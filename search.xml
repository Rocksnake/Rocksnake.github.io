<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>利用Semaphore实现一个限流器</title>
    <url>/2020/08/25/%E5%88%A9%E7%94%A8Semaphore%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E9%99%90%E6%B5%81%E5%99%A8/</url>
    <content><![CDATA[<h1 id="利用Semaphore实现一个限流器"><a href="#利用Semaphore实现一个限流器" class="headerlink" title="利用Semaphore实现一个限流器"></a>利用Semaphore实现一个限流器</h1><blockquote>
<p>Semaphore普遍翻译为信号量，编程世界里，线程能不能执行，要看信号量是不是允许。</p>
</blockquote>
<p>信号量是由大名鼎鼎的计算机科学家迪杰斯特拉（Dijkstra）于 1965 年提出，在这之后的 15 年，信号量一直都是并发编程领域的终结者，直到 1980 年管程被提出来，我们才有了第二选择。目前几乎所有支持并发编程的语言都支持信号量机制。</p>
<h2 id="信号量模型"><a href="#信号量模型" class="headerlink" title="信号量模型"></a>信号量模型</h2><blockquote>
<p>简单概括为：一个计数器、一个等待队列、三个方法</p>
</blockquote>
<p>在信号量模型里，计数器和等待队列对外是透明的，所以只能通过信号量模型提供的三个方法来访问它们，这三个方法分别是：init()、down() 和 up()。</p>
<p><img src="https://static001.geekbang.org/resource/image/6d/5c/6dfeeb9180ff3e038478f2a7dccc9b5c.png" alt="img"></p>
<center>信号量模型图</center>

<p>这三个方法详细的语义具体如下：</p>
<ul>
<li>init()：设置计数器的初始值</li>
<li>down()：计数器的值减1,；如果此时计数器的值小于0，则当前线程将被阻塞，否则当前线程可以继续执行；</li>
<li>up()：计数器的值加 1；如果此时计数器的值小于或者等于 0，则唤醒等待队列中的一个线程，并将其从等待队列中移除。</li>
</ul>
<p>这里提到的 init()、down() 和 up() 三个方法都是原子性的，并且这个原子性是由信号量模型的实现方保证的。</p>
<p>在 Java SDK 里面，信号量模型是由 java.util.concurrent.Semaphore 实现的，Semaphore 这个类能够保证这三个方法都是原子操作。</p>
<p>给出一个参考信号量模型：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Semaphore</span></span>&#123;</span><br><span class="line">  <span class="comment">// 计数器</span></span><br><span class="line">  <span class="keyword">int</span> count;</span><br><span class="line">  <span class="comment">// 等待队列</span></span><br><span class="line">  Queue queue;</span><br><span class="line">  <span class="comment">// 初始化操作</span></span><br><span class="line">  Semaphore(<span class="keyword">int</span> c)&#123;</span><br><span class="line">    <span class="keyword">this</span>.count=c;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// </span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">down</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.count--;</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">this</span>.count&lt;<span class="number">0</span>)&#123;</span><br><span class="line">      <span class="comment">//将当前线程插入等待队列</span></span><br><span class="line">      <span class="comment">//阻塞当前线程</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">up</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.count++;</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">this</span>.count&lt;=<span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">//移除等待队列中的某个线程T</span></span><br><span class="line">      <span class="comment">//唤醒线程T</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>信号量模型里面，down()、up() 这两个操作历史上最早称为 P 操作和 V 操作，所以信号量模型也被称为 PV 原语。</p>
<p>还有些人喜欢用 semWait() 和 semSignal() 来称呼它们，虽然叫法不同，但是语义都是相同的。在 Java SDK 并发包里，down() 和 up() 对应的则是 acquire() 和 release()。</p>
<h2 id="如何使用信号量"><a href="#如何使用信号量" class="headerlink" title="如何使用信号量"></a>如何使用信号量</h2><blockquote>
<p>信号量的模型比较简单，关于信号量模型的使用，可以参考红绿灯，车辆在通过路口前必须检查是否是绿灯，只有绿灯才能通行。</p>
</blockquote>
<p>信号量也是类似的，使用累加器的例子说明，在累加器的例子里面，count+=1操作是个临界区，只允许一个线程执行，也就是说要保证互斥，用信号量怎么控制呢？</p>
<p>就像用互斥锁一样，只要在进入临界区之前执行以下down()操作，退出临界区之前执行一下up()操作就可以。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> count;</span><br><span class="line"><span class="comment">//初始化信号量</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> Semaphore s </span><br><span class="line">    = <span class="keyword">new</span> Semaphore(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//用信号量保证互斥    </span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">addOne</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">//down()</span></span><br><span class="line">  s.acquire();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    count+=<span class="number">1</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="comment">//up()</span></span><br><span class="line">    s.release();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>信号量是如何保证互斥的呢？</p>
<p>假设两个线程 T1 和 T2 同时访问 addOne() 方法，当它们同时调用 acquire() 的时候，由于 acquire() 是一个原子操作，所以只能有一个线程（假设 T1）把信号量里的计数器减为 0，另外一个线程（T2）则是将计数器减为 -1。对于线程 T1，信号量里面的计数器的值是 0，大于等于 0，所以线程 T1 会继续执行；对于线程 T2，信号量里面的计数器的值是 -1，小于 0，按照信号量模型里对 down() 操作的描述，线程 T2 将被阻塞。所以此时只有线程 T1 会进入临界区执行count+=1；。</p>
<p>当线程 T1 执行 release() 操作，也就是 up() 操作的时候，信号量里计数器的值是 -1，加 1 之后的值是 0，小于等于 0，按照信号量模型里对 up() 操作的描述，此时等待队列中的 T2 将会被唤醒。于是 T2 在 T1 执行完临界区代码之后才获得了进入临界区执行的机会，从而保证了互斥性。</p>
<h2 id="快速实现一个限流器"><a href="#快速实现一个限流器" class="headerlink" title="快速实现一个限流器"></a>快速实现一个限流器</h2><blockquote>
<p>我们用信号量实现了一个最简单的互斥锁功能。</p>
</blockquote>
<p>既然Java SDK里面提供了Lock，为啥还要提供一个 Semaphore ？</p>
<p>其实实现一个互斥锁，仅仅是 Semaphore 的部分功能，<strong>Semaphore 还有一个功能是 Lock 不容易实现的</strong>，那就是：<strong>Semaphore 可以允许多个线程访问一个临界区。</strong></p>
<p>现实中也是有这样的需求的，常见的就是工作中遇到的各种池化资源，如连接池、对象池、线程池等，其中，可能最熟悉的就是数据库连接池，同一时刻，一定时允许多个线程同时使用连接池的，当然，每个连接在释放前，是不允许其他线程使用的。</p>
<p>所谓<strong>对象池</strong>，指的是一次性创建出 N 个对象，之后所有的线程重复利用这 N 个对象，当然对象在被释放前，也是不允许其他线程使用的。</p>
<p>对象池，可以用 List 保存实例对象，这个很简单。但关键是限流器的设计，这里的限流，指的是不允许多于 N 个线程同时进入临界区。</p>
<p>那如何快速实现一个这样的限流器呢？这种场景，我立刻就想到了信号量的解决方案。</p>
<p>信号量的计数器，在上面的例子中，我们设置成了 1，这个 1 表示只允许一个线程进入临界区，但如果我们把计数器的值设置成对象池里对象的个数 N，就能完美解决对象池的限流问题了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//对象池</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ObjPool</span>&lt;<span class="title">T</span>, <span class="title">R</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> List&lt;T&gt; pool;</span><br><span class="line">  <span class="comment">// 用信号量实现限流器</span></span><br><span class="line">  <span class="keyword">final</span> Semaphore sem;</span><br><span class="line">  <span class="comment">// 构造函数</span></span><br><span class="line">  ObjPool(<span class="keyword">int</span> size, T t)&#123;</span><br><span class="line">    pool = <span class="keyword">new</span> Vector&lt;T&gt;()&#123;&#125;;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;size; i++)&#123;</span><br><span class="line">      pool.add(t);</span><br><span class="line">    &#125;</span><br><span class="line">    sem = <span class="keyword">new</span> Semaphore(size);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 利用对象池的对象，调用func</span></span><br><span class="line">  <span class="function">R <span class="title">exec</span><span class="params">(Function&lt;T,R&gt; func)</span> </span>&#123;</span><br><span class="line">    T t = <span class="keyword">null</span>;</span><br><span class="line">    sem.acquire();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      t = pool.remove(<span class="number">0</span>);</span><br><span class="line">      <span class="keyword">return</span> func.apply(t);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      pool.add(t);</span><br><span class="line">      sem.release();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 创建对象池</span></span><br><span class="line">ObjPool&lt;Long, String&gt; pool = </span><br><span class="line">  <span class="keyword">new</span> ObjPool&lt;Long, String&gt;(<span class="number">10</span>, <span class="number">2</span>);</span><br><span class="line"><span class="comment">// 通过对象池获取t，之后执行  </span></span><br><span class="line">pool.exec(t -&gt; &#123;</span><br><span class="line">    System.out.println(t);</span><br><span class="line">    <span class="keyword">return</span> t.toString();</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<p>我们用一个 List来保存对象实例，用 Semaphore 实现限流器。</p>
<p>关键的代码是 ObjPool 里面的 exec() 方法，这个方法里面实现了限流的功能。</p>
<p>在这个方法里面，我们首先调用 acquire() 方法（与之匹配的是在 finally 里面调用 release() 方法），假设对象池的大小是 10，信号量的计数器初始化为 10，那么前 10 个线程调用 acquire() 方法，都能继续执行，相当于通过了信号灯，而其他线程则会阻塞在 acquire() 方法上。对于通过信号灯的线程，我们为每个线程分配了一个对象 t（这个分配工作是通过 pool.remove(0) 实现的），分配完之后会执行一个回调函数 func，而函数的参数正是前面分配的对象 t ；执行完回调函数之后，它们就会释放对象（这个释放工作是通过 pool.add(t) 实现的），同时调用 release() 方法来更新信号量的计数器。如果此时信号量里计数器的值小于等于 0，那么说明有线程在等待，此时会自动唤醒等待的线程。</p>
<blockquote>
<p>使用信号量，我们可以轻松地实现一个限流器，使用起来还是非常简单的</p>
</blockquote>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><blockquote>
<p>信号量在其他语言里相对于Java里更加有名，java在并发编程领域重点支持管程模型。</p>
</blockquote>
<p> 管程模型理论上解决了信号量模型的一些不足，主要体现在易用性和工程化方面，例如用信号量解决我们曾经提到过的阻塞队列问题，就比管程模型麻烦很多。</p>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>Q：在上面对象池的例子中，对象保存在了 Vector 中，Vector 是 Java 提供的线程安全的容器，如果我们把 Vector 换成 ArrayList，是否可以呢？</p>
<p>A：</p>
<ul>
<li>和管程相比，信号量可以实现的独特功能就是同时允许多个线程进入临界区，但是信号量不能做的就是同时唤醒多个线程去争抢锁，只能唤醒一个阻塞中的线程，而且信号量模型是没有Condition的概念的，即阻塞线程被醒了直接就运行了而不会去检查此时临界条件是否已经不满足了，基于此考虑信号量模型才会设计出只能让一个线程被唤醒，否则就会出现因为缺少Condition检查而带来的线程安全问题。正因为缺失了Condition，所以用信号量来实现阻塞队列就很麻烦，因为要自己实现类似Condition的逻辑。</li>
<li>需要用线程安全的vector，因为信号量支持多个线程进入临界区，执行list的add和remove方法时可能是多线程并发执行</li>
<li>有同学认为up()中的判断条件应该&gt;=0，我觉得有可能理解为生产者-消费者模式中的生产者了。可以这么想，&gt;0就意味着没有阻塞的线程了，所以只有&lt;=0的情况才需要唤醒一个等待的线程。其实down()和up()是成对出现的，并且是先调用down()获得锁，处理完成再调用up()释放锁，如果信号量初始值为1，应该是不会出现&gt;0的情况的，除非故意调先用up()，这也失去了信号量本身的意义了。</li>
<li>很多人对up()方法的计数器count&lt;=0不理解，可以看下这里：<br>1、反证法验证一下，假如一个线程先执行down()操作，那么此时count的值是0，接着这个线程执行up()操作，此时count的值是1，如果count应该是大于等于0，那么应该唤醒其他线程，可是此时并没有线程在睡眠呀，count的值不应该是大于等于0。<br>2、假如一个线程t1执行down()操作，此时count = 0，然后t1被中断，另外的线程t2执行down()操作，此时count=-1，t2阻塞睡眠，另外的线程t3执行down()操作，count=-2，t3也睡眠。count=-2 说明有两个线程在睡眠，接着t1执行up() 操作，此时count=-1，小于等于0，唤醒t2或者t3其中一个线程，假如计数器count是大于等于0才唤醒其他线程，这明显是不对的。</li>
<li>换ArrayList是不行的，临界区内可能存在多个线程来执行remove操作，出现不可预知的后果</li>
</ul>
]]></content>
      <categories>
        <category>Concurrent Programming</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>Lock and Condition</title>
    <url>/2020/08/24/Lock%20and%20Condition/</url>
    <content><![CDATA[<h1 id="Lock-and-Condition"><a href="#Lock-and-Condition" class="headerlink" title="Lock and Condition"></a>Lock and Condition</h1><h2 id="隐藏在并发包中的管程"><a href="#隐藏在并发包中的管程" class="headerlink" title="隐藏在并发包中的管程"></a>隐藏在并发包中的管程</h2><blockquote>
<p>Java SDK并发包中内容丰富，但最核心的还是其对管程的实现，理论上利用管程，几乎可以实现并发包中的所有工具类。</p>
</blockquote>
<p>我们知道并发编程领域里，两大核心问题：互斥和同步，这两大问题管程都是可以解决的，Java SDK并发包通过Lock和Condition两个接口实现管程，其中Lock用于解决互斥问题，Condition用于解决同步问题。</p>
<p>首先考虑一个问题，就是Java 语言本身提供的 synchronized 也是管程的一种实现，既然 Java 从语言层面已经实现了管程了，那为什么还要在 SDK 里提供另外一种实现呢？</p>
<p>很显然synchronized与Lock有区别，理解这个问题有助于进一步深入。</p>
<h3 id="再造管程的理由"><a href="#再造管程的理由" class="headerlink" title="再造管程的理由"></a>再造管程的理由</h3><p>在 Java 的 1.5 版本中，synchronized 性能不如 SDK 里面的 Lock，但 1.6 版本之后，synchronized 做了很多优化，将性能追了上来，所以 1.6 之后的版本又有人推荐使用 synchronized 了。</p>
<p><font color=red>那么性能问题是否能作为理由呢，显然不能，因为性能问题是可以通过优化解决的。</font></p>
<p>还记得我们前面介绍的死锁问题，有一个破坏不可抢占条件方案，这个方案synchronized没有办法解决，原因是synchronized申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。但我们希望的是：</p>
<blockquote>
<p>对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。</p>
</blockquote>
<p>如果我们重新设计一把互斥锁去解决这个问题，那该怎么设计呢？我觉得有三种方案。</p>
<ol>
<li>能够响应中断。<ul>
<li>synchronized 的问题是，持有锁 A 后，如果尝试获取锁 B 失败，那么线程就进入阻塞状态，一旦发生死锁，就没有任何机会来唤醒阻塞的线程。但如果阻塞状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能够唤醒它，那它就有机会释放曾经持有的锁 A。这样就破坏了不可抢占条件了。</li>
</ul>
</li>
<li>支持超时。<ul>
<li>如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个错误，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。</li>
</ul>
</li>
<li>非阻塞地获取锁。<ul>
<li>如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。</li>
</ul>
</li>
</ol>
<p>这三种方案可以全面弥补 synchronized 的问题。到这里相信你应该也能理解了，这三个方案就是“重复造轮子”的主要原因，体现在 API 上，就是 Lock 接口的三个方法。详情如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 支持中断的API</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">lockInterruptibly</span><span class="params">()</span> </span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> InterruptedException</span>;</span><br><span class="line"><span class="comment">// 支持超时的API</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">tryLock</span><span class="params">(<span class="keyword">long</span> time, TimeUnit unit)</span> </span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> InterruptedException</span>;</span><br><span class="line"><span class="comment">// 支持非阻塞获取锁的API</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">tryLock</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>

<h3 id="如何保证可见性"><a href="#如何保证可见性" class="headerlink" title="如何保证可见性"></a>如何保证可见性</h3><p>Java SDK 里面 Lock 的使用，有一个经典的范例，就是try{}finally{}，在 finally 里面释放锁。其中可见性是怎么保证的呢？ Java 里多线程的可见性是通过 Happens-Before 规则保证的。</p>
<p>synchronized 之所以能够保证可见性，也是因为有一条 synchronized 相关的规则：synchronized 的解锁 Happens-Before 于后续对这个锁的加锁。那 Java SDK 里面 Lock 靠什么保证可见性呢？</p>
<p>Q：线程 T1 对 value 进行了 +=1 操作，那后续的线程 T2 能够看到 value 的正确结果吗？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">X</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Lock rtl =</span><br><span class="line">  <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">  <span class="keyword">int</span> value;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addOne</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 获取锁</span></span><br><span class="line">    rtl.lock();  </span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      value+=<span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="comment">// 保证锁能释放</span></span><br><span class="line">      rtl.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>A：答案必须是肯定的。Java SDK 里面锁的实现非常复杂，它是利用了 volatile 相关的 Happens-Before 规则。Java SDK 里面的 ReentrantLock，内部持有一个 volatile 的成员变量 state，获取锁的时候，会读写 state 的值；解锁的时候，也会读写 state 的值（简化后的代码如下面所示）。也就是说，在执行 value+=1 之前，程序先读写了一次 volatile 变量 state，在执行 value+=1 之后，又读写了一次 volatile 变量 state。根据相关的 Happens-Before 规则：</p>
<ol>
<li>顺序性规则：对于线程 T1，value+=1 Happens-Before 释放锁的操作 unlock()；</li>
<li>volatile 变量规则：由于 state = 1 会先读取 state，所以线程 T1 的 unlock() 操作 Happens-Before 线程 T2 的 lock() 操作；</li>
<li>传递性规则：线程 T1 的 value+=1 Happens-Before 线程 T2 的 lock() 操作。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SampleLock</span> </span>&#123;</span><br><span class="line">  <span class="keyword">volatile</span> <span class="keyword">int</span> state;</span><br><span class="line">  <span class="comment">// 加锁</span></span><br><span class="line">  lock() &#123;</span><br><span class="line">    <span class="comment">// 省略代码无数</span></span><br><span class="line">    state = <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 解锁</span></span><br><span class="line">  unlock() &#123;</span><br><span class="line">    <span class="comment">// 省略代码无数</span></span><br><span class="line">    state = <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>所以，后续线程T2能够看到value的正确结果。</p>
<h3 id="什么是可重入锁"><a href="#什么是可重入锁" class="headerlink" title="什么是可重入锁"></a>什么是可重入锁</h3><p>可重入锁，ReentrantLock，指的是线程可以重复获取同一把锁。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">X</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Lock rtl =</span><br><span class="line">  <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">  <span class="keyword">int</span> value;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 获取锁</span></span><br><span class="line">    rtl.lock();         ②</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> value;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="comment">// 保证锁能释放</span></span><br><span class="line">      rtl.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addOne</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 获取锁</span></span><br><span class="line">    rtl.lock();  </span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      value = <span class="number">1</span> + get(); ①</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="comment">// 保证锁能释放</span></span><br><span class="line">      rtl.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当线程 T1 执行到 ① 处时，已经获取到了锁 rtl ，当在 ① 处调用 get() 方法时，会在 ② 再次对锁 rtl 执行加锁操作。此时，如果锁 rtl 是可重入的，那么线程 T1 可以再次加锁成功；如果锁 rtl 是不可重入的，那么线程 T1 此时会被阻塞。</p>
<p>除了可重入锁，可能你还听说过可重入函数，可重入函数怎么理解呢？指的是线程可以重复调用？</p>
<p>显然不是，所谓<strong>可重入函数</strong>，指的是多个线程可以同时调用该函数，每个线程都能得到正确结果；同时在一个线程内支持线程切换，无论被切换多少次，结果都是正确的。多线程可以同时执行，还支持线程切换，这意味着什么呢？线程安全啊。所以，<strong>可重入函数是线程安全的</strong>。</p>
<h3 id="公平锁与非公平锁"><a href="#公平锁与非公平锁" class="headerlink" title="公平锁与非公平锁"></a>公平锁与非公平锁</h3><p>在使用 ReentrantLock 的时候，你会发现 ReentrantLock 这个类有两个构造函数，一个是无参构造函数，一个是传入 fair 参数的构造函数。</p>
<p>fair 参数代表的是锁的公平策略，如果传入 true 就表示需要构造一个公平锁，反之则表示要构造一个非公平锁。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//无参构造函数：默认非公平锁</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ReentrantLock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    sync = <span class="keyword">new</span> NonfairSync();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//根据公平策略参数创建锁</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ReentrantLock</span><span class="params">(<span class="keyword">boolean</span> fair)</span></span>&#123;</span><br><span class="line">    sync = fair ? <span class="keyword">new</span> FairSync() </span><br><span class="line">                : <span class="keyword">new</span> NonfairSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在介绍管程的时候我们知道了入口等待队列，锁都对应着一个等待队列，如果一个线程没有获得锁，就会进入等待队列，当有线程释放锁的时候，就需要从等待队列中唤醒一个等待的线程。</p>
<ul>
<li><p>如果是公平锁，唤醒的策略就是谁等待的时间长，就唤醒谁，很公平；</p>
</li>
<li><p>如果是非公平锁，则不提供这个保证，有可能等待时间短的线程反而先被唤醒。</p>
</li>
</ul>
<h3 id="用锁的最佳实践"><a href="#用锁的最佳实践" class="headerlink" title="用锁的最佳实践"></a>用锁的最佳实践</h3><blockquote>
<p>锁虽然能解决很多并发问题，但风险很高，可能会导致死锁，也可能影响性能。</p>
</blockquote>
<p>并发大师 Doug Lea《Java 并发编程：设计原则与模式》一书中，推荐的三个用锁的最佳实践，它们分别是：</p>
<ol>
<li>永远只在更新对象的成员变量时加锁</li>
<li>永远只在访问可变的成员变量时加锁</li>
<li>永远不在调用其他对象的方法时加锁</li>
</ol>
<p>这三条规则，前两条估计你一定会认同，最后一条你可能会觉得过于严苛。</p>
<p>但是我还是倾向于你去遵守，因为调用其他对象的方法，实在是太不安全了，也许“其他”方法里面有线程 sleep() 的调用，也可能会有奇慢无比的 I/O 操作，这些都会严重影响性能。更可怕的是，“其他”类的方法可能也会加锁，然后双重加锁就可能导致死锁。</p>
<p><strong>并发问题，本来就难以诊断，所以你一定要让你的代码尽量安全，尽量简单，哪怕有一点可能会出问题，都要努力避免。</strong></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Java SDK 并发包里的 Lock 接口里面的每个方法，你可以感受到，都是经过深思熟虑的。除了支持类似 synchronized 隐式加锁的 lock() 方法外，还支持超时、非阻塞、可中断的方式获取锁，这三种方式为我们编写更加安全、健壮的并发程序提供了很大的便利。</p>
<p>除了并发大师 Doug Lea 推荐的三个最佳实践外，你也可以参考一些诸如：减少锁的持有时间、减小锁的粒度等业界广为人知的规则，其实本质上它们都是相通的，不过是在该加锁的地方加锁而已。</p>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>tryLock() 支持非阻塞方式获取锁，那么是否存在死锁问题呢？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Lock lock</span><br><span class="line">          = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account tar, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span>(<span class="keyword">this</span>.lock.tryLock()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (tar.lock.tryLock()) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">              tar.balance += amt;</span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">              tar.lock.unlock();</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;<span class="comment">//if</span></span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          <span class="keyword">this</span>.lock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;<span class="comment">//if</span></span><br><span class="line">    &#125;<span class="comment">//while</span></span><br><span class="line">  &#125;<span class="comment">//transfer</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>A：</strong></p>
<ul>
<li>存在活锁。这个例子可以稍微改下，成功转账后应该跳出循环。加个随机重试时间避免活锁</li>
<li>有可能活锁，A，B两账户相互转账，各自持有自己lock的锁，都一直在尝试获取对方的锁，形成了活锁</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Lock lock</span><br><span class="line">          = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account tar, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line"><span class="keyword">boolean</span> flag = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">while</span> (flag) &#123;</span><br><span class="line">      <span class="keyword">if</span>(<span class="keyword">this</span>.lock.tryLock(随机数，NANOSECONDS)) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (tar.lock.tryLock(随机数，NANOSECONDS)) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">              tar.balance += amt;</span><br><span class="line">flag = <span class="keyword">false</span>;</span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">              tar.lock.unlock();</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;<span class="comment">//if</span></span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          <span class="keyword">this</span>.lock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;<span class="comment">//if</span></span><br><span class="line">    &#125;<span class="comment">//while</span></span><br><span class="line">  &#125;<span class="comment">//transfer</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Dubbo如何用管程实现异步转同步？"><a href="#Dubbo如何用管程实现异步转同步？" class="headerlink" title="Dubbo如何用管程实现异步转同步？"></a>Dubbo如何用管程实现异步转同步？</h2><p>在上一部分我们说到了 Java SDK 并发包里的 Lock 有别于 synchronized 隐式锁的三个特性：能够响应中断、支持超时和非阻塞地获取锁。</p>
<p>这一部分我们来了解Java SDK 并发包里的 Condition，Condition 实现了管程模型里面的条件变量。</p>
<p>Java语言内置的管程里只有一个条件变量，而Lock &amp; Condition实现的管程是支持多个条件变量的。</p>
<p>在很多并发场景下，支持多个条件变量能够让我们的并发程序可读性更好，实现起来也更容易。例如，实现一个阻塞队列，就需要两个条件变量。</p>
<h3 id="那如何利用两个条件变量快速实现阻塞队列呢？"><a href="#那如何利用两个条件变量快速实现阻塞队列呢？" class="headerlink" title="那如何利用两个条件变量快速实现阻塞队列呢？"></a>那如何利用两个条件变量快速实现阻塞队列呢？</h3><p>一个阻塞队列，需要两个条件变量，一个是队列不空（空队列不允许出队），另一个是队列不满（队列已满不允许入队）。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BlockedQueue</span>&lt;<span class="title">T</span>&gt;</span>&#123;</span><br><span class="line">  <span class="keyword">final</span> Lock lock =</span><br><span class="line">    <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">  <span class="comment">// 条件变量：队列不满  </span></span><br><span class="line">  <span class="keyword">final</span> Condition notFull =</span><br><span class="line">    lock.newCondition();</span><br><span class="line">  <span class="comment">// 条件变量：队列不空  </span></span><br><span class="line">  <span class="keyword">final</span> Condition notEmpty =</span><br><span class="line">    lock.newCondition();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 入队</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">enq</span><span class="params">(T x)</span> </span>&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (队列已满)&#123;</span><br><span class="line">        <span class="comment">// 等待队列不满</span></span><br><span class="line">        notFull.await();</span><br><span class="line">      &#125;  </span><br><span class="line">      <span class="comment">// 省略入队操作...</span></span><br><span class="line">      <span class="comment">//入队后,通知可出队</span></span><br><span class="line">      notEmpty.signal();</span><br><span class="line">    &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 出队</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">deq</span><span class="params">()</span></span>&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (队列已空)&#123;</span><br><span class="line">        <span class="comment">// 等待队列不空</span></span><br><span class="line">        notEmpty.await();</span><br><span class="line">      &#125;  </span><br><span class="line">      <span class="comment">// 省略出队操作...</span></span><br><span class="line">      <span class="comment">//出队后，通知可入队</span></span><br><span class="line">      notFull.signal();</span><br><span class="line">    &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>不过需要注意的是，Lock和Condition实现的管程，线程等待和通知需要调用 await()、signal()、signalAll()，它们的语义和 wait()、notify()、notifyAll() 是相同的。但是不一样的是，Lock&amp;Condition 实现的管程里只能使用前面的 await()、signal()、signalAll()，而后面的 wait()、notify()、notifyAll() 只有在 synchronized 实现的管程里才能使用。如果一不小心在 Lock&amp;Condition 实现的管程里调用了 wait()、notify()、notifyAll()，那程序可就彻底玩完了。</p>
<p>Java SDK 并发包里的 Lock 和 Condition 不过就是管程的一种实现而已，熟悉管程，Lock和Condition自然就熟练于心了。</p>
<p><strong>下面我们看看Dubbo中，Lock和Condition是怎么用的。</strong></p>
<p>首先需要了同步和异步：我们平时写的代码基本都是同步的。同步和异步的区别通俗点讲就是调用方法是否需要等到结果，如果需要等待结果就是同步，反之则是异步。</p>
<p>E.g.：有一个计算圆周率小数点后 100 万位的方法pai1M()，这个方法可能需要执行俩礼拜，如果调用pai1M()之后，线程一直等着计算结果，等俩礼拜之后结果返回，就可以执行 printf(“hello world”)了，这个属于同步；如果调用pai1M()之后，线程不用等待计算结果，立刻就可以执行 printf(“hello world”)，这个就属于异步。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 计算圆周率小说点后100万位 </span></span><br><span class="line"><span class="function">String <span class="title">pai1M</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">//省略代码无数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pai1M()</span><br><span class="line">printf(<span class="string">"hello world"</span>)</span><br></pre></td></tr></table></figure>

<p>同步是java代码默认的处理方式，如果你想让程序支持异步，可以通过以下方法实现：</p>
<ol>
<li>调用方创建一个子线程，在子线程中执行方法调用，称为异步调用；</li>
<li>方法实现的时候，创建一个新的线程执行主要逻辑，主线程直接return，这种方法一般称为异步方法。</li>
</ol>
<h3 id="Dubbo源码分析"><a href="#Dubbo源码分析" class="headerlink" title="Dubbo源码分析"></a>Dubbo源码分析</h3><blockquote>
<p>异步的场景还是很多的，例如TCP协议本身就是异步的，工作中经常用到RPC调用，在TCP协议层面，发送完RPC请求后，线程是不会等待RPC的响应结果。</p>
</blockquote>
<p>可是平时工作中的RPC调用大多数是同步的，这是因为有人帮忙做了异步转同步的事情，就比如RPC框架Dubbo，下面我们具体分析。</p>
<p>对于一个简单的RPC调用，默认情况下sayHello()方法是个同步方法，也就是说，执行service.sayHello(“dubbo”)的时候，线程会停下来等结果。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DemoService service = 初始化部分省略</span><br><span class="line">String message = </span><br><span class="line">  service.sayHello(<span class="string">"dubbo"</span>);</span><br><span class="line">System.out.println(message);</span><br></pre></td></tr></table></figure>

<p>如果此时你将调用线程dump出来的话，会是这个样子：</p>
<p><img src="https://static001.geekbang.org/resource/image/a9/c5/a924d23fc43d31267473f2dc91396ec5.png" alt="img"></p>
<center>调用栈信息</center>

<p>你会发现调用线程阻塞了，线程状态是TIMED_WAITING，本来发送请求是异步的，但是调用线程却阻塞了，说明Dubbo帮我们做了异步转同步的事情。通过调用栈，能看到线程是阻塞在DefaultFuture.get()方法是哪个，可以推断，Dubbo异步转同步的功能应该是通过DefaultFuture这个类实现的。</p>
<p>不过为了理清前后关系，有必要分析调用DefaultFuture.get()之前发生了什么，Fubbolnvoler的108行调用了DefaultFuture.get()，这一行先调用了request(inv, timeout)方法，其实就是发送RPC请求，之后调用get()方法等待RPC返回结果。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DubboInvoker</span></span>&#123;</span><br><span class="line">  <span class="function">Result <span class="title">doInvoke</span><span class="params">(Invocation inv)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 下面这行就是源码中108行</span></span><br><span class="line">    <span class="comment">// 为了便于展示，做了修改</span></span><br><span class="line">    <span class="keyword">return</span> currentClient </span><br><span class="line">      .request(inv, timeout)</span><br><span class="line">      .get();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>DefaultFuture 这个类是很关键，对其做一定程度的精简，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建锁与条件变量</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Lock lock </span><br><span class="line">    = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Condition done </span><br><span class="line">    = lock.newCondition();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用方通过该方法等待结果</span></span><br><span class="line"><span class="function">Object <span class="title">get</span><span class="params">(<span class="keyword">int</span> timeout)</span></span>&#123;</span><br><span class="line">  <span class="keyword">long</span> start = System.nanoTime();</span><br><span class="line">  lock.lock();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">while</span> (!isDone()) &#123;</span><br><span class="line">    done.await(timeout);</span><br><span class="line">      <span class="keyword">long</span> cur=System.nanoTime();</span><br><span class="line">    <span class="keyword">if</span> (isDone() || </span><br><span class="line">          cur-start &gt; timeout)&#123;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  lock.unlock();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!isDone()) &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> returnFromResponse();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// RPC结果是否已经返回</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">isDone</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> response != <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// RPC结果返回时调用该方法   </span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doReceived</span><span class="params">(Response res)</span> </span>&#123;</span><br><span class="line">  lock.lock();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    response = res;</span><br><span class="line">    <span class="keyword">if</span> (done != <span class="keyword">null</span>) &#123;</span><br><span class="line">      done.signal();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    lock.unlock();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>还记得我们的需求，当RPC返回结果之前，阻塞调用线程，让调用线程等待；当RPC返回结果后，唤醒调用线程，让调用线程重新执行。</p>
<p>其实这就是经典的等待-通知机制，大概就已经想到了管程的解决方案了，我们看看Dubbo是怎么实现的：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 创建锁与条件变量</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Lock lock </span><br><span class="line">    = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Condition done </span><br><span class="line">    = lock.newCondition();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用方通过该方法等待结果</span></span><br><span class="line"><span class="function">Object <span class="title">get</span><span class="params">(<span class="keyword">int</span> timeout)</span></span>&#123;</span><br><span class="line">  <span class="keyword">long</span> start = System.nanoTime();</span><br><span class="line">  lock.lock();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">while</span> (!isDone()) &#123;</span><br><span class="line">    done.await(timeout);</span><br><span class="line">      <span class="keyword">long</span> cur=System.nanoTime();</span><br><span class="line">    <span class="keyword">if</span> (isDone() || </span><br><span class="line">          cur-start &gt; timeout)&#123;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  lock.unlock();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!isDone()) &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> returnFromResponse();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// RPC结果是否已经返回</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">isDone</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> response != <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// RPC结果返回时调用该方法   </span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doReceived</span><span class="params">(Response res)</span> </span>&#123;</span><br><span class="line">  lock.lock();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    response = res;</span><br><span class="line">    <span class="keyword">if</span> (done != <span class="keyword">null</span>) &#123;</span><br><span class="line">      done.signal();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    lock.unlock();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>调用线程通过get()方法等待RPC返回结果这个方法里面，看到最熟悉的就是：调用lock()获取锁，在finally里调用unlock()释放锁；获取锁后，通过经典的在循环中调用await()方法来实现等待。</p>
<p>当RPC结果返回时，会调用doReceived()方法，这个方法里面，调用lock()获取锁，在finally里面调用unlock()释放锁，获取锁后通过调用signal()来通知调用线程，结果已经返回，不用继续等待了。</p>
<p>工作中需要异步处理的越来越多，其中有一个主要原因就是有些 API 本身就是异步 API。例如 websocket 也是一个异步的通信协议，如果基于这个协议实现一个简单的 RPC，你也会遇到异步转同步的问题。</p>
<p>现在很多公有云的 API 本身也是异步的，例如创建云主机，就是一个异步的 API，调用虽然成功了，但是云主机并没有创建成功，你需要调用另外一个 API 去轮询云主机的状态。如果你需要在项目内部封装创建云主机的 API，你也会面临异步转同步的问题，因为同步的 API 更易用。</p>
<h3 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Lock and Condition是管程的一种实现，能否用好Lock and Condition取决于管程模型理解得怎样。</p>
<p>Lock&amp;Condition 实现的管程相对于 synchronized 实现的管程来说更加灵活、功能也更丰富。</p>
<p>了解原理比了解实现更能让你快速学好并发编程，关于 Java SDK 并发包里锁和条件变量是如何实现的，可以参考《Java 并发编程的艺术》一书的第 5 章《Java 中的锁》。</p>
<p>可以去查看DefaultFuture <a href="incubator-dubbo/dubbo-remoting/dubbo-remoting-api/src/main/java/org/apache/dubbo/remoting/exchange/support/DefaultFuture.java。">源码</a>，Dubbo<a href="https://github.com/apache/dubbo" target="_blank" rel="noopener">源码</a>。</p>
<h3 id="思考-1"><a href="#思考-1" class="headerlink" title="思考"></a>思考</h3><p>DefaultFuture 里面唤醒等待的线程，用的是 signal()，而不是 signalAll()，是否合理呢？</p>
<p>A：</p>
<ul>
<li>不合理，会导致很多请求超时，看了源码是调用signalAll()</li>
<li>我理解异步的本质是利用多线程提升性能，异步一定是基于一个新开的线程，从调用线程来看是异步的，但是从新开的那个线程来看，正是同步（等待）的，只是对于调用方而言这种同步是透明的。正所谓生活哪有什么岁月静好，只是有人替你负重前行。</li>
<li>提到异步转同步，让我想到这两天看的zookeeper客户端源码，感觉应该也是这个机制，客户端同步模式下发送请求后会执行packet.wait，收到服务端响应后执行packet.notifyAll</li>
<li>DefaultFuture的某个实例并非单线程访问的，可能会有多个线程访问同一个，因此需要用SignalAll通知全部，避免没有通知到正确的线程（不知道DUBBO中DefaultFuture同一个实例会不会共享）</li>
</ul>
]]></content>
      <categories>
        <category>Concurrent Programming</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>Java线程</title>
    <url>/2020/08/23/Java%E7%BA%BF%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="Java-线程的生命周期"><a href="#Java-线程的生命周期" class="headerlink" title="Java 线程的生命周期"></a>Java 线程的生命周期</h1><blockquote>
<p>在 Java 领域，实现并发程序的主要手段就是多线程。</p>
</blockquote>
<p>Java语言里的线程本质上就是操作系统的线程。</p>
<p>线程的生老病死称为生命周期，我们要做的就是搞定生命周期中的各个节点的状态转换机制即可。</p>
<p>虽然不同编程语言对线程进行了封装，但是生命周期基本相似。</p>
<h2 id="通用的线程生命周期"><a href="#通用的线程生命周期" class="headerlink" title="通用的线程生命周期"></a>通用的线程生命周期</h2><blockquote>
<p>初识状态、可运行状态、运行状态、休眠状态、终止状态</p>
</blockquote>
<p><img src="https://static001.geekbang.org/resource/image/9b/e5/9bbc6fa7fb4d631484aa953626cf6ae5.png" alt="img"></p>
<center>通用线程状态转换图-五种模型</center>

<ol>
<li><strong>初始状态</strong>，指的是线程已经被创建，但是还不允许分配 CPU 执行。这个状态属于编程语言特有的，不过这里所谓的被创建，仅仅是在编程语言层面被创建，而在操作系统层面，真正的线程还没有创建。</li>
<li><strong>可运行状态</strong>，指的是线程可以分配 CPU 执行。在这种状态下，真正的操作系统线程已经被成功创建了，所以可以分配 CPU 执行。</li>
<li>当有空闲的 CPU 时，操作系统会将其分配给一个处于可运行状态的线程，被分配到 CPU 的线程的状态就转换成了<strong>运行状态</strong>。</li>
<li>运行状态的线程如果调用一个阻塞的 API（例如以阻塞方式读文件）或者等待某个事件（例如条件变量），那么线程的状态就会转换到休眠状态，同时释放 CPU 使用权，<strong>休眠状态</strong>的线程永远没有机会获得 CPU 使用权。当等待的事件出现了，线程就会从休眠状态转换到可运行状态。</li>
<li>线程执行完或者出现异常就会进入<strong>终止状态</strong>，终止状态的线程不会切换到其他任何状态，进入终止状态也就意味着线程的生命周期结束了。</li>
</ol>
<p>这五种状态在不同编程与原理会有简化合并，例如C语言的POSIX Threads规范，合并了初始状态和可运行状态；Java语言合并了可运行状态和运行状态，这两个状态在操作系统调度层面有用，而JVM层面不关心这两个状态，因为JVM把线程调度交给操作系统处理了。</p>
<p>除了简单合并，这五种状态也有可能被细化，例如Java语言中细化了休眠状态。</p>
<h2 id="Java中现成的生命周期"><a href="#Java中现成的生命周期" class="headerlink" title="Java中现成的生命周期"></a>Java中现成的生命周期</h2><p>Java中线程有六种状态：</p>
<ol>
<li>NEW(初始化状态)</li>
<li>RUNNABLE(可运行/运行状态)</li>
<li>BLOCKED(阻塞状态)</li>
<li>WAITING(无时限等待)</li>
<li>TIMED_WAITNG(有时限等待)</li>
<li>TERMINATED(终止状态)</li>
</ol>
<p>看似复杂，实际上在操作系统层面，Java 线程中的 BLOCKED、WAITING、TIMED_WAITING 是一种状态，即前面我们提到的休眠状态。</p>
<p>也就是说，Java线程处于这三种状态之一，那么这个线程就永远没有CPU的使用权。</p>
<p><img src="https://static001.geekbang.org/resource/image/3f/8c/3f6c6bf95a6e8627bdf3cb621bbb7f8c.png" alt="img"></p>
<center>Java中的线程状态转换</center>

<p>其中，BLOCKED、WAITING、TIMED_WAITING 可以理解为线程导致休眠状态的三种原因。那具体是哪些情形会导致线程从 RUNNABLE 状态转换到这三种状态呢？而这三种状态又是何时转换回 RUNNABLE 的呢？以及 NEW、TERMINATED 和 RUNNABLE 状态是如何转换的？</p>
<h3 id="RUNNABLE-与-BLOCKED-的状态转换"><a href="#RUNNABLE-与-BLOCKED-的状态转换" class="headerlink" title="RUNNABLE 与 BLOCKED 的状态转换"></a>RUNNABLE 与 BLOCKED 的状态转换</h3><blockquote>
<p>只有一种场景会触发这种转换，就是线程等待synchronized的隐式锁。</p>
</blockquote>
<p>synchronized修饰的方法、代码块同一时段只允许一个线程执行，其他线程只能等待，这种情况下，等待的线程就会从RUNNABLE转换到BLOCKED状态。而当等待的线程获得synchronized隐式锁时，就会从BLOCKED转换到RUNNABLE状态。</p>
<p>Q：熟悉操作系统线程的生命周期，可能会有个问题，线程调用阻塞式API时，是否会转换到BLOCKED状态？</p>
<p>A：在操作系统层面，线程是会转换到休眠状态的，但是在 JVM 层面，Java 线程的状态不会发生变化，也就是说 Java 线程的状态会依然保持 RUNNABLE 状态。JVM 层面并不关心操作系统调度相关的状态，因为在 JVM 看来，等待 CPU 使用权（操作系统层面此时处于可执行状态）与等待 I/O（操作系统层面此时处于休眠状态）没有区别，都是在等待某个资源，所以都归入了 RUNNABLE 状态。</p>
<p><strong>所以，平时说到Java 在调用阻塞式 API 时，线程会阻塞，指的是操作系统线程的状态，并不是 Java 线程的状态。</strong></p>
<h3 id="RUNNABLE与WAITING的状态转换"><a href="#RUNNABLE与WAITING的状态转换" class="headerlink" title="RUNNABLE与WAITING的状态转换"></a>RUNNABLE与WAITING的状态转换</h3><blockquote>
<p>总体来说，有三种场景会触发这种转换</p>
</blockquote>
<ol>
<li>获得synchronized隐式锁的线程，调用无参数的Object.wait()方法</li>
<li>调用无参数的Thread.join()方法<ul>
<li>join是一种线程同步方法</li>
<li>E.g.：一个线程对象thread Ａ，当调用A.join()时，执行这条语句的线程会等待thread A执行完，而等待中的这个线程，其状态会从RUNNABLE转换到WAITING。当线程thread A执行完，原来等待它的线程又会从WAITING状态转换到RUNNABLE</li>
</ul>
</li>
<li>调用LockSupport.park()方法<ul>
<li>Java 并发包中的锁，都是基于LockSupport 对象实现的</li>
<li>调用 LockSupport.park() 方法，当前线程会阻塞，线程的状态会从 RUNNABLE 转换到 WAITING。调用 LockSupport.unpark(Thread thread) 可唤醒目标线程，目标线程的状态又会从 WAITING 状态转换到 RUNNABLE。</li>
</ul>
</li>
</ol>
<h3 id="RUNNABLE-与-TIMED-WAITING-的状态转换"><a href="#RUNNABLE-与-TIMED-WAITING-的状态转换" class="headerlink" title="RUNNABLE 与 TIMED_WAITING 的状态转换"></a>RUNNABLE 与 TIMED_WAITING 的状态转换</h3><blockquote>
<p>有五种场景会触发这种转换</p>
</blockquote>
<ol>
<li>调用带超时参数的 Thread.sleep(long millis) 方法；</li>
<li>获得 synchronized 隐式锁的线程，调用带超时参数的 Object.wait(long timeout) 方法；</li>
<li>调用带超时参数的 Thread.join(long millis) 方法；</li>
<li>调用带超时参数的 LockSupport.parkNanos(Object blocker, long deadline) 方法；</li>
<li>调用带超时参数的 LockSupport.parkUntil(long deadline) 方法。</li>
</ol>
<blockquote>
<p>TIMED_WAITING 和 WAITING 状态的区别，仅仅是触发条件多了超时参数。</p>
</blockquote>
<h3 id="从-NEW-到-RUNNABLE-状态"><a href="#从-NEW-到-RUNNABLE-状态" class="headerlink" title="从 NEW 到 RUNNABLE 状态"></a>从 NEW 到 RUNNABLE 状态</h3><blockquote>
<p>Java刚创建出来的Thread对象就是NEW状态</p>
</blockquote>
<p>而创建Thread对象主要有两种方法，</p>
<ol>
<li>继承Thread对象，重写run方法</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 自定义线程对象</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 线程需要执行的代码</span></span><br><span class="line">    ......</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 创建线程对象</span></span><br><span class="line">MyThread myThread = <span class="keyword">new</span> MyThread();</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>实现Runnable接口，重写run方法，并将该实现类作为创建Thread对象的参数。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 实现Runnable接口</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Runner</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 线程需要执行的代码</span></span><br><span class="line">    ......</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 创建线程对象</span></span><br><span class="line">Thread thread = <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runner());</span><br></pre></td></tr></table></figure>

<p>NEW 状态的线程，不会被操作系统调度，因此不会执行。Java 线程要执行，就必须转换到 RUNNABLE 状态。从 NEW 状态转换到 RUNNABLE 状态很简单，只要调用线程对象的 start() 方法就可以了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">MyThread myThread = <span class="keyword">new</span> MyThread();</span><br><span class="line"><span class="comment">// 从NEW状态转换到RUNNABLE状态</span></span><br><span class="line">myThread.start()；</span><br></pre></td></tr></table></figure>

<h3 id="从-RUNNABLE-到-TERMINATED-状态"><a href="#从-RUNNABLE-到-TERMINATED-状态" class="headerlink" title="从 RUNNABLE 到 TERMINATED 状态"></a>从 RUNNABLE 到 TERMINATED 状态</h3><p>线程执行完 run() 方法后，会自动转换到 TERMINATED 状态，当然如果执行 run() 方法的时候异常抛出，也会导致线程终止。</p>
<p>有时候我们需要强制中断 run() 方法的执行，例如 run() 方法访问一个很慢的网络，我们等不下去了，想终止怎么办呢？Java 的 Thread 类里面倒是有个 stop() 方法，不过已经标记为 @Deprecated，所以不建议使用了。正确的姿势其实是调用 interrupt() 方法。</p>
<h4 id="stop-和-interrupt-方法的主要区别"><a href="#stop-和-interrupt-方法的主要区别" class="headerlink" title="stop() 和 interrupt() 方法的主要区别"></a>stop() 和 interrupt() 方法的主要区别</h4><p>stop() 方法会真的杀死线程，不给线程喘息的机会，如果线程持有 ReentrantLock 锁，被 stop() 的线程并不会自动调用 ReentrantLock 的 unlock() 去释放锁，那其他线程就再也没机会获得 ReentrantLock 锁，这实在是太危险了。所以该方法就不建议使用了，类似的方法还有 suspend() 和 resume() 方法，这两个方法同样也都不建议使用了，所以这里也就不多介绍了。</p>
<p>而 interrupt() 方法就温柔多了，interrupt() 方法仅仅是通知线程，线程有机会执行一些后续操作，同时也可以无视这个通知。被 interrupt 的线程，是怎么收到通知的呢？一种是异常，另一种是主动检测。</p>
<ul>
<li><p>当线程 A 处于 WAITING、TIMED_WAITING 状态时，如果其他线程调用线程 A 的 interrupt() 方法，会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发 InterruptedException 异常。上面我们提到转换到 WAITING、TIMED_WAITING 状态的触发条件，都是调用了类似 wait()、join()、sleep() 这样的方法，我们看这些方法的签名，发现都会 throws InterruptedException 这个异常。这个异常的触发条件就是：其他线程调用了该线程的 interrupt() 方法。</p>
</li>
<li><p>当线程 A 处于 RUNNABLE 状态时，并且阻塞在 java.nio.channels.InterruptibleChannel 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 会触发 java.nio.channels.ClosedByInterruptException 这个异常；而阻塞在 java.nio.channels.Selector 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 的 java.nio.channels.Selector 会立即返回。</p>
</li>
<li><p>上面这两种情况属于被中断的线程通过异常的方式获得了通知。还有一种是主动检测，如果线程处于 RUNNABLE 状态，并且没有阻塞在某个 I/O 操作上，例如中断计算圆周率的线程 A，这时就得依赖线程 A 主动检测中断状态了。如果其他线程调用线程 A 的 interrupt() 方法，那么线程 A 可以通过 isInterrupted() 方法，检测是不是自己被中断了。</p>
</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>理解 Java 线程的各种状态以及生命周期对于诊断多线程 Bug 非常有帮助，多线程程序很难调试，出了 Bug 基本上都是靠日志，靠线程 dump 来跟踪问题，分析线程 dump 的一个基本功就是分析线程状态，大部分的死锁、饥饿、活锁问题都需要跟踪分析线程的状态。</p>
<p>可以通过 jstack 命令或者Java VisualVM这个可视化工具将 JVM 所有的线程栈信息导出来，完整的线程栈信息不仅包括线程的当前状态、调用栈，还包括了锁的信息。例如，我曾经写过一个死锁的程序，导出的线程栈明确告诉我发生了死锁，并且将死锁线程的调用栈信息清晰地显示出来了（如下图）。导出线程栈，分析线程状态是诊断并发问题的一个重要工具。</p>
<p><img src="https://static001.geekbang.org/resource/image/67/be/67734e1a062adc7cf7baac7d6c17ddbe.png" alt="img"></p>
<center>发生死锁的线程栈</center>

<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>若要实现当前线程被中断之后，退出while(true)，这样实现是否正确？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Thread th = Thread.currentThread();</span><br><span class="line"><span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">  <span class="keyword">if</span>(th.isInterrupted()) &#123;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 省略业务代码无数</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    Thread.sleep(<span class="number">100</span>);</span><br><span class="line">  &#125;<span class="keyword">catch</span> (InterruptedException e)&#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>A：</p>
<ul>
<li>可能出现无限循环，线程在sleep期间被打断了，抛出一个InterruptedException异常，try catch捕捉此异常，应该重置一下中断标示，因为抛出异常后，中断标示会自动清除掉！</li>
<li>interrupt是中断的意思，在单片机开发领域，用于接收特定的事件，从而执行后续的操作。Java线程中，（通常）使用interrupt作为线程退出的通知事件，告知线程可以结束了。<br>interrupt不会结束线程的运行，在抛出InterruptedException后会清除中断标志（代表可以接收下一个中断信号了），所以我想，interrupt应该也是可以类似单片机一样作为一种通知信号的，只是实现通知的话，Java有其他更好的选择。<br>因InterruptedException退出同步代码块会释放当前线程持有的锁，所以相比外部强制stop是安全的（已手动测试）。sleep、join等会抛出InterruptedException的操作会立即抛出异常，wait在被唤醒之后才会抛出异常（就像阻塞一样，不被打扰）。I/O阻塞在Java中是可运行状态，并发包中的lock是等待状态。</li>
<li>当发起中断之后，Thread.sleep(100);会抛出InterruptedException异常，而这个抛出这个异常会清除当前线程的中断标识，导致th.isInterrupted()一直都是返回false的。</li>
<li>不能中断循环，异常捕获要放在while循环外面</li>
</ul>
<h1 id="创建多少线程才是合适的"><a href="#创建多少线程才是合适的" class="headerlink" title="创建多少线程才是合适的"></a>创建多少线程才是合适的</h1><blockquote>
<p>Java领域，实现并发程序的主要手段就是多线程</p>
</blockquote>
<p>经常碰到这样的问题：各种线程池的线程数量调整成多少是合适的？或者Tomcat 的线程数、Jdbc 连接池的连接数是多少？</p>
<p>设置合适的线程数需要分析：</p>
<ol>
<li>为什么要使用多线程</li>
<li>多线程的应用场景有哪些</li>
</ol>
<h2 id="为什么要使用多线程？"><a href="#为什么要使用多线程？" class="headerlink" title="为什么要使用多线程？"></a>为什么要使用多线程？</h2><blockquote>
<p>本质上就是提升程序性能</p>
</blockquote>
<p>如何度量呢，度量性能是否得到提升？</p>
<p>度量的指标有很多，有两个指标是最核心的，就是吞吐量和延迟。</p>
<ul>
<li>延迟<ul>
<li>指的是发出请求到收到响应这个过程的时间；<ul>
<li>延迟越短，意味着程序执行得越快，性能也就越好。</li>
</ul>
</li>
</ul>
</li>
<li>吞吐量<ul>
<li>指的是在单位时间内能处理请求的数量；<ul>
<li>吞吐量越大，意味着程序能处理的请求越多，性能也就越好。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>二者存在一些联系，同等条件下，延迟越短，吞吐量越大，也隶属于不同维度，一个是时间维度，一个是空间维度，不能互相转换。</p>
<p>提升性能，从度量的角度，主要是降低延迟，提高吞吐量，这也是我们使用多线程的主要目的。</p>
<h2 id="多线程的应用场景"><a href="#多线程的应用场景" class="headerlink" title="多线程的应用场景"></a>多线程的应用场景</h2><p>降低延迟，提高吞吐量的方法：</p>
<ol>
<li>优化算法【属于算法范畴】</li>
<li>将硬件的性能发挥到极致【并发编程】</li>
</ol>
<p>计算机主要有I/O和CPU两类硬件，在并发编程领域，提升性能本质上就是提升硬件的利用率，再具体点来说，就是提升 I/O 的利用率和 CPU 的利用率。</p>
<p>操作系统不是已经解决了硬件的利用率问题，例如：操作系统已经解决了磁盘和网卡的利用率问题，利用中断机制还能避免 CPU 轮询 I/O 状态，也提升了 CPU 的利用率。</p>
<p>但是操作系统解决硬件利用率问题的对象往往是单一的硬件设备，而我们的并发程序，往往需要 CPU 和 I/O 设备相互配合工作，也就是说，我们需要解决 CPU 和 I/O 设备综合利用率的问题。</p>
<p><strong>Q：如何利用多线程来提升 CPU 和 I/O 设备的利用率？</strong></p>
<p><strong>A：</strong></p>
<p>假设程序按照 CPU 计算和 I/O 操作交叉执行的方式运行，而且 CPU 计算和 I/O 操作的耗时是 1:1，</p>
<p>如果只有一个线程，执行 CPU 计算的时候，I/O 设备空闲；执行 I/O 操作的时候，CPU 空闲，所以 CPU 的利用率和 I/O 设备的利用率都是 50%。</p>
<p><img src="https://static001.geekbang.org/resource/image/d1/22/d1d7dfa1d574356cc5cb1019a4b7ca22.png" alt="img"></p>
<center>单线程执行</center>

<p>如果有两个线程，如下图所示，当线程 A 执行 CPU 计算的时候，线程 B 执行 I/O 操作；当线程 A 执行 I/O 操作的时候，线程 B 执行 CPU 计算，这样 CPU 的利用率和 I/O 设备的利用率就都达到了 100%。</p>
<p><img src="https://static001.geekbang.org/resource/image/68/2c/68a415b31b72844eb81889e9f0eb3f2c.png" alt="img"></p>
<center>二线程执行</center>

<p>CPU 的利用率和 I/O 设备的利用率都提升到了 100%，会对性能产生了哪些影响呢？</p>
<p>很容易看出：单位时间处理的请求数量翻了一番，也就是说吞吐量提高了 1 倍。此时可以逆向思维一下，如果 CPU 和 I/O 设备的利用率都很低，那么可以尝试通过增加线程来提高吞吐量。</p>
<p>在单核时代，多线程主要就是用来平衡 CPU 和 I/O 设备的。如果程序只有 CPU 计算，而没有 I/O 操作的话，多线程不但不会提升性能，还会使性能变得更差，原因是增加了线程切换的成本。但是在多核时代，这种纯计算型的程序也可以利用多线程来提升性能。为什么呢？因为利用多核可以降低响应时间。</p>
<p>为了便于理解：</p>
<p>E.g.：计算 1+2+… … +100 亿的值，如果在 4 核的 CPU 上利用 4 个线程执行，线程 A 计算[1，25 亿)，线程 B 计算[25 亿，50 亿)，线程 C 计算[50，75 亿)，线程 D 计算[75 亿，100 亿]，之后汇总，那么理论上应该比一个线程计算[1，100 亿]快将近 4 倍，响应时间能够降到 25%。一个线程，对于 4 核的 CPU，CPU 的利用率只有 25%，而 4 个线程，则能够将 CPU 的利用率提高到 100%。</p>
<p><img src="https://static001.geekbang.org/resource/image/95/8c/95367d49f55e0dfd099f2749c532098c.png" alt="img"></p>
<h2 id="创建多少线程合适"><a href="#创建多少线程合适" class="headerlink" title="创建多少线程合适"></a>创建多少线程合适</h2><p>创建多少线程合适，要看多线程具体的应用场景。我们的程序一般都是 CPU 计算和 I/O 操作交叉执行的，由于 I/O 设备的速度相对于 CPU 来说都很慢，所以大部分情况下，I/O 操作执行的时间相对于 CPU 计算来说都非常长，这种场景我们一般都称为 I/O 密集型计算；和 I/O 密集型计算相对的就是 CPU 密集型计算了，CPU 密集型计算大部分场景下都是纯 CPU 计算。I/O 密集型程序和 CPU 密集型程序，计算最佳线程数的方法是不同的。</p>
<p>下面我们对这两个场景分别说明。</p>
<p>对于 CPU 密集型计算，多线程本质上是提升多核 CPU 的利用率，所以对于一个 4 核的 CPU，每个核一个线程，理论上创建 4 个线程就可以了，再多创建线程也只是增加线程切换的成本。所以，对于 CPU 密集型的计算场景，理论上“线程的数量 =CPU 核数”就是最合适的。不过在工程上，线程的数量一般会设置为“CPU 核数 +1”，这样的话，当线程因为偶尔的内存页失效或其他原因导致阻塞时，这个额外的线程可以顶上，从而保证 CPU 的利用率。</p>
<p>对于 I/O 密集型的计算场景，比如前面我们的例子中，如果 CPU 计算和 I/O 操作的耗时是 1:1，那么 2 个线程是最合适的。如果 CPU 计算和 I/O 操作的耗时是 1:2，那多少个线程合适呢？是 3 个线程，如下图所示：CPU 在 A、B、C 三个线程之间切换，对于线程 A，当 CPU 从 B、C 切换回来时，线程 A 正好执行完 I/O 操作。这样 CPU 和 I/O 设备的利用率都达到了 100%。</p>
<p><img src="https://static001.geekbang.org/resource/image/98/cb/98b71b72f01baf5f0968c7c3a2102fcb.png" alt="img"></p>
<center>三线程执行示意图</center>

<p>通过上面这个例子，我们会发现，对于 I/O 密集型计算场景，最佳的线程数是与程序中 CPU 计算和 I/O 操作的耗时比相关的，我们可以总结出这样一个公式：<br>$$<br>最佳线程数 = 1 + (I/O耗时 / CPU耗时)<br>$$<br>我们令 R=I/O 耗时 / CPU 耗时，综合上图，可以这样理解：当线程 A 执行 IO 操作时，另外 R 个线程正好执行完各自的 CPU 计算。这样 CPU 的利用率就达到了 100%。</p>
<p>不过上面这个公式是针对单核 CPU 的，至于多核 CPU，也很简单，只需要等比扩大就可以了，计算公式如下：<br>$$<br>最佳线程数 =CPU 核数 * [ 1 +（I/O 耗时 / CPU 耗时）]总结<br>$$</p>
<h2 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>很多人都知道线程数不是越多越好，但是设置多少是合适的，却又拿不定主意。其实只要把握住一条原则就可以了，这条原则就是将硬件的性能发挥到极致。上面我们针对 CPU 密集型和 I/O 密集型计算场景都给出了理论上的最佳公式，这些公式背后的目标其实就是将硬件的性能发挥到极致。</p>
<p>对于 I/O 密集型计算场景，I/O 耗时和 CPU 耗时的比值是一个关键参数，不幸的是这个参数是未知的，而且是动态变化的，所以工程上，我们要估算这个参数，然后做各种不同场景下的压测来验证我们的估计。不过工程上，原则还是将硬件的性能发挥到极致，所以压测时，我们需要重点关注 CPU、I/O 设备的利用率和性能指标（响应时间、吞吐量）之间的关系。</p>
<h2 id="思考-1"><a href="#思考-1" class="headerlink" title="思考"></a>思考</h2><p>Q：有些同学对于最佳线程数的设置积累了一些经验值，认为对于 I/O 密集型应用，最佳线程数应该为：2 * CPU 的核数 + 1，你觉得这个经验值合理吗？</p>
<p>A：</p>
<ul>
<li>更多的精力其实应该放在算法的优化上，线程池的配置，按照经验配置一个，随时关注线程池大小对程序的影响即可，具体做法：可以为你的程序配置一个全局的线程池，需要异步执行的任务，扔到这个全局线程池处理，线程池大小按照经验设置，每隔一段时间打印一下线程池的利用率，做到心里有数。看到过太多的代码，遇到要执行一个异步任务就创建一个线程池，导致整个程序的线程池大到爆，完全没必要。而且大多数时候，提高吞吐量可以通过使用缓存、优化业务逻辑、提前计算好等方式来处理，真没有必要太过于关注线程池大小怎么配置，如果小了就改大一点，大了改小一点就好，从老师本文的篇幅也可以看出来。经验值不靠谱的另外一个原因，大多数情况下，一台服务器跑了很多程序，每个程序都有自己的线程池，那CPU如何分配？还是根据实际情况来确定比较好。</li>
<li>定性的io密集或者cpu密集很难在定量的维度上反应出性能瓶颈，而且公式上忽略了线程数增加带来的cpu消耗，性能优化还是要定量比较好，这样不会盲目，比如io已经成为了瓶颈，增加线程或许带来不了性能提升，这个时候是不是可以考虑用cpu换取带宽，压缩数据，或者逻辑上少发送一些。最后一个问题，我的答案是大部分应用环境是合理的，老师也说了是积累了一些调优经验后给出的方案，没有特殊需求，初始值我会选大家都在用伪标准</li>
<li>认为不合理，不能只考虑经验，还有根据是IO密集型或者是CPU密集型，具体问题具体分析。<br>看今天文章内容，分享个实际问题；我们公司服务器都是容器，一个物理机分出好多容器，有个哥们设置线程池数量直接就是：Runtime.getRuntime().availableProcessors() * 2；本来想获取容器的CPU数量 * 2，其实Runtime.getRuntime().availableProcessors()获取到的是物理机CPU合数，一下开启了好多线程 ^_^</li>
<li>理论加经验加实际场景，比如现在大多数公司的系统是以服务的形式来通过docker部署的，每个docker服务其实对应部署的就一个服务，这样的情况下是可以按照理论为基础，再加上实际情况来设置线程池大小的，当然通过各种监控来调整是最好的，但是实际情况是但服务几十上百，除非是核心功能，否则很难通过监控指标来调整线程池大小。理论加经验起码不会让设置跑偏太多，还有就是服务中的各种线程池统一管理是很有必要的</li>
</ul>
<p>Q：</p>
<p>当应用来的请数量过大，此时线程池的线程已经不够使用，排队的队列也已经满了，那么后面的请求就会被丢弃掉，如果这是一个更新数据的请求操作，那么就会出现数据更新丢失。</p>
<p>A：单机有瓶颈，就分布式。数据库有瓶颈，就分库分表分片</p>
<p>Q：</p>
<p>在4核8线程的处理器使用Runtime.availableProcessors()结果是8，超线程技术属于硬件层面上的并发，从cpu硬件来看是一个物理核心有两个逻辑核心，但因为缓存、执行资源等存在共享和竞争，所以两个核心并不能并行工作。超线程技术统计性能提升大概是30%左右，并不是100%。另外，不管设置成4还是8，现代操作系统层面的调度应该是按逻辑核心数，也就是8来调度的（除非禁用超线程技术）。所以我觉得这种情况下，严格来说，4和8都不一定是合适的。</p>
<p>A：</p>
<p>工作中都是按照逻辑核数来的，理论值和经验值只是提供个指导，实际上还是要靠压测。</p>
<h1 id="为什么局部变量是线程安全的"><a href="#为什么局部变量是线程安全的" class="headerlink" title="为什么局部变量是线程安全的"></a>为什么局部变量是线程安全的</h1><p>多个线程同时访问共享变量的时候，会导致并发问题。那在 Java 语言里，是不是所有变量都是共享变量呢？</p>
<p>不少同学会给方法里面的局部变量设置同步，显然这些同学并没有把共享变量搞清楚。那 Java 方法里面的局部变量是否存在并发问题呢？</p>
<p>E.g.：fibonacci() 这个方法，会根据传入的参数 n ，返回 1 到 n 的斐波那契数列，斐波那契数列类似这样： 1、1、2、3、5、8、13、21、34……第 1 项和第 2 项是 1，从第 3 项开始，每一项都等于前两项之和。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 返回斐波那契数列</span></span><br><span class="line"><span class="keyword">int</span>[] fibonacci(<span class="keyword">int</span> n) &#123;</span><br><span class="line">  <span class="comment">// 创建结果数组，保存数列的结果</span></span><br><span class="line">  <span class="keyword">int</span>[] r = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">  <span class="comment">// 初始化第一、第二个数</span></span><br><span class="line">  r[<span class="number">0</span>] = r[<span class="number">1</span>] = <span class="number">1</span>;  <span class="comment">// ①</span></span><br><span class="line">  <span class="comment">// 计算2..n</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; n; i++) &#123;</span><br><span class="line">      r[i] = r[i-<span class="number">2</span>] + r[i-<span class="number">1</span>];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>每次计算完一项，都会更新数组 r 对应位置中的值。</p>
<p>Q：那么，当多个线程调用 fibonacci() 这个方法的时候，数组 r 是否存在数据竞争（Data Race）呢，即多个线程执行①处？</p>
<p>A：局部变量不存在数据竞争，在CPU层面， 是没有方法概念的，CPU眼里只有一条条的指令。</p>
<h2 id="方法是如何被执行的"><a href="#方法是如何被执行的" class="headerlink" title="方法是如何被执行的"></a>方法是如何被执行的</h2><p>高级语言里的普通语句，例如上面的r[i] = r[i-2] + r[i-1];翻译成 CPU 的指令相对简单，可方法的调用就比较复杂了。例如下面这三行代码：第 1 行，声明一个 int 变量 a；第 2 行，调用方法 fibonacci(a)；第 3 行，将 b 赋值给 c。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">7</span>；</span><br><span class="line"><span class="keyword">int</span>[] b = fibonacci(a);</span><br><span class="line"><span class="keyword">int</span>[] c = b;</span><br></pre></td></tr></table></figure>

<p>当调用 fibonacci(a) 的时候，CPU 要先找到方法 fibonacci() 的地址，然后跳转到这个地址去执行代码，最后 CPU 执行完方法 fibonacci() 之后，要能够返回。</p>
<p>首先找到调用方法的下一条语句的地址：也就是int[] c=b;的地址，再跳转到这个地址去执行。 </p>
<p><img src="https://static001.geekbang.org/resource/image/9b/1f/9bd881b545e1c67142486f6594dc9d1f.png" alt="img"></p>
<center>方法的调用过程</center>

<p>以上我们了解了方法调用的过程，那么：</p>
<p>CPU 去哪里找到调用方法的参数和返回地址？</p>
<p>如果你熟悉 CPU 的工作原理，你应该会立刻想到：通过 CPU 的堆栈寄存器。CPU 支持一种栈结构，栈你一定很熟悉了，就像手枪的弹夹，先入后出。因为这个栈是和方法调用相关的，因此经常被称为<strong>调用栈</strong>。</p>
<p>有三个方法 A、B、C，他们的调用关系是 A-&gt;B-&gt;C（A 调用 B，B 调用 C），在运行时，会构建出下面这样的调用栈。每个方法在调用栈里都有自己的独立空间，称为栈帧，每个栈帧里都有对应方法需要的参数和返回地址。当调用方法时，会创建新的栈帧，并压入调用栈；当方法返回时，对应的栈帧就会被自动弹出。也就是说，栈帧和方法是同生共死的。</p>
<p><img src="https://static001.geekbang.org/resource/image/67/c7/674bb47feccbf55cf0b6acc5c92e4fc7.png" alt="img"></p>
<center>调用栈结构</center>

<p>利用栈结构来支持方法调用这个方案非常普遍，以至于 CPU 里内置了栈寄存器。虽然各家编程语言定义的方法千奇百怪，但是方法的内部执行原理却是出奇的一致：都是靠栈结构解决的。Java 语言虽然是靠虚拟机解释执行的，但是方法的调用也是利用栈结构解决的。</p>
<h2 id="局部变量的存放"><a href="#局部变量的存放" class="headerlink" title="局部变量的存放"></a>局部变量的存放</h2><blockquote>
<p>我们知道了方法间的调用在CPU眼里是怎么执行的</p>
</blockquote>
<p>但是，方法内的局部变量存放在哪里呢？</p>
<p>局部变量的作用域是方法内部，也就是说当方法执行完，局部变量就没用了，局部变量应该和方法同生共死。此时你应该会想到调用栈的栈帧，调用栈的栈帧就是和方法同生共死的，所以局部变量放到调用栈里那儿是相当的合理。</p>
<p>事实上，的确是这样的，局部变量就是放到了调用栈里。</p>
<p>于是调用栈的结构就变成了：</p>
<p><img src="https://static001.geekbang.org/resource/image/ec/9c/ece8c32d23e4777c370f594c97762a9c.png" alt="img"></p>
<center>保护局部变量的调用栈结构</center>

<p>学Java的人都知道，new出来的对象是在堆里，局部变量是在栈里，局部变量是和方法同生共死的，一个变量如果想跨越方法的边界，就必须创建在堆里。</p>
<h2 id="调用栈与线程"><a href="#调用栈与线程" class="headerlink" title="调用栈与线程"></a>调用栈与线程</h2><p>Q：两个线程可以同时用不同的参数调用相同的方法，那调用栈和线程之间是什么关系呢？</p>
<p>A：每个线程都有自己独立的调用栈。因为如果不是这样，那两个线程就互相干扰了。</p>
<p><img src="https://static001.geekbang.org/resource/image/84/1a/840cb955e521bd51776dbcdad3dba11a.png" alt="img"></p>
<center>线程与调用栈的关系</center>

<p>现在我们再来考虑那个问题：Java 方法里面的局部变量是否存在并发问题？</p>
<p>现在应该很清楚了，一点问题都没有。因为每个线程都有自己的调用栈，局部变量保存在线程各自的调用栈里面，不会共享，所以自然也就没有并发问题。再次重申一遍：没有共享，就没有伤害。</p>
<h2 id="线程封闭"><a href="#线程封闭" class="headerlink" title="线程封闭"></a>线程封闭</h2><p>方法里的局部变量，因为不会和其他线程共享，所以没有并发问题，这个思路很好，已经成为解决并发问题的一个重要技术，同时还有个响当当的名字叫做<strong>线程封闭</strong></p>
<blockquote>
<p>仅在单线程内访问数据。由于不存在共享，所以即便不同步也不会有并发问题，性能足够好。</p>
</blockquote>
<p>采用线程封闭技术的案例非常多，例如：</p>
<p>从数据库连接池里获取的连接 Connection，在 JDBC 规范里并没有要求这个 Connection 必须是线程安全的。数据库连接池通过线程封闭技术，保证一个 Connection 一旦被一个线程获取之后，在这个线程关闭 Connection 之前的这段时间里，不会再分配给其他线程，从而保证了 Connection 不会有并发问题。</p>
<h2 id="Conclusion-2"><a href="#Conclusion-2" class="headerlink" title="Conclusion"></a>Conclusion</h2><blockquote>
<p>调用栈是一个通用的计算机概念，所有的编程语言都会涉及到</p>
</blockquote>
<h2 id="思考-2"><a href="#思考-2" class="headerlink" title="思考"></a>思考</h2><p>Q：递归调用太深，可能导致栈溢出。你思考一下原因是什么？有哪些解决方案呢？</p>
<p>A：</p>
<p>栈溢出原因：</p>
<p>因为每调用一个方法就会在栈上创建一个栈帧，方法调用结束后就会弹出该栈帧，而栈的大小不是无限的，所以递归调用次数过多的话就会导致栈溢出。</p>
<p>而递归调用的特点是每递归一次，就要创建一个新的栈帧，而且还要保留之前的环境（栈帧），直到遇到结束条件。所以递归调用一定要明确好结束条件，不要出现死循环，而且要避免栈太深。</p>
<p>解决方法：</p>
<ol>
<li><p>简单粗暴，不要使用递归，使用循环替代。缺点：代码逻辑不够清晰；</p>
</li>
<li><p>限制递归次数；</p>
</li>
<li><p>使用尾递归</p>
<ul>
<li>尾递归是指在方法返回时只调用自己本身，且不能包含表达式。编译器或解释器会把尾递归做优化，使递归方法不论调用多少次，都只占用一个栈帧，所以不会出现栈溢出。然而，Java没有尾递归优化。</li>
</ul>
</li>
</ol>
<h1 id="如何用面向对象思想写好并发程序"><a href="#如何用面向对象思想写好并发程序" class="headerlink" title="如何用面向对象思想写好并发程序"></a>如何用面向对象思想写好并发程序</h1><p>很多时候，代码设计之初都是直接按照单线程的思路来写，而忽略了本应该重视的并发问题。</p>
<p>面向对象思想与并发编程有关系吗？</p>
<p>本来没关系，他们分属于两个不同的领域，但是在Java语言里，融合效果还是不错的，在Java语言中，面向对象思想能够让并发编程变得更简单。</p>
<p>具体如何才能用面向对象思路写好并发程序呢？</p>
<ul>
<li>封装共享变量</li>
<li>识别共享变量间的约束条件</li>
<li>制定并发访问策略</li>
</ul>
<h2 id="封装共享变量"><a href="#封装共享变量" class="headerlink" title="封装共享变量"></a>封装共享变量</h2><blockquote>
<p>面向对象思想里面有一个很重要的特性是封装</p>
</blockquote>
<p>将属性和实现细节封装在对象内部，外界对象只能通过目标对象提供的公共方法来间接访问这些内部属性。</p>
<p>通俗地讲，和门票管理模型匹配度相对较高，球场里的作为就是对象属性，球场入口就是对象的公共方法，我们把共享变量作为对象的属性，对于共享变量的访问路径就是对象的公共方法，所有入口都要安排检票程序就相当于我们前面提到的并发访问策略。</p>
<p>利用面向对象思想写并发程序的思路，其实就是将共享变量作为对象属性封装在内部，对所有公共方法指定并发访问策略。</p>
<p>E.g.：统计程序的计数器</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Counter</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> value;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">long</span> <span class="title">get</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">long</span> <span class="title">addOne</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ++value;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>计数器程序共享变量只有一个</strong>，就是 value，我们把它作为 Counter 类的属性，并且将两个公共方法 get() 和 addOne() 声明为同步方法，这样 Counter 类就成为一个线程安全的类了。</p>
<p>实际工作中，经常面临的情况往往是很多的共享变量，多个共享变量，如果每一个都考虑其并发安全问题，会做很多不必要的工作，因为很多共享变量的值不会变，对于这些不会变化的共享变量，建议使用final关键字修饰，既能避免并发问题，也能表明设计意图。</p>
<h2 id="识别共享变量间的约束条件"><a href="#识别共享变量间的约束条件" class="headerlink" title="识别共享变量间的约束条件"></a>识别共享变量间的约束条件</h2><blockquote>
<p>非常重要</p>
<p>这些约束条件决定了并发访问策略</p>
</blockquote>
<p>E.g.：库存管理里面有个合理库存的概念，库存量不能太高，也不能太低，有一个上限和下限。</p>
<p>关于约束条件，可以用程序进行模拟：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*<span class="doctag">@param</span>:upper:库存上限</span></span><br><span class="line"><span class="comment">*<span class="doctag">@param</span>:lower:库存上限</span></span><br><span class="line"><span class="comment">*用了AtomicLong原子 类</span></span><br><span class="line"><span class="comment">*原子类是线程安全的，set方法不需要同步</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SafeWM</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 库存上限</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> AtomicLong upper =</span><br><span class="line">        <span class="keyword">new</span> AtomicLong(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// 库存下限</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> AtomicLong lower =</span><br><span class="line">        <span class="keyword">new</span> AtomicLong(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// 设置库存上限</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">setUpper</span><span class="params">(<span class="keyword">long</span> v)</span></span>&#123;</span><br><span class="line">    upper.set(v);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 设置库存下限</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">setLower</span><span class="params">(<span class="keyword">long</span> v)</span></span>&#123;</span><br><span class="line">    lower.set(v);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 省略其他业务代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>代码貌似是没问题的，但是<strong>忽略了一个约束条件</strong>，就是库存下限要小于库存上限。</p>
<p>这个约束条件能够直接加到上面的 set 方法上吗？我们先直接加一下看看效果（如下面代码所示）。我们在 setUpper() 和 setLower() 中增加了参数校验，这乍看上去好像是对的，但其实存在并发问题，问题在于存在竞态条件。这里我顺便插一句，其实当你看到代码里出现 if 语句的时候，就应该立刻意识到可能存在竞态条件。</p>
<p>我们假设库存的下限和上限分别是 (2,10)，线程 A 调用 setUpper(5) 将上限设置为 5，线程 B 调用 setLower(7) 将下限设置为 7，如果线程 A 和线程 B 完全同时执行，你会发现线程 A 能够通过参数校验，因为这个时候，下限还没有被线程 B 设置，还是 2，而 5&gt;2；线程 B 也能够通过参数校验，因为这个时候，上限还没有被线程 A 设置，还是 10，而 7&lt;10。当线程 A 和线程 B 都通过参数校验后，就把库存的下限和上限设置成 (7, 5) 了，显然此时的结果是<strong>不符合库存下限要小于库存上限这个约束条件</strong>的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SafeWM</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 库存上限</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> AtomicLong upper =</span><br><span class="line">        <span class="keyword">new</span> AtomicLong(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// 库存下限</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> AtomicLong lower =</span><br><span class="line">        <span class="keyword">new</span> AtomicLong(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// 设置库存上限</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">setUpper</span><span class="params">(<span class="keyword">long</span> v)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 检查参数合法性</span></span><br><span class="line">    <span class="keyword">if</span> (v &lt; lower.get()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">    &#125;</span><br><span class="line">    upper.set(v);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 设置库存下限</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">setLower</span><span class="params">(<span class="keyword">long</span> v)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 检查参数合法性</span></span><br><span class="line">    <span class="keyword">if</span> (v &gt; upper.get()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">    &#125;</span><br><span class="line">    lower.set(v);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 省略其他业务代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在没有识别出<strong>库存下限要小于库存上限</strong>这个约束条件之前，我们制定的并发访问策略是利用原子类，但是这个策略，完全不能保证<strong>库存下限要小于库存上限</strong>这个约束条件。所以说，在设计阶段，我们<strong>一定要识别出所有共享变量之间的约束条件，如果约束条件识别不足，很可能导致制定的并发访问策略南辕北辙。</strong></p>
<p>共享变量之间的约束条件，反映在代码里，基本上都会有 if 语句，所以，一定要特别注意竞态条件。</p>
<h2 id="制定并发访问策略"><a href="#制定并发访问策略" class="headerlink" title="制定并发访问策略"></a>制定并发访问策略</h2><p>制定并发访问策略，是一个非常复杂的事情。应该说整个专栏都是在尝试搞定它。不过从方案上来看，无外乎就是以下“三件事”。</p>
<ol>
<li>避免共享：避免共享的技术主要是利于线程本地存储以及为每个任务分配独立的线程。</li>
<li>不变模式：这个在 Java 领域应用的很少，但在其他领域却有着广泛的应用，例如 Actor 模式、CSP 模式以及函数式编程的基础都是不变模式。</li>
<li>管程及其他同步工具：Java 领域万能的解决方案是管程，但是对于很多特定场景，使用 Java 并发包提供的读写锁、并发容器等同步工具会更好。</li>
</ol>
<p>除了这些方案之外，还有一些宏观的原则需要你了解。这些宏观原则，有助于你写出“健壮”的并发程序。这些原则主要有以下三条。</p>
<ol>
<li>优先使用成熟的工具类：Java SDK 并发包里提供了丰富的工具类，基本上能满足你日常的需要，建议你熟悉它们，用好它们，而不是自己再“发明轮子”，毕竟并发工具类不是随随便便就能发明成功的。</li>
<li>迫不得已时才使用低级的同步原语：低级的同步原语主要指的是 synchronized、Lock、Semaphore 等，这些虽然感觉简单，但实际上并没那么简单，一定要小心使用。</li>
<li>避免过早优化：安全第一，并发程序首先要保证安全，出现性能瓶颈后再优化。在设计期和开发期，很多人经常会情不自禁地预估性能的瓶颈，并对此实施优化，但残酷的现实却是：性能瓶颈不是你想预估就能预估的。</li>
</ol>
<h2 id="Conclusion-3"><a href="#Conclusion-3" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>利用面向对象思想编写并发程序，一个关键点就是利用面向对象里的封装特性。</p>
<p>对共享变量进行封装，要避免“逸出”，所谓“逸出”简单讲就是共享变量逃逸到对象的外面。</p>
<h2 id="思考-3"><a href="#思考-3" class="headerlink" title="思考"></a>思考</h2><p>类 SafeWM 不满足库存下限要小于库存上限这个约束条件，那你来试试修改一下，让它能够在并发条件下满足库存下限要小于库存上限这个约束条件。</p>
<p><img src="https://img-blog.csdnimg.cn/20200824152158744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>对于两个互相比较的变量来说，赋值的时候只能加锁来控制。但是这也会带来性能问题，不过可以采用读锁和写锁来优化，申请写锁了就互斥，读锁可以并发访问，这样性能相对粗粒度的锁来说会高点。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DBPush</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">static</span> DBPush dbPush = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">DBPush</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> DBPush <span class="title">getInStance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (dbPush == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (DBPush<span class="class">.<span class="keyword">class</span>) </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (dbPush == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    dbPush = <span class="keyword">new</span> DBPush();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dbPush;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SafeWM</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 库存上限</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> AtomicLong upper =</span><br><span class="line">        <span class="keyword">new</span> AtomicLong(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// 库存下限</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> AtomicLong lower =</span><br><span class="line">        <span class="keyword">new</span> AtomicLong(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// 设置库存上限</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">setUpper</span><span class="params">(<span class="keyword">long</span> v)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">      <span class="comment">// 检查参数合法性</span></span><br><span class="line">      <span class="keyword">if</span> (v &lt; lower.get()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">      &#125;</span><br><span class="line">      upper.set(v);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 设置库存下限</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">setLower</span><span class="params">(<span class="keyword">long</span> v)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">      <span class="comment">// 检查参数合法性</span></span><br><span class="line">      <span class="keyword">if</span> (v &gt; upper.get()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">      &#125;</span><br><span class="line">      lower.set(v);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 省略其他业务代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Concurrent Programming</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>管程-并发编程的万能钥匙</title>
    <url>/2020/08/22/%E7%AE%A1%E7%A8%8B-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E4%B8%87%E8%83%BD%E9%92%A5%E5%8C%99/</url>
    <content><![CDATA[<h1 id="管程-并发编程的万能钥匙"><a href="#管程-并发编程的万能钥匙" class="headerlink" title="管程-并发编程的万能钥匙"></a>管程-并发编程的万能钥匙</h1><blockquote>
<p>Java 语言在 1.5 之前，提供的唯一的并发原语就是管程，而且 1.5 之后提供的 SDK 并发包，也是以管程技术为基础的。</p>
</blockquote>
<p>可以说，管程就是一把解决并发问题的万能钥匙。</p>
<p>为什么 Java 在 1.5 之前仅仅提供了 synchronized 关键字及 wait()、notify()、notifyAll() 这三个方法？</p>
<p>操作系统原理中我们知道信号量能解决所有并发问题，但是Java采用的是管程技术，synchronized关键字及wait、notify、notifyAll这三个方法都是管程的组成部分。</p>
<p>管程和信号量是等价的，所谓等价指的是用管程能够实现信号量，也能用信号量实现管程。</p>
<p>管程指的是管理共享变量以及对共享变量的操作过程，让他们支持并发。</p>
<h2 id="MESA模型"><a href="#MESA模型" class="headerlink" title="MESA模型"></a>MESA模型</h2><p>管程发展史上，先后出现过三种不同的管程模型，</p>
<ul>
<li>Hasen 模型</li>
<li>Hoare 模型</li>
<li>MESA 模型【Java管程实现参考】</li>
</ul>
<p>面对两大并发领域的核心问题：互斥和同步，管程都能解决。</p>
<ul>
<li><p>互斥：同一时刻只允许一个线程访问共享资源；</p>
</li>
<li><p>同步：即线程之间如何通信、协作。这两大问题</p>
</li>
</ul>
<h3 id="如何解决互斥问题"><a href="#如何解决互斥问题" class="headerlink" title="如何解决互斥问题"></a>如何解决互斥问题</h3><blockquote>
<p>管程 解决互斥问题：将共享变量及其对共享变量的操作统一封装起来。</p>
</blockquote>
<p><img src="https://static001.geekbang.org/resource/image/59/c4/592e33c4339c443728cdf82ab3d318c4.png" alt="img"></p>
<center>管程模型的代码化语义</center>

<ol>
<li>管程 X 将共享变量 queue 这个队列和相关的操作入队 enq()、出队 deq() 都封装起来了；</li>
<li>线程 A 和线程 B 如果想访问共享变量 queue，只能通过调用管程提供的 enq()、deq() 方法来实现；</li>
<li>enq()、deq() 保证互斥性，只允许一个线程进入管程。</li>
</ol>
<blockquote>
<p>管程模型和面向对象高度契合</p>
</blockquote>
<h3 id="如何解决同步问题"><a href="#如何解决同步问题" class="headerlink" title="如何解决同步问题"></a>如何解决同步问题</h3><blockquote>
<p>管程引入了条件变量的概念，而且每一个条件变量都对应有一个等待队列</p>
</blockquote>
<p><img src="https://static001.geekbang.org/resource/image/83/65/839377608f47e7b3b9c79b8fad144065.png" alt="img"></p>
<center>MESA管程模型</center>

<p>在管程模型里，共享变量和对共享变量的操作是被封装起来的，图中最外层的框就代表封装的意思。框的上面只有一个入口，并且在入口旁边还有一个入口等待队列。当多个线程同时试图进入管程内部时，只允许一个线程进入，其他线程则在入口等待队列中等待。这个过程类似就医流程的分诊，只允许一个患者就诊，其他患者都在门口等待。</p>
<p><strong>条件变量和等待队列的作用：</strong> 解决线程同步的问题。</p>
<p>假设有个线程 T1 执行出队操作，不过需要注意的是执行出队操作，有个前提条件，就是队列不能是空的，而队列不空这个前提条件就是管程里的条件变量。 </p>
<p><strong>如果线程 T1 进入管程后恰好发现队列是空的，那怎么办呢？等待啊，去哪里等呢？</strong></p>
<p>就去条件变量对应的等待队列里面等。此时线程 T1 就去“队列不空”这个条件变量的等待队列中等待。这个过程类似于大夫发现你要去验个血，于是给你开了个验血的单子，你呢就去验血的队伍里排队。线程 T1 进入条件变量的等待队列后，是允许其他线程进入管程的。这和你去验血的时候，医生可以给其他患者诊治，道理都是一样的。</p>
<p>再假设之后另外一个线程 T2 执行入队操作，入队操作执行成功之后，“队列不空”这个条件对于线程 T1 来说已经满足了，此时线程 T2 要通知 T1，告诉它需要的条件已经满足了。当线程 T1 得到通知后，会从等待队列里面出来，但是出来之后不是马上执行，而是重新进入到入口等待队列里面。这个过程类似你验血完，回来找大夫，需要重新分诊。</p>
<p>条件变量及其等待队列我们讲清楚了，下面再说说 wait()、notify()、notifyAll() 这三个操作。前面提到线程 T1 发现“队列不空”这个条件不满足，需要进到对应的等待队列里等待。这个过程就是通过调用 wait() 来实现的。如果我们用对象 A 代表“队列不空”这个条件，那么线程 T1 需要调用 A.wait()。同理当“队列不空”这个条件满足时，线程 T2 需要调用 A.notify() 来通知 A 等待队列中的一个线程，此时这个队列里面只有线程 T1。至于 notifyAll() 这个方法，它可以通知等待队列中的所有线程。</p>
<p>这里我还是来一段代码再次说明一下吧。下面的代码实现的是一个阻塞队列，阻塞队列有两个操作分别是入队和出队，这两个方法都是先获取互斥锁，类比管程模型中的入口。</p>
<ol>
<li>对于入队操作，如果队列已满，就需要等待直到队列不满，所以这里用了notFull.await();</li>
<li>对于出队操作，如果队列为空，就需要等待直到队列不空，所以就用了notEmpty.await();</li>
<li>如果入队成功，那么队列就不空了，就需要通知条件变量：队列不空notEmpty对应的等待队列。</li>
<li>如果出队成功，那就队列就不满了，就需要通知条件变量：队列不满notFull对应的等待队列。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BlockedQueue</span>&lt;<span class="title">T</span>&gt;</span>&#123;</span><br><span class="line">  <span class="keyword">final</span> Lock lock =</span><br><span class="line">    <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">  <span class="comment">// 条件变量：队列不满  </span></span><br><span class="line">  <span class="keyword">final</span> Condition notFull =</span><br><span class="line">    lock.newCondition();</span><br><span class="line">  <span class="comment">// 条件变量：队列不空  </span></span><br><span class="line">  <span class="keyword">final</span> Condition notEmpty =</span><br><span class="line">    lock.newCondition();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 入队</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">enq</span><span class="params">(T x)</span> </span>&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (队列已满)&#123;</span><br><span class="line">        <span class="comment">// 等待队列不满 </span></span><br><span class="line">        notFull.await();</span><br><span class="line">      &#125;  </span><br><span class="line">      <span class="comment">// 省略入队操作...</span></span><br><span class="line">      <span class="comment">//入队后,通知可出队</span></span><br><span class="line">      notEmpty.signal();</span><br><span class="line">    &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 出队</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">deq</span><span class="params">()</span></span>&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (队列已空)&#123;</span><br><span class="line">        <span class="comment">// 等待队列不空</span></span><br><span class="line">        notEmpty.await();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 省略出队操作...</span></span><br><span class="line">      <span class="comment">//出队后，通知可入队</span></span><br><span class="line">      notFull.signal();</span><br><span class="line">    &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们用了 Java 并发包里面的 Lock 和 Condition，如果你看着吃力，也没关系，后面我们还会详细介绍，这个例子只是先让你明白条件变量及其等待队列是怎么回事。需要注意的是：await() 和前面我们提到的 wait() 语义是一样的；signal() 和前面我们提到的 notify() 语义是一样的。</p>
<h3 id="wait-的正确姿势"><a href="#wait-的正确姿势" class="headerlink" title="wait() 的正确姿势"></a>wait() 的正确姿势</h3><blockquote>
<p>对于 MESA 管程来说，有一个编程范式，就是需要在一个 while 循环里面调用 wait()。这个是 MESA 管程特有的。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(条件不满足) &#123;</span><br><span class="line">  wait();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Hasen 模型、Hoare 模型和 MESA 模型的一个核心区别就是当条件满足后，如何通知相关线程。管程要求同一时刻只允许一个线程执行，那当线程 T2 的操作使线程 T1 等待的条件满足时，T1 和 T2 究竟谁可以执行呢？</p>
<ol>
<li>Hasen 模型里面，要求 notify() 放在代码的最后，这样 T2 通知完 T1 后，T2 就结束了，然后 T1 再执行，这样就能保证同一时刻只有一个线程执行。</li>
<li>Hoare 模型里面，T2 通知完 T1 后，T2 阻塞，T1 马上执行；等 T1 执行完，再唤醒 T2，也能保证同一时刻只有一个线程执行。但是相比 Hasen 模型，T2 多了一次阻塞唤醒操作。</li>
<li>MESA 管程里面，T2 通知完 T1 后，T2 还是会接着执行，T1 并不立即执行，仅仅是从条件变量的等待队列进到入口等待队列里面。这样做的好处是 notify() 不用放到代码的最后，T2 也没有多余的阻塞唤醒操作。但是也有个副作用，就是当 T1 再次执行的时候，可能曾经满足的条件，现在已经不满足了，所以需要以循环方式检验条件变量。</li>
</ol>
<h3 id="notify-何时可以使用"><a href="#notify-何时可以使用" class="headerlink" title="notify() 何时可以使用"></a>notify() 何时可以使用</h3><p>还有一个需要注意的地方，就是 notify() 和 notifyAll() 的使用，前面章节，我曾经介绍过，除非经过深思熟虑，否则尽量使用 notifyAll()。那什么时候可以使用 notify() 呢？需要满足以下三个条件：</p>
<ol>
<li>所有等待线程拥有相同的等待条件；</li>
<li>所有等待线程被唤醒后，执行相同的操作；</li>
<li>只需要唤醒一个线程。</li>
</ol>
<p>比如上面阻塞队列的例子中，对于“队列不满”这个条件变量，其阻塞队列里的线程都是在等待“队列不满”这个条件，反映在代码里就是下面这 3 行代码。对所有等待线程来说，都是执行这 3 行代码，重点是 while 里面的等待条件是完全相同的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (队列已满)&#123;</span><br><span class="line">  <span class="comment">// 等待队列不满</span></span><br><span class="line">  notFull.await();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>所有等待线程被唤醒后执行的操作也是相同的，都是下面这几行：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 省略入队操作...</span></span><br><span class="line"><span class="comment">//入队后,通知可出队</span></span><br><span class="line">notEmpty.signal();</span><br></pre></td></tr></table></figure>

<p>同时也满足第 3 条，只需要唤醒一个线程。所以上面阻塞队列的代码，使用 signal() 是可以的。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Java 参考了 MESA 模型，语言内置的管程（synchronized）对 MESA 模型进行了精简。MESA 模型中，条件变量可以有多个，Java 语言内置的管程里只有一个条件变量。具体如下图所示。</p>
<p><img src="https://static001.geekbang.org/resource/image/57/fa/57e4d94e90226b70be3d57024f5333fa.png" alt="img"></p>
<center>Java中的管程</center>

<p>Java 内置的管程方案（synchronized）使用简单，synchronized 关键字修饰的代码块，在编译期会自动生成相关加锁和解锁的代码，但是仅支持一个条件变量；而 Java SDK 并发包实现的管程支持多个条件变量，不过并发包里的锁，需要开发人员自己进行加锁和解锁操作。</p>
<p>并发编程里两大核心问题——互斥和同步，都可以由管程来帮你解决。学好管程，理论上所有的并发问题你都可以解决，并且很多并发工具类底层都是管程实现的，所以学好管程，就是相当于掌握了一把并发编程的万能钥匙。</p>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p><strong>Q：</strong> wait() 方法，在 Hasen 模型和 Hoare 模型里面，都是没有参数的，而在 MESA 模型里面，增加了超时参数，你觉得这个参数有必要吗？</p>
<p><strong>A：</strong></p>
<p>1.管程是一种概念，任何语言都可以通用。</p>
<p>2.在java中，每个加锁的对象都绑定着一个管程（监视器）</p>
<p>3.线程访问加锁对象，就是去拥有一个监视器的过程。如一个病人去门诊室看医生，医生是共享资源，门锁锁定医生，病人去看医生，就是访问医生这个共享资源，门诊室其实是监视器（管程）。</p>
<p>4.所有线程访问共享资源，都需要先拥有监视器。就像所有病人看病都需要先拥有进入门诊室的资格。</p>
<p>5.监视器至少有两个等待队列。一个是进入监视器的等待队列一个是条件变量对应的等待队列。后者可以有多个。就像一个病人进入门诊室诊断后，需要去验血，那么它需要去抽血室排队等待。另外一个病人心脏不舒服，需要去拍胸片，去拍摄室等待。</p>
<p>6.监视器要求的条件满足后，位于条件变量下等待的线程需要重新在门诊室门外排队，等待进入监视器。就像抽血的那位，抽完后，拿到了化验单，然后，重新回到门诊室等待，然后进入看病，然后退出，医生通知下一位进入。</p>
<p>总结起来就是，管程就是一个对象监视器。任何线程想要访问该资源，就要排队进入监控范围。进入之后，接受检查，不符合条件，则要继续等待，直到被通知，然后继续进入监视器。</p>
<p>有hasen 是执行完，再去唤醒另外一个线程。能够保证线程的执行。hoare，是中断当前线程，唤醒另外一个线程，执行玩再去唤醒，也能够保证完成。而mesa是进入等待队列，不一定有机会能够执行。</p>
<p>MESA模型和其他两种模型相比可以实现更好的公平性，因为唤醒只是把你放到队列里而不保证你一定可以执行，最后能不能执行还是要看你自己可不可以抢得到执行权也就是入口，其他两种模型是显式地唤醒，有点内定的意思了。</p>
]]></content>
      <categories>
        <category>Concurrent Programming</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>The theory of concurrent programming</title>
    <url>/2020/08/22/Concurrent%20programming/</url>
    <content><![CDATA[<h1 id="The-theory-of-concurrent-programming"><a href="#The-theory-of-concurrent-programming" class="headerlink" title="The theory of concurrent programming"></a>The theory of concurrent programming</h1><h2 id="来由"><a href="#来由" class="headerlink" title="来由"></a>来由</h2><blockquote>
<p>借助数据库和类似Tomcat中间件而不用写并发程序的时代已经过去了。</p>
</blockquote>
<p>举个例子，Java里的synchronized、wait()/notify() 相关的知识很琐碎，看懂难，会用更难。</p>
<p>但实际上 synchronized、wait()、notify() 不过是操作系统领域里管程模型的一种实现而已，Java SDK 并发包里的条件变量 Condition 也是管程里的概念，如果站在管程这个理论模型的高度而不是单独理解，用起来就得心应手了。</p>
<hr>
<p><strong>管程</strong></p>
<blockquote>
<p>一种解决并发问题的模型，与信号量在逻辑上是等价的，但管程更易用</p>
</blockquote>
<p>很多语言都支持管程。</p>
<hr>
<p>Java SDK并发包乃是并发大师 Doug Lea 出品，其章法在哪里呢？</p>
<p><strong>并发编程其实就是三个核心问题</strong>【且Java SDK并发包大部分内容都是按照这三个维度组织的】：</p>
<ol>
<li>分工<ul>
<li>如何高效地拆解任务并分配给线程</li>
<li>E.g.：Fork/Join框架</li>
</ul>
</li>
<li>同步<ul>
<li>线程之间如何协作</li>
<li>E.g.：CountDownLatch</li>
</ul>
</li>
<li>互斥<ul>
<li>保证同一时刻只允许一个线程访问共享资源</li>
<li>E.g.：可重入锁</li>
</ul>
</li>
</ol>
<blockquote>
<p>Java SDK仅仅是针对并发问题开发出来的工具而已</p>
</blockquote>
<h2 id="并发编程BUG的源头"><a href="#并发编程BUG的源头" class="headerlink" title="并发编程BUG的源头"></a>并发编程BUG的源头</h2><blockquote>
<p>CPU、内存、I/O设备三者中间存在一个核心矛盾就是速度差异</p>
<p>CPU 一天对内存一年；内存一天对I/O设备十年</p>
</blockquote>
<p>为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系结构、操作系统、编译程序都做出了贡献：</p>
<ul>
<li>CPU 增加了缓存，以均衡与内存的速度差异；</li>
<li>操作系统增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异；</li>
<li>编译程序优化指令执行次序，使得缓存能够得到更加合理地利用。</li>
</ul>
<blockquote>
<p>正是这些造就了并发编程的BUG神出鬼没</p>
</blockquote>
<h3 id="缓存导致的可见性问题"><a href="#缓存导致的可见性问题" class="headerlink" title="缓存导致的可见性问题"></a>缓存导致的可见性问题</h3><blockquote>
<p>一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为可见性</p>
</blockquote>
<p>单核时代，所有的线程都是在一颗 CPU 上执行，CPU 缓存与内存的数据一致性容易解决。因为所有线程都是操作同一个 CPU 的缓存，一个线程对缓存的写，对另外一个线程来说一定是可见的。</p>
<p><img src="https://static001.geekbang.org/resource/image/a0/da/a07e8182819e2b260ce85b2167d446da.png" alt="img"></p>
<center>CPU缓存与内存的关系</center>

<p>多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存。</p>
<p><img src="https://static001.geekbang.org/resource/image/e2/ea/e2aa76928b2bc135e08e7590ca36e0ea.png" alt="img"></p>
<center>多核CPU的缓存与内存关系</center>

<h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><blockquote>
<p>多核场景下的可见性问题</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test1</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">add10K</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> idx = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(idx++ &lt; <span class="number">10000</span>) &#123;</span><br><span class="line">      count += <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Test1 test = <span class="keyword">new</span> Test1();</span><br><span class="line">    <span class="comment">// 创建两个线程，执行add()操作</span></span><br><span class="line">    Thread th1 = <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">      test.add10K();</span><br><span class="line">    &#125;);</span><br><span class="line">    Thread th2 = <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">      test.add10K();</span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="comment">// 启动两个线程</span></span><br><span class="line">    th1.start();</span><br><span class="line">    th2.start();</span><br><span class="line">    <span class="comment">// 等待两个线程执行结束</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">		th1.join();</span><br><span class="line">	&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">		<span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">		e.printStackTrace();</span><br><span class="line">	&#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">		th2.join();</span><br><span class="line">	&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">		<span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">		e.printStackTrace();</span><br><span class="line">	&#125;</span><br><span class="line">    System.out.println(<span class="string">"count:"</span>+ count);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其运行结果并非我们的直觉2000，而是一个10000到20000之间的随机数。</p>
<p>假设线程 A 和线程 B 同时开始执行，那么第一次都会将 count=0 读到各自的 CPU 缓存里，执行完 count+=1 之后，各自 CPU 缓存里的值都是 1，同时写入内存后，我们会发现内存中是 1，而不是我们期望的 2。之后由于各自的 CPU 缓存里都有了 count 的值，两个线程都是基于 CPU 缓存里的 count 值来计算，所以导致最终 count 的值都是小于 20000 的。这就是缓存的可见性问题。</p>
<p>循环 10000 次 count+=1 操作如果改为循环 1 亿次，你会发现效果更明显，最终 count 的值接近 1 亿，而不是 2 亿。如果循环 10000 次，count 的值接近 20000，原因是两个线程不是同时启动的，有一个时差。</p>
<h3 id="线程切换带来的原子性问题"><a href="#线程切换带来的原子性问题" class="headerlink" title="线程切换带来的原子性问题"></a>线程切换带来的原子性问题</h3><blockquote>
<p>I/O太慢，所以早期系统发明了多进程</p>
</blockquote>
<p>操作系统允许某个进程执行一小段时间，例如 50 毫秒，过了 50 毫秒操作系统就会重新选择一个进程来执行（我们称为“任务切换”），这个 50 毫秒称为“时间片”。【可参考<a href="https://rocksnake.github.io/2020/03/05/CPU-Sched/" target="_blank" rel="noopener">CPU Sched</a>】</p>
<p><img src="https://static001.geekbang.org/resource/image/25/fb/254b129b145d80e9bb74123d6e620efb.png" alt="img"></p>
<center>线程切换</center>

<p>在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标记为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，唤醒后的进程就有机会重新获得 CPU 的使用权了。</p>
<p>这里的进程在等待 IO 时之所以会释放 CPU 使用权，是为了让 CPU 在这段等待时间里可以做别的事情，这样一来 CPU 的使用率就上来了；此外，如果这时有另外一个进程也读文件，读文件的操作就会排队，磁盘驱动在完成一个进程的读操作后，发现有排队的任务，就会立即启动下一个读操作，这样 IO 的使用率也上来了。</p>
<blockquote>
<p>支持多进程分时复用在操作系统发展史上有里程碑意义</p>
<ul>
<li><p>早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。</p>
</li>
<li><p>现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。</p>
</li>
</ul>
</blockquote>
<p>Java并发程序都是基于多线程的，自然会涉及到任务切换，任务切换的时机大多数都是在时间片结束的时候，高级语言里一条语句往往需要多条CPU指令完成。</p>
<p>E.g.：count += 1</p>
<ul>
<li>指令 1：首先，需要把变量 count 从内存加载到 CPU 的寄存器；</li>
<li>指令 2：之后，在寄存器中执行 +1 操作；</li>
<li>指令 3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存）。</li>
</ul>
<p>操作系统做任务切换，可以在任何一条CPU指令执行完，对于上述三条指令，我们假设count = 0，如果线程A在指令1执行完后做线程切换，线程A和线程B按照图示执行，那么就会两个线程都执行了count+=1的操作，得到1而不是2。</p>
<p><img src="https://static001.geekbang.org/resource/image/33/63/33777c468872cb9a99b3cdc1ff597063.png" alt="img"></p>
<center>非原子操作的执行路径</center>

<p>我们潜意识里面觉得 count+=1 这个操作是一个不可分割的整体，就像一个原子一样，线程的切换可以发生在 count+=1 之前，也可以发生在 count+=1 之后，但就是不会发生在中间。</p>
<p>我们把一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性。<strong>CPU 能保证的原子操作是 CPU 指令级别的，而不是高级语言的操作符，这是违背我们直觉的地方。</strong>因此，很多时候我们需要在高级语言层面保证操作的原子性。</p>
<h3 id="编译优化带来的有序性问题"><a href="#编译优化带来的有序性问题" class="headerlink" title="编译优化带来的有序性问题"></a>编译优化带来的有序性问题</h3><blockquote>
<p>有序性指的是程序按照代码的先后顺序执行</p>
</blockquote>
<p>编译器为了优化性能，有时候会改变程序中语句的先后顺序，例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”。</p>
<p><font color=red>Java领域里有一个经典的案例就是利用双重检查创建单例对象</font></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *在获取实例getInstance()方法中</span></span><br><span class="line"><span class="comment"> *首先判断instance是否为空</span></span><br><span class="line"><span class="comment"> *如果为空锁定Singleton.class并再次检查instance是否为空</span></span><br><span class="line"><span class="comment"> *如果还为空则创建Singleton的一个实例</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">  <span class="keyword">static</span> Singleton instance;</span><br><span class="line">  <span class="function"><span class="keyword">static</span> Singleton <span class="title">getInstance</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">synchronized</span>(Singleton<span class="class">.<span class="keyword">class</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="keyword">null</span>)</span><br><span class="line">          instance = <span class="keyword">new</span> Singleton();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> instance;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>假设有两个线程 A、B 同时调用 getInstance() 方法，他们会同时发现 instance == null ，于是同时对 Singleton.class 加锁，此时 JVM 保证只有一个线程能够加锁成功（假设是线程 A），另外一个线程则会处于等待状态（假设是线程 B）；</p>
<p>线程 A 会创建一个 Singleton 实例，之后释放锁，锁释放后，线程 B 被唤醒，线程 B 再次尝试加锁，此时是可以加锁成功的，加锁成功后，线程 B 检查 instance == null 时会发现，已经创建过 Singleton 实例了，所以线程 B 不会再创建一个 Singleton 实例。</p>
<p>我们的直觉是这样，但实际上，getInstance() 方法并不完美，问题就出现在new操作上，</p>
<p>我们认为的new操作应该是：① 分配一块内存M；② 在内存上初始化Singleton对象；③ 然后M的地址赋值给instance变量</p>
<p>但是实际上优化后的执行路径是：</p>
<ol>
<li>分配一块内存 M；</li>
<li>将 M 的地址赋值给 instance 变量；</li>
<li>最后在内存 M 上初始化 Singleton 对象。</li>
</ol>
<p>所以就会出现：我们假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 instance != null ，所以直接返回 instance，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。</p>
<p><img src="https://static001.geekbang.org/resource/image/64/d8/64c955c65010aae3902ec918412827d8.png" alt="img"></p>
<center>双重检查创建单例的异常执行路径</center>

<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>Q：32位机器上对long型变量进行加减操作存在并发隐患。</p>
<p>A：long类型64位，在32位的机器上，对long类型的数据操作通常需要多条指令组合出来，无法保证原子性。</p>
<p>原因就是文章里的bug源头之二：线程切换带来的原子性问题。<br>非volatile类型的long和double型变量是8字节64位的，32位机器读或写这个变量时得把人家咔嚓分成两个32位操作，可能一个线程读了某个值的高32位，低32位已经被另一个线程改了。所以官方推荐最好把long\double 变量声明为volatile或是同步加锁synchronize以避免并发问题。<a href="https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.7" target="_blank" rel="noopener">Reference</a></p>
<h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><h3 id="解决可见性和有序性问题"><a href="#解决可见性和有序性问题" class="headerlink" title="解决可见性和有序性问题"></a>解决可见性和有序性问题</h3><blockquote>
<p>Java 内存模型</p>
<p>Java 内存模型规范了 JVM 如何提供按需禁用缓存和编译优化的方法。具体来说，这些方法包括 volatile、synchronized 和 final 三个关键字，以及六项 Happens-Before 规则。</p>
</blockquote>
<p>我们都知道导致可见性的原因是缓存，导致有序性的原因是编译优化，那解决可见性、有序性最直接的办法就是禁用缓存和编译优化。</p>
<p><font color=red>但是这样虽然问题解决了，但是程序的性能达不到最佳</font></p>
<p><strong>合理的方案</strong>应该是<strong>按需禁用缓存以及编译优化</strong>，所谓“按需禁用”其实就是指按照程序员的要求来禁用。所以，为了解决可见性和有序性问题，只需要提供给程序员按需禁用缓存和编译优化的方法即可。</p>
<h4 id="volatile"><a href="#volatile" class="headerlink" title="volatile"></a>volatile</h4><blockquote>
<p>禁用CPU缓存</p>
</blockquote>
<p>E.g.：声明一个 volatile 变量 volatile int x = 0，其代表的含义：对这个变量的读写，不能使用 CPU 缓存，必须从内存中读取或者写入。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*假设线程 A 执行 writer() 方法，按照 volatile 语义，会把变量 “v=true” 写入内存；</span></span><br><span class="line"><span class="comment">*假设线程 B 执行 reader() 方法，同样按照 volatile 语义，线程 B 会从内存中读取变量 v，</span></span><br><span class="line"><span class="comment">*如果线程 B 看到 “v == true” 时，那么线程 B 看到的变量 x 是多少呢？</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//取决于Java版本，在低于 1.5 版本上运行，x 可能是 42，也有可能是 0；如果在 1.5 以上的版本上运行，x 就是等于 42。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VolatileExample</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> x = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">volatile</span> <span class="keyword">boolean</span> v = <span class="keyword">false</span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    x = <span class="number">42</span>;</span><br><span class="line">    v = <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reader</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (v == <span class="keyword">true</span>) &#123;</span><br><span class="line">      <span class="comment">// 这里x会是多少呢？</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在Java1.5时对volatile语义进行了增强，通过Happens-Before 规则的方法。</p>
<h4 id="Happens-Before-规则"><a href="#Happens-Before-规则" class="headerlink" title="Happens-Before 规则"></a>Happens-Before 规则</h4><blockquote>
<p>前面一个操作的结果对后续操作是可见的。</p>
<p>约束了编译器的优化行为，虽然允许编译器优化，但是要求优化后一定遵守规则</p>
</blockquote>
<h5 id="程序的顺序性规划"><a href="#程序的顺序性规划" class="headerlink" title="程序的顺序性规划"></a>程序的顺序性规划</h5><blockquote>
<p>在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作。</p>
</blockquote>
<p>E.g.：代码 “x = 42;” Happens-Before 于代码 “v = true;”。</p>
<p>符合单线程思维：程序前面对某个变量的修改一定是对后续操作可见的</p>
<h5 id="volatile-变量规则"><a href="#volatile-变量规则" class="headerlink" title="volatile 变量规则"></a>volatile 变量规则</h5><blockquote>
<p>对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作。</p>
</blockquote>
<p>对一个 volatile 变量的写操作相对于后续对这个 volatile 变量的读操作可见，这怎么看都是禁用缓存的意思啊，貌似和 1.5 版本以前的语义没有变化啊？如果单看这个规则，的确是这样，但是如果我们关联一下规则 3，就有点不一样的感觉了。</p>
<h5 id="传递性"><a href="#传递性" class="headerlink" title="传递性"></a>传递性</h5><blockquote>
<p>如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C。</p>
</blockquote>
<p><img src="https://static001.geekbang.org/resource/image/b1/e1/b1fa541e98c74bc2a033d9ac5ae7fbe1.png" alt="img"></p>
<center>示例代码中的传递性规则</center>

<ol>
<li>“x=42” Happens-Before 写变量 “v=true” ，这是规则 1 的内容；</li>
<li>写变量“v=true” Happens-Before 读变量 “v=true”，这是规则 2 的内容 。</li>
</ol>
<p>再根据这个传递性规则，我们得到结果：“x=42” Happens-Before 读变量“v=true”。这意味着什么呢？</p>
<p>如果线程 B 读到了“v=true”，那么线程 A 设置的“x=42”对线程 B 是可见的。也就是说，线程 B 能看到 “x == 42” ，有没有一种恍然大悟的感觉？这就是 1.5 版本对 volatile 语义的增强，这个增强意义重大，1.5 版本的并发工具包（java.util.concurrent）就是靠 volatile 语义来搞定可见性的，</p>
<h5 id="管程中锁的规则"><a href="#管程中锁的规则" class="headerlink" title="管程中锁的规则"></a>管程中锁的规则</h5><blockquote>
<p>对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。</p>
<p>我们前面提到过：管程是一种通用的同步原语，在 Java 中指的就是 synchronized，synchronized 是 Java 里对管程的实现。</p>
</blockquote>
<p>管程中的锁在 Java 里是隐式实现的，例如下面的代码，在进入同步块之前，会自动加锁，而在代码块执行完会自动释放锁，加锁以及释放锁都是编译器帮我们实现的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123; <span class="comment">//此处自动加锁</span></span><br><span class="line">  <span class="comment">// x是共享变量,初始值=10</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>.x &lt; <span class="number">12</span>) &#123;</span><br><span class="line">    <span class="keyword">this</span>.x = <span class="number">12</span>; </span><br><span class="line">  &#125;  </span><br><span class="line">&#125; <span class="comment">//此处自动解锁</span></span><br></pre></td></tr></table></figure>

<p>假设 x 的初始值是 10，线程 A 执行完代码块后 x 的值会变成 12（执行完自动释放锁），线程 B 进入代码块时，能够看到线程 A 对 x 的写操作，也就是线程 B 能够看到 x==12。</p>
<h5 id="线程start-规则"><a href="#线程start-规则" class="headerlink" title="线程start()规则"></a>线程start()规则</h5><blockquote>
<p>关于线程启动的，主线程 A 启动子线程 B 后，子线程 B 能够看到主线程在启动子线程 B 前的操作。</p>
</blockquote>
<p>如果线程 A 调用线程 B 的 start() 方法（即在线程 A 中启动线程 B），那么该 start() 操作 Happens-Before 于线程 B 中的任意操作。具体可参考下面示例代码。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Thread B = <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">  <span class="comment">// 主线程调用B.start()之前</span></span><br><span class="line">  <span class="comment">// 所有对共享变量的修改，此处皆可见</span></span><br><span class="line">  <span class="comment">// 此例中，var==77</span></span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 此处对共享变量var修改</span></span><br><span class="line"><span class="keyword">var</span> = <span class="number">77</span>;</span><br><span class="line"><span class="comment">// 主线程启动子线程</span></span><br><span class="line">B.start();</span><br></pre></td></tr></table></figure>

<h5 id="线程-join-规则"><a href="#线程-join-规则" class="headerlink" title="线程 join() 规则"></a>线程 join() 规则</h5><blockquote>
<p>关于线程等待的，主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B 的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。</p>
<p>看到：指的是对共享变量的操作</p>
</blockquote>
<p>如果在线程 A 中，调用线程 B 的 join() 并成功返回，那么线程 B 中的任意操作 Happens-Before 于该 join() 操作的返回。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Thread B = <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">  <span class="comment">// 此处对共享变量var修改</span></span><br><span class="line">  <span class="keyword">var</span> = <span class="number">66</span>;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 例如此处对共享变量修改，</span></span><br><span class="line"><span class="comment">// 则这个修改结果对线程B可见</span></span><br><span class="line"><span class="comment">// 主线程启动子线程</span></span><br><span class="line">B.start();</span><br><span class="line">B.join()</span><br><span class="line"><span class="comment">// 子线程所有对共享变量的修改</span></span><br><span class="line"><span class="comment">// 在主线程调用B.join()之后皆可见</span></span><br><span class="line"><span class="comment">// 此例中，var==66</span></span><br></pre></td></tr></table></figure>

<h4 id="final"><a href="#final" class="headerlink" title="final"></a>final</h4><blockquote>
<p>我们提到volatile为禁用缓存以及编译优化，final可以告诉编译器优化得更好一些。</p>
</blockquote>
<p>final修饰变量时，初衷是告诉编译器，这个变量生而不变，可劲优化。</p>
<p>问题类似于上一期提到的利用双重检查方法创建单例，构造函数的错误重排导致线程可能看到 final 变量的值会变化。详细的案例可以参考<a href="http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#finalWrong" target="_blank" rel="noopener">Reference</a></p>
<p>在 1.5 以后 Java 内存模型对 final 类型变量的重排进行了约束。现在只要我们提供正确构造函数没有“逸出”，就不会出问题了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*在构造函数里面将 this 赋值给了全局变量 global.obj，这就是“逸出”，</span></span><br><span class="line"><span class="comment">*线程通过 global.obj 读取 x 是有可能读到 0 的。因此我们一定要避免“逸出”。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// 以下代码来源于【参考1】</span></span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> x;</span><br><span class="line"><span class="comment">// 错误的构造函数</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FinalFieldExample</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  x = <span class="number">3</span>;</span><br><span class="line">  y = <span class="number">4</span>;</span><br><span class="line">  <span class="comment">// 此处就是讲this逸出，</span></span><br><span class="line">  global.obj = <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Reference：</p>
<ol>
<li><p><a href="http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html" target="_blank" rel="noopener">JSR 133 (Java Memory Model) FAQ</a></p>
</li>
<li><p><a href="http://ifeve.com/jmm-faq/" target="_blank" rel="noopener">Java 内存模型 FAQ</a></p>
</li>
<li><p><a href="extension://idghocbbahafpfhjnfhpbfbmpegphmmp/assets/pdf/web/viewer.html?file=https%3A%2F%2Fwww.cs.umd.edu%2F~pugh%2Fjava%2FmemoryModel%2Fjsr133.pdf">JSR-133: JavaTM Memory Model and Thread Specification</a></p>
</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20200821133643927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>Q: 有一个共享变量 abc，在一个线程里设置了 abc 的值 abc=3，有哪些办法可以让其他线程能够看到abc==3？</p>
<p>A: </p>
<ol>
<li>声明共享变量abc，并使用volatile关键字修饰abc</li>
<li>声明共享变量abc，在synchronized关键字对abc的赋值代码块加锁，由于Happen-before管程锁的规则，可以使得后续的线程可以看到abc的值。</li>
<li>A线程启动后，使用A.JOIN()方法来完成运行，后续线程再启动，则一定可以看到abc==3</li>
</ol>
<h3 id="解决原子性问题"><a href="#解决原子性问题" class="headerlink" title="解决原子性问题"></a>解决原子性问题</h3><blockquote>
<p>一个或者多个操作在CPU执行的过程中不被中断的特性，称为原子性</p>
</blockquote>
<p>原子性问题的源头是线程切换，而操作系统做线程切换是依CPU中断的，所以禁止CPU发生中断就能够禁止线程切换。</p>
<p>这样的方案在单核CPU上可行，但是对于多核CPU，例如前面的问题，long型变量时64位，在32位CPU上执行写操作会被拆分成两次操作(写高32位和写低32位)</p>
<p><img src="https://static001.geekbang.org/resource/image/38/28/381b657801c48b3399f19d946bad9e28.png" alt="img"></p>
<p>在单核 CPU 场景下，同一时刻只有一个线程执行，禁止 CPU 中断，意味着操作系统不会重新调度线程，也就是禁止了线程切换，获得 CPU 使用权的线程就可以不间断地执行，所以两次写操作一定是：要么都被执行，要么都没有被执行，具有原子性。</p>
<p>但是在多核场景下，同一时刻，有可能有两个线程同时在执行，一个线程执行在 CPU-1 上，一个线程执行在 CPU-2 上，此时禁止 CPU 中断，只能保证 CPU 上的线程连续执行，并不能保证同一时刻只有一个线程执行，如果这两个线程同时写 long 型变量高 32 位的话，那就有可能出现我们开头提及的诡异 Bug 了。</p>
<p>“同一时刻只有一个线程执行”这个条件非常重要，我们称之为互斥。如果我们能够保证对共享变量的修改是互斥的，那么，无论是单核 CPU 还是多核 CPU，就都能保证原子性了。</p>
<h4 id="简易锁模型"><a href="#简易锁模型" class="headerlink" title="简易锁模型"></a>简易锁模型</h4><blockquote>
<p>谈到互斥，首要的解决方案肯定是锁</p>
</blockquote>
<p><img src="https://static001.geekbang.org/resource/image/3d/a2/3df991e7de14a788b220468836cd48a2.png" alt="img"></p>
<center>简易锁模型</center>

<p>我们把一段需要互斥执行的代码称为临界区，线程在进入临界区之前，首先尝试加锁lock()，如果成功，则进入临界区，此时我们称这个线程持有锁，否则就等待，直到持有锁的线程解锁；持有锁的线程执行完临界区的代码后，执行解锁unlock()。</p>
<h4 id="改进锁模型"><a href="#改进锁模型" class="headerlink" title="改进锁模型"></a>改进锁模型</h4><p><img src="https://static001.geekbang.org/resource/image/28/2f/287008c8137a43fa032e68a0c23c172f.png" alt="img"></p>
<ol>
<li>把临界区要保护的资源标注出来，如图中临界区里增加了一个元素：受保护的资源 R；</li>
<li>保护资源 R 就得为它创建一把锁 LR；</li>
<li>针对这把锁 LR，我们还需在进出临界区时添上加锁操作和解锁操作。</li>
</ol>
<p>在锁 LR 和受保护资源之间，我特地用一条线做了关联，这个关联关系非常重要。很多并发 Bug 的出现都是因为把它忽略了，然后就出现了类似锁自家门来保护他家资产的事情，这样的 Bug 非常不好诊断，因为潜意识里我们认为已经正确加锁了。</p>
<h4 id="Java语言提供的锁技术：synchronized"><a href="#Java语言提供的锁技术：synchronized" class="headerlink" title="Java语言提供的锁技术：synchronized"></a>Java语言提供的锁技术：synchronized</h4><blockquote>
<p>可以用来修饰方法、也可以修饰代码块</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">X</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 修饰非静态方法</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 临界区</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 修饰静态方法</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">bar</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 临界区</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 修饰代码块</span></span><br><span class="line">  Object obj = <span class="keyword">new</span> Object()；</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">baz</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(obj) &#123;</span><br><span class="line">      <span class="comment">// 临界区</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可能从这些用法中我们没有发现lock()和unlock()，这两个操作是被Java自动默认加上的，在synchronized修饰的方法或代码块前后自动加上lock()和unlock() ，好处是一定是成对出现的，忘记unlock()比较致命，容易导致线程一直等待。</p>
<p>那么synchronized里的加锁 lock() 和解锁 unlock() 锁定的对象在哪里呢？</p>
<p>上面的代码我们看到只有修饰代码块的时候，锁定了一个 obj 对象，那修饰方法的时候锁定的是什么呢？这个也是 Java 的一条隐式规则：</p>
<ul>
<li>当修饰静态方法的时候，锁定的是当前类的 Class 对象，在上面的例子中就是 Class X；</li>
<li>当修饰非静态方法的时候，锁定的是当前实例对象 this。</li>
</ul>
<p>对于上面的例子，synchronized修饰静态方法相当于：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">X</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 修饰静态方法</span></span><br><span class="line">  <span class="keyword">synchronized</span>(X<span class="class">.<span class="keyword">class</span>) <span class="title">static</span> <span class="title">void</span> <span class="title">bar</span>() </span>&#123;</span><br><span class="line">    <span class="comment">// 临界区</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>修饰非静态方法，相当于：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">X</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 修饰非静态方法</span></span><br><span class="line">  <span class="keyword">synchronized</span>(<span class="keyword">this</span>) <span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 临界区</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="用synchronized解决count-1的问题"><a href="#用synchronized解决count-1的问题" class="headerlink" title="用synchronized解决count+=1的问题"></a>用synchronized解决count+=1的问题</h5><blockquote>
<p>SafeCalc这个类有两个方法：</p>
<ul>
<li>get方法，获得value</li>
<li>addOne方法，给value加1，并且用synchronized修饰</li>
</ul>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SafeCalc</span> </span>&#123;</span><br><span class="line">  <span class="keyword">long</span> value = <span class="number">0L</span>;</span><br><span class="line">  <span class="function"><span class="keyword">long</span> <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">addOne</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    value += <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>addOne方法首先被肯定的是被修饰后，无论是单核还是多核，只有一个线程能够执行addOne方法，所以一定能保证原子性操作，那是否有可见性问题呢？</p>
<p>我们记得管程中锁的规则：对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。</p>
<p>管程，就是我们这里提到的synchronized，我们知道synchronized修饰的临界区是互斥的，也就是说同一时刻只有一个线程执行临界区的代码；而所谓“对一个锁解锁Happens-Before后续对这个锁的加锁“，指的是前一个线程的解锁操作对后一个线程的加锁操作可见，综合 Happens-Before 的传递性原则，我们就能得出前一个线程在临界区修改的共享变量（该操作在解锁之前），对后续进入临界区（该操作在加锁之后）的线程是可见的。</p>
<p>按照这个规则，如果多个线程同时执行 addOne() 方法，可见性是可以保证的，也就说如果有 1000 个线程执行 addOne() 方法，最终结果一定是 value 的值增加了 1000。看到这个结果，我们长出一口气，问题终于解决了。</p>
<p>但也许，你一不小心就忽视了 get() 方法。执行 addOne() 方法后，value 的值对 get() 方法是可见的吗？这个可见性是没法保证的。管程中锁的规则，是只保证后续对这个锁的加锁的可见性，而 get() 方法并没有加锁操作，所以可见性没法保证。那如何解决呢？很简单，就是 get() 方法也 synchronized 一下，完整的代码如下所示。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SafeCalc</span> </span>&#123;</span><br><span class="line">  <span class="keyword">long</span> value = <span class="number">0L</span>;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">long</span> <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">addOne</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    value += <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的代码转换为我们提到的锁模型，就是下面图示这个样子。get() 方法和 addOne() 方法都需要访问 value 这个受保护的资源，这个资源用 this 这把锁来保护。线程要进入临界区 get() 和 addOne()，必须先获得 this 这把锁，这样 get() 和 addOne() 也是互斥的。</p>
<p><img src="https://static001.geekbang.org/resource/image/26/f6/26a84ffe2b4a6ae67c8093d29473e1f6.png" alt="img"></p>
<center>保护临界区get和addOne的示意图</center>

<p>这个模型更像现实世界里面球赛门票的管理，一个座位只允许一个人使用，这个座位就是“受保护资源”，球场的入口就是 Java 类里的方法，而门票就是用来保护资源的“锁”，Java 里的检票工作是由 synchronized 解决的。</p>
<h4 id="锁和受保护资源的关系"><a href="#锁和受保护资源的关系" class="headerlink" title="锁和受保护资源的关系"></a>锁和受保护资源的关系</h4><blockquote>
<p>受保护资源和锁之间的关联关系是 N:1 的关系</p>
</blockquote>
<p>拿前面球赛门票的管理来类比，就是一个座位，我们只能用一张票来保护，如果多发了重复的票，那就要打架了。现实世界里，我们可以用多把锁来保护同一个资源，但在并发领域是不行的，并发领域的锁和现实世界的锁不是完全匹配的。不过倒是可以用同一把锁来保护多个资源，这个对应到现实世界就是我们所谓的“包场”了。</p>
<p>接着上面的那个例子稍作改动，把 value 改成静态变量，把 addOne() 方法改成静态方法，此时 get() 方法和 addOne() 方法是否存在并发问题呢？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SafeCalc</span> </span>&#123;</span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">long</span> value = <span class="number">0L</span>;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">long</span> <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">addOne</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    value += <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果你仔细观察，就会发现改动后的代码是用两个锁保护一个资源。这个受保护的资源就是静态变量 value，两个锁分别是 this 和 SafeCalc.class。我们可以用下面这幅图来形象描述这个关系。由于临界区 get() 和 addOne() 是用两个锁保护的，因此这两个临界区没有互斥关系，临界区 addOne() 对 value 的修改对临界区 get() 也没有可见性保证，这就导致并发问题了。</p>
<p>锁保护了资源，同时锁和资源是有对应关系的，所以从资源出发，也可以区分两把锁是不是同一个。 参见下一节课的笔记，可以看出Java里没有单独的锁类型，锁就是用一个对象来表示的，锁和被保护资源之间的对应关系是靠代码逻辑实现的。</p>
<p><img src="https://static001.geekbang.org/resource/image/60/be/60551e006fca96f581f3dc25424226be.png" alt="img"></p>
<center>两把锁保护一个资源的示意图</center>

<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><blockquote>
<p>并发问题首要想到的解决办法还是加锁，因为加锁能够保证执行临界区代码的互斥性。</p>
</blockquote>
<p><strong>真正用好互斥锁：</strong> 临界区的代码是操作受保护资源的路径，类似于球场的入口，入口一定要检票，也就是要加锁，但不是随便一把锁都能有效。所以必须深入分析锁定的对象和受保护资源的关系，综合考虑受保护资源的访问路径，多方面考量才能用好互斥锁。</p>
<p>synchronized 是 Java 在语言层面提供的互斥原语，其实 Java 里面还有很多其他类型的锁，但作为互斥锁，原理都是相通的：锁，一定有一个要锁定的对象，至于这个锁定的对象要保护的资源以及在哪里加锁 / 解锁，就属于设计层面的事情了。</p>
<h4 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SafeCalc</span> </span>&#123;</span><br><span class="line">  <span class="keyword">long</span> value = <span class="number">0L</span>;</span><br><span class="line">  <span class="function"><span class="keyword">long</span> <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">new</span> Object()) &#123;</span><br><span class="line">      <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">addOne</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">new</span> Object()) &#123;</span><br><span class="line">      value += <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>synchronized这个使用方式正确吗？有哪些问题呢？能解决可见性和原子性问题吗？</p>
<ul>
<li>加锁的本质就是在锁对象的对象头中写入当前线程id，但是new object每次在内存中都是新对象，所以加锁无效</li>
<li>经过JVM逃逸分析的优化后，这段代码会被直接优化掉，所以在运行时是无锁的</li>
<li>sync锁的对象monitor指针指向一个ObjectMonitor对象，所有线程加入他的entrylist里面，去cas抢锁，更改state加1拿锁，执行完代码，释放锁state减1，和aqs机制差不多，只是所有线程不阻塞，cas抢锁，没有队列，属于非公平锁。wait的时候，线程进waitset休眠，等待notify唤醒</li>
<li>多把锁保护同一个资源，就像一个厕所坑位，有N多门可以进去，没有丝毫保护效果，管理员一看，还不如把门都撤了，弄成开放式(编译器代码优化)😂。</li>
<li>两把不同的锁，不能保护临界资源。而且这种new出来只在一个地方使用的对象，其它线程不能对它解锁，这个锁会被编译器优化掉。和没有syncronized代码块效果是相同的</li>
<li>不能，因为new了，所以不是同一把锁。老师您好，我对那 synchronized的理解是这样，它并不能改变CPU时间片切换的特点，只是当其他线程要访问这个资源时，发现锁还未释放，所以只能在外面等待，不知道理解是否正确</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">A</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> Integer b = <span class="number">1</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       System.out.println(<span class="string">"A is begin!"</span>);</span><br><span class="line">       <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"a"</span>);</span><br><span class="line">           <span class="comment">// System.out.println(b);</span></span><br><span class="line">           <span class="keyword">if</span> (b.equals(<span class="number">2</span>))</span><br><span class="line">              <span class="keyword">break</span>;</span><br><span class="line">       &#125;</span><br><span class="line"> </span><br><span class="line">       System.out.println(<span class="string">"A is finish!"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">       A a = <span class="keyword">new</span> A();</span><br><span class="line">       <span class="comment">//线程A</span></span><br><span class="line">       <span class="keyword">new</span> Thread(a).start();</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">       &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">       &#125;</span><br><span class="line">       a.b = <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">我们知道这个程序会出现可见性问题。</span><br><span class="line">但是在<span class="keyword">while</span>内加上System.out.println(b)后 当主线程修改b的值后 线程A居然能够取得最新值 可见性问题得到解决</span><br><span class="line">System.out.println(b)的实现如下</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">println</span><span class="params">(String x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">            print(x);</span><br><span class="line">            newLine();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">Doug Lea大神的Concurrent Programming in Java一书中有这样一个片段来描述<span class="keyword">synchronized</span>这个关键字：</span><br><span class="line"> </span><br><span class="line">这里英文就不放出来了 字数超过两千……</span><br><span class="line">这篇文章也有提及https:<span class="comment">//www.jianshu.com/p/3c06ffbf0d52</span></span><br><span class="line"> </span><br><span class="line">简单翻译一下：从本质上来说，当线程释放一个锁时会强制性的将工作内存中之前所有的写操作都刷新到主内存中去，而获取一个锁则会强制性的加载可访问到的值到线程工作内存中来。虽然锁操作只对同步方法和同步代码块这一块起到作用，但是影响的却是线程执行操作所使用的所有字段。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">也就是说当调用System.out.println(<span class="string">"a"</span>)时当前线程的缓存会被重新刷新过，所以才能够读到这个值最新值</span><br><span class="line"> ---------------------------------------------------------</span><br><span class="line">然后问题来了</span><br><span class="line">问题<span class="number">1</span>:</span><br><span class="line">首先上面的说法不知道是不是真的是这样。</span><br><span class="line">然后我在下面加了System.out.println(b) 结果打印出来的是旧值，但是下面的b.equals(<span class="number">2</span>)却能通过 这里没弄明白 我觉得应该是编译器进行了优化?因为现在大三能力不够，还没学会看<span class="class"><span class="keyword">class</span>文件 没法验证</span></span><br><span class="line"><span class="class"> </span></span><br><span class="line">问题2:</span><br><span class="line">网上找了一些文章</span><br><span class="line">有些人的说法是：打印是IO操作，而IO操作会引起线程的切换，线程切换会导致线程原本的缓存失效，从而也会读取到修改后的值。</span><br><span class="line"> </span><br><span class="line">我尝试着将打印换成File file = <span class="keyword">new</span> File(<span class="string">"D://1.txt"</span>);这句代码，程序也能够正常的结束。当然，在这里也可以尝试将将打印替换成<span class="keyword">synchronized</span>(A<span class="class">.<span class="keyword">class</span>)</span>&#123; &#125;这句空同步代码块，发现程序也能够正常结束。</span><br><span class="line"> </span><br><span class="line">这里有个问题就是 线程切换时会把之前操作的相关数据保存到内存里，切换回来后会把内存里的数据重新加载到寄存器里吗，这样说的话 就算切换也是获取不到修改后的值的,不知道是什么做到能够读到这个修改后的值的？</span><br><span class="line"> </span><br><span class="line">问题<span class="number">3</span>:</span><br><span class="line">是不是</span><br><span class="line">线程执行过程中，操作系统会随机性的把缓存刷到内存</span><br><span class="line">线程结束后一定会把缓存里的数据刷到内存</span><br><span class="line"></span><br><span class="line"> ---------------------------------------------------------</span><br><span class="line">在评论里好多大神 能学到好多东西😄😄</span><br><span class="line">作者回复: <span class="number">1</span>. println的代码里锁的<span class="keyword">this</span>指的是你的控制台，这个锁跟你的代码没关系，而且println里也没有写操作，所以println不会导致强刷缓存。</span><br><span class="line"></span><br><span class="line">我觉得是因为println产生了IO，IO相对CPU来说，太慢，所以这个期间大概率的会把缓存的值写入内存。也有可能这个线程被调度到了其他的CPU上，压根没有缓存，所以只能从内存取数。你调用sleep，效果应该也差不多。</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>. 线程切换显然不足以保证可见性，保证的可见性只能靠hb规则。</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>. 线程结束后，不一定会强刷缓存。否则Join的规则就没必要了</span><br><span class="line"></span><br><span class="line">并发问题本来就是小概率的事件，尤其有了IO操作之后，概率就更低了。</span><br></pre></td></tr></table></figure>

<h3 id="如何用一把锁保护多个资源？"><a href="#如何用一把锁保护多个资源？" class="headerlink" title="如何用一把锁保护多个资源？"></a>如何用一把锁保护多个资源？</h3><blockquote>
<p>前面我们提到，可以用一把锁来保护多个资源，但是不能用多把锁来保护一个资源。</p>
</blockquote>
<p>当我们要保护多个资源时，首先要区分这些资源是否存在关联关系。</p>
<h4 id="保护没有关联关系的多个资源"><a href="#保护没有关联关系的多个资源" class="headerlink" title="保护没有关联关系的多个资源"></a>保护没有关联关系的多个资源</h4><blockquote>
<p>在现实世界里，球场的座位和电影院的座位就是没有关联关系的，这种场景非常容易解决，那就是球赛有球赛的门票，电影院有电影院的门票，各自管理各自的。</p>
</blockquote>
<p>账户类 Account 有两个成员变量，分别是账户余额 balance 和账户密码 password。取款 withdraw() 和查看余额 getBalance() 操作会访问账户余额 balance，我们创建一个 final 对象 balLock 作为锁（类比球赛门票）；而更改密码 updatePassword() 和查看密码 getPassword() 操作会修改账户密码 password，我们创建一个 final 对象 pwLock 作为锁（类比电影票）。不同的资源用不同的锁保护，各自管各自的，很简单。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 锁：保护账户余额</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Object balLock</span><br><span class="line">    = <span class="keyword">new</span> Object();</span><br><span class="line">  <span class="comment">// 账户余额  </span></span><br><span class="line">  <span class="keyword">private</span> Integer balance;</span><br><span class="line">  <span class="comment">// 锁：保护账户密码</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Object pwLock</span><br><span class="line">    = <span class="keyword">new</span> Object();</span><br><span class="line">  <span class="comment">// 账户密码</span></span><br><span class="line">  <span class="keyword">private</span> String password;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 取款</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">withdraw</span><span class="params">(Integer amt)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(balLock) &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt)&#123;</span><br><span class="line">        <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="comment">// 查看余额</span></span><br><span class="line">  <span class="function">Integer <span class="title">getBalance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(balLock) &#123;</span><br><span class="line">      <span class="keyword">return</span> balance;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 更改密码</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">updatePassword</span><span class="params">(String pw)</span></span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(pwLock) &#123;</span><br><span class="line">      <span class="keyword">this</span>.password = pw;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="comment">// 查看密码</span></span><br><span class="line">  <span class="function">String <span class="title">getPassword</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(pwLock) &#123;</span><br><span class="line">      <span class="keyword">return</span> password;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当然，我们也可以用一把互斥锁来保护多个资源，例如我们可以用 this 这一把锁来管理账户类里所有的资源：账户余额和用户密码。具体实现很简单，示例程序中所有的方法都增加同步关键字 synchronized 就可以了。</p>
<p>但是用一把锁有个问题，就是性能太差，会导致取款、查看余额、修改密码、查看密码这四个操作都是串行的。而我们用两把锁，取款和修改密码是可以并行的。<strong>用不同的锁对受保护资源进行精细化管理</strong>，能够提升性能。这种锁还有个名字，叫<strong>细粒度锁</strong>。</p>
<h4 id="保护有关联关系的多个资源"><a href="#保护有关联关系的多个资源" class="headerlink" title="保护有关联关系的多个资源"></a>保护有关联关系的多个资源</h4><p>如果多个资源是有关联关系的，那这个问题就有点复杂了。例如银行业务里面的转账操作，账户 A 减少 100 元，账户 B 增加 100 元。这两个账户就是有关联关系的。那对于像转账这种有关联关系的操作，我们应该怎么去解决呢？</p>
<p>先把这个问题代码化。我们声明了个账户类：Account，该类有一个成员变量余额：balance，还有一个用于转账的方法：transfer()，然后怎么保证转账操作 transfer() 没有并发问题呢？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">      <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">      target.balance += amt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>直觉告诉我，用户synchronized关键字修饰以下transfer()方法就可以，于是：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">transfer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">      <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">      target.balance += amt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在这段代码中，临界区内有两个资源，分别是转出账户的余额 this.balance 和转入账户的余额 target.balance，并且用的是一把锁 this，符合我们前面提到的，多个资源可以用一把锁来保护。</p>
<p><font color=red>但是，真的是这样吗？</font></p>
<p>看似正确，问题就出在this这把锁上，this这把锁可以保护自己的余额this.balance，却保护不了别人的余额target.balance，不能用自家的锁来保护被人家的资产。</p>
<p><img src="https://static001.geekbang.org/resource/image/1b/d8/1ba92a09d1a55a6a1636318f30c155d8.png" alt="img"></p>
<center>用锁this保护this.balance和target.balance</center>

<p><strong>假设有 A、B、C 三个账户，余额都是 200 元，我们用两个线程分别执行两个转账操作：账户 A 转给账户 B 100 元，账户 B 转给账户 C 100 元，最后我们期望的结果应该是账户 A 的余额是 100 元，账户 B 的余额是 200 元， 账户 C 的余额是 300 元。</strong></p>
<p>假设线程 1 执行账户 A 转账户 B 的操作，线程 2 执行账户 B 转账户 C 的操作。这两个线程分别在两颗 CPU 上同时执行，那它们是互斥的吗？我们期望是，但实际上并不是。因为线程 1 锁定的是账户 A 的实例（A.this），而线程 2 锁定的是账户 B 的实例（B.this），所以这两个线程可以同时进入临界区 transfer()。同时进入临界区的结果是什么呢？线程 1 和线程 2 都会读到账户 B 的余额为 200，导致最终账户 B 的余额可能是 300（线程 1 后于线程 2 写 B.balance，线程 2 写的 B.balance 值被线程 1 覆盖），可能是 100（线程 1 先于线程 2 写 B.balance，线程 1 写的 B.balance 值被线程 2 覆盖），就是不可能是 200。</p>
<p><img src="https://static001.geekbang.org/resource/image/a4/27/a46b4a1e73671d6e6f1bdb26f6c87627.png" alt="img"></p>
<center>并发转账</center>

<h4 id="使用锁的正确姿势"><a href="#使用锁的正确姿势" class="headerlink" title="使用锁的正确姿势"></a>使用锁的正确姿势</h4><blockquote>
<p>前面我们提到用同一把锁来保护多个资源，就是包场</p>
<p>只要我们的锁能覆盖所有受保护的资源就可以。</p>
</blockquote>
<p>在上面的例子中，this 是对象级别的锁，所以 A 对象和 B 对象都有自己的锁，如何让 A 对象和 B 对象共享一把锁呢？</p>
<p>首先想到的方法是可以让所有对象都持有一个唯一性的对象，这个对象在创建 Account 时传入。我们把 Account 默认构造函数变为 private，同时增加一个带 Object lock 参数的构造函数，创建 Account 对象时，传入相同的 lock，这样所有的 Account 对象都会共享这个 lock 了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Object lock；</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="title">Account</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="comment">// 创建Account时传入同一个lock对象</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Account</span><span class="params">(Object lock)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.lock = lock;</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 此处检查所有对象共享的锁</span></span><br><span class="line">    <span class="keyword">synchronized</span>(lock) &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">        <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">        target.balance += amt;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个办法确实能解决问题，但是有点小瑕疵，要求在创建Account对象的时候必须传入同一个对象，如果创建 Account 对象时，传入的 lock 不是同一个对象，那可就惨了，会出现锁自家门来保护他家资产的荒唐事。在真实的项目场景中，创建 Account 对象的代码很可能分散在多个工程中，传入共享的 lock 真的很难。</p>
<p>所以，上面的方案缺乏实践的可行性，我们需要更好的方案。还真有，就是用 Account.class 作为共享的锁。Account.class 是所有 Account 对象共享的，而且这个对象是 Java 虚拟机在加载 Account 类的时候创建的，所以我们不用担心它的唯一性。使用 Account.class 作为共享的锁，我们就无需在创建 Account 对象时传入了，代码更简单。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(Account<span class="class">.<span class="keyword">class</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">        <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">        target.balance += amt;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://static001.geekbang.org/resource/image/52/7c/527cd65f747abac3f23390663748da7c.png" alt="img"></p>
<center>使用共享的锁 Account.class 来保护不同对象的临界区</center>

<h4 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>如何保护多个资源关键是要分析多个资源之间的关系。如果资源之间没有关系，很好处理，每个资源一把锁就可以了。如果资源之间有关联关系，就要选择一个粒度更大的锁，这个锁应该能够覆盖所有相关的资源。除此之外，还要梳理出有哪些访问路径，所有的访问路径都要设置合适的锁，这个过程可以类比一下门票管理。</p>
<p>关联关系如果用更具体、更专业的语言来描述的话，其实是一种“原子性”特征，在前面的文章中，我们提到的原子性，主要是面向 CPU 指令的，转账操作的原子性则是属于是面向高级语言的，不过它们本质上是一样的。</p>
<p>“原子性”的本质是什么？其实不是不可分割，不可分割只是外在表现，其本质是多个资源间有一致性的要求，操作的中间状态对外不可见。例如，在 32 位的机器上写 long 型变量有中间状态（只写了 64 位中的 32 位），在银行转账的操作中也有中间状态（账户 A 减少了 100，账户 B 还没来得及发生变化）。所以解决原子性问题，是要保证中间状态对外不可见。</p>
<h4 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h4><p><strong>Q：</strong> 我们用了两把不同的锁来分别保护账户余额、账户密码，创建锁的时候，我们用的是：private final Object xxxLock = new Object();，如果账户余额用 this.balance 作为互斥锁，账户密码用 this.password 作为互斥锁，你觉得是否可以呢？</p>
<p><strong>A： ** **不能用可变对象做锁</strong>，用this.balance 和this.password 都不行。在同一个账户多线程访问时候，A线程取款进行this.balance-=amt;时候此时this.balance对应的值已经发生变换，线程B再次取款时拿到的balance对应的值并不是A线程中的，也就是说不能把可变的对象当成一把锁。this.password 虽然说是String修饰但也会改变，所以也不行。</p>
<p>不能用balance和password做为锁对象。这两个对象balance是Integer，password是String都是不可变变对象，一但对他们进行赋值就会变成新的对象，加的锁就失效了</p>
<h3 id="一不小心就死锁了，怎么办？"><a href="#一不小心就死锁了，怎么办？" class="headerlink" title="一不小心就死锁了，怎么办？"></a>一不小心就死锁了，怎么办？</h3><p>前面我们用Account.class作为互斥锁，来解决银行业务里面的转账问题，虽然这个方案不存在并发问题，但是所有账户的转账操作都是串行的。</p>
<p>例如账户A转账户B、账户C转账户D这两个转账操作现实世界里是可以并行的。但是在这个方案里却被串行化了，这样的话，性能太差。</p>
<p>试想互联网支付盛行的当下，8 亿网民每人每天一笔交易，每天就是 8 亿笔交易；每笔交易都对应着一次转账操作，8 亿笔交易就是 8 亿次转账操作，也就是说平均到每秒就是近 1 万次转账操作，若所有的转账操作都串行，性能完全不能接受。</p>
<p>那我们就尝试把性能提升一下。</p>
<h4 id="向现实世界要答案"><a href="#向现实世界要答案" class="headerlink" title="向现实世界要答案"></a>向现实世界要答案</h4><p>现实世界里，账户转账操作是支持并发的，而且绝对是真正的并行，银行所有的窗口都可以做转账操作。只要我们能仿照现实世界做转账操作，串行的问题就解决了。</p>
<p>想象一个真实场景，古代没有信息化，账户的存在形式真的就是一个账本，而且每个账户都有一个账本，这些账本都统一放在文件架上，银行柜员在给我们做转账时，要去文件架上把转出账本和转入账本都拿到手，然后做转账。这个柜员在拿账本的时候可能遇到以下三种情况：</p>
<ol>
<li>文件架上恰好有转出账本和转入账本，那就同时拿走；</li>
<li>如果文件架上只有转出账本和转入账本之一，那这个柜员就先把文件架上有的账本拿到手，同时等着其他柜员把另外一个账本送回来；</li>
<li>转出账本和转入账本都没有，那这个柜员就等着两个账本都被送回来。</li>
</ol>
<p>这个过程用两把锁就实现了，转出账本一把，转入账本另一把，在transfer()方法内部，首先尝试所的那个转出账户this（先把转出账本拿到手），然后尝试锁定转入账户 target（再把转入账本拿到手），只有当两者都成功时，才执行转账操作。</p>
<p><img src="https://static001.geekbang.org/resource/image/cb/55/cb18e672732ab76fc61d60bdf66bf855.png" alt="img"></p>
<center>两个转账操作并行</center>

<p>经过这样的优化，账户A转账户B和账户C转账户D这两个转账操作就可以并行了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 锁定转出账户</span></span><br><span class="line">    <span class="keyword">synchronized</span>(<span class="keyword">this</span>) &#123;              </span><br><span class="line">      <span class="comment">// 锁定转入账户</span></span><br><span class="line">      <span class="keyword">synchronized</span>(target) &#123;           </span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">          <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">          target.balance += amt;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="没有免费的午餐"><a href="#没有免费的午餐" class="headerlink" title="没有免费的午餐"></a>没有免费的午餐</h4><p>上述实现看似很完美，相对于用Account.class作为互斥锁，锁定的范围太大，而我们锁定两个账户范围就小多了，这样的锁，我们称为细粒度锁。</p>
<blockquote>
<p>使用细粒度锁可以提高并行度，是性能优化的一个重要手段。</p>
</blockquote>
<p><font color=red>但是,使用细粒度锁是有代价的，这个代价就是可能会导致死锁。</font></p>
<p>首先我们来看一个特殊场景，如果有客户找柜员张三做个转账业务：账户 A 转账户 B 100 元，此时另一个客户找柜员李四也做个转账业务：账户 B 转账户 A 100 元，于是张三和李四同时都去文件架上拿账本，这时候有可能凑巧张三拿到了账本 A，李四拿到了账本 B。张三拿到账本 A 后就等着账本 B（账本 B 已经被李四拿走），而李四拿到账本 B 后就等着账本 A（账本 A 已经被张三拿走），他们要等多久呢？他们会永远等待下去…因为张三不会把账本 A 送回去，李四也不会把账本 B 送回去。</p>
<p><img src="https://static001.geekbang.org/resource/image/f2/88/f293dc0d92b7c8255bd0bc790fc2a088.png" alt="img"></p>
<center>转账业务中的“死等”</center>

<p>现实世界里的死等，就是编程领域的死锁了。死锁的一个比较专业的定义是：一组互相竞争资源的线程因互相等待，导致“永久”阻塞的现象。</p>
<p>上面转账的代码是怎么发生死锁的呢？我们假设线程 T1 执行账户 A 转账户 B 的操作，账户 A.transfer(账户 B)；同时线程 T2 执行账户 B 转账户 A 的操作，账户 B.transfer(账户 A)。当 T1 和 T2 同时执行完①处的代码时，T1 获得了账户 A 的锁（对于 T1，this 是账户 A），而 T2 获得了账户 B 的锁（对于 T2，this 是账户 B）。之后 T1 和 T2 在执行②处的代码时，T1 试图获取账户 B 的锁时，发现账户 B 已经被锁定（被 T2 锁定），所以 T1 开始等待；T2 则试图获取账户 A 的锁时，发现账户 A 已经被锁定（被 T1 锁定），所以 T2 也开始等待。于是 T1 和 T2 会无期限地等待下去，也就是我们所说的死锁了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 锁定转出账户</span></span><br><span class="line">    <span class="keyword">synchronized</span>(<span class="keyword">this</span>)&#123;     ①</span><br><span class="line">      <span class="comment">// 锁定转入账户</span></span><br><span class="line">      <span class="keyword">synchronized</span>(target)&#123; ②</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">          <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">          target.balance += amt;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>关于这种现象，我们还可以借助资源分配图来可视化锁的占用情况（资源分配图是个有向图，它可以描述资源和线程的状态）。</p>
<p>其中，资源用方形节点表示，线程用圆形节点表示；资源中的点指向线程的边表示线程已经获得该资源，线程指向资源的边则表示线程请求资源，但尚未得到。转账发生死锁时的资源分配图就如下图所示，一个“各据山头死等”的尴尬局面。</p>
<p><img src="https://static001.geekbang.org/resource/image/82/1c/829d69c7d32c3ad1b89d89fc56017d1c.png" alt="img"></p>
<center>转账发生死锁时的资源分配问题</center>

<p>发现了问题，那么</p>
<h4 id="如何预防死锁呢"><a href="#如何预防死锁呢" class="headerlink" title="如何预防死锁呢"></a>如何预防死锁呢</h4><p>并发程序一旦死锁，一般没有特别好的方法，很多时候我们只能重启应用。因此，解决死锁问题最好的办法还是规避死锁。</p>
<p>那如何避免死锁呢？要避免死锁就需要分析死锁发生的条件，有个叫 Coffman 的牛人早就总结过了，只有以下这四个条件都发生时才会出现死锁：</p>
<ol>
<li>互斥，共享资源 X 和 Y 只能被一个线程占用；</li>
<li>占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X；</li>
<li>不可抢占，其他线程不能强行抢占线程 T1 占有的资源；</li>
<li>循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待。</li>
</ol>
<p>反过来分析，也就是说只要我们破坏其中一个，就可以成功避免死锁的发生。</p>
<p>其中，互斥这个条件我们没有办法破坏，因为我们用锁为的就是互斥。不过其他三个条件都是有办法破坏掉的，到底如何做呢？</p>
<ol>
<li>对于“占用且等待”这个条件，我们可以一次性申请所有的资源，这样就不存在等待了。</li>
<li>对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。</li>
<li>对于“循环等待”这个条件，可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，这样线性化后自然就不存在循环了。</li>
</ol>
<h5 id="破坏占用且等待条件"><a href="#破坏占用且等待条件" class="headerlink" title="破坏占用且等待条件"></a>破坏占用且等待条件</h5><p>从理论上讲，要破坏这个条件，可以一次性申请所有资源。在现实世界里，就拿前面我们提到的转账操作来讲，它需要的资源有两个，一个是转出账户，另一个是转入账户，当这两个账户同时被申请时，我们该怎么解决这个问题呢？</p>
<p>可以增加一个账本管理员，然后只允许账本管理员从文件架上拿账本，也就是说柜员不能直接在文件架上拿账本，必须通过账本管理员才能拿到想要的账本。例如，张三同时申请账本 A 和 B，账本管理员如果发现文件架上只有账本 A，这个时候账本管理员是不会把账本 A 拿下来给张三的，只有账本 A 和 B 都在的时候才会给张三。这样就保证了“一次性申请所有资源”。</p>
<p><img src="https://static001.geekbang.org/resource/image/27/db/273af8c2ee60bd659f18673d2af005db.png" alt="img"></p>
<center>通过账本管理员拿账本</center>

<p>对应到编程领域，“同时申请”这个操作是一个临界区，我们也需要一个角色（Java 里面的类）来管理这个临界区，我们就把这个角色定为 Allocator。它有两个重要功能，分别是：同时申请资源 apply() 和同时释放资源 free()。账户 Account 类里面持有一个 Allocator 的单例（必须是单例，只能由一个人来分配资源）。当账户 Account 在执行转账操作的时候，首先向 Allocator 同时申请转出账户和转入账户这两个资源，成功后再锁定这两个资源；当转账操作执行完，释放锁之后，我们需通知 Allocator 同时释放转出账户和转入账户这两个资源。具体的代码实现如下。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Allocator</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> List&lt;Object&gt; als =</span><br><span class="line">    <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="comment">// 一次性申请所有资源</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">apply</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    Object from, Object to)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(als.contains(from) ||</span><br><span class="line">         als.contains(to))&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      als.add(from);</span><br><span class="line">      als.add(to);  </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 归还资源</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">free</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    Object from, Object to)</span></span>&#123;</span><br><span class="line">    als.remove(from);</span><br><span class="line">    als.remove(to);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="comment">// actr应该为单例</span></span><br><span class="line">  <span class="keyword">private</span> Allocator actr;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 一次性申请转出账户和转入账户，直到成功</span></span><br><span class="line">    <span class="keyword">while</span>(!actr.apply(<span class="keyword">this</span>, target))</span><br><span class="line">      ；</span><br><span class="line">    <span class="keyword">try</span>&#123;</span><br><span class="line">      <span class="comment">// 锁定转出账户</span></span><br><span class="line">      <span class="keyword">synchronized</span>(<span class="keyword">this</span>)&#123;              </span><br><span class="line">        <span class="comment">// 锁定转入账户</span></span><br><span class="line">        <span class="keyword">synchronized</span>(target)&#123;           </span><br><span class="line">          <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt)&#123;</span><br><span class="line">            <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">            target.balance += amt;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      actr.free(<span class="keyword">this</span>, target)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="破坏不可抢占条件"><a href="#破坏不可抢占条件" class="headerlink" title="破坏不可抢占条件"></a>破坏不可抢占条件</h5><p>破坏不可抢占条件看上去很简单，核心是要能够主动释放它占有的资源，这一点 synchronized 是做不到的。原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。</p>
<p>你可能会质疑，“Java 作为排行榜第一的语言，这都解决不了？”你的怀疑很有道理，Java 在语言层次确实没有解决这个问题，不过在 SDK 层面还是解决了的，java.util.concurrent 这个包下面提供的 Lock 是可以轻松解决这个问题的。关于这个话题，咱们后面会详细讲。</p>
<h5 id="破坏循环等待条件"><a href="#破坏循环等待条件" class="headerlink" title="破坏循环等待条件"></a>破坏循环等待条件</h5><p>破坏这个条件，需要对资源进行排序，然后按序申请资源。这个实现非常简单，我们假设每个账户都有不同的属性 id，这个 id 可以作为排序字段，申请的时候，我们可以按照从小到大的顺序来申请。比如下面代码中，①~⑥处的代码对转出账户（this）和转入账户（target）排序，然后按照序号从小到大的顺序锁定账户。这样就不存在“循环”等待了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> id;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    Account left = <span class="keyword">this</span>        ①</span><br><span class="line">    Account right = target;    ②</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.id &gt; target.id) &#123; ③</span><br><span class="line">      left = target;           ④</span><br><span class="line">      right = <span class="keyword">this</span>;            ⑤</span><br><span class="line">    &#125;                          ⑥</span><br><span class="line">    <span class="comment">// 锁定序号小的账户</span></span><br><span class="line">    <span class="keyword">synchronized</span>(left)&#123;</span><br><span class="line">      <span class="comment">// 锁定序号大的账户</span></span><br><span class="line">      <span class="keyword">synchronized</span>(right)&#123; </span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt)&#123;</span><br><span class="line">          <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">          target.balance += amt;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Conclusion-2"><a href="#Conclusion-2" class="headerlink" title="Conclusion"></a>Conclusion</h4><blockquote>
<p>利用现实世界的模型来构思解决方案</p>
</blockquote>
<p>在利用现实模型建模的时候，我们还要仔细对比现实世界和编程世界里的各角色之间的差异。</p>
<p>用细粒度锁来锁定多个资源时，要注意死锁的问题，识别出风险很重要。</p>
<p>预防死锁主要是破坏三个条件中的一个，有了这个思路后，实现就简单了。但仍需注意的是，有时候预防死锁成本也是很高的。例如上面转账那个例子，我们破坏占用且等待条件的成本就比破坏循环等待条件的成本高，破坏占用且等待条件，我们也是锁了所有的账户，而且还是用了死循环 while(!actr.apply(this, target));方法，不过好在 apply() 这个方法基本不耗时。 在转账这个例子中，破坏循环等待条件就是成本最低的一个方案。</p>
<p>所以我们在选择具体方案的时候，还需要评估一下操作成本，从中选择一个成本最低的方案。</p>
<h4 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h4><p>破坏占用且等待条件，我们也是锁了所有的账户，而且还是用了死循环 while(!actr.apply(this, target));这个方法，那它比synchronized(Account.class) 有没有性能优势呢？</p>
<ul>
<li>synchronized(Account.class) 锁了Account类相关的所有操作。相当于文中说的包场了，只要与Account有关联，通通需要等待当前线程操作完成。while死循环的方式只锁定了当前操作的两个相关的对象。两种影响到的范围不同。</li>
<li>最简单的方案: 遇到死锁，我就是用资源id的从小到大的顺序去申请锁解决的</li>
<li>用top命令查看Java线程的cpu利用率，用jstack来dump线程。开发环境可以用 java visualvm查看线程执行情况</li>
<li>while 循环就是一个自旋锁机制吧，自旋锁的话要关注它的循环时间，不能一直循环下去，不然会浪费 cpu 资源。自旋锁在JVM里是一种特殊的锁机制，自诩不会阻塞线程的。咱们这个其实还是会阻塞线程的。不过原理都一样，你这样理解也没问题。</li>
<li>最常见的就是B转A的同时，A转账给B，那么先锁B再锁A，但是，另一个线程是先锁A再锁B，然而，如果两个线程同时执行，那么就是出现死锁的情况，线程T1锁了A请求锁B，此时线程T2锁了B请求锁A，都在等着对方释放锁，然而自己都不会释放锁，故死锁。<br>最简单的办法，就是无论哪个线程执行的时候，都按照顺序加锁，即按照A和B的id大小来加锁，这样，无论哪个线程执行的时候，都会先加锁A，再加锁B，A被加锁，则等待释放。这样就不会被死锁了。</li>
</ul>
<h3 id="用“等待-通知”机制优化循环等待"><a href="#用“等待-通知”机制优化循环等待" class="headerlink" title="用“等待-通知”机制优化循环等待"></a>用“等待-通知”机制优化循环等待</h3>]]></content>
      <categories>
        <category>Feedback</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>The theory of concurrent programming</title>
    <url>/2020/08/20/The%20theory%20of%20concurrent%20programming/</url>
    <content><![CDATA[<h1 id="The-theory-of-concurrent-programming"><a href="#The-theory-of-concurrent-programming" class="headerlink" title="The theory of concurrent programming"></a>The theory of concurrent programming</h1><h2 id="来由"><a href="#来由" class="headerlink" title="来由"></a>来由</h2><blockquote>
<p>借助数据库和类似Tomcat中间件而不用写并发程序的时代已经过去了。</p>
</blockquote>
<p>举个例子，Java里的synchronized、wait()/notify() 相关的知识很琐碎，看懂难，会用更难。</p>
<p>但实际上 synchronized、wait()、notify() 不过是操作系统领域里管程模型的一种实现而已，Java SDK 并发包里的条件变量 Condition 也是管程里的概念，如果站在管程这个理论模型的高度而不是单独理解，用起来就得心应手了。</p>
<hr>
<p><strong>管程</strong></p>
<blockquote>
<p>一种解决并发问题的模型，与信号量在逻辑上是等价的，但管程更易用</p>
</blockquote>
<p>很多语言都支持管程。</p>
<hr>
<p>Java SDK并发包乃是并发大师 Doug Lea 出品，其章法在哪里呢？</p>
<p><strong>并发编程其实就是三个核心问题</strong>【且Java SDK并发包大部分内容都是按照这三个维度组织的】：</p>
<ol>
<li>分工<ul>
<li>如何高效地拆解任务并分配给线程</li>
<li>E.g.：Fork/Join框架</li>
</ul>
</li>
<li>同步<ul>
<li>线程之间如何协作</li>
<li>E.g.：CountDownLatch</li>
</ul>
</li>
<li>互斥<ul>
<li>保证同一时刻只允许一个线程访问共享资源</li>
<li>E.g.：可重入锁</li>
</ul>
</li>
</ol>
<blockquote>
<p>Java SDK仅仅是针对并发问题开发出来的工具而已</p>
</blockquote>
<h2 id="并发编程BUG的源头"><a href="#并发编程BUG的源头" class="headerlink" title="并发编程BUG的源头"></a>并发编程BUG的源头</h2><blockquote>
<p>CPU、内存、I/O设备三者中间存在一个核心矛盾就是速度差异</p>
<p>CPU 一天对内存一年；内存一天对I/O设备十年</p>
</blockquote>
<p>为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系结构、操作系统、编译程序都做出了贡献：</p>
<ul>
<li>CPU 增加了缓存，以均衡与内存的速度差异；</li>
<li>操作系统增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异；</li>
<li>编译程序优化指令执行次序，使得缓存能够得到更加合理地利用。</li>
</ul>
<blockquote>
<p>正是这些造就了并发编程的BUG神出鬼没</p>
</blockquote>
<h3 id="缓存导致的可见性问题"><a href="#缓存导致的可见性问题" class="headerlink" title="缓存导致的可见性问题"></a>缓存导致的可见性问题</h3><blockquote>
<p>一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为可见性</p>
</blockquote>
<p>单核时代，所有的线程都是在一颗 CPU 上执行，CPU 缓存与内存的数据一致性容易解决。因为所有线程都是操作同一个 CPU 的缓存，一个线程对缓存的写，对另外一个线程来说一定是可见的。</p>
<p><img src="https://static001.geekbang.org/resource/image/a0/da/a07e8182819e2b260ce85b2167d446da.png" alt="img"></p>
<center>CPU缓存与内存的关系</center>

<p>多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存。</p>
<p><img src="https://static001.geekbang.org/resource/image/e2/ea/e2aa76928b2bc135e08e7590ca36e0ea.png" alt="img"></p>
<center>多核CPU的缓存与内存关系</center>

<h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><blockquote>
<p>多核场景下的可见性问题</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test1</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">add10K</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> idx = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(idx++ &lt; <span class="number">10000</span>) &#123;</span><br><span class="line">      count += <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Test1 test = <span class="keyword">new</span> Test1();</span><br><span class="line">    <span class="comment">// 创建两个线程，执行add()操作</span></span><br><span class="line">    Thread th1 = <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">      test.add10K();</span><br><span class="line">    &#125;);</span><br><span class="line">    Thread th2 = <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">      test.add10K();</span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="comment">// 启动两个线程</span></span><br><span class="line">    th1.start();</span><br><span class="line">    th2.start();</span><br><span class="line">    <span class="comment">// 等待两个线程执行结束</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">		th1.join();</span><br><span class="line">	&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">		<span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">		e.printStackTrace();</span><br><span class="line">	&#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">		th2.join();</span><br><span class="line">	&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">		<span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">		e.printStackTrace();</span><br><span class="line">	&#125;</span><br><span class="line">    System.out.println(<span class="string">"count:"</span>+ count);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其运行结果并非我们的直觉2000，而是一个10000到20000之间的随机数。</p>
<p>假设线程 A 和线程 B 同时开始执行，那么第一次都会将 count=0 读到各自的 CPU 缓存里，执行完 count+=1 之后，各自 CPU 缓存里的值都是 1，同时写入内存后，我们会发现内存中是 1，而不是我们期望的 2。之后由于各自的 CPU 缓存里都有了 count 的值，两个线程都是基于 CPU 缓存里的 count 值来计算，所以导致最终 count 的值都是小于 20000 的。这就是缓存的可见性问题。</p>
<p>循环 10000 次 count+=1 操作如果改为循环 1 亿次，你会发现效果更明显，最终 count 的值接近 1 亿，而不是 2 亿。如果循环 10000 次，count 的值接近 20000，原因是两个线程不是同时启动的，有一个时差。</p>
<h3 id="线程切换带来的原子性问题"><a href="#线程切换带来的原子性问题" class="headerlink" title="线程切换带来的原子性问题"></a>线程切换带来的原子性问题</h3><blockquote>
<p>I/O太慢，所以早期系统发明了多进程</p>
</blockquote>
<p>操作系统允许某个进程执行一小段时间，例如 50 毫秒，过了 50 毫秒操作系统就会重新选择一个进程来执行（我们称为“任务切换”），这个 50 毫秒称为“时间片”。【可参考<a href="https://rocksnake.github.io/2020/03/05/CPU-Sched/" target="_blank" rel="noopener">CPU Sched</a>】</p>
<p><img src="https://static001.geekbang.org/resource/image/25/fb/254b129b145d80e9bb74123d6e620efb.png" alt="img"></p>
<center>线程切换</center>

<p>在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标记为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，唤醒后的进程就有机会重新获得 CPU 的使用权了。</p>
<p>这里的进程在等待 IO 时之所以会释放 CPU 使用权，是为了让 CPU 在这段等待时间里可以做别的事情，这样一来 CPU 的使用率就上来了；此外，如果这时有另外一个进程也读文件，读文件的操作就会排队，磁盘驱动在完成一个进程的读操作后，发现有排队的任务，就会立即启动下一个读操作，这样 IO 的使用率也上来了。</p>
<blockquote>
<p>支持多进程分时复用在操作系统发展史上有里程碑意义</p>
<ul>
<li><p>早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。</p>
</li>
<li><p>现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。</p>
</li>
</ul>
</blockquote>
<p>Java并发程序都是基于多线程的，自然会涉及到任务切换，任务切换的时机大多数都是在时间片结束的时候，高级语言里一条语句往往需要多条CPU指令完成。</p>
<p>E.g.：count += 1</p>
<ul>
<li>指令 1：首先，需要把变量 count 从内存加载到 CPU 的寄存器；</li>
<li>指令 2：之后，在寄存器中执行 +1 操作；</li>
<li>指令 3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存）。</li>
</ul>
<p>操作系统做任务切换，可以在任何一条CPU指令执行完，对于上述三条指令，我们假设count = 0，如果线程A在指令1执行完后做线程切换，线程A和线程B按照图示执行，那么就会两个线程都执行了count+=1的操作，得到1而不是2。</p>
<p><img src="https://static001.geekbang.org/resource/image/33/63/33777c468872cb9a99b3cdc1ff597063.png" alt="img"></p>
<center>非原子操作的执行路径</center>

<p>我们潜意识里面觉得 count+=1 这个操作是一个不可分割的整体，就像一个原子一样，线程的切换可以发生在 count+=1 之前，也可以发生在 count+=1 之后，但就是不会发生在中间。</p>
<p>我们把一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性。<strong>CPU 能保证的原子操作是 CPU 指令级别的，而不是高级语言的操作符，这是违背我们直觉的地方。</strong>因此，很多时候我们需要在高级语言层面保证操作的原子性。</p>
<h3 id="编译优化带来的有序性问题"><a href="#编译优化带来的有序性问题" class="headerlink" title="编译优化带来的有序性问题"></a>编译优化带来的有序性问题</h3><blockquote>
<p>有序性指的是程序按照代码的先后顺序执行</p>
</blockquote>
<p>编译器为了优化性能，有时候会改变程序中语句的先后顺序，例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”。</p>
<p><font color=red>Java领域里有一个经典的案例就是利用双重检查创建单例对象</font></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *在获取实例getInstance()方法中</span></span><br><span class="line"><span class="comment"> *首先判断instance是否为空</span></span><br><span class="line"><span class="comment"> *如果为空锁定Singleton.class并再次检查instance是否为空</span></span><br><span class="line"><span class="comment"> *如果还为空则创建Singleton的一个实例</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">  <span class="keyword">static</span> Singleton instance;</span><br><span class="line">  <span class="function"><span class="keyword">static</span> Singleton <span class="title">getInstance</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">synchronized</span>(Singleton<span class="class">.<span class="keyword">class</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="keyword">null</span>)</span><br><span class="line">          instance = <span class="keyword">new</span> Singleton();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> instance;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>假设有两个线程 A、B 同时调用 getInstance() 方法，他们会同时发现 instance == null ，于是同时对 Singleton.class 加锁，此时 JVM 保证只有一个线程能够加锁成功（假设是线程 A），另外一个线程则会处于等待状态（假设是线程 B）；</p>
<p>线程 A 会创建一个 Singleton 实例，之后释放锁，锁释放后，线程 B 被唤醒，线程 B 再次尝试加锁，此时是可以加锁成功的，加锁成功后，线程 B 检查 instance == null 时会发现，已经创建过 Singleton 实例了，所以线程 B 不会再创建一个 Singleton 实例。</p>
<p>我们的直觉是这样，但实际上，getInstance() 方法并不完美，问题就出现在new操作上，</p>
<p>我们认为的new操作应该是：① 分配一块内存M；② 在内存上初始化Singleton对象；③ 然后M的地址赋值给instance变量</p>
<p>但是实际上优化后的执行路径是：</p>
<ol>
<li>分配一块内存 M；</li>
<li>将 M 的地址赋值给 instance 变量；</li>
<li>最后在内存 M 上初始化 Singleton 对象。</li>
</ol>
<p>所以就会出现：我们假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 instance != null ，所以直接返回 instance，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。</p>
<p><img src="https://static001.geekbang.org/resource/image/64/d8/64c955c65010aae3902ec918412827d8.png" alt="img"></p>
<center>双重检查创建单例的异常执行路径</center>

<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>Q：32位机器上对long型变量进行加减操作存在并发隐患。</p>
<p>A：long类型64位，在32位的机器上，对long类型的数据操作通常需要多条指令组合出来，无法保证原子性。</p>
<p>原因就是文章里的bug源头之二：线程切换带来的原子性问题。<br>非volatile类型的long和double型变量是8字节64位的，32位机器读或写这个变量时得把人家咔嚓分成两个32位操作，可能一个线程读了某个值的高32位，低32位已经被另一个线程改了。所以官方推荐最好把long\double 变量声明为volatile或是同步加锁synchronize以避免并发问题。<a href="https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.7" target="_blank" rel="noopener">Reference</a></p>
<h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><h3 id="解决可见性和有序性问题"><a href="#解决可见性和有序性问题" class="headerlink" title="解决可见性和有序性问题"></a>解决可见性和有序性问题</h3><blockquote>
<p>Java 内存模型</p>
<p>Java 内存模型规范了 JVM 如何提供按需禁用缓存和编译优化的方法。具体来说，这些方法包括 volatile、synchronized 和 final 三个关键字，以及六项 Happens-Before 规则。</p>
</blockquote>
<p>我们都知道导致可见性的原因是缓存，导致有序性的原因是编译优化，那解决可见性、有序性最直接的办法就是禁用缓存和编译优化。</p>
<p><font color=red>但是这样虽然问题解决了，但是程序的性能达不到最佳</font></p>
<p><strong>合理的方案</strong>应该是<strong>按需禁用缓存以及编译优化</strong>，所谓“按需禁用”其实就是指按照程序员的要求来禁用。所以，为了解决可见性和有序性问题，只需要提供给程序员按需禁用缓存和编译优化的方法即可。</p>
<h4 id="volatile"><a href="#volatile" class="headerlink" title="volatile"></a>volatile</h4><blockquote>
<p>禁用CPU缓存</p>
</blockquote>
<p>E.g.：声明一个 volatile 变量 volatile int x = 0，其代表的含义：对这个变量的读写，不能使用 CPU 缓存，必须从内存中读取或者写入。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*假设线程 A 执行 writer() 方法，按照 volatile 语义，会把变量 “v=true” 写入内存；</span></span><br><span class="line"><span class="comment">*假设线程 B 执行 reader() 方法，同样按照 volatile 语义，线程 B 会从内存中读取变量 v，</span></span><br><span class="line"><span class="comment">*如果线程 B 看到 “v == true” 时，那么线程 B 看到的变量 x 是多少呢？</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//取决于Java版本，在低于 1.5 版本上运行，x 可能是 42，也有可能是 0；如果在 1.5 以上的版本上运行，x 就是等于 42。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VolatileExample</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> x = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">volatile</span> <span class="keyword">boolean</span> v = <span class="keyword">false</span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    x = <span class="number">42</span>;</span><br><span class="line">    v = <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reader</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (v == <span class="keyword">true</span>) &#123;</span><br><span class="line">      <span class="comment">// 这里x会是多少呢？</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在Java1.5时对volatile语义进行了增强，通过Happens-Before 规则的方法。</p>
<h4 id="Happens-Before-规则"><a href="#Happens-Before-规则" class="headerlink" title="Happens-Before 规则"></a>Happens-Before 规则</h4><blockquote>
<p>前面一个操作的结果对后续操作是可见的。</p>
<p>约束了编译器的优化行为，虽然允许编译器优化，但是要求优化后一定遵守规则</p>
</blockquote>
<h5 id="程序的顺序性规划"><a href="#程序的顺序性规划" class="headerlink" title="程序的顺序性规划"></a>程序的顺序性规划</h5><blockquote>
<p>在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作。</p>
</blockquote>
<p>E.g.：代码 “x = 42;” Happens-Before 于代码 “v = true;”。</p>
<p>符合单线程思维：程序前面对某个变量的修改一定是对后续操作可见的</p>
<h5 id="volatile-变量规则"><a href="#volatile-变量规则" class="headerlink" title="volatile 变量规则"></a>volatile 变量规则</h5><blockquote>
<p>对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作。</p>
</blockquote>
<p>对一个 volatile 变量的写操作相对于后续对这个 volatile 变量的读操作可见，这怎么看都是禁用缓存的意思啊，貌似和 1.5 版本以前的语义没有变化啊？如果单看这个规则，的确是这样，但是如果我们关联一下规则 3，就有点不一样的感觉了。</p>
<h5 id="传递性"><a href="#传递性" class="headerlink" title="传递性"></a>传递性</h5><blockquote>
<p>如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C。</p>
</blockquote>
<p><img src="https://static001.geekbang.org/resource/image/b1/e1/b1fa541e98c74bc2a033d9ac5ae7fbe1.png" alt="img"></p>
<center>示例代码中的传递性规则</center>

<ol>
<li>“x=42” Happens-Before 写变量 “v=true” ，这是规则 1 的内容；</li>
<li>写变量“v=true” Happens-Before 读变量 “v=true”，这是规则 2 的内容 。</li>
</ol>
<p>再根据这个传递性规则，我们得到结果：“x=42” Happens-Before 读变量“v=true”。这意味着什么呢？</p>
<p>如果线程 B 读到了“v=true”，那么线程 A 设置的“x=42”对线程 B 是可见的。也就是说，线程 B 能看到 “x == 42” ，有没有一种恍然大悟的感觉？这就是 1.5 版本对 volatile 语义的增强，这个增强意义重大，1.5 版本的并发工具包（java.util.concurrent）就是靠 volatile 语义来搞定可见性的，</p>
<h5 id="管程中锁的规则"><a href="#管程中锁的规则" class="headerlink" title="管程中锁的规则"></a>管程中锁的规则</h5><blockquote>
<p>对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。</p>
<p>我们前面提到过：管程是一种通用的同步原语，在 Java 中指的就是 synchronized，synchronized 是 Java 里对管程的实现。</p>
</blockquote>
<p>管程中的锁在 Java 里是隐式实现的，例如下面的代码，在进入同步块之前，会自动加锁，而在代码块执行完会自动释放锁，加锁以及释放锁都是编译器帮我们实现的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123; <span class="comment">//此处自动加锁</span></span><br><span class="line">  <span class="comment">// x是共享变量,初始值=10</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>.x &lt; <span class="number">12</span>) &#123;</span><br><span class="line">    <span class="keyword">this</span>.x = <span class="number">12</span>; </span><br><span class="line">  &#125;  </span><br><span class="line">&#125; <span class="comment">//此处自动解锁</span></span><br></pre></td></tr></table></figure>

<p>假设 x 的初始值是 10，线程 A 执行完代码块后 x 的值会变成 12（执行完自动释放锁），线程 B 进入代码块时，能够看到线程 A 对 x 的写操作，也就是线程 B 能够看到 x==12。</p>
<h5 id="线程start-规则"><a href="#线程start-规则" class="headerlink" title="线程start()规则"></a>线程start()规则</h5><blockquote>
<p>关于线程启动的，主线程 A 启动子线程 B 后，子线程 B 能够看到主线程在启动子线程 B 前的操作。</p>
</blockquote>
<p>如果线程 A 调用线程 B 的 start() 方法（即在线程 A 中启动线程 B），那么该 start() 操作 Happens-Before 于线程 B 中的任意操作。具体可参考下面示例代码。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Thread B = <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">  <span class="comment">// 主线程调用B.start()之前</span></span><br><span class="line">  <span class="comment">// 所有对共享变量的修改，此处皆可见</span></span><br><span class="line">  <span class="comment">// 此例中，var==77</span></span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 此处对共享变量var修改</span></span><br><span class="line"><span class="keyword">var</span> = <span class="number">77</span>;</span><br><span class="line"><span class="comment">// 主线程启动子线程</span></span><br><span class="line">B.start();</span><br></pre></td></tr></table></figure>

<h5 id="线程-join-规则"><a href="#线程-join-规则" class="headerlink" title="线程 join() 规则"></a>线程 join() 规则</h5><blockquote>
<p>关于线程等待的，主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B 的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。</p>
<p>看到：指的是对共享变量的操作</p>
</blockquote>
<p>如果在线程 A 中，调用线程 B 的 join() 并成功返回，那么线程 B 中的任意操作 Happens-Before 于该 join() 操作的返回。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Thread B = <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">  <span class="comment">// 此处对共享变量var修改</span></span><br><span class="line">  <span class="keyword">var</span> = <span class="number">66</span>;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 例如此处对共享变量修改，</span></span><br><span class="line"><span class="comment">// 则这个修改结果对线程B可见</span></span><br><span class="line"><span class="comment">// 主线程启动子线程</span></span><br><span class="line">B.start();</span><br><span class="line">B.join()</span><br><span class="line"><span class="comment">// 子线程所有对共享变量的修改</span></span><br><span class="line"><span class="comment">// 在主线程调用B.join()之后皆可见</span></span><br><span class="line"><span class="comment">// 此例中，var==66</span></span><br></pre></td></tr></table></figure>

<h4 id="final"><a href="#final" class="headerlink" title="final"></a>final</h4><blockquote>
<p>我们提到volatile为禁用缓存以及编译优化，final可以告诉编译器优化得更好一些。</p>
</blockquote>
<p>final修饰变量时，初衷是告诉编译器，这个变量生而不变，可劲优化。</p>
<p>问题类似于上一期提到的利用双重检查方法创建单例，构造函数的错误重排导致线程可能看到 final 变量的值会变化。详细的案例可以参考<a href="http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#finalWrong" target="_blank" rel="noopener">Reference</a></p>
<p>在 1.5 以后 Java 内存模型对 final 类型变量的重排进行了约束。现在只要我们提供正确构造函数没有“逸出”，就不会出问题了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*在构造函数里面将 this 赋值给了全局变量 global.obj，这就是“逸出”，</span></span><br><span class="line"><span class="comment">*线程通过 global.obj 读取 x 是有可能读到 0 的。因此我们一定要避免“逸出”。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// 以下代码来源于【参考1】</span></span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> x;</span><br><span class="line"><span class="comment">// 错误的构造函数</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FinalFieldExample</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  x = <span class="number">3</span>;</span><br><span class="line">  y = <span class="number">4</span>;</span><br><span class="line">  <span class="comment">// 此处就是讲this逸出，</span></span><br><span class="line">  global.obj = <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Reference：</p>
<ol>
<li><p><a href="http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html" target="_blank" rel="noopener">JSR 133 (Java Memory Model) FAQ</a></p>
</li>
<li><p><a href="http://ifeve.com/jmm-faq/" target="_blank" rel="noopener">Java 内存模型 FAQ</a></p>
</li>
<li><p><a href="extension://idghocbbahafpfhjnfhpbfbmpegphmmp/assets/pdf/web/viewer.html?file=https%3A%2F%2Fwww.cs.umd.edu%2F~pugh%2Fjava%2FmemoryModel%2Fjsr133.pdf">JSR-133: JavaTM Memory Model and Thread Specification</a></p>
</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20200821133643927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>Q: 有一个共享变量 abc，在一个线程里设置了 abc 的值 abc=3，有哪些办法可以让其他线程能够看到abc==3？</p>
<p>A: </p>
<ol>
<li>声明共享变量abc，并使用volatile关键字修饰abc</li>
<li>声明共享变量abc，在synchronized关键字对abc的赋值代码块加锁，由于Happen-before管程锁的规则，可以使得后续的线程可以看到abc的值。</li>
<li>A线程启动后，使用A.JOIN()方法来完成运行，后续线程再启动，则一定可以看到abc==3</li>
</ol>
<h3 id="解决原子性问题"><a href="#解决原子性问题" class="headerlink" title="解决原子性问题"></a>解决原子性问题</h3><blockquote>
<p>一个或者多个操作在CPU执行的过程中不被中断的特性，称为原子性</p>
</blockquote>
<p>原子性问题的源头是线程切换，而操作系统做线程切换是依CPU中断的，所以禁止CPU发生中断就能够禁止线程切换。</p>
<p>这样的方案在单核CPU上可行，但是对于多核CPU，例如前面的问题，long型变量时64位，在32位CPU上执行写操作会被拆分成两次操作(写高32位和写低32位)</p>
<p><img src="https://static001.geekbang.org/resource/image/38/28/381b657801c48b3399f19d946bad9e28.png" alt="img"></p>
<p>在单核 CPU 场景下，同一时刻只有一个线程执行，禁止 CPU 中断，意味着操作系统不会重新调度线程，也就是禁止了线程切换，获得 CPU 使用权的线程就可以不间断地执行，所以两次写操作一定是：要么都被执行，要么都没有被执行，具有原子性。</p>
<p>但是在多核场景下，同一时刻，有可能有两个线程同时在执行，一个线程执行在 CPU-1 上，一个线程执行在 CPU-2 上，此时禁止 CPU 中断，只能保证 CPU 上的线程连续执行，并不能保证同一时刻只有一个线程执行，如果这两个线程同时写 long 型变量高 32 位的话，那就有可能出现我们开头提及的诡异 Bug 了。</p>
<p>“同一时刻只有一个线程执行”这个条件非常重要，我们称之为互斥。如果我们能够保证对共享变量的修改是互斥的，那么，无论是单核 CPU 还是多核 CPU，就都能保证原子性了。</p>
<h4 id="简易锁模型"><a href="#简易锁模型" class="headerlink" title="简易锁模型"></a>简易锁模型</h4><blockquote>
<p>谈到互斥，首要的解决方案肯定是锁</p>
</blockquote>
<p><img src="https://static001.geekbang.org/resource/image/3d/a2/3df991e7de14a788b220468836cd48a2.png" alt="img"></p>
<center>简易锁模型</center>

<p>我们把一段需要互斥执行的代码称为临界区，线程在进入临界区之前，首先尝试加锁lock()，如果成功，则进入临界区，此时我们称这个线程持有锁，否则就等待，直到持有锁的线程解锁；持有锁的线程执行完临界区的代码后，执行解锁unlock()。</p>
<h4 id="改进锁模型"><a href="#改进锁模型" class="headerlink" title="改进锁模型"></a>改进锁模型</h4><p><img src="https://static001.geekbang.org/resource/image/28/2f/287008c8137a43fa032e68a0c23c172f.png" alt="img"></p>
<ol>
<li>把临界区要保护的资源标注出来，如图中临界区里增加了一个元素：受保护的资源 R；</li>
<li>保护资源 R 就得为它创建一把锁 LR；</li>
<li>针对这把锁 LR，我们还需在进出临界区时添上加锁操作和解锁操作。</li>
</ol>
<p>在锁 LR 和受保护资源之间，我特地用一条线做了关联，这个关联关系非常重要。很多并发 Bug 的出现都是因为把它忽略了，然后就出现了类似锁自家门来保护他家资产的事情，这样的 Bug 非常不好诊断，因为潜意识里我们认为已经正确加锁了。</p>
<h4 id="Java语言提供的锁技术：synchronized"><a href="#Java语言提供的锁技术：synchronized" class="headerlink" title="Java语言提供的锁技术：synchronized"></a>Java语言提供的锁技术：synchronized</h4><blockquote>
<p>可以用来修饰方法、也可以修饰代码块</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">X</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 修饰非静态方法</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 临界区</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 修饰静态方法</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">bar</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 临界区</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 修饰代码块</span></span><br><span class="line">  Object obj = <span class="keyword">new</span> Object()；</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">baz</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(obj) &#123;</span><br><span class="line">      <span class="comment">// 临界区</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可能从这些用法中我们没有发现lock()和unlock()，这两个操作是被Java自动默认加上的，在synchronized修饰的方法或代码块前后自动加上lock()和unlock() ，好处是一定是成对出现的，忘记unlock()比较致命，容易导致线程一直等待。</p>
<p>那么synchronized里的加锁 lock() 和解锁 unlock() 锁定的对象在哪里呢？</p>
<p>上面的代码我们看到只有修饰代码块的时候，锁定了一个 obj 对象，那修饰方法的时候锁定的是什么呢？这个也是 Java 的一条隐式规则：</p>
<ul>
<li>当修饰静态方法的时候，锁定的是当前类的 Class 对象，在上面的例子中就是 Class X；</li>
<li>当修饰非静态方法的时候，锁定的是当前实例对象 this。</li>
</ul>
<p>对于上面的例子，synchronized修饰静态方法相当于：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">X</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 修饰静态方法</span></span><br><span class="line">  <span class="keyword">synchronized</span>(X<span class="class">.<span class="keyword">class</span>) <span class="title">static</span> <span class="title">void</span> <span class="title">bar</span>() </span>&#123;</span><br><span class="line">    <span class="comment">// 临界区</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>修饰非静态方法，相当于：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">X</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 修饰非静态方法</span></span><br><span class="line">  <span class="keyword">synchronized</span>(<span class="keyword">this</span>) <span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 临界区</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="用synchronized解决count-1的问题"><a href="#用synchronized解决count-1的问题" class="headerlink" title="用synchronized解决count+=1的问题"></a>用synchronized解决count+=1的问题</h5><blockquote>
<p>SafeCalc这个类有两个方法：</p>
<ul>
<li>get方法，获得value</li>
<li>addOne方法，给value加1，并且用synchronized修饰</li>
</ul>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SafeCalc</span> </span>&#123;</span><br><span class="line">  <span class="keyword">long</span> value = <span class="number">0L</span>;</span><br><span class="line">  <span class="function"><span class="keyword">long</span> <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">addOne</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    value += <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>addOne方法首先被肯定的是被修饰后，无论是单核还是多核，只有一个线程能够执行addOne方法，所以一定能保证原子性操作，那是否有可见性问题呢？</p>
<p>我们记得管程中锁的规则：对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。</p>
<p>管程，就是我们这里提到的synchronized，我们知道synchronized修饰的临界区是互斥的，也就是说同一时刻只有一个线程执行临界区的代码；而所谓“对一个锁解锁Happens-Before后续对这个锁的加锁“，指的是前一个线程的解锁操作对后一个线程的加锁操作可见，综合 Happens-Before 的传递性原则，我们就能得出前一个线程在临界区修改的共享变量（该操作在解锁之前），对后续进入临界区（该操作在加锁之后）的线程是可见的。</p>
<p>按照这个规则，如果多个线程同时执行 addOne() 方法，可见性是可以保证的，也就说如果有 1000 个线程执行 addOne() 方法，最终结果一定是 value 的值增加了 1000。看到这个结果，我们长出一口气，问题终于解决了。</p>
<p>但也许，你一不小心就忽视了 get() 方法。执行 addOne() 方法后，value 的值对 get() 方法是可见的吗？这个可见性是没法保证的。管程中锁的规则，是只保证后续对这个锁的加锁的可见性，而 get() 方法并没有加锁操作，所以可见性没法保证。那如何解决呢？很简单，就是 get() 方法也 synchronized 一下，完整的代码如下所示。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SafeCalc</span> </span>&#123;</span><br><span class="line">  <span class="keyword">long</span> value = <span class="number">0L</span>;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">long</span> <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">addOne</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    value += <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的代码转换为我们提到的锁模型，就是下面图示这个样子。get() 方法和 addOne() 方法都需要访问 value 这个受保护的资源，这个资源用 this 这把锁来保护。线程要进入临界区 get() 和 addOne()，必须先获得 this 这把锁，这样 get() 和 addOne() 也是互斥的。</p>
<p><img src="https://static001.geekbang.org/resource/image/26/f6/26a84ffe2b4a6ae67c8093d29473e1f6.png" alt="img"></p>
<center>保护临界区get和addOne的示意图</center>

<p>这个模型更像现实世界里面球赛门票的管理，一个座位只允许一个人使用，这个座位就是“受保护资源”，球场的入口就是 Java 类里的方法，而门票就是用来保护资源的“锁”，Java 里的检票工作是由 synchronized 解决的。</p>
<h4 id="锁和受保护资源的关系"><a href="#锁和受保护资源的关系" class="headerlink" title="锁和受保护资源的关系"></a>锁和受保护资源的关系</h4><blockquote>
<p>受保护资源和锁之间的关联关系是 N:1 的关系</p>
</blockquote>
<p>拿前面球赛门票的管理来类比，就是一个座位，我们只能用一张票来保护，如果多发了重复的票，那就要打架了。现实世界里，我们可以用多把锁来保护同一个资源，但在并发领域是不行的，并发领域的锁和现实世界的锁不是完全匹配的。不过倒是可以用同一把锁来保护多个资源，这个对应到现实世界就是我们所谓的“包场”了。</p>
<p>接着上面的那个例子稍作改动，把 value 改成静态变量，把 addOne() 方法改成静态方法，此时 get() 方法和 addOne() 方法是否存在并发问题呢？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SafeCalc</span> </span>&#123;</span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">long</span> value = <span class="number">0L</span>;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">long</span> <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">addOne</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    value += <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果你仔细观察，就会发现改动后的代码是用两个锁保护一个资源。这个受保护的资源就是静态变量 value，两个锁分别是 this 和 SafeCalc.class。我们可以用下面这幅图来形象描述这个关系。由于临界区 get() 和 addOne() 是用两个锁保护的，因此这两个临界区没有互斥关系，临界区 addOne() 对 value 的修改对临界区 get() 也没有可见性保证，这就导致并发问题了。</p>
<p>锁保护了资源，同时锁和资源是有对应关系的，所以从资源出发，也可以区分两把锁是不是同一个。 参见下一节课的笔记，可以看出Java里没有单独的锁类型，锁就是用一个对象来表示的，锁和被保护资源之间的对应关系是靠代码逻辑实现的。</p>
<p><img src="https://static001.geekbang.org/resource/image/60/be/60551e006fca96f581f3dc25424226be.png" alt="img"></p>
<center>两把锁保护一个资源的示意图</center>

<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><blockquote>
<p>并发问题首要想到的解决办法还是加锁，因为加锁能够保证执行临界区代码的互斥性。</p>
</blockquote>
<p><strong>真正用好互斥锁：</strong> 临界区的代码是操作受保护资源的路径，类似于球场的入口，入口一定要检票，也就是要加锁，但不是随便一把锁都能有效。所以必须深入分析锁定的对象和受保护资源的关系，综合考虑受保护资源的访问路径，多方面考量才能用好互斥锁。</p>
<p>synchronized 是 Java 在语言层面提供的互斥原语，其实 Java 里面还有很多其他类型的锁，但作为互斥锁，原理都是相通的：锁，一定有一个要锁定的对象，至于这个锁定的对象要保护的资源以及在哪里加锁 / 解锁，就属于设计层面的事情了。</p>
<h4 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SafeCalc</span> </span>&#123;</span><br><span class="line">  <span class="keyword">long</span> value = <span class="number">0L</span>;</span><br><span class="line">  <span class="function"><span class="keyword">long</span> <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">new</span> Object()) &#123;</span><br><span class="line">      <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">addOne</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">new</span> Object()) &#123;</span><br><span class="line">      value += <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>synchronized这个使用方式正确吗？有哪些问题呢？能解决可见性和原子性问题吗？</p>
<ul>
<li>加锁的本质就是在锁对象的对象头中写入当前线程id，但是new object每次在内存中都是新对象，所以加锁无效</li>
<li>经过JVM逃逸分析的优化后，这段代码会被直接优化掉，所以在运行时是无锁的</li>
<li>sync锁的对象monitor指针指向一个ObjectMonitor对象，所有线程加入他的entrylist里面，去cas抢锁，更改state加1拿锁，执行完代码，释放锁state减1，和aqs机制差不多，只是所有线程不阻塞，cas抢锁，没有队列，属于非公平锁。wait的时候，线程进waitset休眠，等待notify唤醒</li>
<li>多把锁保护同一个资源，就像一个厕所坑位，有N多门可以进去，没有丝毫保护效果，管理员一看，还不如把门都撤了，弄成开放式(编译器代码优化)😂。</li>
<li>两把不同的锁，不能保护临界资源。而且这种new出来只在一个地方使用的对象，其它线程不能对它解锁，这个锁会被编译器优化掉。和没有syncronized代码块效果是相同的</li>
<li>不能，因为new了，所以不是同一把锁。老师您好，我对那 synchronized的理解是这样，它并不能改变CPU时间片切换的特点，只是当其他线程要访问这个资源时，发现锁还未释放，所以只能在外面等待，不知道理解是否正确</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">A</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> Integer b = <span class="number">1</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       System.out.println(<span class="string">"A is begin!"</span>);</span><br><span class="line">       <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"a"</span>);</span><br><span class="line">           <span class="comment">// System.out.println(b);</span></span><br><span class="line">           <span class="keyword">if</span> (b.equals(<span class="number">2</span>))</span><br><span class="line">              <span class="keyword">break</span>;</span><br><span class="line">       &#125;</span><br><span class="line"> </span><br><span class="line">       System.out.println(<span class="string">"A is finish!"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">       A a = <span class="keyword">new</span> A();</span><br><span class="line">       <span class="comment">//线程A</span></span><br><span class="line">       <span class="keyword">new</span> Thread(a).start();</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">       &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">       &#125;</span><br><span class="line">       a.b = <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">我们知道这个程序会出现可见性问题。</span><br><span class="line">但是在<span class="keyword">while</span>内加上System.out.println(b)后 当主线程修改b的值后 线程A居然能够取得最新值 可见性问题得到解决</span><br><span class="line">System.out.println(b)的实现如下</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">println</span><span class="params">(String x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">            print(x);</span><br><span class="line">            newLine();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">Doug Lea大神的Concurrent Programming in Java一书中有这样一个片段来描述<span class="keyword">synchronized</span>这个关键字：</span><br><span class="line"> </span><br><span class="line">这里英文就不放出来了 字数超过两千……</span><br><span class="line">这篇文章也有提及https:<span class="comment">//www.jianshu.com/p/3c06ffbf0d52</span></span><br><span class="line"> </span><br><span class="line">简单翻译一下：从本质上来说，当线程释放一个锁时会强制性的将工作内存中之前所有的写操作都刷新到主内存中去，而获取一个锁则会强制性的加载可访问到的值到线程工作内存中来。虽然锁操作只对同步方法和同步代码块这一块起到作用，但是影响的却是线程执行操作所使用的所有字段。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">也就是说当调用System.out.println(<span class="string">"a"</span>)时当前线程的缓存会被重新刷新过，所以才能够读到这个值最新值</span><br><span class="line"> ---------------------------------------------------------</span><br><span class="line">然后问题来了</span><br><span class="line">问题<span class="number">1</span>:</span><br><span class="line">首先上面的说法不知道是不是真的是这样。</span><br><span class="line">然后我在下面加了System.out.println(b) 结果打印出来的是旧值，但是下面的b.equals(<span class="number">2</span>)却能通过 这里没弄明白 我觉得应该是编译器进行了优化?因为现在大三能力不够，还没学会看<span class="class"><span class="keyword">class</span>文件 没法验证</span></span><br><span class="line"><span class="class"> </span></span><br><span class="line">问题2:</span><br><span class="line">网上找了一些文章</span><br><span class="line">有些人的说法是：打印是IO操作，而IO操作会引起线程的切换，线程切换会导致线程原本的缓存失效，从而也会读取到修改后的值。</span><br><span class="line"> </span><br><span class="line">我尝试着将打印换成File file = <span class="keyword">new</span> File(<span class="string">"D://1.txt"</span>);这句代码，程序也能够正常的结束。当然，在这里也可以尝试将将打印替换成<span class="keyword">synchronized</span>(A<span class="class">.<span class="keyword">class</span>)</span>&#123; &#125;这句空同步代码块，发现程序也能够正常结束。</span><br><span class="line"> </span><br><span class="line">这里有个问题就是 线程切换时会把之前操作的相关数据保存到内存里，切换回来后会把内存里的数据重新加载到寄存器里吗，这样说的话 就算切换也是获取不到修改后的值的,不知道是什么做到能够读到这个修改后的值的？</span><br><span class="line"> </span><br><span class="line">问题<span class="number">3</span>:</span><br><span class="line">是不是</span><br><span class="line">线程执行过程中，操作系统会随机性的把缓存刷到内存</span><br><span class="line">线程结束后一定会把缓存里的数据刷到内存</span><br><span class="line"></span><br><span class="line"> ---------------------------------------------------------</span><br><span class="line">在评论里好多大神 能学到好多东西😄😄</span><br><span class="line">作者回复: <span class="number">1</span>. println的代码里锁的<span class="keyword">this</span>指的是你的控制台，这个锁跟你的代码没关系，而且println里也没有写操作，所以println不会导致强刷缓存。</span><br><span class="line"></span><br><span class="line">我觉得是因为println产生了IO，IO相对CPU来说，太慢，所以这个期间大概率的会把缓存的值写入内存。也有可能这个线程被调度到了其他的CPU上，压根没有缓存，所以只能从内存取数。你调用sleep，效果应该也差不多。</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>. 线程切换显然不足以保证可见性，保证的可见性只能靠hb规则。</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>. 线程结束后，不一定会强刷缓存。否则Join的规则就没必要了</span><br><span class="line"></span><br><span class="line">并发问题本来就是小概率的事件，尤其有了IO操作之后，概率就更低了。</span><br></pre></td></tr></table></figure>

<h3 id="如何用一把锁保护多个资源？"><a href="#如何用一把锁保护多个资源？" class="headerlink" title="如何用一把锁保护多个资源？"></a>如何用一把锁保护多个资源？</h3><blockquote>
<p>前面我们提到，可以用一把锁来保护多个资源，但是不能用多把锁来保护一个资源。</p>
</blockquote>
<p>当我们要保护多个资源时，首先要区分这些资源是否存在关联关系。</p>
<h4 id="保护没有关联关系的多个资源"><a href="#保护没有关联关系的多个资源" class="headerlink" title="保护没有关联关系的多个资源"></a>保护没有关联关系的多个资源</h4><blockquote>
<p>在现实世界里，球场的座位和电影院的座位就是没有关联关系的，这种场景非常容易解决，那就是球赛有球赛的门票，电影院有电影院的门票，各自管理各自的。</p>
</blockquote>
<p>账户类 Account 有两个成员变量，分别是账户余额 balance 和账户密码 password。取款 withdraw() 和查看余额 getBalance() 操作会访问账户余额 balance，我们创建一个 final 对象 balLock 作为锁（类比球赛门票）；而更改密码 updatePassword() 和查看密码 getPassword() 操作会修改账户密码 password，我们创建一个 final 对象 pwLock 作为锁（类比电影票）。不同的资源用不同的锁保护，各自管各自的，很简单。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 锁：保护账户余额</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Object balLock</span><br><span class="line">    = <span class="keyword">new</span> Object();</span><br><span class="line">  <span class="comment">// 账户余额  </span></span><br><span class="line">  <span class="keyword">private</span> Integer balance;</span><br><span class="line">  <span class="comment">// 锁：保护账户密码</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Object pwLock</span><br><span class="line">    = <span class="keyword">new</span> Object();</span><br><span class="line">  <span class="comment">// 账户密码</span></span><br><span class="line">  <span class="keyword">private</span> String password;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 取款</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">withdraw</span><span class="params">(Integer amt)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(balLock) &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt)&#123;</span><br><span class="line">        <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="comment">// 查看余额</span></span><br><span class="line">  <span class="function">Integer <span class="title">getBalance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(balLock) &#123;</span><br><span class="line">      <span class="keyword">return</span> balance;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 更改密码</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">updatePassword</span><span class="params">(String pw)</span></span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(pwLock) &#123;</span><br><span class="line">      <span class="keyword">this</span>.password = pw;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="comment">// 查看密码</span></span><br><span class="line">  <span class="function">String <span class="title">getPassword</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(pwLock) &#123;</span><br><span class="line">      <span class="keyword">return</span> password;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当然，我们也可以用一把互斥锁来保护多个资源，例如我们可以用 this 这一把锁来管理账户类里所有的资源：账户余额和用户密码。具体实现很简单，示例程序中所有的方法都增加同步关键字 synchronized 就可以了。</p>
<p>但是用一把锁有个问题，就是性能太差，会导致取款、查看余额、修改密码、查看密码这四个操作都是串行的。而我们用两把锁，取款和修改密码是可以并行的。<strong>用不同的锁对受保护资源进行精细化管理</strong>，能够提升性能。这种锁还有个名字，叫<strong>细粒度锁</strong>。</p>
<h4 id="保护有关联关系的多个资源"><a href="#保护有关联关系的多个资源" class="headerlink" title="保护有关联关系的多个资源"></a>保护有关联关系的多个资源</h4><p>如果多个资源是有关联关系的，那这个问题就有点复杂了。例如银行业务里面的转账操作，账户 A 减少 100 元，账户 B 增加 100 元。这两个账户就是有关联关系的。那对于像转账这种有关联关系的操作，我们应该怎么去解决呢？</p>
<p>先把这个问题代码化。我们声明了个账户类：Account，该类有一个成员变量余额：balance，还有一个用于转账的方法：transfer()，然后怎么保证转账操作 transfer() 没有并发问题呢？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">      <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">      target.balance += amt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>直觉告诉我，用户synchronized关键字修饰以下transfer()方法就可以，于是：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">transfer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">      <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">      target.balance += amt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在这段代码中，临界区内有两个资源，分别是转出账户的余额 this.balance 和转入账户的余额 target.balance，并且用的是一把锁 this，符合我们前面提到的，多个资源可以用一把锁来保护。</p>
<p><font color=red>但是，真的是这样吗？</font></p>
<p>看似正确，问题就出在this这把锁上，this这把锁可以保护自己的余额this.balance，却保护不了别人的余额target.balance，不能用自家的锁来保护被人家的资产。</p>
<p><img src="https://static001.geekbang.org/resource/image/1b/d8/1ba92a09d1a55a6a1636318f30c155d8.png" alt="img"></p>
<center>用锁this保护this.balance和target.balance</center>

<p><strong>假设有 A、B、C 三个账户，余额都是 200 元，我们用两个线程分别执行两个转账操作：账户 A 转给账户 B 100 元，账户 B 转给账户 C 100 元，最后我们期望的结果应该是账户 A 的余额是 100 元，账户 B 的余额是 200 元， 账户 C 的余额是 300 元。</strong></p>
<p>假设线程 1 执行账户 A 转账户 B 的操作，线程 2 执行账户 B 转账户 C 的操作。这两个线程分别在两颗 CPU 上同时执行，那它们是互斥的吗？我们期望是，但实际上并不是。因为线程 1 锁定的是账户 A 的实例（A.this），而线程 2 锁定的是账户 B 的实例（B.this），所以这两个线程可以同时进入临界区 transfer()。同时进入临界区的结果是什么呢？线程 1 和线程 2 都会读到账户 B 的余额为 200，导致最终账户 B 的余额可能是 300（线程 1 后于线程 2 写 B.balance，线程 2 写的 B.balance 值被线程 1 覆盖），可能是 100（线程 1 先于线程 2 写 B.balance，线程 1 写的 B.balance 值被线程 2 覆盖），就是不可能是 200。</p>
<p><img src="https://static001.geekbang.org/resource/image/a4/27/a46b4a1e73671d6e6f1bdb26f6c87627.png" alt="img"></p>
<center>并发转账</center>

<h4 id="使用锁的正确姿势"><a href="#使用锁的正确姿势" class="headerlink" title="使用锁的正确姿势"></a>使用锁的正确姿势</h4><blockquote>
<p>前面我们提到用同一把锁来保护多个资源，就是包场</p>
<p>只要我们的锁能覆盖所有受保护的资源就可以。</p>
</blockquote>
<p>在上面的例子中，this 是对象级别的锁，所以 A 对象和 B 对象都有自己的锁，如何让 A 对象和 B 对象共享一把锁呢？</p>
<p>首先想到的方法是可以让所有对象都持有一个唯一性的对象，这个对象在创建 Account 时传入。我们把 Account 默认构造函数变为 private，同时增加一个带 Object lock 参数的构造函数，创建 Account 对象时，传入相同的 lock，这样所有的 Account 对象都会共享这个 lock 了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Object lock；</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="title">Account</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="comment">// 创建Account时传入同一个lock对象</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Account</span><span class="params">(Object lock)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.lock = lock;</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 此处检查所有对象共享的锁</span></span><br><span class="line">    <span class="keyword">synchronized</span>(lock) &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">        <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">        target.balance += amt;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个办法确实能解决问题，但是有点小瑕疵，要求在创建Account对象的时候必须传入同一个对象，如果创建 Account 对象时，传入的 lock 不是同一个对象，那可就惨了，会出现锁自家门来保护他家资产的荒唐事。在真实的项目场景中，创建 Account 对象的代码很可能分散在多个工程中，传入共享的 lock 真的很难。</p>
<p>所以，上面的方案缺乏实践的可行性，我们需要更好的方案。还真有，就是用 Account.class 作为共享的锁。Account.class 是所有 Account 对象共享的，而且这个对象是 Java 虚拟机在加载 Account 类的时候创建的，所以我们不用担心它的唯一性。使用 Account.class 作为共享的锁，我们就无需在创建 Account 对象时传入了，代码更简单。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(Account<span class="class">.<span class="keyword">class</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">        <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">        target.balance += amt;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://static001.geekbang.org/resource/image/52/7c/527cd65f747abac3f23390663748da7c.png" alt="img"></p>
<center>使用共享的锁 Account.class 来保护不同对象的临界区</center>

<h4 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>如何保护多个资源关键是要分析多个资源之间的关系。如果资源之间没有关系，很好处理，每个资源一把锁就可以了。如果资源之间有关联关系，就要选择一个粒度更大的锁，这个锁应该能够覆盖所有相关的资源。除此之外，还要梳理出有哪些访问路径，所有的访问路径都要设置合适的锁，这个过程可以类比一下门票管理。</p>
<p>关联关系如果用更具体、更专业的语言来描述的话，其实是一种“原子性”特征，在前面的文章中，我们提到的原子性，主要是面向 CPU 指令的，转账操作的原子性则是属于是面向高级语言的，不过它们本质上是一样的。</p>
<p>“原子性”的本质是什么？其实不是不可分割，不可分割只是外在表现，其本质是多个资源间有一致性的要求，操作的中间状态对外不可见。例如，在 32 位的机器上写 long 型变量有中间状态（只写了 64 位中的 32 位），在银行转账的操作中也有中间状态（账户 A 减少了 100，账户 B 还没来得及发生变化）。所以解决原子性问题，是要保证中间状态对外不可见。</p>
<h4 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h4><p><strong>Q：</strong> 我们用了两把不同的锁来分别保护账户余额、账户密码，创建锁的时候，我们用的是：private final Object xxxLock = new Object();，如果账户余额用 this.balance 作为互斥锁，账户密码用 this.password 作为互斥锁，你觉得是否可以呢？</p>
<p><strong>A： ** **不能用可变对象做锁</strong>，用this.balance 和this.password 都不行。在同一个账户多线程访问时候，A线程取款进行this.balance-=amt;时候此时this.balance对应的值已经发生变换，线程B再次取款时拿到的balance对应的值并不是A线程中的，也就是说不能把可变的对象当成一把锁。this.password 虽然说是String修饰但也会改变，所以也不行。</p>
<p>不能用balance和password做为锁对象。这两个对象balance是Integer，password是String都是不可变变对象，一但对他们进行赋值就会变成新的对象，加的锁就失效了</p>
<h3 id="一不小心就死锁了，怎么办？"><a href="#一不小心就死锁了，怎么办？" class="headerlink" title="一不小心就死锁了，怎么办？"></a>一不小心就死锁了，怎么办？</h3><p>前面我们用Account.class作为互斥锁，来解决银行业务里面的转账问题，虽然这个方案不存在并发问题，但是所有账户的转账操作都是串行的。</p>
<p>例如账户A转账户B、账户C转账户D这两个转账操作现实世界里是可以并行的。但是在这个方案里却被串行化了，这样的话，性能太差。</p>
<p>试想互联网支付盛行的当下，8 亿网民每人每天一笔交易，每天就是 8 亿笔交易；每笔交易都对应着一次转账操作，8 亿笔交易就是 8 亿次转账操作，也就是说平均到每秒就是近 1 万次转账操作，若所有的转账操作都串行，性能完全不能接受。</p>
<p>那我们就尝试把性能提升一下。</p>
<h4 id="向现实世界要答案"><a href="#向现实世界要答案" class="headerlink" title="向现实世界要答案"></a>向现实世界要答案</h4><p>现实世界里，账户转账操作是支持并发的，而且绝对是真正的并行，银行所有的窗口都可以做转账操作。只要我们能仿照现实世界做转账操作，串行的问题就解决了。</p>
<p>想象一个真实场景，古代没有信息化，账户的存在形式真的就是一个账本，而且每个账户都有一个账本，这些账本都统一放在文件架上，银行柜员在给我们做转账时，要去文件架上把转出账本和转入账本都拿到手，然后做转账。这个柜员在拿账本的时候可能遇到以下三种情况：</p>
<ol>
<li>文件架上恰好有转出账本和转入账本，那就同时拿走；</li>
<li>如果文件架上只有转出账本和转入账本之一，那这个柜员就先把文件架上有的账本拿到手，同时等着其他柜员把另外一个账本送回来；</li>
<li>转出账本和转入账本都没有，那这个柜员就等着两个账本都被送回来。</li>
</ol>
<p>这个过程用两把锁就实现了，转出账本一把，转入账本另一把，在transfer()方法内部，首先尝试所的那个转出账户this（先把转出账本拿到手），然后尝试锁定转入账户 target（再把转入账本拿到手），只有当两者都成功时，才执行转账操作。</p>
<p><img src="https://static001.geekbang.org/resource/image/cb/55/cb18e672732ab76fc61d60bdf66bf855.png" alt="img"></p>
<center>两个转账操作并行</center>

<p>经过这样的优化，账户A转账户B和账户C转账户D这两个转账操作就可以并行了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 锁定转出账户</span></span><br><span class="line">    <span class="keyword">synchronized</span>(<span class="keyword">this</span>) &#123;              </span><br><span class="line">      <span class="comment">// 锁定转入账户</span></span><br><span class="line">      <span class="keyword">synchronized</span>(target) &#123;           </span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">          <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">          target.balance += amt;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="没有免费的午餐"><a href="#没有免费的午餐" class="headerlink" title="没有免费的午餐"></a>没有免费的午餐</h4><p>上述实现看似很完美，相对于用Account.class作为互斥锁，锁定的范围太大，而我们锁定两个账户范围就小多了，这样的锁，我们称为细粒度锁。</p>
<blockquote>
<p>使用细粒度锁可以提高并行度，是性能优化的一个重要手段。</p>
</blockquote>
<p><font color=red>但是,使用细粒度锁是有代价的，这个代价就是可能会导致死锁。</font></p>
<p>首先我们来看一个特殊场景，如果有客户找柜员张三做个转账业务：账户 A 转账户 B 100 元，此时另一个客户找柜员李四也做个转账业务：账户 B 转账户 A 100 元，于是张三和李四同时都去文件架上拿账本，这时候有可能凑巧张三拿到了账本 A，李四拿到了账本 B。张三拿到账本 A 后就等着账本 B（账本 B 已经被李四拿走），而李四拿到账本 B 后就等着账本 A（账本 A 已经被张三拿走），他们要等多久呢？他们会永远等待下去…因为张三不会把账本 A 送回去，李四也不会把账本 B 送回去。</p>
<p><img src="https://static001.geekbang.org/resource/image/f2/88/f293dc0d92b7c8255bd0bc790fc2a088.png" alt="img"></p>
<center>转账业务中的“死等”</center>

<p>现实世界里的死等，就是编程领域的死锁了。死锁的一个比较专业的定义是：一组互相竞争资源的线程因互相等待，导致“永久”阻塞的现象。</p>
<p>上面转账的代码是怎么发生死锁的呢？我们假设线程 T1 执行账户 A 转账户 B 的操作，账户 A.transfer(账户 B)；同时线程 T2 执行账户 B 转账户 A 的操作，账户 B.transfer(账户 A)。当 T1 和 T2 同时执行完①处的代码时，T1 获得了账户 A 的锁（对于 T1，this 是账户 A），而 T2 获得了账户 B 的锁（对于 T2，this 是账户 B）。之后 T1 和 T2 在执行②处的代码时，T1 试图获取账户 B 的锁时，发现账户 B 已经被锁定（被 T2 锁定），所以 T1 开始等待；T2 则试图获取账户 A 的锁时，发现账户 A 已经被锁定（被 T1 锁定），所以 T2 也开始等待。于是 T1 和 T2 会无期限地等待下去，也就是我们所说的死锁了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 锁定转出账户</span></span><br><span class="line">    <span class="keyword">synchronized</span>(<span class="keyword">this</span>)&#123;     ①</span><br><span class="line">      <span class="comment">// 锁定转入账户</span></span><br><span class="line">      <span class="keyword">synchronized</span>(target)&#123; ②</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">          <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">          target.balance += amt;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>关于这种现象，我们还可以借助资源分配图来可视化锁的占用情况（资源分配图是个有向图，它可以描述资源和线程的状态）。</p>
<p>其中，资源用方形节点表示，线程用圆形节点表示；资源中的点指向线程的边表示线程已经获得该资源，线程指向资源的边则表示线程请求资源，但尚未得到。转账发生死锁时的资源分配图就如下图所示，一个“各据山头死等”的尴尬局面。</p>
<p><img src="https://static001.geekbang.org/resource/image/82/1c/829d69c7d32c3ad1b89d89fc56017d1c.png" alt="img"></p>
<center>转账发生死锁时的资源分配问题</center>

<p>发现了问题，那么</p>
<h4 id="如何预防死锁呢"><a href="#如何预防死锁呢" class="headerlink" title="如何预防死锁呢"></a>如何预防死锁呢</h4><p>并发程序一旦死锁，一般没有特别好的方法，很多时候我们只能重启应用。因此，解决死锁问题最好的办法还是规避死锁。</p>
<p>那如何避免死锁呢？要避免死锁就需要分析死锁发生的条件，有个叫 Coffman 的牛人早就总结过了，只有以下这四个条件都发生时才会出现死锁：</p>
<ol>
<li>互斥，共享资源 X 和 Y 只能被一个线程占用；</li>
<li>占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X；</li>
<li>不可抢占，其他线程不能强行抢占线程 T1 占有的资源；</li>
<li>循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待。</li>
</ol>
<p>反过来分析，也就是说只要我们破坏其中一个，就可以成功避免死锁的发生。</p>
<p>其中，互斥这个条件我们没有办法破坏，因为我们用锁为的就是互斥。不过其他三个条件都是有办法破坏掉的，到底如何做呢？</p>
<ol>
<li>对于“占用且等待”这个条件，我们可以一次性申请所有的资源，这样就不存在等待了。</li>
<li>对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。</li>
<li>对于“循环等待”这个条件，可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，这样线性化后自然就不存在循环了。</li>
</ol>
<h5 id="破坏占用且等待条件"><a href="#破坏占用且等待条件" class="headerlink" title="破坏占用且等待条件"></a>破坏占用且等待条件</h5><p>从理论上讲，要破坏这个条件，可以一次性申请所有资源。在现实世界里，就拿前面我们提到的转账操作来讲，它需要的资源有两个，一个是转出账户，另一个是转入账户，当这两个账户同时被申请时，我们该怎么解决这个问题呢？</p>
<p>可以增加一个账本管理员，然后只允许账本管理员从文件架上拿账本，也就是说柜员不能直接在文件架上拿账本，必须通过账本管理员才能拿到想要的账本。例如，张三同时申请账本 A 和 B，账本管理员如果发现文件架上只有账本 A，这个时候账本管理员是不会把账本 A 拿下来给张三的，只有账本 A 和 B 都在的时候才会给张三。这样就保证了“一次性申请所有资源”。</p>
<p><img src="https://static001.geekbang.org/resource/image/27/db/273af8c2ee60bd659f18673d2af005db.png" alt="img"></p>
<center>通过账本管理员拿账本</center>

<p>对应到编程领域，“同时申请”这个操作是一个临界区，我们也需要一个角色（Java 里面的类）来管理这个临界区，我们就把这个角色定为 Allocator。它有两个重要功能，分别是：同时申请资源 apply() 和同时释放资源 free()。账户 Account 类里面持有一个 Allocator 的单例（必须是单例，只能由一个人来分配资源）。当账户 Account 在执行转账操作的时候，首先向 Allocator 同时申请转出账户和转入账户这两个资源，成功后再锁定这两个资源；当转账操作执行完，释放锁之后，我们需通知 Allocator 同时释放转出账户和转入账户这两个资源。具体的代码实现如下。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Allocator</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> List&lt;Object&gt; als =</span><br><span class="line">    <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="comment">// 一次性申请所有资源</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">apply</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    Object from, Object to)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(als.contains(from) ||</span><br><span class="line">         als.contains(to))&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      als.add(from);</span><br><span class="line">      als.add(to);  </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 归还资源</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">free</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    Object from, Object to)</span></span>&#123;</span><br><span class="line">    als.remove(from);</span><br><span class="line">    als.remove(to);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="comment">// actr应该为单例</span></span><br><span class="line">  <span class="keyword">private</span> Allocator actr;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 一次性申请转出账户和转入账户，直到成功</span></span><br><span class="line">    <span class="keyword">while</span>(!actr.apply(<span class="keyword">this</span>, target))</span><br><span class="line">      ；</span><br><span class="line">    <span class="keyword">try</span>&#123;</span><br><span class="line">      <span class="comment">// 锁定转出账户</span></span><br><span class="line">      <span class="keyword">synchronized</span>(<span class="keyword">this</span>)&#123;              </span><br><span class="line">        <span class="comment">// 锁定转入账户</span></span><br><span class="line">        <span class="keyword">synchronized</span>(target)&#123;           </span><br><span class="line">          <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt)&#123;</span><br><span class="line">            <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">            target.balance += amt;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      actr.free(<span class="keyword">this</span>, target)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="破坏不可抢占条件"><a href="#破坏不可抢占条件" class="headerlink" title="破坏不可抢占条件"></a>破坏不可抢占条件</h5><p>破坏不可抢占条件看上去很简单，核心是要能够主动释放它占有的资源，这一点 synchronized 是做不到的。原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。</p>
<p>你可能会质疑，“Java 作为排行榜第一的语言，这都解决不了？”你的怀疑很有道理，Java 在语言层次确实没有解决这个问题，不过在 SDK 层面还是解决了的，java.util.concurrent 这个包下面提供的 Lock 是可以轻松解决这个问题的。关于这个话题，咱们后面会详细讲。</p>
<h5 id="破坏循环等待条件"><a href="#破坏循环等待条件" class="headerlink" title="破坏循环等待条件"></a>破坏循环等待条件</h5><p>破坏这个条件，需要对资源进行排序，然后按序申请资源。这个实现非常简单，我们假设每个账户都有不同的属性 id，这个 id 可以作为排序字段，申请的时候，我们可以按照从小到大的顺序来申请。比如下面代码中，①~⑥处的代码对转出账户（this）和转入账户（target）排序，然后按照序号从小到大的顺序锁定账户。这样就不存在“循环”等待了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> id;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    Account left = <span class="keyword">this</span>        ①</span><br><span class="line">    Account right = target;    ②</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.id &gt; target.id) &#123; ③</span><br><span class="line">      left = target;           ④</span><br><span class="line">      right = <span class="keyword">this</span>;            ⑤</span><br><span class="line">    &#125;                          ⑥</span><br><span class="line">    <span class="comment">// 锁定序号小的账户</span></span><br><span class="line">    <span class="keyword">synchronized</span>(left)&#123;</span><br><span class="line">      <span class="comment">// 锁定序号大的账户</span></span><br><span class="line">      <span class="keyword">synchronized</span>(right)&#123; </span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt)&#123;</span><br><span class="line">          <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">          target.balance += amt;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Conclusion-2"><a href="#Conclusion-2" class="headerlink" title="Conclusion"></a>Conclusion</h4><blockquote>
<p>利用现实世界的模型来构思解决方案</p>
</blockquote>
<p>在利用现实模型建模的时候，我们还要仔细对比现实世界和编程世界里的各角色之间的差异。</p>
<p>用细粒度锁来锁定多个资源时，要注意死锁的问题，识别出风险很重要。</p>
<p>预防死锁主要是破坏三个条件中的一个，有了这个思路后，实现就简单了。但仍需注意的是，有时候预防死锁成本也是很高的。例如上面转账那个例子，我们破坏占用且等待条件的成本就比破坏循环等待条件的成本高，破坏占用且等待条件，我们也是锁了所有的账户，而且还是用了死循环 while(!actr.apply(this, target));方法，不过好在 apply() 这个方法基本不耗时。 在转账这个例子中，破坏循环等待条件就是成本最低的一个方案。</p>
<p>所以我们在选择具体方案的时候，还需要评估一下操作成本，从中选择一个成本最低的方案。</p>
<h4 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h4><p>破坏占用且等待条件，我们也是锁了所有的账户，而且还是用了死循环 while(!actr.apply(this, target));这个方法，那它比synchronized(Account.class) 有没有性能优势呢？</p>
<ul>
<li>synchronized(Account.class) 锁了Account类相关的所有操作。相当于文中说的包场了，只要与Account有关联，通通需要等待当前线程操作完成。while死循环的方式只锁定了当前操作的两个相关的对象。两种影响到的范围不同。</li>
<li>最简单的方案: 遇到死锁，我就是用资源id的从小到大的顺序去申请锁解决的</li>
<li>用top命令查看Java线程的cpu利用率，用jstack来dump线程。开发环境可以用 java visualvm查看线程执行情况</li>
<li>while 循环就是一个自旋锁机制吧，自旋锁的话要关注它的循环时间，不能一直循环下去，不然会浪费 cpu 资源。自旋锁在JVM里是一种特殊的锁机制，自诩不会阻塞线程的。咱们这个其实还是会阻塞线程的。不过原理都一样，你这样理解也没问题。</li>
<li>最常见的就是B转A的同时，A转账给B，那么先锁B再锁A，但是，另一个线程是先锁A再锁B，然而，如果两个线程同时执行，那么就是出现死锁的情况，线程T1锁了A请求锁B，此时线程T2锁了B请求锁A，都在等着对方释放锁，然而自己都不会释放锁，故死锁。<br>最简单的办法，就是无论哪个线程执行的时候，都按照顺序加锁，即按照A和B的id大小来加锁，这样，无论哪个线程执行的时候，都会先加锁A，再加锁B，A被加锁，则等待释放。这样就不会被死锁了。</li>
</ul>
<h3 id="用“等待-通知”机制优化循环等待"><a href="#用“等待-通知”机制优化循环等待" class="headerlink" title="用“等待-通知”机制优化循环等待"></a>用“等待-通知”机制优化循环等待</h3><p>上边我们说过，在破坏占用且等待条件的时候，如果转出账本和转入账本不满足同时在文件架上这个条件，就用死循环的方式来循环等待，核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 一次性申请转出账户和转入账户，直到成功</span></span><br><span class="line"><span class="keyword">while</span>(!actr.apply(<span class="keyword">this</span>, target))</span><br><span class="line">  ；</span><br></pre></td></tr></table></figure>

<p>如果 apply() 操作耗时非常短，而且并发冲突量也不大时，这个方案还挺不错的，因为这种场景下，循环上几次或者几十次就能一次性获取转出账户和转入账户了。但是如果 apply() 操作耗时长，或者并发冲突量大的时候，循环等待这种方案就不适用了，因为在这种场景下，可能要循环上万次才能获取到锁，太消耗 CPU 了。</p>
<p>其实在这种场景下，最好的方案应该是：如果线程要求的条件（转出账本和转入账本同在文件架上）不满足，则线程阻塞自己，进入等待状态；当线程要求的条件（转出账本和转入账本同在文件架上）满足后，通知等待的线程重新执行。其中，使用线程阻塞的方式就能避免循环等待消耗 CPU 的问题。</p>
<blockquote>
<p>Java 也是支持等待-通知机制的</p>
</blockquote>
<h4 id="就医流程"><a href="#就医流程" class="headerlink" title="就医流程"></a>就医流程</h4><p>从一个就医流程迁移到一个完整的等待-通知机制的步骤：</p>
<p>线程首先获取互斥锁，当线程要求的条件不满足时，释放互斥锁，进入等待状态；当要求的条件满足时，通知等待的线程，重新获取互斥锁。</p>
<h4 id="用-synchronized-实现等待-通知机制"><a href="#用-synchronized-实现等待-通知机制" class="headerlink" title="用 synchronized 实现等待 - 通知机制"></a>用 synchronized 实现等待 - 通知机制</h4><blockquote>
<p>Java 语言里，等待-通知机制可以有多种实现方式，比如 Java 语言内置的 synchronized 配合wait()、notify()、notifyAll() 这三个方法就能轻松实现</p>
</blockquote>
<p>那么如何实现：</p>
<p>同一时刻，只允许一个线程进入 synchronized 保护的临界区（这个临界区可以看作大夫的诊室），当有一个线程进入临界区后，其他线程就只能进入图中左边的等待队列里等待（相当于患者分诊等待）。这个等待队列和互斥锁是一对一的关系，每个互斥锁都有自己独立的等待队列。</p>
<p><img src="https://static001.geekbang.org/resource/image/c6/d0/c6640129fde927be8882ca90981613d0.png" alt="img"></p>
<center>wait操作工作原理</center>

<p>在并发程序中，当一个线程进入临界区后，由于某些条件不满足，需要进入等待状态，Java 对象的 wait() 方法就能够满足这种需求。如上图所示，当调用 wait() 方法后，当前线程就会被阻塞，并且进入到右边的等待队列中，<strong>这个等待队列也是互斥锁的等待队列</strong>。 </p>
<p>线程在进入等待队列的同时，<strong>会释放持有的互斥锁</strong>，线程释放锁后，其他线程就有机会获得锁，并进入临界区了。</p>
<p>那线程要求的条件满足时，该怎么通知这个等待的线程呢？很简单，就是 Java 对象的 notify() 和 notifyAll() 方法。我在下面这个图里为你大致描述了这个过程，当条件满足时调用 notify()，会通知等待队列（互斥锁的等待队列）中的线程，告诉它条件曾经满足过。</p>
<p><img src="https://static001.geekbang.org/resource/image/1b/8c/1b3e999c300166a84f2e8cc7a4b8f78c.png" alt="img"></p>
<center>notify 操作工作原理</center>

<p>为什么说是曾经满足过呢？因为notify只能保证在通知时间点，条件是满足的。而被通知线程的执行时间点和通知的时间点基本上不会重合，所以当线程执行的时候，很可能条件已经不满足了（保不齐有其他线程插队）。</p>
<p>除此之外，被通知的线程要想重新执行，仍然需要获取到互斥锁（因为曾经获取的锁在调用 wait() 时已经释放了）。</p>
<p>上面我们一直强调 wait()、notify()、notifyAll() 方法操作的等待队列是互斥锁的等待队列，所以如果 synchronized 锁定的是 this，那么对应的一定是 this.wait()、this.notify()、this.notifyAll()；如果 synchronized 锁定的是 target，那么对应的一定是 target.wait()、target.notify()、target.notifyAll() 。而且 wait()、notify()、notifyAll() 这三个方法能够被调用的前提是已经获取了相应的互斥锁，所以我们会发现 wait()、notify()、notifyAll() 都是在 synchronized{}内部被调用的。如果在 synchronized{}外部调用，或者锁定的 this，而用 target.wait() 调用的话，JVM 会抛出一个运行时异常：java.lang.IllegalMonitorStateException。</p>
<h4 id="一个更好地资源分配器"><a href="#一个更好地资源分配器" class="headerlink" title="一个更好地资源分配器"></a>一个更好地资源分配器</h4><blockquote>
<p>搞懂等待-通知机制的基本原理之后，如何解决一次性申请转出账户和转入账户的问题？</p>
</blockquote>
<p>需要考虑四个要素：</p>
<ol>
<li>互斥锁，我们之前提到的Allocator需要是单例的，所以我们可以用this作为互斥锁</li>
<li>线程要求的条件，转出账户和转入账户都没有被分配过</li>
<li>何时等待，线程要求的条件不满足就等待</li>
<li>何时通知，当有线程释放账户时就通知</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">while</span>(条件不满足) &#123;</span><br><span class="line">  wait();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>利用这种范式可以解决上面提到的<strong>条件曾经满足过</strong>这个问题，因为当wait()返回时，有可能条件已经发生变化了，曾经条件满足，但是现在已经不满足了，所以要重新检验条件是否满足，范式，意味着是经典做法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Allocator</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> List&lt;Object&gt; als;</span><br><span class="line">  <span class="comment">// 一次性申请所有资源</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    Object from, Object to)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 经典写法</span></span><br><span class="line">    <span class="keyword">while</span>(als.contains(from) ||</span><br><span class="line">         als.contains(to))&#123;</span><br><span class="line">      <span class="keyword">try</span>&#123;</span><br><span class="line">        wait();</span><br><span class="line">      &#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">      &#125;   </span><br><span class="line">    &#125; </span><br><span class="line">    als.add(from);</span><br><span class="line">    als.add(to);  </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 归还资源</span></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">free</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    Object from, Object to)</span></span>&#123;</span><br><span class="line">    als.remove(from);</span><br><span class="line">    als.remove(to);</span><br><span class="line">    notifyAll();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="尽量使用notifyAll"><a href="#尽量使用notifyAll" class="headerlink" title="尽量使用notifyAll()"></a>尽量使用notifyAll()</h4><p>上面的代码中使用此方法实现通知机制，不使用notify是因为二者是有区别的。</p>
<ul>
<li>notify是会随机地通知等待队列中的一个进程</li>
<li>notifyAll会通知等待队列中的所有线程</li>
</ul>
<p>从感觉上来讲，应该是 notify() 更好一些，因为即便通知所有线程，也只有一个线程能够进入临界区。但那所谓的感觉往往都蕴藏着风险，实际上使用 notify() 也很有风险，它的风险在于可能导致某些线程永远不会被通知到。</p>
<p>假设我们有资源 A、B、C、D，线程 1 申请到了 AB，线程 2 申请到了 CD，此时线程 3 申请 AB，会进入等待队列（AB 分配给线程 1，线程 3 要求的条件不满足），线程 4 申请 CD 也会进入等待队列。我们再假设之后线程 1 归还了资源 AB，如果使用 notify() 来通知等待队列中的线程，有可能被通知的是线程 4，但线程 4 申请的是 CD，所以此时线程 4 还是会继续等待，而真正该唤醒的线程 3 就再也没有机会被唤醒了。</p>
<p>所以除非经过深思熟虑，否则尽量使用 notifyAll()。</p>
<h4 id="Conclusion-3"><a href="#Conclusion-3" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>等待 - 通知机制是一种非常普遍的线程间协作的方式。项目中我们经常使用轮询的方式来等待某个状态，其实很多情况下都可以用今天我们介绍的等待 - 通知机制来优化。</p>
<p>Java 语言内置的 synchronized 配合 wait()、notify()、notifyAll() 这三个方法可以快速实现这种机制，但是它们的使用看上去还是有点复杂，所以你需要认真理解等待队列和 wait()、notify()、notifyAll() 的关系。</p>
<p>还是那句话，结合现实，对问题的理解会更深入。</p>
<h4 id="问题-4"><a href="#问题-4" class="headerlink" title="问题"></a>问题</h4><p>wait方法和sleep方法都能让当前线程挂起一段时间，它们的区别是什么?</p>
<ol>
<li>wait会释放所有锁而sleep不会释放锁资源</li>
<li>wait只能在同步方法和同步块中使用，而sleep任何地方都可以</li>
<li>wait无需捕捉异常，而sleep需要</li>
</ol>
<p>wait()方法与sleep()方法的不同之处在于，wait()方法会释放对象的“锁标志”。当调用某一对象的wait()方法后，会使当前线程暂停执行，并将当前线程放入对象等待池中，直到调用了notify()方法后，将从对象等待池中移出任意一个线程并放入锁标志等待池中，只有锁标志等待池中的线程可以获取锁标志，它们随时准备争夺锁的拥有权。当调用了某个对象的notifyAll()方法，会将对象等待池中的所有线程都移动到该对象的锁标志等待池。</p>
<p>sleep()方法需要指定等待的时间，它可以让当前正在执行的线程在指定的时间内暂停执行，进入阻塞状态，该方法既可以让其他同优先级或者高优先级的线程得到执行的机会，也可以让低优先级的线程得到执行机会。但是sleep()方法不会释放“锁标志”，也就是说如果有synchronized同步块，其他线程仍然不能访问共享数据。</p>
<blockquote>
<p>到此我们基本认识了并发编程。接下来需要发现并解决一部分问题，以得到更大程度地提升。</p>
</blockquote>
<h2 id="发现问题"><a href="#发现问题" class="headerlink" title="发现问题"></a>发现问题</h2><p>那么这三个问题就是安全性问题、活跃性问题和性能问题。</p>
<h3 id="安全性问题"><a href="#安全性问题" class="headerlink" title="安全性问题"></a>安全性问题</h3><blockquote>
<p>我们一定经常听到：“这个方法不是线程安全的”，“这个类不是线程安全的”</p>
</blockquote>
<p>什么是线程安全呢？其实本质上就是正确性，正确性的含义就是程序按照我们期望的执行。</p>
<p>如何才能写出线程安全的程序呢？我们在(一)中认知了Bug的三个源头，原子性问题、可见性问题和有序性问题。也就是说，理论上线程安全的程序，就要避免出现这三个问题。</p>
<p>但是并不是所有的代码都需要认真地分析是否存在这三个问题，只有一种情况需要就是，<strong>存在共享数据并且该数据会发生变化，通俗地讲就是多个线程会同时读写同一数据。</strong></p>
<p>如果能够做到不共享数据或者数据状态不发生变化，不就能够保证线程的安全性了嘛。有不少技术方案都是基于这个理论的，例如线程本地存储（Thread Local Storage，TLS）、不变模式等等。</p>
<p>现实生活中，必须共享会发生变化的数据。</p>
<p>当多个线程同时访问同一数据，并且至少有一个线程会写这个数据的时候，如果我们不采取防护措施，那么就会导致并发 Bug，对此还有一个专业的术语，叫做数据竞争（Data Race）。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*多个线程调用add10k()方法时就会发生数据竞争</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">add10K</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> idx = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(idx++ &lt; <span class="number">10000</span>) &#123;</span><br><span class="line">      count += <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们首先想到是不是在访问数据的地方，加个锁保护一下就能解决所有的并发问题了呢，对于上面的示例，增加两个被synchronized修饰的get和set方法，add10k方法里面通过get和set方法来访问value变量。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*所有访问共享变量 value 的地方，我们都增加了互斥锁，此时是不存在数据竞争的。</span></span><br><span class="line"><span class="comment">*但很显然修改后的 add10K() 方法并不是线程安全的。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">long</span> <span class="title">get</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> count；</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">long</span> v)</span></span>&#123;</span><br><span class="line">    count = v;</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">add10K</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> idx = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(idx++ &lt; <span class="number">10000</span>) &#123;</span><br><span class="line">      set(get()+<span class="number">1</span>)      </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>假设 count=0，当两个线程同时执行 get() 方法时，get() 方法会返回相同的值 0，两个线程执行 get()+1 操作，结果都是 1，之后两个线程再将结果 1 写入了内存。你本来期望的是 2，而结果却是 1。</p>
<p>这种问题，官方的称呼叫做<strong>竞态条件</strong>，</p>
<p>所谓竞态条件，指的是程序的执行结果依赖线程执行的顺序。如果两个线程完全同时执行，那么结果是 1；如果两个线程是前后执行，那么结果就是 2。</p>
<p>在并发环境里，线程的执行顺序是不确定的，如果程序存在竞态条件问题，那就意味着程序执行的结果是不确定的，而执行结果不确定这可是个大 Bug。</p>
<p><strong>结合例子说明下竞态条件：</strong>前面我们提到过转账操作，有个判断条件就是转出金额不能大于账户金额，但在并发环境里面，如果不加控制，当多个线程同时对一个账号执行转出操作时，就可能出现超额转出问题，假设账户Ａ有月200，线程1和线程2都要从账户A中转出150。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*有可能线程 1 和线程 2 同时执行到第 6 行，这样线程 1 和线程 2 都会发现转出金额 150 小于账户余额 200，于是就会发生超额转出的情况。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> balance;</span><br><span class="line">  <span class="comment">// 转账</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      Account target, <span class="keyword">int</span> amt)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.balance &gt; amt) &#123;</span><br><span class="line">      <span class="keyword">this</span>.balance -= amt;</span><br><span class="line">      target.balance += amt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>所以，也可以按照</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (状态变量 满足 执行条件) &#123;</span><br><span class="line">  执行操作</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这样来理解竞态条件，在并发场景中，程序的执行依赖于某个状态变量。</p>
<p>当某个线程发现状态变量满足执行条件后，开始执行操作；可是就在这个线程执行操作的时候，其他线程同时修改了状态变量，导致状态变量不满足执行条件了。</p>
<p>当然很多场景下，这个条件不是显式的，例如前面 addOne 的例子中，set(get()+1) 这个复合操作，其实就隐式依赖 get() 的结果。</p>
<p>Q：面对数据竞争和竞态条件问题，又该如何保证线程的安全性呢？</p>
<p>A：这两类问题，都可以用互斥这个技术方案，而实现互斥的方案有很多，CPU 提供了相关的互斥指令，操作系统、编程语言也会提供相关的 API。从逻辑上来看，我们可以统一归为：锁。</p>
<h3 id="活跃性问题"><a href="#活跃性问题" class="headerlink" title="活跃性问题"></a>活跃性问题</h3><blockquote>
<p>某个操作无法执行下去，常见的死锁就是一种活跃性问题，当然出了死锁外，还有两种情况，分别是活锁和饥饿。</p>
</blockquote>
<p>发生“死锁”后线程会互相等待，而且会一直等待下去，在技术上的表现形式是线程永久地“阻塞”了。</p>
<ul>
<li>活锁：有时线程虽然没有发生阻塞，但仍然会存在执行不下去的情况，这就是所谓的。</li>
<li>饥饿：线程因无法访问所需资源而无法执行下去的情况。<ul>
<li>如果线程优先级“不均”，在 CPU 繁忙的情况下，优先级低的线程得到执行的机会很小，就可能发生线程“饥饿”；持有锁的线程，如果执行的时间过长，也可能导致“饥饿”问题。</li>
</ul>
</li>
</ul>
<p><strong>解决活锁的方案很简单：</strong>尝试等待一个随机事件就可以，简单有效，在Raft分布式一致性算法中用到。</p>
<p><strong>解决饥饿的方案很简单：</strong></p>
<ol>
<li>保证资源充足</li>
<li>公平地分配资源</li>
<li>避免持有锁的线程长时间执行</li>
</ol>
<p>1、3的适用场景有限，很多场景下，资源的稀缺性是无法 解决的，持有锁的线程执行的时间也很难缩短，2用的地方更多一些。</p>
<p>Q：如何公平地分配资源呢？</p>
<p>A：并发编程里，主要是使用公平锁。所谓公平锁，是一种先来后到的方案，线程的等待是有顺序的，排在等待队列前面的线程会优先获得资源。</p>
<h3 id="性能问题"><a href="#性能问题" class="headerlink" title="性能问题"></a>性能问题</h3><p>使用“锁”要非常小心，但是如果小心过度，也可能出“性能问题”。“锁”的过度使用可能导致串行化的范围过大，这样就不能够发挥多线程的优势了，而我们之所以使用多线程搞并发程序，为的就是提升性能。</p>
<p>所以我们要尽量减少串行，那串行对性能的影响是怎么样的呢？假设串行百分比是 5%，我们用多核多线程相比单核单线程能提速多少呢？</p>
<p>有个阿姆达尔（Amdahl）定律，代表了处理器并行运算之后效率提升的能力，它正好可以解决这个问题，具体公式如下：<br>$$<br>S = \frac{1}{(1-p)+\frac{p}{n}}<br>$$<br>注：n：CPU核数，p：并行百分比，(1-p)：串行百分比</p>
<p><strong>串行比</strong>：临界区都是串行的，非临界区都是并行的，用单线程执行临界区的时间/用单线程执行(临界区+非临界区)的时间就是串行百分比。</p>
<p>假设 CPU 的核数（也就是 n）无穷大，那加速比 S 的极限就是 20。也就是说，如果我们的串行率是 5%，那么我们无论采用什么技术，最高也就只能提高 20 倍的性能。</p>
<p><strong>Q：使用锁的时候一定要关注对性能的影响。 那怎么才能避免锁带来的性能问题呢？</strong></p>
<p><strong>A：</strong>Java SDK 并发包里之所以有那么多东西，有很大一部分原因就是要提升在某个特定领域的性能。</p>
<ol>
<li>使用无锁的算法和数据结构<ul>
<li>线程本地存储 (Thread Local Storage, TLS)、写入时复制 (Copy-on-write)、乐观锁等；Java 并发包里面的原子类也是一种无锁的数据结构；Disruptor 则是一个无锁的内存队列…</li>
</ul>
</li>
<li>减少锁持有的时间<ul>
<li>互斥锁本质上是将并行的程序串行化，所以要增加并行度，一定要减少持有锁的时间。<ul>
<li>使用细粒度的锁，一个典型的例子就是 Java 并发包里的 ConcurrentHashMap，它使用了所谓分段锁的技术（这个技术后面我们会详细介绍）；还可以使用读写锁，也就是读是无锁的，只有写的时候才会互斥。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>性能方面的度量指标有多个，最重要的三个：</p>
<ul>
<li>吞吐量：指的是单位时间内能处理的请求数量。吞吐量越高，说明性能越好；</li>
<li>延迟：指的是从发出请求到收到响应的时间。延迟越小，说明性能越好；</li>
<li>并发量：指的是能同时处理的请求数量，一般来说随着并发量的增加、延迟也会增加。所以延迟这个指标，一般都会是基于并发量来说的。例如并发量是 1000 的时候，延迟是 50 毫秒。</li>
</ul>
<p>并发编程是一个复杂的技术领域，</p>
<ul>
<li>微观上涉及到原子性问题、可见性问题和有序性问题，</li>
<li>宏观则表现为安全性、活跃性以及性能问题。</li>
</ul>
<p>我们在设计并发程序的时候，主要是从宏观出发，也就是要重点关注它的安全性、活跃性以及性能。安全性方面要注意数据竞争和竞态条件，活跃性方面需要注意死锁、活锁、饥饿等问题，性能方面我们虽然介绍了两个方案，但是遇到具体问题，你还是要具体分析，根据特定的场景选择合适的数据结构和算法。</p>
<p>要解决问题，首先要把问题分析清楚。同样，要写好并发程序，首先要了解并发程序相关的问题。</p>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>Java 语言提供的 Vector 是一个线程安全的容器，检查以下代码是否存在并发问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">addIfNotExist</span><span class="params">(Vector v, </span></span></span><br><span class="line"><span class="function"><span class="params">    Object o)</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(!v.contains(o)) &#123;</span><br><span class="line">    v.add(o);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>vector是线程安全，指的是它方法单独执行的时候没有并发正确性问题，并不代表把它的操作组合在一起问木有，而这个程序显然有竞态条件问题。</li>
<li>contains和add之间不是原子操作，有可能重复添加。</li>
<li>Vector实现线程安全是通过给主要的写方法加了synchronized，类似contains这样的读方法并没有synchronized，该题的问题就出在不是线程安全的contains方法，两个线程如果同时执行到if(!v.contains(o)) 是可以都通过的，这时就会执行两次add方法，重复添加。也就是竞态条件问题。</li>
</ul>
]]></content>
      <categories>
        <category>Concurrent Programming</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>Playground-of-sort-algorithm</title>
    <url>/2020/07/15/Playground%20of%20sort%20algorithm/</url>
    <content><![CDATA[<h1 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h1><p>面向各种排序算法，</p>
<ul>
<li><p>基本：【冒泡(Bubble)，插入(Insertion)】;</p>
</li>
<li><p>常用：【归并(Merge)，快速(Quick)，拓扑(Topological)】；</p>
</li>
<li><p>其他：【堆(Heap)、桶(Bucket)】；</p>
</li>
</ul>
<blockquote>
<p>归并、快速、拓扑的思想是解决绝大部分排序问题的关键，堆和桶排序经常可以在线性时间复杂度内解决问题。</p>
</blockquote>
<h2 id="冒泡"><a href="#冒泡" class="headerlink" title="冒泡"></a>冒泡</h2><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//定义一个布尔变量 hasChange，用来标记每轮遍历中是否发生了交换</span></span><br><span class="line">    <span class="keyword">boolean</span> hasChange = <span class="literal">true</span>; </span><br><span class="line"></span><br><span class="line">    <span class="comment">//每轮遍历开始，将 hasChange 设置为 false</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length - <span class="number">1</span> &amp;&amp; hasChange; i++) &#123;</span><br><span class="line">        hasChange = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//进行两两比较，如果发现当前的数比下一个数还大，那么就交换这两个数，同时记录一下有交换发生</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; nums.length - <span class="number">1</span> - i; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[j] &gt; nums[j + <span class="number">1</span>]) &#123;</span><br><span class="line">                swap(nums, j, j + <span class="number">1</span>);</span><br><span class="line">                hasChange = <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><h4 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h4><p>假设数组的元素个数是 n，由于在整个排序的过程中，我们是直接在给定的数组里面进行元素的两两交换，所以空间复杂度是 O(1)。</p>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ol>
<li><p>给定的数组按照顺序已经排好</p>
<ul>
<li>在这种情况下，我们只需要进行 n−1 次的比较，两两交换次数为 0，时间复杂度是 O(n)。这是最好的情况。</li>
</ul>
</li>
<li><p>给定的数组按照逆序排列</p>
<ul>
<li>在这种情况下，我们需要进行 n(n-1) / 2 次比较，时间复杂度是 O($n^2$)。这是最坏的情况。</li>
</ul>
</li>
<li><p>给定的数组杂乱无章</p>
<ul>
<li>在这种情况下，平均时间复杂度是 O($n^2$)。</li>
</ul>
</li>
</ol>
<p>由此可见，冒泡排序的时间复杂度是 O($n^2$)。它是一种<strong>稳定</strong>的排序算法。（稳定是指如果数组里两个相等的数，那么排序前后这两个相等的数的相对位置保持不变。）</p>
<h2 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h2><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 将数组的第一个元素当作已经排好序的，从第二个元素，即 i 从 1 开始遍历数组</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>, j, current; i &lt; nums.length; i++) &#123;</span><br><span class="line">        <span class="comment">// 外围循环开始，把当前 i 指向的值用 current 保存</span></span><br><span class="line">        current = nums[i];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指针 j 内循环，和 current 值比较，若 j 所指向的值比 current 值大，则该数右移一位</span></span><br><span class="line">        <span class="keyword">for</span> (j = i - <span class="number">1</span>; j &gt;= <span class="number">0</span> &amp;&amp; nums[j] &gt; current; j--) &#123;</span><br><span class="line">            nums[j + <span class="number">1</span>] = nums[j];</span><br><span class="line">            &#125;</span><br><span class="line">    </span><br><span class="line">        <span class="comment">// 内循环结束，j+1 所指向的位置就是 current 值插入的位置</span></span><br><span class="line">        nums[j + <span class="number">1</span>] = current;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="分析-1"><a href="#分析-1" class="headerlink" title="分析"></a>分析</h3><h4 id="空间复杂度-1"><a href="#空间复杂度-1" class="headerlink" title="空间复杂度"></a>空间复杂度</h4><p>假设数组的元素个数是 n，由于在整个排序的过程中，是直接在给定的数组里面进行元素的两两交换，空间复杂度是 O(1)。</p>
<h4 id="时间复杂度-1"><a href="#时间复杂度-1" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ol>
<li><p>给定的数组按照顺序已经排好</p>
<ul>
<li>只需要进行 n-1 次的比较，两两交换次数为 0，时间复杂度是 O(n)。这是最好的情况。</li>
</ul>
</li>
<li><p>给定的数组按照逆序排列</p>
<ul>
<li>在这种情况下，我们需要进行 n(n-1) / 2 次比较，时间复杂度是 O($n^2$)。这是最坏的情况。</li>
</ul>
</li>
<li><p>给定的数组杂乱无章</p>
<ul>
<li>在这种情况下，平均时间复杂度是 O($n^2$)。</li>
</ul>
<p>由此可见，和冒泡排序一样，插入排序的时间复杂度是 O($n^2$)，并且它也是一种稳定的排序算法。</p>
</li>
</ol>
<p>E.g.: LeetCode 147 链表插入排序</p>
<h2 id="归并"><a href="#归并" class="headerlink" title="归并"></a>归并</h2><blockquote>
<p>基本思想：分治</p>
<p>不断分组到很小的数组，然后进行排序合并</p>
</blockquote>
<h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="keyword">int</span>[] A, <span class="keyword">int</span> lo, <span class="keyword">int</span> hi)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 判断是否只剩下最后一个元素</span></span><br><span class="line">  <span class="keyword">if</span> (lo &gt;= hi) <span class="keyword">return</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 从中间将数组分成两个部分</span></span><br><span class="line">  <span class="keyword">int</span> mid = lo + (hi - lo) / <span class="number">2</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 分别递归地将左右两半排好序</span></span><br><span class="line">  sort(A, lo, mid);</span><br><span class="line">  sort(A, mid + <span class="number">1</span>, hi);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将排好序的左右两半合并  </span></span><br><span class="line">  merge(A, lo, mid, hi);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//归并操作</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> lo, <span class="keyword">int</span> mid, <span class="keyword">int</span> hi)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 复制一份原来的数组</span></span><br><span class="line">    <span class="keyword">int</span>[] copy = nums.clone();</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 定义一个 k 指针表示从什么位置开始修改原来的数组，i 指针表示左半边的起始位置，j 表示右半边的起始位置</span></span><br><span class="line">    <span class="keyword">int</span> k = lo, i = lo, j = mid + <span class="number">1</span>;</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">while</span> (k &lt;= hi) &#123;</span><br><span class="line">        <span class="keyword">if</span> (i &gt; mid) &#123;</span><br><span class="line">            nums[k++] = copy[j++];</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (j &gt; hi) &#123;</span><br><span class="line">          nums[k++] = copy[i++];</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (copy[j] &lt; copy[i]) &#123;</span><br><span class="line">          nums[k++] = copy[j++];</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          nums[k++] = copy[i++];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="分析-2"><a href="#分析-2" class="headerlink" title="分析"></a>分析</h3><h4 id="空间复杂度-2"><a href="#空间复杂度-2" class="headerlink" title="空间复杂度"></a>空间复杂度</h4><p>由于合并 n 个元素需要分配一个大小为 n 的额外数组，合并完成之后，这个数组的空间就会被释放，所以算法的空间复杂度就是 O(n)。归并排序也是稳定的排序算法。</p>
<h4 id="时间复杂度-2"><a href="#时间复杂度-2" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><blockquote>
<p>归并算法是一个不断递归的过程。</p>
</blockquote>
<p><strong>举例：</strong>数组的元素个数是 n，时间复杂度是 T(n) 的函数。</p>
<p><strong>解法：</strong>把这个规模为 n 的问题分成两个规模分别为 n/2 的子问题，每个子问题的时间复杂度就是 T(n/2)，那么两个子问题的复杂度就是 2×T(n/2)。当两个子问题都得到了解决，即两个子数组都排好了序，需要将它们合并，一共有 n 个元素，每次都要进行最多 n-1 次的比较，所以合并的复杂度是 O(n)。由此我们得到了递归复杂度公式：T(n) = 2×T(n/2) + O(n)。</p>
<p> 对于公式求解，不断地把一个规模为 n 的问题分解成规模为 n/2 的问题，一直分解到规模大小为 1。如果 n 等于 2，只需要分一次；如果 n 等于 4，需要分 2 次。这里的次数是按照规模大小的变化分类的。</p>
<p> 以此类推，对于规模为 n 的问题，一共要进行 log(n) 层的大小切分。在每一层里，我们都要进行合并，所涉及到的元素其实就是数组里的所有元素，因此，每一层的合并复杂度都是 O(n)，所以整体的复杂度就是 O(nlogn)。</p>
<h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><blockquote>
<p>采用了分治的思想</p>
</blockquote>
]]></content>
      <categories>
        <category>Feedback</category>
      </categories>
      <tags>
        <tag>面试总结</tag>
      </tags>
  </entry>
  <entry>
    <title>Advance-Data-Structure</title>
    <url>/2020/07/13/Advance%20Data%20Structure/</url>
    <content><![CDATA[<blockquote>
<p>总结一部分数据结构</p>
</blockquote>
<h2 id="优先队列"><a href="#优先队列" class="headerlink" title="优先队列"></a>优先队列</h2><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p>能保证每次取出的元素都是队列中优先级别最高的，优先级别是可以自定义的。评判标准如数值大小以及更加复杂的计算。</p>
<h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>从一堆杂乱无章的数据当中按照一定的顺序(或优先级)逐步筛选出部分乃至全部的数据。</p>
<p><strong>For Example:</strong> 任意一个数组，找出前 k 大的数。</p>
<p><strong>解法1：</strong>先对数组进行排序，然后依次输出前 k 大的数，复杂度将会是 O(nlogn)，其中 n 是数组的元素个数。</p>
<p><strong>解法2：</strong>使用优先队列，复杂度优化成 O(k + nlogk)。当数据量很大(即 n 很大时)，而 k 相对较小，显然使用优先级队列可以有效地降低算法的复杂度。要找出前 k 大的数，不需要对所有的数进行排序。</p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>优先级的本质是一个二叉堆(Binary Heap)，利用一个数组结构来实现的<a href="https://www.cnblogs.com/-citywall123/p/11788764.html" target="_blank" rel="noopener">完全二叉树</a>。换句话说，优先队列的本质是一个数组，数组里的每个元素既有可能是其他元素的父节点，也有可能是其他元素的子节点，而且每个父节点只能有两个子节点，很像一个二叉树的结构。</p>
<hr>
<p>若设二叉树的深度为k，除第 k 层外，其它各层 (1～k-1) 的结点数都达到最大个数，第k 层所有的结点都<strong>连续集中在最左边</strong>，这就是完全二叉树。</p>
<p><img src="https://img2018.cnblogs.com/i-beta/1468919/201911/1468919-20191103194739538-2034251878.png" alt="img"></p>
<hr>
<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ol>
<li>数组里的第一个元素 array[0] 拥有最高的优先级别</li>
<li>给定一个下标 i ，那么对于元素 array[i] 而言：<ul>
<li>它的父节点所对应的元素下标是 (i - 1) / 2</li>
<li>它的左孩子对应元素下标是 2 x i + 1</li>
<li>它的右孩子所对应的元素下标是 2 × i + 2</li>
</ul>
</li>
<li>数组里每个元素的优先级别都要高于它两个孩子的优先级别。</li>
</ol>
<h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><h4 id="向上筛选-sift-up-bubble-up"><a href="#向上筛选-sift-up-bubble-up" class="headerlink" title="向上筛选( sift up / bubble up)"></a>向上筛选( sift up / bubble up)</h4><ul>
<li><p>当有新的数据加入到优先队列中，新的数据首先被放置在二叉堆的底部。</p>
</li>
<li><p>不断进行向上筛选的操作，即如果发现该数据的优先级别比父节点的优先级别还要高，那么就和父节点的元素相互交换，再接着往上进行比较，直到无法再继续交换为止。</p>
</li>
</ul>
<img src="http://s0.lgstatic.com/i/image2/M01/90/EC/CgotOV2ISXaAJ9iGACXUNreouXo038.gif" alt="img" style="zoom:50%;" />

<p><strong>时间复杂度：</strong>由于二叉堆是一棵完全二叉树，并假设堆的大小为 k，因此整个过程其实就是沿着树的高度往上爬，所以只需要 O(logk) 的时间。</p>
<h4 id="向下筛选-sift-down-bubble-down"><a href="#向下筛选-sift-down-bubble-down" class="headerlink" title="向下筛选( sift down / bubble down)"></a>向下筛选( sift down / bubble down)</h4><ul>
<li><p>当堆顶的元素被取出时，要更新堆顶的元素来作为下一次按照优先级顺序被取出的对象，需要将堆底部的元素放置到堆顶，然后不断地对它执行向下筛选的操作。</p>
</li>
<li><p>将该元素和它的两个孩子节点对比优先级，如果优先级最高的是其中一个孩子，就将该元素和那个孩子进行交换，然后反复进行下去，直到无法继续交换为止。</p>
</li>
</ul>
<img src="http://s0.lgstatic.com/i/image2/M01/90/CC/CgoB5l2ISa-Af-7tAB97MaSBBWo211.gif" alt="img" style="zoom:50%;" />

<p><strong>时间复杂度：</strong>整个过程就是沿着树的高度往下爬，所以时间复杂度也是 O(logk)。</p>
<blockquote>
<p>因此，无论是添加新的数据还是取出堆顶的元素，都需要 O(logk) 的时间。</p>
</blockquote>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>优先队列的初始化是一个最重要的时间复杂度，是分析运用优先队列性能时必不可少的，也是经常容易弄错的地方。</p>
<p><strong>举例：</strong>有 n 个数据，需要创建一个大小为 n 的堆。</p>
<p><strong>误区：</strong>每当把一个数据加入到堆里，都要对其执行向上筛选的操作，这样一来就是 O(nlogn)。</p>
<p><strong>解法：</strong>在创建这个堆的过程中，二叉树的大小是从 1 逐渐增长到 n 的，所以整个算法的复杂度经过推导，最终的结果是 O(n)。</p>
<blockquote>
<p>初始化一个大小为 n 的堆，所需要的时间是 O(n) 。</p>
</blockquote>
<p><strong>E.g.:</strong> LeetCode 347: 给定一个非空的整数数组，返回其中出现频率前 k 高的元素。</p>
<p><strong>说明：</strong></p>
<p>你可以假设给定的 k 总是合理的，且 1 ≤ k ≤ 数组中不相同的元素的个数。</p>
<p>你的算法的时间复杂度必须优于 O(nlogn) ，n 是数组的大小</p>
<p><strong>示例：</strong>car，car，book，desk，desk，desk</p>
<p><strong>解题思路</strong></p>
<p>这道题的输入是一个字符串数组，数组里的元素可能会重复一次甚至多次，要求按顺序输出前 k 个出现次数最多的字符串。</p>
<ul>
<li><p>解这类求”前 k 个”的题目，关键是看如何定义优先级以及优先队列中元素的数据结构。</p>
</li>
<li><p>题目中有”前 k 个“这样的字眼，应该很自然地联想到优先队列。</p>
</li>
<li><p>优先级别可以由字符串出现的次数来决定，出现的次数越多，优先级别越高，反之越低。</p>
</li>
<li><p>统计词频的最佳数据结构就是哈希表（Hash Map），利用一个哈希表，就能快速地知道每个单词出现的次数。</p>
</li>
<li><p>将单词和其出现的次数作为一个新的对象来构建一个优先队列，那么这个问题就很轻而易举地解决了。</p>
</li>
</ul>
<p><strong>建议：</strong>这道题是利用优先队列处理问题的典型，建议好好练习。</p>
<pre><code>    Desk (3)
    /    \
car(2)   book(1)          </code></pre><h2 id="图"><a href="#图" class="headerlink" title="图"></a>图</h2><h3 id="基本知识点"><a href="#基本知识点" class="headerlink" title="基本知识点"></a>基本知识点</h3><p>图可以说是所有数据结构里面<strong>知识点最丰富</strong>的一个，最基本的知识点如下。</p>
<ul>
<li><p>阶（Order）、度：出度（Out-Degree）、入度（In-Degree）</p>
</li>
<li><p>树（Tree）、森林（Forest）、环（Loop）</p>
</li>
<li><p>有向图（Directed Graph）、无向图（Undirected Graph）、完全有向图、完全无向图</p>
</li>
<li><p>连通图（Connected Graph）、连通分量（Connected Component）</p>
</li>
<li><p>存储和表达方式：邻接矩阵（Adjacency Matrix）、邻接链表（Adjacency List）</p>
</li>
</ul>
<p>围绕图的<strong>算法</strong>也是五花八门。</p>
<ul>
<li><p>图的遍历：深度优先、广度优先</p>
</li>
<li><p>环的检测：有向图、无向图</p>
</li>
<li><p>拓扑排序</p>
</li>
<li><p>最短路径算法：Dijkstra、Bellman-Ford、Floyd Warshall</p>
</li>
<li><p>连通性相关算法：Kosaraju、Tarjan、求解孤岛的数量、判断是否为树</p>
</li>
<li><p>图的着色、旅行商问题等</p>
</li>
</ul>
<h3 id="Important"><a href="#Important" class="headerlink" title="Important"></a>Important</h3><blockquote>
<p>必须掌握</p>
</blockquote>
<ul>
<li><p>图的存储和表达方式：邻接矩阵（Adjacency Matrix）、邻接链表（Adjacency List）</p>
</li>
<li><p>图的遍历：深度优先、广度优先</p>
</li>
<li><p>二部图的检测（Bipartite）、树的检测、环的检测：有向图、无向图</p>
</li>
<li><p>拓扑排序</p>
</li>
<li><p>联合-查找算法（Union-Find）</p>
</li>
<li><p>最短路径：Dijkstra、Bellman-Ford</p>
</li>
</ul>
<p>其中，环的检测、二部图的检测、树的检测以及拓扑排序都是基于图的遍历，尤其是深度优先方式的遍历，而遍历可以在邻接矩阵或者邻接链表上进行。</p>
<p>至于最短路径算法，应该要能区分不同的特点，明确要用的算法。</p>
<p><strong>E.g.：</strong> LeetCode 785：给定一个无向图 graph，当这个图为二部图时返回 true。</p>
<p><strong>提示：</strong>如果能将一个图的节点集合分割成两个独立的子集 A 和 B，并使图中的每一条边的两个节点一个来自 A 集合，一个来自 B 集合，就将这个图称为二部图。</p>
<p><strong>解题思路：</strong>判断一个给定的任意图是否为二部图，就必须要对该图进行一次遍历：①深度优先；②广度优先</p>
<p>二部图，图的所有顶点可以分为两个子集 U 和 V，子集里的顶点互不直接相连，图里面所有边，一头连着子集 U 里的顶点，一头连着子集 V 里的顶点。</p>
<img src="http://s0.lgstatic.com/i/image2/M01/90/EC/CgotOV2ISc-ADjNDAK_6wbp-nzI430.gif" alt="img" style="zoom:50%;" />

<p><strong>解析：</strong></p>
<ol>
<li><p>给图里的顶点涂上颜色，子集 U 里的顶点都涂上红色，子集 V 里的顶点都涂上蓝色。</p>
</li>
<li><p>开始遍历这个图的所有顶点，想象一下手里握有红色和蓝色的画笔，每次交替地给遍历当中遇到的顶点涂上颜色。</p>
</li>
<li><p>如果这个顶点还没有颜色，那就给它涂上颜色，然后换成另外一支画笔。</p>
</li>
<li><p>下一个顶点，如果发现这个顶点已经涂上了颜色，而且颜色跟我手里画笔的颜色不同，那么表示这个顶点它既能在子集 U 里，也能在子集 V 里。</p>
</li>
<li><p>所以，它不是一个二部图。</p>
</li>
</ol>
<h2 id="前缀树（Trie）"><a href="#前缀树（Trie）" class="headerlink" title="前缀树（Trie）"></a>前缀树（Trie）</h2><h3 id="应用场景-1"><a href="#应用场景-1" class="headerlink" title="应用场景"></a>应用场景</h3><p>前缀树被广泛地运用在字典查找当中，也被称为字典树。</p>
<p><strong>举例：</strong>给定一系列字符串，这些字符串构成了一种字典，要求你在这个字典当中找出所有以“ABC”开头的字符串。</p>
<p><strong>解法 1：暴力搜索</strong></p>
<p>直接遍历一遍字典，然后逐个判断每个字符串是否由“ABC”开头。假设字典很大，有 N 个单词，要对比的不是“ABC”，而是任意的，那不妨假设所要对比的开头平均长度为 M，那么时间复杂度是 O(M×N)。</p>
<p><strong>解法 2：前缀树</strong></p>
<p>如果用前缀树头帮助对字典的存储进行优化，那么可以把搜索的时间复杂度下降为 O(M)，其中 M 表示字典里最长的那个单词的字符个数，在很多情况下，字典里的单词个数 N 是远远大于 M 的。因此，前缀树在这种场合中是非常高效的。</p>
<p><strong>经典应用</strong></p>
<ul>
<li><p>网站上的搜索框会罗列出以搜索文字作为开头的相关搜索信息，这里运用了前缀树进行后端的快速检索。</p>
</li>
<li><p>汉字拼音输入法的联想输出功能也运用了前缀树。</p>
</li>
</ul>
<p><strong>举例：</strong>假如有一个字典，字典里面有如下词：”A”，”to”，”tea”，”ted”，”ten”，”i”，”in”，”inn”，每个单词还能有自己的一些权重值，那么用前缀树来构建这个字典将会是如下的样子：</p>
<p><img src="http://s0.lgstatic.com/i/image2/M01/90/D0/CgotOV2ILXyAAbuPAAHuMjoQ0_M307.png" alt="img"></p>
<h3 id="性质-1"><a href="#性质-1" class="headerlink" title="性质"></a>性质</h3><ol>
<li><p>每个节点至少包含两个基本属性。</p>
<ul>
<li>children：数组或者集合，罗列出每个分支当中包含的所有字符</li>
<li>isEnd：布尔值，表示该节点是否为某字符串的结尾</li>
</ul>
</li>
<li><p>前缀树的根节点是空的</p>
<ul>
<li>所谓空，即只利用到这个节点的 children 属性，即只关心在这个字典里，有哪些打头的字符。</li>
</ul>
</li>
<li><p>除了根节点，其他所有节点都有可能是单词的结尾，叶子节点一定都是单词的结尾。</p>
</li>
</ol>
<h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><blockquote>
<p>前缀树最基本的操作就是两个：创建和搜索。</p>
</blockquote>
<ol>
<li>创建</li>
</ol>
<blockquote>
<ul>
<li><p>遍历一遍输入的字符串，对每个字符串的字符进行遍历</p>
</li>
<li><p>从前缀树的根节点开始，将每个字符加入到节点的 children 字符集当中。</p>
</li>
<li><p>如果字符集已经包含了这个字符，则跳过。</p>
</li>
<li><p>如果当前字符是字符串的最后一个，则把当前节点的 isEnd 标记为真。</p>
</li>
</ul>
<blockquote>
<p>由上，创建的方法很直观。</p>
</blockquote>
<p><strong>前缀树真正强大的地方在于</strong>，每个节点还能用来保存额外的信息，比如可以用来记录拥有相同前缀的所有字符串。因此，当用户输入某个前缀时，就能在 O(1) 的时间内给出对应的推荐字符串。</p>
</blockquote>
<ol start="2">
<li>搜索</li>
</ol>
<blockquote>
<p>与创建方法类似，从前缀树的根节点出发，逐个匹配输入的前缀字符，如果遇到了就继续往下一层搜索，如果没遇到，就立即返回。</p>
</blockquote>
<h3 id="例题分析"><a href="#例题分析" class="headerlink" title="例题分析"></a>例题分析</h3><p>LeetCode 第 212 题：给定一个二维网格 board 和一个字典中的单词列表 words，找出所有同时在二维网格和字典中出现的单词。</p>
<p><img src="http://s0.lgstatic.com/i/image2/M01/90/B0/CgoB5l2ILXyAYLwPAAAO6ajgsHk324.png" alt="img"></p>
<p>单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母在一个单词中不允许被重复使用。</p>
<p>说明：你可以假设所有输入都由小写字母 a-z 组成。</p>
<h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><p>这是一道出现较为频繁的难题，题目给出了一个二维的字符矩阵，然后还给出了一个字典，现在要求在这个字符矩阵中找到出现在字典里的单词。</p>
<p>由于字符矩阵的每个点都能作为一个字符串的开头，所以必须得尝试从矩阵中的所有字符出发，上下左右一步步地走，然后去和字典进行匹配，如果发现那些经过的字符能组成字典里的单词，就把它记录下来。 </p>
<blockquote>
<p>可以借用深度优先的算法来实现</p>
</blockquote>
<p><img src="http://s0.lgstatic.com/i/image2/M01/90/CD/CgoB5l2IShaAfIDFAAElACD4d7I232.png" alt="img"></p>
<h3 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h3><p><strong>字典匹配的解法 1：</strong>每次都循环遍历字典，看看是否存在字典里面，如果把输入的字典变为哈希集合的话，似乎只需要 O(1) 的时间就能完成匹配。</p>
<p>但是，这样并不能进行前缀的对比，即，必须每次都要进行一次全面的深度优先搜索，或者搜索的长度为字典里最长的字符串长度，这样还是不够高效。</p>
<p><strong>字典匹配的解法 2：</strong>对比字符串的前缀，借助前缀树来重新构建字典。</p>
<p>假如在矩阵里遇到了一个字符”V”，而字典里根本就没有以“V”开头的字符串，则不需要将深度优先搜索进行下去，可以大大地提高搜索效率。</p>
<p>构建好了前缀树之后，每次从矩阵里的某个字符出发进行搜索的时候，同步地对前缀树进行对比，如果发现字符一直能被找到，就继续进行下去，一步一步地匹配，直到在前缀树里发现一个完整的字符串，把它输出即可。</p>
<h2 id="线段树-Segment-Tree"><a href="#线段树-Segment-Tree" class="headerlink" title="线段树(Segment Tree)"></a>线段树(Segment Tree)</h2><p><strong>举例：</strong>假设有一个数组 array[0 … n-1]， 里面有 n 个元素，现在要经常对这个数组做两件事。</p>
<ul>
<li><p>更新数组元素的数值</p>
</li>
<li><p>求数组任意一段区间里元素的总和（或者平均值）</p>
</li>
</ul>
<p><strong>解法 1：</strong>遍历一遍数组。</p>
<blockquote>
<p>时间复杂度 O(n)。</p>
</blockquote>
<p><strong>解法 2：</strong>线段树。</p>
<ul>
<li><p>线段树，就是一种按照二叉树的形式存储数据的结构，每个节点保存的都是数组里某一段的总和。</p>
</li>
<li><p>适用于数据很多，而且需要频繁更新并求和的操作。</p>
</li>
</ul>
<blockquote>
<p>时间复杂度 O(logn)。</p>
</blockquote>
<h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><p><strong>举例：</strong>数组是 [1, 3, 5, 7, 9, 11]，那么它的线段树如下。</p>
<p><img src="http://s0.lgstatic.com/i/image2/M01/90/D0/CgotOV2ILX2AB5E_AABPrKDb2WM573.png" alt="img"></p>
<p>根节点保存的是从下标 0 到下标 5 的所有元素的总和，即 36。左右两个子节点分别保存左右两半元素的总和。按照这样的逻辑不断地切分下去，最终的叶子节点保存的就是每个元素的数值。</p>
<p><strong>解法：</strong></p>
<ol>
<li>更新数组里某个元素的数值</li>
</ol>
<p>从线段树的根节点出发，更新节点的数值，它保存的是数组元素的总和。修改的元素有可能会落在线段树里一些区间里，至少叶子节点是肯定需要更新的，所以，要做的是从根节点往下，判断元素的下标是否在左边还是右边，然后更新分支里的节点大小。因此，复杂度就是遍历树的高度，即 O(logn)。</p>
<ol start="2">
<li>对数组某个区间段里的元素进行求和</li>
</ol>
<p>方法和更新操作类似，首先从根节点出发，判断所求的区间是否落在节点所代表的区间中。如果所要求的区间完全包含了节点所代表的区间，那么就得加上该节点的数值，意味着该节点所记录的区间总和只是所要求解总和的一部分。接下来，不断地往下寻找其他的子区间，最终得出所要求的总和。</p>
<p>建议：线段树的实现书写起来有些繁琐，需要不断地练习。</p>
<h3 id="例题分析-1"><a href="#例题分析-1" class="headerlink" title="例题分析"></a>例题分析</h3><p><strong>LeetCode 第 315 题：</strong>给定一个整数数组 nums，按要求返回一个新数组 counts，使得数组 counts 有该性质——counts[i] 的值是 nums[i] 右侧小于 nums[i] 的元素的数量。</p>
<blockquote>
<p>示例</p>
</blockquote>
<blockquote>
<p>输入：[5, 2, 6, 1]</p>
</blockquote>
<blockquote>
<p>输出：[2, 1, 1, 0] </p>
</blockquote>
<blockquote>
<p>解释</p>
</blockquote>
<ul>
<li><p>5 的右侧有 2 个更小的元素（2 和 1）</p>
</li>
<li><p>2 的右侧仅有 1 个更小的元素（1）</p>
</li>
<li><p>6 的右侧有 1 个更小的元素（1）</p>
</li>
<li><p>1 的右侧有 0 个更小的元素</p>
</li>
</ul>
<h3 id="解题思路-1"><a href="#解题思路-1" class="headerlink" title="解题思路"></a>解题思路</h3><p>给定一个数组 nums，里面都是一些整数，现在要求打印输出一个新的数组 counts，counts 数组的每个元素 counts[i] 表示 nums 中第 i 个元素右边有多少个数小于 nums[i]。</p>
<p>例如，输入数组是 [5, 2, 6, 1]，应该输出的结果是 [2, 1, 1, 0]。</p>
<p> 因为，对于 5，右边有两个数比它小，分别是 2 和 1，所以输出的结果中，第一个元素是 2；对于 2，右边只有 1 比它小，所以第二个元素是 1，类推。</p>
<p>如果使用线段树解法，需要理清线段树的每个节点应该需要包含什么样的信息。</p>
<p>线段树每个节点记录的区间是数组下标所形成的区间，然而对于这道题，因为要统计的是比某个数还要小的数的总和，如果把分段的区间设计成按照数值的大小来划分，并记录下在这个区间中的数的总和，就能快速地知道比当前数还要小的数有多少个。</p>
<p><img src="http://s0.lgstatic.com/i/image2/M01/90/ED/CgotOV2IStmAJxNFAHcB4XzkqCg286.gif" alt="img"></p>
<ol>
<li><p>首先，让从线段树的根节点开始，根节点记录的是数组里最小值到最大值之间的所有元素的总和，然后分割根节点成左区间和右区间，不断地分割下去。</p>
</li>
<li><p>初始化，每个节点记录的在此区间内的元素数量是 0，接下来从数组的最后一位开始往前遍历，每次遍历，判断这个数落在哪个区间，那么那个区间的数量加一。</p>
</li>
<li><p>遇到 1，把它加入到线段树里，此时线段树里各个节点所统计的数量会发生变化。</p>
</li>
<li><p>当前所遇到的最小值就是 1。</p>
</li>
<li><p>把 6 加入到线段树里。</p>
</li>
<li><p>求比 6 小的数有多少个，即查询线段树，从 1 到 5 之间有多少个数。</p>
</li>
<li><p>从根节点开始查询。由于所要查询的区间是 1 到 5，无法包含根节点的区间 1 到 6，所以继续往下查询。</p>
</li>
<li><p>左边，区间 1 到 3 被完全包含在 1 到 5 之间，把该节点所统计好的数返回。</p>
</li>
<li><p>右边，区间 1 到 5 跟区间 4 到 6 有交叉，继续往下看，区间 4 到 5 完全被包含在 1 到 5 之间，所以可以马上返回，并把统计的数量相加。</p>
</li>
<li><p>最后得出，在当前位置，在 6 的右边比 6 小的数只有一个。</p>
</li>
</ol>
<blockquote>
<p>通过这样的方法，每次把当前的数用线段树进行个数统计，然后再计算出比它小的数即可。算法复杂度是 O(nlogm)。</p>
</blockquote>
<h2 id="树状数组（Fenwick-Tree-Binary-Indexed-Tree）"><a href="#树状数组（Fenwick-Tree-Binary-Indexed-Tree）" class="headerlink" title="树状数组（Fenwick Tree / Binary Indexed Tree）"></a>树状数组（Fenwick Tree / Binary Indexed Tree）</h2><h3 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h3><p><strong>举例：</strong>假设有一个数组 array[0 … n-1]， 里面有 n 个元素，现在要经常对这个数组做两件事。</p>
<ul>
<li><p>更新数组元素的数值</p>
</li>
<li><p>求数组前 k 个元素的总和（或者平均值）</p>
</li>
</ul>
<p><strong>解法 1：</strong>线段树。</p>
<p>线段树能在 O(logn) 的时间里更新和求解前 k 个元素的总和。</p>
<p><strong>解法 2：</strong>树状数组。</p>
<ul>
<li><p>该问题只要求求解前 k 个元素的总和，并不要求任意一个区间。</p>
</li>
<li><p>树状数组可以在 O(logn) 的时间里完成上述的操作。</p>
</li>
<li><p>相对于线段树的实现，树状数组显得更简单。</p>
</li>
</ul>
<h3 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h3><blockquote>
<p>树状数组的数据结构有以下几个重要的基本特征。</p>
</blockquote>
<ul>
<li><p>它是利用数组来表示多叉树的结构，在这一点上和优先队列有些类似，只不过，优先队列是用数组来表示完全二叉树，而树状数组是多叉树。</p>
</li>
<li><p>树状数组的第一个元素是空节点。</p>
</li>
<li><p>如果节点 tree[y] 是 tree[x] 的父节点，那么需要满足条件：y = x - (x &amp; (-x))。</p>
</li>
</ul>
<blockquote>
<p>建议：由于树状数组所解决的问题跟线段树有些类似，所以不花篇幅进行问题的讨论。LeetCode 上有很多经典的题目可以用树状数组来解决，比如 LeetCode 第 308 题，求一个动态变化的二维矩阵里，任意子矩阵里的数的总和。</p>
</blockquote>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ol>
<li>优先队列</li>
</ol>
<p>经常出现在考题里的，它的实现过程比较繁琐，但是很多编程语言里都有它的实现，所以在解决面试中的问题时，实行“拿来主义”即可。</p>
<p>鼓励你自己练习实现一个优先队列，在实现它的过程中更好地去了解它的结构和特点。</p>
<ol start="2">
<li>图</li>
</ol>
<p>被广泛运用的数据结构，很多涉及大数据的问题都得运用到图论的知识。</p>
<p>比如在社交网络里，每个人可以用图的顶点表示，人与人直接的关系可以用图的边表示；再比如，在地图上，要求解从起始点到目的地，如何行驶会更快捷，需要运用图论里的最短路径算法。</p>
<ol start="3">
<li>前缀树</li>
</ol>
<p>出现在许多面试的难题当中。</p>
<p>因为很多时候你得自己实现一棵前缀树，所以你要能熟练地书写它的实现以及运用它。</p>
<ol start="4">
<li>线段树和树状数组</li>
</ol>
<p>应用场合比较明确。</p>
<p>例如，问题变为在一幅图片当中修改像素的颜色，然后求解任意矩形区间的灰度平均值，那么可以考虑采用二维的线段树了。</p>
]]></content>
      <categories>
        <category>Feedback</category>
      </categories>
      <tags>
        <tag>面试总结</tag>
      </tags>
  </entry>
  <entry>
    <title>Combine RNN with Neural ODEs</title>
    <url>/2020/05/13/Combine-RNN-with-NeuralODES/</url>
    <content><![CDATA[<center><font size=5><b>Intro to Neural ODEs</b></font></center>
<div align=right><a href="https://medium.com/@abaietto5297" target="_blank" rel="noopener" >——Reference</a></div>


<h1 id="ResNets"><a href="#ResNets" class="headerlink" title="ResNets"></a>ResNets</h1><blockquote>
<p>Neural ODEs comes from ResNets</p>
</blockquote>
<p><strong>As these models grew to hundreds of layers deep, ResNets’  performance decreased. Deep learning had reached its limit. We need state-of-the-art performance to train deeper networks.</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200507170347952.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>It also directly adds the input to the output, this shortcut connection improves the model since, at the worst, the residual block does not do anything.</strong></p>
<p><strong>One final thought. A ResNet can be described by the following equation:</strong></p>
<p>$$<br>h_{t+1} = h_{t}+f(h_{t},\theta_{t})<br>$$</p>
<blockquote>
<p>h - value of the hidden layer;<br>t - tell us which layer we are look at</p>
</blockquote>
<p><strong>the next hidden layer is the sum of the input and a function of the input as we have seen.</strong> </p>
<blockquote>
<p>Find introduction to ResNets in <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">Reference</a></p>
</blockquote>
<h1 id="Euler’s-Method"><a href="#Euler’s-Method" class="headerlink" title="Euler’s Method"></a>Euler’s Method</h1><blockquote>
<p>How Neural ODEs work</p>
</blockquote>
<p><strong>Above equation seems like calculus, and if you don’t remember from calculus class, the Euler’s method is the simplest way to approximate the solution of a differential equation with initial value.</strong></p>
<p>$$<br>Initial value problem:y’(t)=f(t,y(t)), y(t_{0})=y_{0}<br>$$</p>
<p>$$<br>Euler’s Method: y_{n+1}=y_{n}+hf(t_{n},y_{n})<br>$$</p>
<blockquote>
<p>through this we can find numerical approximation</p>
</blockquote>
<p><strong>Euler’s method and ResNets equation are identical, the only difference being the step size $h$, that is multiplied by the function. Because of this similarity, we can think ResNets is underlying differential equation.</strong> </p>
<p><strong>Instead of going from diffeq to Euler’s method, we can reverse engineer the problem. Starting from the ResNet, the resulting differential equation is</strong></p>
<p>$$<br>Neural ODE: {\frac{dh(t)}{dt}} = f(h(t),t,\theta)<br>$$</p>
<p><strong>which describes the dynamics of our model.</strong></p>
<h1 id="The-Basics"><a href="#The-Basics" class="headerlink" title="The Basics"></a>The Basics</h1><blockquote>
<p>how a Neural ODE works</p>
</blockquote>
<p><strong>The Neural ODEs combines two concepts: deep learning and differential equations, we use the most simple methods - Euler’s method to make predictions.</strong></p>
<p><strong>Q:</strong><br>&ensp;&ensp;How do we train it?<br><strong>A:</strong><br>&ensp;&ensp;Adjoint method</p>
<p><strong>Include using another numerical solver to run backwards through time (backpropagating) and updating the model’s parameters.</strong></p>
<ul>
<li><strong>Defining the architecture:</strong></li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200507195623982.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ul>
<li><strong>Defining a neural ODE:</strong></li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200507195830197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li><strong>Put it all together into one:</strong></li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200507201039301.png" alt="在这里插入图片描述"></p>
<h1 id="Adjoint-Method"><a href="#Adjoint-Method" class="headerlink" title="Adjoint Method"></a>Adjoint Method</h1><blockquote>
<p>part 3 - how a Neural ODE backpropagates with the adjoint method<br>this part - Adjoint Method</p>
</blockquote>
<h1 id="Model-Comparison"><a href="#Model-Comparison" class="headerlink" title="Model Comparison"></a>Model Comparison</h1><blockquote>
<p>Start with a simple machine learning model to showcase its strengths and weaknesses</p>
</blockquote>
<ul>
<li><p><strong>ResNets model with lower time per epoch, and Neural ODEs model with more time.</strong></p>
</li>
<li><p><strong>ResNets model with more memory, and ODE model with O(1) space usage.</strong></p>
</li>
</ul>
<p><strong>Overall, one of the main benefits is the constant memory usage while training the model. However, this comes at the cost of training time.</strong></p>
<h1 id="VAE-Variational-Autoencoders"><a href="#VAE-Variational-Autoencoders" class="headerlink" title="VAE[Variational Autoencoders]"></a>VAE[Variational Autoencoders]</h1><blockquote>
<p>Premise:</p>
<blockquote>
<p>Generative model: Able to generate samples like those in training data</p>
</blockquote>
</blockquote>
<p><strong>VAE is a directed generative model with observed and latent variables, which give us a atent space to sample from.</strong></p>
<p><strong>In view of the application that interpolate between sentences, Ｉ will use VAE to connect RNN and ODE．</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200513211119741.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt=""></p>
<center>VAE Architecture</center>

<p><img src="https://img-blog.csdnimg.cn/20200513211818629.png#pic_center" alt="在这里插入图片描述"></p>
<center>VAE Design</center>

<p><strong>When as Inference model, the input $x$ is passed to the encoder network, producing an approximate posterior $q(z|x)$ over latent variables.</strong></p>
<blockquote>
<p>Sentence prediction by conventional autoencoder</p>
<blockquote>
<p>Sentences produced by greedily decoding from points between two sentence encodings with a conventional autoencoder. The intermediate sentences are not plausible English.</p>
</blockquote>
<p>VAE Language Model</p>
<blockquote>
<p>Words are represented using a learned dictionary of embedding words</p>
</blockquote>
<p>VAE sentence interpolation </p>
<blockquote>
<ul>
<li>Paths between random points in VAE space</li>
<li>Intermediate sentences are grammatical</li>
<li>Topic and syntactic structure are consistent</li>
</ul>
</blockquote>
</blockquote>
<h1 id="Breakdown-of-another-deep-learning-breakthrough"><a href="#Breakdown-of-another-deep-learning-breakthrough" class="headerlink" title="Breakdown of another deep learning breakthrough"></a>Breakdown of another deep learning breakthrough</h1><p><img src="https://img-blog.csdnimg.cn/20200508200122409.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li><strong>First, we encode the input sequence with some “standard” time series algorithms, let’s say RNN to obtain the primary embedding of the process</strong></li>
<li><strong>Run the embedding through the Neural ODE to get the “continuous” embedding</strong></li>
<li><strong>Recover initial sequence from the “continuous” embedding in VAE fashion</strong></li>
</ul>
<h2 id="VAE-as-a-generative-model"><a href="#VAE-as-a-generative-model" class="headerlink" title="VAE as a generative model"></a>VAE as a generative model</h2><blockquote>
<p>variational autoencoder approach</p>
</blockquote>
<p><strong>A generative model through sampling procedure:</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200508214509100.png#pic_center" alt="在这里插入图片描述"><br><strong>Training :</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200508215054992.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="Define-Model"><a href="#Define-Model" class="headerlink" title="Define Model"></a>Define Model</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNEncoder</span><span class="params">(nn.moudle)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_dim,hidden_dim,latent_dim)</span>:</span></span><br><span class="line">		super(RNNEncoder,self).__init__()</span><br><span class="line">		self.input_dim = input_dim</span><br><span class="line">		self.hidden_dim = hidden_dim</span><br><span class="line">		self.latent_dim = latent_dim</span><br><span class="line"></span><br><span class="line">		self.rnn = nn.GRU(input_dim+<span class="number">1</span>,hidden_dim)</span><br><span class="line">		self.hid2lat = nn.Linear(hidden_dim,<span class="number">2</span>*latent_dim)</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x,t)</span>:</span></span><br><span class="line">		<span class="comment"># Concatenate time to input</span></span><br><span class="line">		t = t.clone()</span><br><span class="line">		t[<span class="number">1</span>:] = t[:<span class="number">-1</span>] = t[<span class="number">1</span>:]</span><br><span class="line">		t[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">		xt = torch.cat((x,t),dim=<span class="number">-1</span>)</span><br><span class="line">		_,h0 = self.rnn(xt.flip((<span class="number">0</span>,))) <span class="comment">#Reversed</span></span><br><span class="line">		<span class="comment"># Compute latent dimension</span></span><br><span class="line">		z0 = self.hid2lat(h0[<span class="number">0</span>])</span><br><span class="line">		z0_mean = z0[:,:self.latent_dim]</span><br><span class="line">		z0_log_var = z0[:,self.latent_dim:]</span><br><span class="line">		<span class="keyword">return</span> z0_mean,z0_log_var</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralODEDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,output_dim,hidden_dim,latent_dim)</span>:</span></span><br><span class="line">		super(NeuralODEDcoder,self).__init__()</span><br><span class="line">		self.output_dim = output_dim</span><br><span class="line">		self.hidden_dim = hidden_dim</span><br><span class="line">		self.latent_dim = latent_dim</span><br><span class="line">	</span><br><span class="line">		func = NNODEF(latent_dim,hidden_dim,time_invariant=<span class="literal">True</span>)</span><br><span class="line">		self.ode = NeuralODE(func)</span><br><span class="line">		self.l2h = nn.Linear(latent_dim,hidden_dim)</span><br><span class="line">		self.h2o = nn.Linear(hidden_dim,output_dim)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,z0,t)</span>:</span></span><br><span class="line">	zs = self.ode(z0,t,return_whole_sequence=<span class="literal">True</span>)</span><br><span class="line">	hs = self.l2h(zs)</span><br><span class="line">	xs = self.h2o(hs)</span><br><span class="line">	<span class="keyword">return</span> xs</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ODEVAE</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,output_dim,hidden_dim,latent_dim)</span>:</span></span><br><span class="line">		super(ODEVAE,self).__init__()</span><br><span class="line">		self.output_dim = output_dim</span><br><span class="line">		self.hidden_dim = hidden_dim</span><br><span class="line">		self.latent_dim = latent_dim</span><br><span class="line">		</span><br><span class="line">		self.encoder = RNNEncoder(output_dim,hidden_dim,latent_dim)</span><br><span class="line">		self.decoder = NeuralODEDecoder(output_dim,hidden_dim,latent_dim)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x,t,MAP=False)</span>:</span></span><br><span class="line">	z_mean,z_log_var = self.encoder(x,t)</span><br><span class="line">	<span class="keyword">if</span> MAP:</span><br><span class="line">		z = z_mean</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		z = z_mean + torch.randn_like(z_mean)*torch.exp(<span class="number">0.5</span>=z_log_var)</span><br><span class="line">	x_p = self.decoder(z,t)</span><br><span class="line">	<span class="keyword">return</span> x_p,z,z_mean,z_log_var</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_with_seed</span><span class="params">(self,seed_x,t)</span>:</span></span><br><span class="line">	seed_t_len = seed_x.shape[<span class="number">0</span>]</span><br><span class="line">	z_mean,z_log_var = self.encoder(seed_x,t[:seed_t_len])</span><br><span class="line">	x_p = self.decoder(z_mean,t)</span><br><span class="line">	<span class="keyword">return</span> x_p</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>ODEs</tag>
      </tags>
  </entry>
  <entry>
    <title>Model Fusion</title>
    <url>/2020/04/04/Model-Fusion/</url>
    <content><![CDATA[<p><strong>Quote：</strong></p>
<p>&ensp;<a href="https://rocksnake.github.io/2020/03/31/Modeling%20and%20tuning/" target="_blank" rel="noopener">Modeling and tuning</a></p>
<p>&ensp;&ensp;&ensp;&ensp;在此文中介绍到的多种机器学习类型中，我们通常会用到其中的 2 个甚至 2 个以上。如果我们要同时利用这些个模型，就需要将分别的训练结果结合起来作为总的训练结果。</p>
<blockquote>
<p>此过程称之为 “ 模型融合 “</p>
</blockquote>
<p>面向对象：多种调参后的模型。接下来介绍三种提升分数较为显著的方法：</p>
<ul>
<li><div id="return">简单的加权融合；</div></li>
<li><a href="#stack">stacking / blending；</a></li>
<li><a href="#boosting">boosting / bagging【在xgboost，Adaboost,GBDT中已经用到】。</a></li>
</ul>
<h1 id="简单的加权融合"><a href="#简单的加权融合" class="headerlink" title="简单的加权融合"></a>简单的加权融合</h1><ul>
<li>回归（分类概率）：算术平均融合（Arithmetic mean），几何平均融合（Geometric mean）；</li>
<li>分类：投票（Voting)</li>
<li>综合：排序融合(Rank averaging)，log融合</li>
</ul>
<h2 id="加权平均"><a href="#加权平均" class="headerlink" title="加权平均"></a>加权平均</h2><blockquote>
<p>面向模型：<strong>回归 \ 分类概率模型</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 生成一些简单的样本数据，test_prei 代表第i个模型的预测值</span></span><br><span class="line">test_pre1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pre2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pre3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_test_true 代表第模型的真实值</span></span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure>
<p><strong>加权平均函数的定义</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义结果的加权平均函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Weighted_method</span><span class="params">(test_pre1,test_pre2,test_pre3,w=[<span class="number">1</span>/<span class="number">3</span>,<span class="number">1</span>/<span class="number">3</span>,<span class="number">1</span>/<span class="number">3</span>])</span>:</span></span><br><span class="line">    Weighted_result = w[<span class="number">0</span>]*pd.Series(test_pre1)+w[<span class="number">1</span>]*pd.Series(test_pre2)+w[<span class="number">2</span>]*pd.Series(test_pre3)</span><br><span class="line">    <span class="keyword">return</span> Weighted_result</span><br></pre></td></tr></table></figure>
<p><strong>模型训练【3个】</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="comment"># 各模型的预测结果计算MAE</span></span><br><span class="line">print(<span class="string">'Pred1 MAE:'</span>,metrics.mean_absolute_error(y_test_true, test_pre1))</span><br><span class="line">print(<span class="string">'Pred2 MAE:'</span>,metrics.mean_absolute_error(y_test_true, test_pre2))</span><br><span class="line">print(<span class="string">'Pred3 MAE:'</span>,metrics.mean_absolute_error(y_test_true, test_pre3))</span><br></pre></td></tr></table></figure>
<p><strong>加权计算</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 根据加权计算MAE</span></span><br><span class="line">w = [<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.3</span>] <span class="comment"># 定义比重权值</span></span><br><span class="line">Weighted_pre = Weighted_method(test_pre1,test_pre2,test_pre3,w)</span><br><span class="line">print(<span class="string">'Weighted_pre MAE:'</span>,metrics.mean_absolute_error(y_test_true, Weighted_pre))</span><br></pre></td></tr></table></figure>

<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Weighted_pre MAE: <span class="number">0.0575</span></span><br></pre></td></tr></table></figure>

<h2 id="mean-平均"><a href="#mean-平均" class="headerlink" title="mean 平均"></a>mean 平均</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 定义结果的加权平均函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Mean_method</span><span class="params">(test_pre1,test_pre2,test_pre3)</span>:</span></span><br><span class="line">    Mean_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=<span class="number">1</span>).mean(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> Mean_result</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Mean_pre = Mean_method(test_pre1,test_pre2,test_pre3)</span><br><span class="line">print(<span class="string">'Mean_pre MAE:'</span>,metrics.mean_absolute_error(y_test_true, Mean_pre))</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Mean_pre MAE: <span class="number">0.0666666666667</span></span><br></pre></td></tr></table></figure>

<h2 id="median平均"><a href="#median平均" class="headerlink" title="median平均"></a>median平均</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 定义结果的加权平均函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Median_method</span><span class="params">(test_pre1,test_pre2,test_pre3)</span>:</span></span><br><span class="line">    Median_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=<span class="number">1</span>).median(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> Median_result</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Median_pre = Median_method(test_pre1,test_pre2,test_pre3)</span><br><span class="line">print(<span class="string">'Median_pre MAE:'</span>,metrics.mean_absolute_error(y_test_true, Median_pre))</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Median_pre MAE: <span class="number">0.075</span></span><br></pre></td></tr></table></figure>

<h2 id="Voting投票机制"><a href="#Voting投票机制" class="headerlink" title="Voting投票机制"></a>Voting投票机制</h2><blockquote>
<p>Voting即投票机制，分为软投票和硬投票两种，其原理采用少数服从多数的思想。</p>
</blockquote>
<blockquote>
<p>面向模型：分类模型</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score,roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">硬投票：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票数最多的类为最终被预测的类。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line">x=iris.data</span><br><span class="line">y=iris.target</span><br><span class="line">x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">clf1 = XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">150</span>, max_depth=<span class="number">3</span>, min_child_weight=<span class="number">2</span>, subsample=<span class="number">0.7</span>,</span><br><span class="line">                     colsample_bytree=<span class="number">0.6</span>, objective=<span class="string">'binary:logistic'</span>)</span><br><span class="line">clf2 = RandomForestClassifier(n_estimators=<span class="number">50</span>, max_depth=<span class="number">1</span>, min_samples_split=<span class="number">4</span>,</span><br><span class="line">                              min_samples_leaf=<span class="number">63</span>,oob_score=<span class="literal">True</span>)</span><br><span class="line">clf3 = SVC(C=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 硬投票</span></span><br><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">'xgb'</span>, clf1), (<span class="string">'rf'</span>, clf2), (<span class="string">'svc'</span>, clf3)], voting=<span class="string">'hard'</span>)</span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> zip([clf1, clf2, clf3, eclf], [<span class="string">'XGBBoosting'</span>, <span class="string">'Random Forest'</span>, <span class="string">'SVM'</span>, <span class="string">'Ensemble'</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">    print(<span class="string">"Accuracy: %0.2f (+/- %0.2f) [%s]"</span> % (scores.mean(), scores.std(), label))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">软投票：和硬投票原理相同，增加了设置权重的功能，可以为不同模型设置不同权重，进而区别模型不同的重要度。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">x=iris.data</span><br><span class="line">y=iris.target</span><br><span class="line">x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">clf1 = XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">150</span>, max_depth=<span class="number">3</span>, min_child_weight=<span class="number">2</span>, subsample=<span class="number">0.8</span>,</span><br><span class="line">                     colsample_bytree=<span class="number">0.8</span>, objective=<span class="string">'binary:logistic'</span>)</span><br><span class="line">clf2 = RandomForestClassifier(n_estimators=<span class="number">50</span>, max_depth=<span class="number">1</span>, min_samples_split=<span class="number">4</span>,</span><br><span class="line">                              min_samples_leaf=<span class="number">63</span>,oob_score=<span class="literal">True</span>)</span><br><span class="line">clf3 = SVC(C=<span class="number">0.1</span>, probability=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 软投票</span></span><br><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">'xgb'</span>, clf1), (<span class="string">'rf'</span>, clf2), (<span class="string">'svc'</span>, clf3)], voting=<span class="string">'soft'</span>, weights=[<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">clf1.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> zip([clf1, clf2, clf3, eclf], [<span class="string">'XGBBoosting'</span>, <span class="string">'Random Forest'</span>, <span class="string">'SVM'</span>, <span class="string">'Ensemble'</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">    print(<span class="string">"Accuracy: %0.2f (+/- %0.2f) [%s]"</span> % (scores.mean(), scores.std(), label))</span><br></pre></td></tr></table></figure>

<p><a href="#return">返回…</a></p>
<h1 id="stacking-blending"><a href="#stacking-blending" class="headerlink" title="stacking / blending"></a><div id="stack">stacking / blending</div></h1><ul>
<li>构建多层模型，并利用预测结果再拟合预测。<h2 id="Stack"><a href="#Stack" class="headerlink" title="Stack"></a>Stack</h2><blockquote>
<p>stacking是一种分层模型集成框架。</p>
</blockquote>
</li>
</ul>
<p>以两层为例，第一层由多个基学习器组成，其输入为原始训练集，第二层的模型则是以第一层基学习器的输出作为训练集进行再训练，从而得到完整的stacking模型, stacking两层模型都使用了全部的训练数据。</p>
<blockquote>
<p>理论介绍</p>
</blockquote>
<p>简单来说 stacking 就是当用初始训练数据学习出若干个基学习器后，将这几个学习器的预测结果作为新的训练集，来学习一个新的学习器。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2p1cHRlci1vc3Mub3NzLWNuLWhhbmd6aG91LmFsaXl1bmNzLmNvbS9wdWJsaWMvZmlsZXMvaW1hZ2UvMjMyNjU0MTA0Mi8xNTg0NDQ4NzkzMjMxXzZUeWdqWHdqTmIuanBn?x-oss-process=image/format,png"></center>

<p>将个体学习器结合在一起的时候使用的方法叫做结合策略。对于分类问题，我们可以使用投票法来选择输出最多的类。对于回归问题，我们可以将分类器输出的结果求平均值。</p>
<p>上面说的投票法和平均法都是很有效的结合策略，还有一种结合策略是使用另外一个机器学习算法来将个体机器学习器的结果结合在一起，这个方法就是Stacking。</p>
<p>在stacking方法中，我们把个体学习器叫做初级学习器，用于结合的学习器叫做次级学习器或元学习器（meta-learner），次级学习器用于训练的数据叫做次级训练集。次级训练集是在训练集上用初级学习器得到的。</p>
<blockquote>
<p>算法</p>
</blockquote>
<center><img src="http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/2326541042/1584448806789_1ElRtHaacw.jpg"></center>

<ul>
<li>过程1-3 是训练出来个体学习器，也就是初级学习器。</li>
<li>过程5-9是 使用训练出来的个体学习器来得预测的结果，这个预测的结果当做次级学习器的训练集。</li>
<li>过程11 是用初级学习器预测的结果训练出次级学习器，得到我们最后训练的模型。</li>
</ul>
<blockquote>
<p>推导 &amp; 解析【三步走战略】</p>
</blockquote>
<p>首先，我们先从一种“不那么正确”但是容易懂的Stacking方法讲起。</p>
<p>Stacking模型本质上是一种分层的结构，这里简单起见，只分析二级Stacking.假设我们有2个基模型 Model1_1、Model1_2 和 一个次级模型Model2</p>
<p><strong>Step 1.</strong> 基模型 Model1_1，对训练集train训练，然后用于预测 train 和 test 的标签列，分别是P1，T1</p>
<p>Model1_1 模型训练:</p>
<p>$$<br>\left(\begin{array}{c}{\vdots} \ {X_{train}} \ {\vdots}\end{array}\right) \overbrace{\Longrightarrow}^{\ text {Model1_1 Train} }\left(\begin{array}{c}{\vdots} \ {Y}_{True} \ {\vdots}\end{array}\right)<br>$$</p>
<p>训练后的模型 Model1_1 分别在 train 和 test 上预测，得到预测标签分别是P1，T1</p>
<p>$$<br>\left(\begin{array}{c}{\vdots} \ {X_{train}} \ {\vdots}\end{array}\right) \overbrace{\Longrightarrow}^{\ text {Model1_1 Predict} }\left(\begin{array}{c}{\vdots} \ {P}_{1} \ {\vdots}\end{array}\right)<br>$$</p>
<p>$$<br>\left(\begin{array}{c}{\vdots} \ {X_{test}} \ {\vdots}\end{array}\right) \overbrace{\Longrightarrow}^{\ text {Model1_1 Predict} }\left(\begin{array}{c}{\vdots} \ {T_{1}} \ {\vdots}\end{array}\right)<br>$$</p>
<p><strong>Step 2.</strong> 基模型 Model1_2 ，对训练集train训练，然后用于预测train和test的标签列，分别是P2，T2</p>
<p>Model1_2 模型训练:</p>
<p>$$<br>\left(\begin{array}{c}{\vdots} \ {X_{train}} \ {\vdots}\end{array}\right) \overbrace{\Longrightarrow}^{\ text {Model1_2 Train} }\left(\begin{array}{c}{\vdots} \ {Y}_{True} \ {\vdots}\end{array}\right)<br>$$</p>
<p>训练后的模型 Model1_2 分别在 train 和 test 上预测，得到预测标签分别是P2，T2</p>
<p>$$<br>\left(\begin{array}{c}{\vdots} \ {X_{train}} \ {\vdots}\end{array}\right) \overbrace{\Longrightarrow}^{\ text {Model1_2 Predict} }\left(\begin{array}{c}{\vdots} \ {P}_{2} \ {\vdots}\end{array}\right)<br>$$</p>
<p>$$<br>\left(\begin{array}{c}{\vdots} \ {X_{test}} \ {\vdots}\end{array}\right) \overbrace{\Longrightarrow}^{\ text {Model1_2 Predict} }\left(\begin{array}{c}{\vdots} \ {T_{2}} \ {\vdots}\end{array}\right)<br>$$</p>
<p><strong>Step 3.</strong> 分别把P1,P2以及T1,T2合并，得到一个新的训练集和测试集train2,test2.</p>
<p>$$<br>\overbrace{\left(\begin{array}{c}{\vdots} \ {P_{1}} \ {\vdots}\end{array} \begin{array}{c}{\vdots} \ {P_{2}} \ {\vdots}\end{array} \right)}  ^ {text {Train_2 }}<br>and<br>\overbrace{\left(\begin{array}{c}{\vdots} \ {T_{1}} \ {\vdots}\end{array} \begin{array}{c}{\vdots} \ {T_{2}} \ {\vdots}\end{array} \right)}^{text {Test_2 }}<br>$$</p>
<p>再用 次级模型 Model2 以真实训练集标签为标签训练,以train2为特征进行训练，预测test2,得到最终的测试集预测的标签列 $Y_{Pre}$。</p>
<p>$$<br>\overbrace{\left(\begin{array}{c}{\vdots} \ {P_{1}} \ {\vdots}\end{array} \begin{array}{c}{\vdots} \ {P_{2}} \ {\vdots}\end{array} \right)}^{\ text {Train_2 }} \overbrace{\Longrightarrow}^{\text {Model2 Train} }\left(\begin{array}{c}{\vdots} \ {Y}_{True} \ {\vdots}\end{array}\right)<br>$$</p>
<p>$$<br>\overbrace{\left(\begin{array}{c}{\vdots} \ {T_{1}} \ {\vdots}\end{array} \begin{array}{c}{\vdots} \ {T_{2}} \ {\vdots}\end{array} \right)}^{\ text {Test_2 }} \overbrace{\Longrightarrow}^{\ text {Model1_2 Predict} }\left(\begin{array}{c}{\vdots} \ {Y}_{Pre} \ {\vdots}\end{array}\right)<br>$$</p>
<p>这就是我们两层堆叠的一种基本的原始思路想法。在不同模型预测的结果基础上再加一层模型，进行再训练，从而得到模型最终的预测。</p>
<p>Stacking本质上就是这么直接的思路。</p>
<p>但是直接这样有时对于如果训练集和测试集分布不那么一致的情况下是有一点问题的，其问题在于<strong>用初始模型训练的标签再利用真实标签进行再训练，毫无疑问会导致一定的模型过拟合训练集，这样或许模型在测试集上的泛化能力或者说效果会有一定的下降</strong>。</p>
<p>因此现在的问题变成了如何<strong>降低</strong>再训练的<strong>过拟合性</strong>，这里我们一般有两种方法。</p>
<ul>
<li>次级模型尽量选择简单的线性模型</li>
<li>利用K折交叉验证</li>
</ul>
<p><strong>K-折交叉验证：</strong></p>
<p><strong>训练：</strong></p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2p1cHRlci1vc3Mub3NzLWNuLWhhbmd6aG91LmFsaXl1bmNzLmNvbS9wdWJsaWMvZmlsZXMvaW1hZ2UvMjMyNjU0MTA0Mi8xNTg0NDQ4ODE5NjMyX1l2Sk9YTWswMlAuanBn?x-oss-process=image/format,png"></center>

<p><strong>预测：</strong></p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2p1cHRlci1vc3Mub3NzLWNuLWhhbmd6aG91LmFsaXl1bmNzLmNvbS9wdWJsaWMvZmlsZXMvaW1hZ2UvMjMyNjU0MTA0Mi8xNTg0NDQ4ODI2MjAzX2s4S1B5OW43RDkuanBn?x-oss-process=image/format,png"></center>

<blockquote>
<p>面向模型：<strong>回归 \ 分类概率模型</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Stacking_method</span><span class="params">(train_reg1,train_reg2,train_reg3,y_train_true,test_pre1,test_pre2,test_pre3,model_L2= linear_model.LinearRegression<span class="params">()</span>)</span>:</span></span><br><span class="line">    model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)],axis=<span class="number">1</span>).values,y_train_true)</span><br><span class="line">    Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=<span class="number">1</span>).values)</span><br><span class="line">    <span class="keyword">return</span> Stacking_result</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 生成一些简单的样本数据，test_prei 代表第i个模型的预测值</span></span><br><span class="line">train_reg1 = [<span class="number">3.2</span>, <span class="number">8.2</span>, <span class="number">9.1</span>, <span class="number">5.2</span>]</span><br><span class="line">train_reg2 = [<span class="number">2.9</span>, <span class="number">8.1</span>, <span class="number">9.0</span>, <span class="number">4.9</span>]</span><br><span class="line">train_reg3 = [<span class="number">3.1</span>, <span class="number">7.9</span>, <span class="number">9.2</span>, <span class="number">5.0</span>]</span><br><span class="line"><span class="comment"># y_test_true 代表第模型的真实值</span></span><br><span class="line">y_train_true = [<span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">5</span>] </span><br><span class="line"></span><br><span class="line">test_pre1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pre2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pre3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_test_true 代表第模型的真实值</span></span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_L2= linear_model.LinearRegression()</span><br><span class="line">Stacking_pre = Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,</span><br><span class="line">                               test_pre1,test_pre2,test_pre3,model_L2)</span><br><span class="line">print(<span class="string">'Stacking_pre MAE:'</span>,metrics.mean_absolute_error(y_test_true, Stacking_pre))</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Stacking_pre MAE: <span class="number">0.0421348314607</span></span><br></pre></td></tr></table></figure>

<p>可以发现模型结果相对于之前有进一步的提升，这是我们需要注意的一点是，对于第二层Stacking的模型不宜选取的过于复杂，这样会导致模型在训练集上过拟合，从而使得在测试集上并不能达到很好的效果。</p>
<h2 id="Blending"><a href="#Blending" class="headerlink" title="Blending"></a>Blending</h2><blockquote>
<p>其实和Stacking是一种类似的多层模型融合的形式</p>
</blockquote>
<p>其主要思路是把原始的训练集先分成两部分，比如70%的数据作为新的训练集，剩下30%的数据作为测试集。</p>
<ul>
<li><p>在第一层，我们在这70%的数据上训练多个模型，然后去预测那30%数据的label，同时也预测test集的label。</p>
</li>
<li><p>在第二层，我们就直接用这30%数据在第一层预测的结果做为新特征继续训练，然后用test集第一层预测的label做特征，用第二层训练的模型做进一步预测</p>
</li>
</ul>
<p><strong>其优点在于：</strong></p>
<ol>
<li>比stacking简单（因为不用进行k次的交叉验证来获得stacker feature）</li>
<li>避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集</li>
</ol>
<p><strong>缺点在于：</strong></p>
<ol>
<li>使用了很少的数据（第二阶段的blender只使用training set10%的量）</li>
<li>blender可能会过拟合</li>
<li>stacking使用多次的交叉验证会比较稳健</li>
</ol>
<p><a href="#return">返回…</a></p>
<h1 id="boosting-bagging"><a href="#boosting-bagging" class="headerlink" title="boosting / bagging"></a><div id="boosting">boosting / bagging</div></h1><blockquote>
<p>在xgboost，Adaboost,GBDT中已经用到</p>
</blockquote>
<ul>
<li>多树的提升方法</li>
</ul>
<p><strong>Reference:</strong> <a href="https://rocksnake.github.io/2020/03/31/Modeling%20and%20tuning/" target="_blank" rel="noopener">Modeling and tuning</a></p>
<h1 id="Others-Methods"><a href="#Others-Methods" class="headerlink" title="Others Methods"></a>Others Methods</h1><p>将特征放进模型中预测，并将预测结果变换并作为新的特征加入原有特征中再经过模型预测结果  (Stacking变化)</p>
<blockquote>
<p>可以反复预测多次将结果加入最后的特征中</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Ensemble_add_feature</span><span class="params">(train,test,target,clfs)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># n_flods = 5</span></span><br><span class="line">    <span class="comment"># skf = list(StratifiedKFold(y, n_folds=n_flods))</span></span><br><span class="line"></span><br><span class="line">    train_ = np.zeros((train.shape[<span class="number">0</span>],len(clfs*<span class="number">2</span>)))</span><br><span class="line">    test_ = np.zeros((test.shape[<span class="number">0</span>],len(clfs*<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j,clf <span class="keyword">in</span> enumerate(clfs):</span><br><span class="line">        <span class="string">'''依次训练各个单模型'''</span></span><br><span class="line">        <span class="comment"># print(j, clf)</span></span><br><span class="line">        <span class="string">'''使用第1个部分作为预测，第2部分来训练模型，获得其预测的输出作为第2部分的新特征。'''</span></span><br><span class="line">        <span class="comment"># X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]</span></span><br><span class="line"></span><br><span class="line">        clf.fit(train,target)</span><br><span class="line">        y_train = clf.predict(train)</span><br><span class="line">        y_test = clf.predict(test)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 新特征生成</span></span><br><span class="line">        train_[:,j*<span class="number">2</span>] = y_train**<span class="number">2</span></span><br><span class="line">        test_[:,j*<span class="number">2</span>] = y_test**<span class="number">2</span></span><br><span class="line">        train_[:, j+<span class="number">1</span>] = np.exp(y_train)</span><br><span class="line">        test_[:, j+<span class="number">1</span>] = np.exp(y_test)</span><br><span class="line">        <span class="comment"># print("val auc Score: %f" % r2_score(y_predict, dataset_d2[:, j]))</span></span><br><span class="line">        print(<span class="string">'Method '</span>,j)</span><br><span class="line">    </span><br><span class="line">    train_ = pd.DataFrame(train_)</span><br><span class="line">    test_ = pd.DataFrame(test_)</span><br><span class="line">    <span class="keyword">return</span> train_,test_</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score, train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">clf = LogisticRegression()</span><br><span class="line"></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>,:]</span><br><span class="line"></span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=<span class="number">0.3</span>)</span><br><span class="line">x_train = pd.DataFrame(x_train) ; x_test = pd.DataFrame(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型融合中使用到的各个单模型</span></span><br><span class="line">clfs = [LogisticRegression(),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'entropy'</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line">New_train,New_test = Ensemble_add_feature(x_train,x_test,y_train,clfs)</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression()</span><br><span class="line"><span class="comment"># clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)</span></span><br><span class="line">clf.fit(New_train, y_train)</span><br><span class="line">y_emb = clf.predict_proba(New_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Val auc Score of stacking: %f"</span> % (roc_auc_score(y_test, y_emb)))</span><br></pre></td></tr></table></figure>
<p><strong>Conclusion：</strong></p>
<p>比赛的融合是提分和提升模型鲁棒性的一种重要方法：</p>
<ol>
<li><p><strong>结果层面的融合</strong>，这种是最常见的融合方法，其可行的融合方法也有很多，比如根据结果的得分进行加权融合，还可以做Log，exp处理等。在做结果融合的时候，有一个很重要的条件是模型结果的得分要比较近似，然后结果的差异要比较大，这样的结果融合往往有比较好的效果提升。</p>
</li>
<li><p><strong>特征层面的融合</strong>，这个层面其实感觉不叫融合，准确说可以叫分割，很多时候如果我们用同种模型训练，可以把特征进行切分给不同的模型，然后在后面进行模型或者结果融合有时也能产生比较好的效果。</p>
</li>
<li><p><strong>模型层面的融合</strong>，模型层面的融合可能就涉及模型的堆叠和设计，比如加Staking层，部分模型的结果作为特征输入等，这些就需要多实验和思考了，基于模型层面的融合最好不同模型类型要有一定的差异，用同种模型不同的参数的收益一般是比较小的。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>模型融合</tag>
      </tags>
  </entry>
  <entry>
    <title>Modeling and tuning</title>
    <url>/2020/03/31/Modeling-and-tuning/</url>
    <content><![CDATA[<p>在XGBoost、LightGBM、CatBoost三个模型介绍与参数调优的基础上，进一步拓展模型类型和参数调优方法。</p>
<blockquote>
<p><a href="https://rocksnake.github.io/2020/03/27/Boost%20Tuning/" target="_blank" rel="noopener">Boosting Tuning</a></p>
</blockquote>
<ol>
<li><div id="return">线性回归模型；</div></li>
<li><a href="#Model_performance_verification">模型性能验证；</a></li>
<li><a href="#Embedded_Feature_Selection">嵌入式特征选择；</a></li>
<li><a href="#compare">模型对比；</a></li>
<li><a href="#tuning">模型调参。</a></li>
</ol>
<h1 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h1><ul>
<li>线性回归对于特征的要求；</li>
<li>处理长尾分布；</li>
<li>理解线性回归模型；</li>
</ul>
<blockquote>
<p>针对线性回归模型的参数类型、损失函数、优化方法</p>
<ul>
<li>参见<a href="https://zhuanlan.zhihu.com/p/49480391" target="_blank" rel="noopener">知乎介绍</a></li>
</ul>
</blockquote>
<p>简而言之：</p>
<p>$$<br>y = w * x + b<br>$$</p>
<p>在多元线性回归中，并不是所用特征越多越好；选择少量、合适的特征既可以避免过拟合，也可以增加模型解释度。可以通过 3 种方法来选择特征：最优子集选择、向前或向后逐步选择、交叉验证法。</p>
<p><strong>结合一个例子进行分析：</strong></p>
<p>e.g. <a href="https://www.yuque.com/docs/share/d61bec98-faf9-4eb5-8ab4-7e5cbee77014?#" target="_blank" rel="noopener">线性回归模型 + 五折交叉验证 + 模拟真实业务</a></p>
<hr>
<p><strong>补充：</strong></p>
<center><b>五折交叉验证</b></center>

<p>在使用训练集对参数进行训练的时候，经常会发现人们通常会将一整个训练集分为三个部分（比如mnist手写训练集）。一般分为：训练集（train_set），评估集（valid_set），测试集（test_set）这三个部分。这其实是为了保证训练效果而特意设置的。其中测试集很好理解，其实就是完全不参与训练的数据，仅仅用来观测测试效果的数据。而训练集和评估集则牵涉到下面的知识了。</p>
<p>因为在实际的训练中，训练的结果对于训练集的拟合程度通常还是挺好的（初始条件敏感），但是对于训练集之外的数据的拟合程度通常就不那么令人满意了。因此我们通常并不会把所有的数据集都拿来训练，而是分出一部分来（这一部分不参加训练）对训练集生成的参数进行测试，相对客观的判断这些参数对训练集之外的数据的符合程度。这种思想就称为交叉验证（Cross Validation）</p>
<center><b>模拟真实业务情况</b></center>

<p>我们不具有预知未来的能力，五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况。</p>
<p>因此我们可以采用时间顺序对数据集进行分割，保证最终的结果与五折交叉验证差距不大。</p>
<hr>
<p><a href="#return">返回…</a></p>
<h1 id="模型性能验证"><a href="#模型性能验证" class="headerlink" title="模型性能验证"></a><div id="Model_performance_verification">模型性能验证</div></h1><ul>
<li>评价函数与目标函数；</li>
<li>交叉验证方法；</li>
<li>留一验证方法；</li>
<li>针对时间序列问题的验证；</li>
<li>绘制学习率曲线和验证曲线；</li>
</ul>
<p>在此前<a href="https://www.yuque.com/docs/share/d61bec98-faf9-4eb5-8ab4-7e5cbee77014?#" target="_blank" rel="noopener">线性回归模型 + 五折交叉验证 + 模拟真实业务</a>例子的基础上我们通过绘制学习率曲线和验证曲线进行模型性能的验证。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve, validation_curve</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">? learning_curve</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span><span class="params">(estimator, title, X, y, ylim=None, cv=None,n_jobs=<span class="number">1</span>, train_size=np.linspace<span class="params">(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span> )</span>)</span>:</span>  </span><br><span class="line">    plt.figure()  </span><br><span class="line">    plt.title(title)  </span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  </span><br><span class="line">        plt.ylim(*ylim)  </span><br><span class="line">    plt.xlabel(<span class="string">'Training example'</span>)  </span><br><span class="line">    plt.ylabel(<span class="string">'score'</span>)  </span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_size, scoring = make_scorer(mean_absolute_error))  </span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)  </span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)  </span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)  </span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)  </span><br><span class="line">    plt.grid()<span class="comment">#区域  </span></span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,  </span><br><span class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>,  </span><br><span class="line">                     color=<span class="string">"r"</span>)  </span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,  </span><br><span class="line">                     test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>,  </span><br><span class="line">                     color=<span class="string">"g"</span>)  </span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">'r'</span>,  </span><br><span class="line">             label=<span class="string">"Training score"</span>)  </span><br><span class="line">    plt.plot(train_sizes, test_scores_mean,<span class="string">'o-'</span>,color=<span class="string">"g"</span>,  </span><br><span class="line">             label=<span class="string">"Cross-validation score"</span>)  </span><br><span class="line">    plt.legend(loc=<span class="string">"best"</span>)  </span><br><span class="line">    <span class="keyword">return</span> plt</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_learning_curve(LinearRegression(), <span class="string">'Liner_model'</span>, train_X[:<span class="number">1000</span>], train_y_ln[:<span class="number">1000</span>], ylim=(<span class="number">0.0</span>, <span class="number">0.5</span>), cv=<span class="number">5</span>, n_jobs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train = sample_feature[continuous_feature_names + [<span class="string">'price'</span>]].dropna()</span><br><span class="line"></span><br><span class="line">train_X = train[continuous_feature_names]</span><br><span class="line">train_y = train[<span class="string">'price'</span>]</span><br><span class="line">train_y_ln = np.log(train_y + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p><a href="#return">返回…</a></p>
<h1 id="嵌入式特征选择"><a href="#嵌入式特征选择" class="headerlink" title="嵌入式特征选择"></a><div id="Embedded_Feature_Selection">嵌入式特征选择</div></h1><ul>
<li>Lasso回归；</li>
<li>Ridge回归；</li>
<li>决策树；</li>
</ul>
<hr>
<p>学前知识：</p>
<ul>
<li>用简单易懂的语言描述「过拟合 overfitting」<a href="https://www.zhihu.com/question/32246256/answer/55320482" target="_blank" rel="noopener">学习资源</a></li>
<li>模型复杂度与模型的泛化能力 <a href="http://yangyingming.com/article/434/" target="_blank" rel="noopener">学习资源</a></li>
<li>正则化的直观理解 <a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">学习资源</a></li>
</ul>
<hr>
<p>在<strong>过滤式</strong>和<strong>包裹式</strong>特征选择方法中，特征选择过程与学习器训练过程有明显的分别。</p>
<p>而嵌入式特征选择在学习器训练过程中自动地进行特征选择。嵌入式选择最常用的是<strong>L1正则化</strong>与<strong>L2正则化</strong>。在对线性回归模型加入两种正则化方法后，他们分别变成了<strong>岭回归</strong>与<strong>Lasso回归</strong>。</p>
<h2 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h2><blockquote>
<p>在拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。</p>
</blockquote>
<p>因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』</p>
<center><img src="https://img-blog.csdnimg.cn/20200331161150829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70"></center>

<h2 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h2><blockquote>
<p>有助于生成一个稀疏权值矩阵，进而可以用于特征选择。</p>
</blockquote>
<center><img src="https://img-blog.csdnimg.cn/20200331161249549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70"></center>

<h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><p>除此之外，决策树通过信息熵或GINI指数选择分裂节点时，优先选择的分裂特征也更加重要，这同样是一种特征选择的方法。XGBoost与LightGBM模型中的model_importance指标正是基于此计算的。</p>
<p><a href="#return">返回…</a></p>
<h1 id="模型对比"><a href="#模型对比" class="headerlink" title="模型对比"></a><div id="compare">模型对比</div></h1><p>除了线性模型以外，还有许多我们常用的非线性模型如下，在此篇幅有限不再一一讲解原理。我们选择了部分常用模型与线性模型进行效果比对。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPRegressor</span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBRegressor</span><br><span class="line"><span class="keyword">from</span> lightgbm.sklearn <span class="keyword">import</span> LGBMRegressor</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">models = [LinearRegression(),</span><br><span class="line">          DecisionTreeRegressor(),</span><br><span class="line">          RandomForestRegressor(),</span><br><span class="line">          GradientBoostingRegressor(),</span><br><span class="line">          MLPRegressor(solver=<span class="string">'lbfgs'</span>, max_iter=<span class="number">100</span>), </span><br><span class="line">          XGBRegressor(n_estimators = <span class="number">100</span>, objective=<span class="string">'reg:squarederror'</span>), </span><br><span class="line">          LGBMRegressor(n_estimators = <span class="number">100</span>)]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = dict()</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model_name = str(model).split(<span class="string">'('</span>)[<span class="number">0</span>]</span><br><span class="line">    scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error))</span><br><span class="line">    result[model_name] = scores</span><br><span class="line">    print(model_name + <span class="string">' is finished'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = pd.DataFrame(result)</span><br><span class="line">result.index = [<span class="string">'cv'</span> + str(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<p><a href="#return">返回…</a></p>
<h1 id="模型调参"><a href="#模型调参" class="headerlink" title="模型调参"></a><div id="tuning">模型调参</div></h1><p>介绍三种常用的调参方法：</p>
<ul>
<li>贪心算法 <a href="https://www.jianshu.com/p/ab89df9759c8" target="_blank" rel="noopener">学习资源</a></li>
<li>网格调参 <a href="https://blog.csdn.net/weixin_43172660/article/details/83032029" target="_blank" rel="noopener">学习资源</a></li>
<li>贝叶斯调参 <a href="https://blog.csdn.net/linxid/article/details/81189154" target="_blank" rel="noopener">学习资源</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## LGB的参数集合：</span></span><br><span class="line"></span><br><span class="line">objective = [<span class="string">'regression'</span>, <span class="string">'regression_l1'</span>, <span class="string">'mape'</span>, <span class="string">'huber'</span>, <span class="string">'fair'</span>]</span><br><span class="line"></span><br><span class="line">num_leaves = [<span class="number">3</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">20</span>,<span class="number">40</span>, <span class="number">55</span>]</span><br><span class="line">max_depth = [<span class="number">3</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">20</span>,<span class="number">40</span>, <span class="number">55</span>]</span><br><span class="line">bagging_fraction = []</span><br><span class="line">feature_fraction = []</span><br><span class="line">drop_rate = []</span><br></pre></td></tr></table></figure>
<h2 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_obj = dict()</span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> objective:</span><br><span class="line">    model = LGBMRegressor(objective=obj)</span><br><span class="line">    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_obj[obj] = score</span><br><span class="line">    </span><br><span class="line">best_leaves = dict()</span><br><span class="line"><span class="keyword">for</span> leaves <span class="keyword">in</span> num_leaves:</span><br><span class="line">    model = LGBMRegressor(objective=min(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>], num_leaves=leaves)</span><br><span class="line">    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_leaves[leaves] = score</span><br><span class="line">    </span><br><span class="line">best_depth = dict()</span><br><span class="line"><span class="keyword">for</span> depth <span class="keyword">in</span> max_depth:</span><br><span class="line">    model = LGBMRegressor(objective=min(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          num_leaves=min(best_leaves.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          max_depth=depth)</span><br><span class="line">    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_depth[depth] = score</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.lineplot(x=[<span class="string">'0_initial'</span>,<span class="string">'1_turning_obj'</span>,<span class="string">'2_turning_leaves'</span>,<span class="string">'3_turning_depth'</span>], y=[<span class="number">0.143</span> ,min(best_obj.values()), min(best_leaves.values()), min(best_depth.values())])</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200331162154268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70"></center>

<h2 id="Grid-Search-调参"><a href="#Grid-Search-调参" class="headerlink" title="Grid Search 调参"></a>Grid Search 调参</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parameters = &#123;<span class="string">'objective'</span>: objective , <span class="string">'num_leaves'</span>: num_leaves, <span class="string">'max_depth'</span>: max_depth&#125;</span><br><span class="line">model = LGBMRegressor()</span><br><span class="line">clf = GridSearchCV(model, parameters, cv=<span class="number">5</span>)</span><br><span class="line">clf = clf.fit(train_X, train_y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf.best_params_</span><br><span class="line">model = LGBMRegressor(objective=<span class="string">'regression'</span>,</span><br><span class="line">                          num_leaves=<span class="number">55</span>,</span><br><span class="line">                          max_depth=<span class="number">15</span>)</span><br><span class="line">                    </span><br><span class="line">np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br></pre></td></tr></table></figure>
<h2 id="贝叶斯调参"><a href="#贝叶斯调参" class="headerlink" title="贝叶斯调参"></a>贝叶斯调参</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rf_cv</span><span class="params">(num_leaves, max_depth, subsample, min_child_samples)</span>:</span></span><br><span class="line">    val = cross_val_score(</span><br><span class="line">        LGBMRegressor(objective = <span class="string">'regression_l1'</span>,</span><br><span class="line">            num_leaves=int(num_leaves),</span><br><span class="line">            max_depth=int(max_depth),</span><br><span class="line">            subsample = subsample,</span><br><span class="line">            min_child_samples = int(min_child_samples)</span><br><span class="line">        ),</span><br><span class="line">        X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error)</span><br><span class="line">    ).mean()</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - val</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rf_bo = BayesianOptimization(</span><br><span class="line">    rf_cv,</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="string">'num_leaves'</span>: (<span class="number">2</span>, <span class="number">100</span>),</span><br><span class="line">    <span class="string">'max_depth'</span>: (<span class="number">2</span>, <span class="number">100</span>),</span><br><span class="line">    <span class="string">'subsample'</span>: (<span class="number">0.1</span>, <span class="number">1</span>),</span><br><span class="line">    <span class="string">'min_child_samples'</span> : (<span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rf_bo.maximize()</span><br><span class="line"><span class="number">1</span> - rf_bo.max[<span class="string">'target'</span>]</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>学习资源补充：</strong></p>
<ul>
<li>决策树模型 <a href="https://zhuanlan.zhihu.com/p/65304798" target="_blank" rel="noopener">Source</a></li>
<li>GBDT模型 <a href="https://zhuanlan.zhihu.com/p/45145899" target="_blank" rel="noopener">Source</a></li>
<li>XGBoost模型 <a href="https://zhuanlan.zhihu.com/p/86816771" target="_blank" rel="noopener">Source</a></li>
<li>LightGBM模型 <a href="https://zhuanlan.zhihu.com/p/89360721" target="_blank" rel="noopener">Source</a></li>
</ul>
<p>推荐教材：</p>
<ul>
<li>《机器学习》 <a href="https://book.douban.com/subject/26708119/" target="_blank" rel="noopener">Source</a></li>
<li>《统计学习方法》 <a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">Source</a></li>
<li>《Python大战机器学习》 <a href="https://book.douban.com/subject/26987890/" target="_blank" rel="noopener">Source</a></li>
<li>《面向机器学习的特征工程》 <a href="https://book.douban.com/subject/26826639/" target="_blank" rel="noopener">Source</a></li>
<li>《数据科学家访谈录》 <a href="https://book.douban.com/subject/30129410/" target="_blank" rel="noopener">Source</a></li>
</ul>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>Model&amp;tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>Boosting Tuning</title>
    <url>/2020/03/27/Boost-Tuning/</url>
    <content><![CDATA[<div id="Boosting">Boosting Tuning</div>

<ol>
<li><a href="#xgb">XGBoost；</a></li>
<li><a href="#lgb">LightGBM;</a></li>
<li><a href="#cat">CatBoost</a></li>
</ol>
<h1 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a><div id="xgb">XGBoost</div></h1><blockquote>
<p>efficient、flexible</p>
</blockquote>
<p>Reference：<a href="https://xgboost.readthedocs.io/en/latest/" target="_blank" rel="noopener">XGBoost Documentation</a></p>
<p>Xgboost 是一种高度复杂的算法可以处理各种各样的数据，利用 Xgboost 构建模型简单，但是用 Xgboost 来调参提升模型却很难。</p>
<h2 id="XGBoost-的优势"><a href="#XGBoost-的优势" class="headerlink" title="XGBoost 的优势"></a>XGBoost 的优势</h2><ol>
<li><strong>Regularization（正则化）</strong>；【可以减少过拟合】</li>
<li><strong>Parallel Processing（并行处理）</strong>；【Boosting 是串行算法】</li>
<li><strong>High Flexibility（高度灵活）</strong>；【自定义优化目标与评估标准】</li>
<li><strong>Handling Missing Values（处理缺失值）</strong> 【内置程序】</li>
<li><strong>Tree Pruning（树剪枝）</strong>  【参数max_depth】</li>
<li><strong>Built-in Cross-Validation（内置的交叉验证）</strong> </li>
<li><strong>Continue on Existing Model（继续现有模型）</strong> 【基于上一次运行迭代】</li>
</ol>
<h2 id="参数介绍"><a href="#参数介绍" class="headerlink" title="参数介绍"></a>参数介绍</h2><ol>
<li><strong>General Parameters（通用参数）：</strong> 设置整体功能</li>
<li><strong>Booster Parameters（提升参数）：</strong> 选择你每一步的booster (树or回归）</li>
<li><strong>Learning Task Parameters（学习任务参数）：</strong> 指导优化任务的执行</li>
</ol>
<p><strong>参数详细介绍参见：</strong> <a href="https://blog.csdn.net/u010665216/article/details/78532619" target="_blank" rel="noopener">Blog</a></p>
<p>XGBoost 的参数较多，如果需要认知各个参数的参考上述 Blog， 我直接开始说参数调优…</p>
<h2 id="Xgboost参数调优"><a href="#Xgboost参数调优" class="headerlink" title="Xgboost参数调优"></a>Xgboost参数调优</h2><blockquote>
<p>通用方法</p>
</blockquote>
<ul>
<li>选择一个相对<strong>较高的学习率</strong>。通常来说学习率设置为0.1。但是对于不同的问题可以讲学习率设置在0.05-0.3。通过交叉验证来寻找符合学习率的最佳树的个数。</li>
<li>当确定好学习率与最佳树的个数时，调整树的<strong>某些特定参数。</strong>比如：max_depth, min_child_weight, gamma, subsample, colsample_bytree</li>
<li>调整<strong>正则化参数</strong> ，比如： lambda, alpha。这个主要是为了减少模型复杂度和提高运行速度的。适当地减少过拟合。</li>
<li><strong>降低学习速率</strong>，选择最优参数</li>
</ul>
<h3 id="修正学习速率及调参估计量"><a href="#修正学习速率及调参估计量" class="headerlink" title="修正学习速率及调参估计量"></a>修正学习速率及调参估计量</h3><blockquote>
<p>给定一个初始值，方便确定其他参数，可随意给定</p>
</blockquote>
<h3 id="调整max-depth-和min-child-weight"><a href="#调整max-depth-和min-child-weight" class="headerlink" title="调整max_depth 和min_child_weight"></a>调整max_depth 和min_child_weight</h3><blockquote>
<blockquote>
<p>max_depth:树的最大深度，控制过拟合</p>
</blockquote>
<blockquote>
<p>min_child_weight:一个子集的所有观察值的最小权重和</p>
</blockquote>
</blockquote>
<blockquote>
<p>需要调整参数的选择依据：对结果的影响程度</p>
</blockquote>
<p><strong>方法：</strong></p>
<p>先预设一个较大的值，通过一个范围和步进值进行迭代，不断缩小范围，找寻最优组合，类似于最小二乘法。</p>
<h3 id="调整gamma"><a href="#调整gamma" class="headerlink" title="调整gamma"></a>调整gamma</h3><blockquote>
<p>这个指定了一个结点被分割时，所需要的最小损失函数减小的大小。  </p>
</blockquote>
<p>这个值一般来说需要根据损失函数来调整。</p>
<blockquote>
<p>和上一步一致的方法，同样是设置范围和步进值进行迭代，找寻最优值。</p>
</blockquote>
<h3 id="调整subsample-和colsample-bytree"><a href="#调整subsample-和colsample-bytree" class="headerlink" title="调整subsample 和colsample_bytree"></a>调整subsample 和colsample_bytree</h3><blockquote>
<p>分别是<strong>采样率</strong>和<strong>特征采样率</strong></p>
</blockquote>
<p>方法还和之前一致。</p>
<h3 id="调整正则化参数"><a href="#调整正则化参数" class="headerlink" title="调整正则化参数"></a>调整正则化参数</h3><blockquote>
<p>reg_alpha</p>
</blockquote>
<p>此参数使用相对较少，主要是用来调整过拟合，调优选择精度最高的值。</p>
<h3 id="减小学习率"><a href="#减小学习率" class="headerlink" title="减小学习率"></a>减小学习率</h3><blockquote>
<p>通过减小学习率并增加树的数量</p>
</blockquote>
<p>参数调优用例： <a href="https://gitee.com/orayang_admin/Xgboost-tuning" target="_blank" rel="noopener" title="Xgboost-tuning">Xgboost-tuning</a></p>
<p><a href="#Boosting">返回…</a></p>
<h1 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a><div id="lgb">LightGBM</div></h1><blockquote>
<p>LightGBM 的优点就是快的一逼的同时精度还高</p>
</blockquote>
<p>LGB 不需要通过所有样本计算信息增益了，而是选择梯度大的样本来计算信息增益，所以更精；而且内置特征降维技术，所以更快。</p>
<p>如果一个样本的梯度较小，证明这个样本训练的误差已经很小了，所以不需要计算了。GBDT的梯度算出来实际上就是残差，梯度小残差就小，所以该样本拟合较好，不需要去拟合他们了。但是我们这样相当于改变数据分布，无法避免信息损失，还是会导致精度下降。</p>
<p><strong>LGB 核心：</strong></p>
<ol>
<li>在保留大梯度样本的同时，随机地保留一些小梯度样本，同时放大了小梯度样本带来的信息增益。</li>
</ol>
<p>e.g. 首先把样本按照梯度排序，选出梯度最大的$a%$个样本，然后在剩下小梯度数据中随机选取$b%$个样本，在计算信息增益的时候，将选出来$b%$个小梯度样本的信息增益扩大 $1 -  \frac{a}{b}$ 倍。</p>
<ol start="2">
<li>内置了特征降维技术，思想就是合并那些冲突小的稀疏特征。</li>
</ol>
<p>e.g. 对于一列特征[1,nan,1,nan,1]和一列特征[nan,1,nan,1,nan]，他们正好可以合并成一列特征[1,2,1,2,1]。LGB的目标就是在于找到这样的特征并且将他们合并在一起。</p>
<p>Reference:  <a href="https://zhuanlan.zhihu.com/p/89360721" target="_blank" rel="noopener">无痛看懂LightGBM原文</a></p>
<h2 id="参数调优"><a href="#参数调优" class="headerlink" title="参数调优"></a>参数调优</h2><p>一般步骤如下：</p>
<ol>
<li>选择较高的学习率，大概0.1附近；</li>
<li>对决策树基本参数调参；</li>
<li>正则化参数调参；</li>
<li>最后降低学习率，这里是为了最后提高准确率。</li>
</ol>
<p>可见对于基于决策树的模型，调参的方法都是大同小异，LGB 的参数调优步骤和 XGB 的调优步骤基本相同。</p>
<h3 id="学习率和估计器及数目"><a href="#学习率和估计器及数目" class="headerlink" title="学习率和估计器及数目"></a>学习率和估计器及数目</h3><blockquote>
<p>给定一个初始值，方便设置其他参数</p>
</blockquote>
<p>通常先把学习率先定一个较高的值，取 learning_rate = 0.1，其次确定估计器boosting/boost/boosting_type的类型，不过默认都会选gbdt。</p>
<p>为了确定估计器的数目，也就是boosting迭代的次数，也可以说是残差树的数目，参数名为n_estimators/num_iterations/num_round/num_boost_round。我们可以先将该参数设成一个较大的数，然后在cv结果中查看最优的迭代次数。</p>
<p>注意：在硬件条件允许的情况下，学习率尽可能小</p>
<h3 id="调整-max-depth-和-num-leaves"><a href="#调整-max-depth-和-num-leaves" class="headerlink" title="调整 max_depth 和 num_leaves"></a>调整 max_depth 和 num_leaves</h3><blockquote>
<p><strong>max_depth</strong> ：设置树深度，深度越大可能过拟合</p>
</blockquote>
<blockquote>
<p><strong>num_leaves</strong>：树的复杂程度</p>
</blockquote>
<p>因为 LightGBM 使用的是 leaf-wise 的算法，因此在调节树的复杂程度时，使用的是 num_leaves 而不是 max_depth。大致换算关系：$num_leaves = 2^(max_depth)$，但是它的值的设置应该小于 $2^(max_depth)$，否则可能会导致过拟合。</p>
<p><strong>方法：</strong></p>
<p>sklearn 里的 GridSearchCV() 函数进行搜索。</p>
<h3 id="调整-min-data-in-leaf-和-min-sum-hessian-in-leaf"><a href="#调整-min-data-in-leaf-和-min-sum-hessian-in-leaf" class="headerlink" title="调整 min_data_in_leaf 和 min_sum_hessian_in_leaf"></a>调整 min_data_in_leaf 和 min_sum_hessian_in_leaf</h3><blockquote>
<p>min_data_in_leaf:  取决于训练数据的样本个数和num_leaves.</p>
</blockquote>
<blockquote>
<p>min_sum_hessian_in_leaf: 使一个结点分裂的最小海森值之和</p>
</blockquote>
<p>方法同上：还是利用 GridSearchCV 函数</p>
<h3 id="调整-feature-fraction-和-bagging-fraction"><a href="#调整-feature-fraction-和-bagging-fraction" class="headerlink" title="调整 feature_fraction 和 bagging_fraction"></a>调整 feature_fraction 和 bagging_fraction</h3><blockquote>
<p>feature_fraction: 进行特征的子抽样</p>
</blockquote>
<blockquote>
<p>bagging_fraction+bagging_freq参数必须同时设置，bagging_fraction相当于subsample样本采样</p>
</blockquote>
<h3 id="调整正则化参数-1"><a href="#调整正则化参数-1" class="headerlink" title="调整正则化参数"></a>调整正则化参数</h3><blockquote>
<p>lambda_l1(reg_alpha), lambda_l2(reg_lambda)</p>
</blockquote>
<p>两者分别对应l1正则化和l2正则化。分别对应 l1 正则化和 l2 正则化</p>
<p>方法仍然同上。</p>
<h3 id="降低learning-rate"><a href="#降低learning-rate" class="headerlink" title="降低learning_rate"></a>降低learning_rate</h3><p>之前我们设置了较高的学习率，是为了让收敛更快，但是现在我们需要提高精度，使用较低的学习速率，以及使用更多的决策树n_estimators来训练数据。</p>
<p><strong>方法：</strong></p>
<p>LightGBM 的 cv( ) 函数</p>
<p>参数调优实例：<a href="https://blog.csdn.net/dzysunshine/article/details/92124011" target="_blank" rel="noopener">Blog</a></p>
<p><a href="#Boosting">返回…</a></p>
<h1 id="CatBoost"><a href="#CatBoost" class="headerlink" title="CatBoost"></a><div id="cat">CatBoost</div></h1><blockquote>
<p>Gradient Boosting(梯度提升) + Categorical Features(类别型特征)</p>
</blockquote>
<h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ol>
<li><strong>类别型特征的处理</strong>； 【降低拟合的同时将数据集随机排列进而将全部数据集用来学习】</li>
<li><strong>特征组合</strong>；【贪婪策略，二值处理】</li>
<li><strong>克服梯度偏差</strong> 【梯度步长的无偏估计、GBDT 技术构建新树】</li>
<li><strong>快速评分</strong> 【oblivious 树作为基本预测器转换索引为二进制向量，进一步使用二进制特征来计算模型预测值】</li>
<li><strong>基于GPU实现快速学习</strong> 【密集的数值特征、完美哈希_类别型特征、多 GPU 支持】</li>
</ol>
<p>Reference：<a href="https://cloud.tencent.com/developer/news/372336" target="_blank" rel="noopener">CatBoost “超强战斗力”的算法</a></p>
<p>Result：e.g.<br><img src="https://ask.qcloudimg.com/http-save/developer-news/gd0hpq4ceo.jpeg?imageView2/2/w/1620" alt="null"></p>
<h2 id="参数调优-1"><a href="#参数调优-1" class="headerlink" title="参数调优"></a>参数调优</h2><p>具有鲁棒性的 CatBoost 简直就是机器学习框架界的一匹黑马，不仅训练准确性超强，就连参数调优都特别舒适。</p>
<p>在 CatBoost Document 原文中，针对 CatBoost Tuning 是如此描述的</p>
<blockquote>
<p>CatBoost provides a flexible interface for parameter tuning and can be configured to suit different tasks.</p>
</blockquote>
<h3 id="One-hot-encoding"><a href="#One-hot-encoding" class="headerlink" title="One-hot encoding"></a>One-hot encoding</h3><p><strong>注意：</strong></p>
<p>由于 One-hot encoding 对训练速度和训练结果的影响较大，所以千万不要在预处理的时候使用</p>
<p><strong>适用性:</strong></p>
<p>当分类特征值较少时，选择 One-hot encoding 会取得很好的效果</p>
<p>使用过程中 One-hot encoding 不会显著提高模型质量，但是如果模型质量需要提高，可以选择修改内置参数，而不需要预处理数据。</p>
<p><strong>参数详细：</strong>  <a href="https://catboost.ai/docs/concepts/parameter-tuning.html#one-hot-enc" target="_blank" rel="noopener">Reference</a></p>
<h3 id="Number-of-trees"><a href="#Number-of-trees" class="headerlink" title="Number of trees"></a>Number of trees</h3><ul>
<li><p>CatBoost 在参数调优时，如果要调整任何其他参数，首先应该确定模型没有非正常拟合现象。所以我们有必要分析数据集的度量值和迭代次数。</p>
</li>
<li><p>可以通过将迭代次数设置为一个较大值，使用  overfitting detector 参数并且将 use best model options 参数调大，这样一来，模型训练结果只包含前 k 个最佳迭代。【k-最佳损失值的索引】</p>
</li>
<li><p>用于选择最佳模型的度量可能与用于优化目标值的度量不同。</p>
</li>
</ul>
<p><strong>参数详细：</strong> <a href="https://catboost.ai/docs/concepts/parameter-tuning.html#trees-number" target="_blank" rel="noopener">Reference</a></p>
<h3 id="Tree-depth"><a href="#Tree-depth" class="headerlink" title="Tree depth"></a>Tree depth</h3><blockquote>
<p>在大多数情况下，最佳深度范围是 4 ~ 10，建议深度6 ~ 10</p>
</blockquote>
<p>The maximum depth of the trees is limited to 8 for pairwise modes (YetiRank, PairLogitPairwise and QueryCrossEntropy) when the training is performed on GPU.</p>
<p>GPU 上训练时，树的最大深度被限制在了 8。</p>
<p><strong>参数详细：</strong> <a href="https://catboost.ai/docs/concepts/parameter-tuning.html#tree-depth" target="_blank" rel="noopener">Reference</a></p>
<h3 id="Border-count"><a href="#Border-count" class="headerlink" title="Border count"></a>Border count</h3><blockquote>
<p>数字特征的分割数</p>
</blockquote>
<p>此参数的值会影响 GPU 上的训练速度，值越小，训练速度越快。【Detail：<a href="https://catboost.ai/docs/concepts/speed-up-training.html#splis-numerical-features" target="_blank" rel="noopener">Number of splits for numerical features</a>】</p>
<p>分割数 128 一般情况下来说足够了，但是如果想要更好的训练质量，可以将此参数设置为 254.</p>
<p><strong>参数详细：</strong>  <a href="https://catboost.ai/docs/concepts/parameter-tuning.html#border-count" target="_blank" rel="noopener">Reference</a></p>
<h3 id="其他参数："><a href="#其他参数：" class="headerlink" title="其他参数："></a>其他参数：</h3><blockquote>
<p>参见：<a href="https://catboost.ai/docs/concepts/parameter-tuning.html" target="_blank" rel="noopener">Reference</a></p>
</blockquote>
<p>说了这么多，其实参数调优只能从表面上解决问题，如果想要实质性或更大的优化，还需要在<strong>特征工程</strong>和<strong>模型集成</strong>上大做文章。</p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>Boost</tag>
      </tags>
  </entry>
  <entry>
    <title>Feature Engineering for Data Mining</title>
    <url>/2020/03/26/Feature-Engineering-for-Data-Mining/</url>
    <content><![CDATA[<p><strong>重要性：</strong></p>
<p>特征工程是比赛中重要的一部分，调参带来的效益往往是有限的，特征工程选取的好坏决定最终的结果。</p>
<p><strong>必要性：</strong></p>
<p>特征工程是和模型结合在一起的，其能更好地表示潜在问题的特征，从而提高机器学习的性能。</p>
<p><strong>特征工程是很低的，容易入门，但是若想精通就需要花费较大的功夫了，正所谓师傅领进门，修行靠个人。</strong></p>
<p><strong>目标：</strong></p>
<p>学习特征工程我们的目的是：对于特征进行进一步分析，并对于数据进行处理。</p>
<p><strong>内容：</strong></p>
<blockquote>
<p>本文我将从以下几点介绍我认识的特征工程</p>
</blockquote>
<ol>
<li><a href="#1_title">异常处理；</a></li>
<li><a href="#2_title">特征归一化/标准化；</a></li>
<li><a href="#3_title">数据分桶；</a></li>
<li><a href="#4_title">缺失值处理；</a></li>
<li><a href="#5_title">特征构造；</a></li>
<li><a href="#6_title">特征筛选；</a></li>
<li><a href="#7_title">降维。</a></li>
</ol>
<h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a><div id="1_title">异常处理</div></h2><blockquote>
<p>通常有以下几种方法</p>
</blockquote>
<ul>
<li><div id="return"><font color=sky-blue>通过箱线图（或 3-Sigma）分析删除异常值；</font></div></li>
<li><a href="#box_cox">BOX-COX 转换（处理有偏分布）；</a></li>
<li><a href="#tile_cut">长尾截断；</a></li>
</ul>
<p><strong>异常值处理：</strong></p>
<blockquote>
<p>通过箱线图，分析并删去异常值</p>
</blockquote>
<ul>
<li>上四分位数Q3 是指数据从小到大排列，取其3/4处位置的分位数</li>
<li>下四分位数Q1 是指数据从小到大排列，取其1/4处位置的分位数</li>
<li>IQR = Q3-Q1指的是四分位距，即上四分位数和下四分位数的差值</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 给出一个数据异常值处理的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outliers_proc</span><span class="params">(data, col_name, scale=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用于清洗异常值，默认用 box_plot（scale=3）进行清洗</span></span><br><span class="line"><span class="string">    :param data: 接收 pandas 数据格式</span></span><br><span class="line"><span class="string">    :param col_name: pandas 列名</span></span><br><span class="line"><span class="string">    :param scale: 尺度</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">box_plot_outliers</span><span class="params">(data_ser, box_scale)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        利用箱线图去除异常值</span></span><br><span class="line"><span class="string">        :param data_ser: 接收 pandas.Series 数据格式</span></span><br><span class="line"><span class="string">        :param box_scale: 箱线图尺度，</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        iqr = box_scale * (data_ser.quantile(<span class="number">0.75</span>) - data_ser.quantile(<span class="number">0.25</span>))</span><br><span class="line">        val_low = data_ser.quantile(<span class="number">0.25</span>) - iqr</span><br><span class="line">        val_up = data_ser.quantile(<span class="number">0.75</span>) + iqr</span><br><span class="line">        rule_low = (data_ser &lt; val_low)</span><br><span class="line">        rule_up = (data_ser &gt; val_up)</span><br><span class="line">        <span class="keyword">return</span> (rule_low, rule_up), (val_low, val_up)</span><br><span class="line"></span><br><span class="line">    data_n = data.copy()</span><br><span class="line">    data_series = data_n[col_name]</span><br><span class="line">    rule, value = box_plot_outliers(data_series, box_scale=scale)</span><br><span class="line">    index = np.arange(data_series.shape[<span class="number">0</span>])[rule[<span class="number">0</span>] | rule[<span class="number">1</span>]]</span><br><span class="line">    print(<span class="string">"Delete number is: &#123;&#125;"</span>.format(len(index)))</span><br><span class="line">    data_n = data_n.drop(index)</span><br><span class="line">    data_n.reset_index(drop=<span class="literal">True</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    print(<span class="string">"Now column number is: &#123;&#125;"</span>.format(data_n.shape[<span class="number">0</span>]))</span><br><span class="line">    index_low = np.arange(data_series.shape[<span class="number">0</span>])[rule[<span class="number">0</span>]]</span><br><span class="line">    outliers = data_series.iloc[index_low]</span><br><span class="line">    print(<span class="string">"Description of data less than the lower bound is:"</span>)</span><br><span class="line">    print(pd.Series(outliers).describe())</span><br><span class="line">    index_up = np.arange(data_series.shape[<span class="number">0</span>])[rule[<span class="number">1</span>]]</span><br><span class="line">    outliers = data_series.iloc[index_up]</span><br><span class="line">    print(<span class="string">"Description of data larger than the upper bound is:"</span>)</span><br><span class="line">    print(pd.Series(outliers).describe())</span><br><span class="line">    </span><br><span class="line">    fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">    sns.boxplot(y=data[col_name], data=data, palette=<span class="string">"Set1"</span>, ax=ax[<span class="number">0</span>])</span><br><span class="line">    sns.boxplot(y=data_n[col_name], data=data_n, palette=<span class="string">"Set1"</span>, ax=ax[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> data_n</span><br></pre></td></tr></table></figure>
<p><a href="#return">返回..</a></p>
<div id="box_cox"><b>BOX-COX 转换（处理有偏分布）</b></div><br/>

<blockquote>
<p>将数据转化为正态分布</p>
</blockquote>
<p>我们都知道 <a href="https://www.cnblogs.com/my-love-is-python/p/10322080.html" target="_blank" rel="noopener">log 变换</a> 可以使数据在一定程度上符合正态分布，特别是有尾数据分布。</p>
<p>但是…采用这种方法，</p>
<ul>
<li><p>较小数据之间的差异将会变大(因为对数函数的斜率很小)，而较大数据之间的差异将减少(可能某分布中较大数据的斜率很小)。</p>
</li>
<li><p>如果你拓展了左尾的差异，减少了右尾的差异，结果将是方差恒定、形状对称的正态分布(无论均值大小如何)。</p>
</li>
</ul>
<p>参见【Reference】：<a href="http://blog.sina.com.cn/s/blog_839b4f640102vqen.html" target="_blank" rel="noopener">使用Box-Cox转换的益处</a> 、<a href="https://wenku.baidu.com/view/96140c8376a20029bd642de3.html" target="_blank" rel="noopener">BoxCox 变换方法及其实现运用</a></p>
<p><a href="#return">返回..</a></p>
<div id="tile_cut"><b>长尾截断</b></div><br/>

<p>对于连续型数值特征，有时精度太高可能只是噪声，并不具备太多的信息，也使得特征维度急剧上升。因此可以保留一定的精度，之后当作类别特征进行处理。对于长尾的数据，可以先进行对数缩放，再进行精度截断，之后可以当做类别变量做二值化处理。</p>
<p><img src="https://img-blog.csdnimg.cn/20200326112942490.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="特征归一化-标准化"><a href="#特征归一化-标准化" class="headerlink" title="特征归一化 / 标准化"></a><div id="2_title">特征归一化 / 标准化</div></h2><p><strong>思想：</strong></p>
<ul>
<li>标准化（转换为标准正态分布）；</li>
<li>归一化（映射到 [0,1] 区间）； - Sigmoid 函数</li>
<li>针对幂律分布，可以采用公式：$log( \frac{1+x}{1+median})$</li>
</ul>
<p><strong>算法：</strong></p>
<ol>
<li>Z-score</li>
<li>min-max</li>
<li>行归一化</li>
</ol>
<p>特征尺度不一致需要标准化，以下算法会受尺度影响</p>
<ol start="4">
<li>KNN，依赖距离</li>
<li>K-meas</li>
<li>逻辑回归，支持向量机，神经网络等如果使用梯度下降来学习权重；</li>
<li>主成分分析，特征向量偏向于较大的列</li>
</ol>
<p>在机器学习 Pipeline 中使用归一化</p>
<h2 id="数据分桶"><a href="#数据分桶" class="headerlink" title="数据分桶"></a><div id="3_title">数据分桶</div></h2><p><strong>Q:</strong></p>
<p>为什么要做数据分桶呢？</p>
<p><strong>A:</strong></p>
<ol>
<li>离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；</li>
<li>离散后的特征对异常值更具鲁棒性，如 age&gt;30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；</li>
<li>LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；</li>
<li>离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；</li>
<li>特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化</li>
<li>e.t.c 当然还有很多原因，LightGBM 在改进 XGBoost 时就增加了数据分桶，增强了模型的泛化性</li>
</ol>
<ul>
<li>等频分桶；</li>
<li>等距分桶；</li>
<li>Best-KS 分桶（类似利用基尼指数进行二分类）；</li>
<li>卡方分桶；</li>
</ul>
<p>参见【Reference】：<a href="https://flashgene.com/archives/81897.html" target="_blank" rel="noopener">浅谈微视推荐系统中的特征工程</a></p>
<h2 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a><div id="4_title">缺失值处理</div></h2><ul>
<li>不处理（针对类似 XGBoost 等树模型）；</li>
<li>删除（缺失数据太多）；</li>
<li>插值补全，包括均值/中位数/众数/建模预测/多重插补/压缩感知补全/矩阵补全等；</li>
<li>分箱，缺失值一个箱；</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200326151308471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>参见【Reference】：<a href="https://www.zhihu.com/question/26639110" target="_blank" rel="noopener">机器学习中如何处理缺失数据？</a>、<a href="http://cda.pinggu.org/view/20394.html" target="_blank" rel="noopener">数据缺失值的4种处理方法</a></p>
<h2 id="特征构造"><a href="#特征构造" class="headerlink" title="特征构造"></a><div id="5_title">特征构造</div></h2><ul>
<li>构造统计量特征，报告计数、求和、比例、标准差等；</li>
<li>时间特征，包括相对时间和绝对时间，节假日，双休日等；</li>
<li>地理信息，包括分箱，分布编码等方法；</li>
<li>非线性变换，包括 log/ 平方/ 根号等；</li>
<li>特征组合，特征交叉；</li>
</ul>
<blockquote>
<p>训练集和测试集放在一起，方便构造特征 【contact】</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Train_data[<span class="string">'train'</span>]=<span class="number">1</span></span><br><span class="line">Test_data[<span class="string">'train'</span>]=<span class="number">0</span></span><br><span class="line">data = pd.concat([Train_data, Test_data], ignore_index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>检查空数据</p>
</blockquote>
<p>可以选择删除也可以留下</p>
<ul>
<li>删除缺失数据占总样本量过大，不建议删去；</li>
<li>当要交到xgboost模型中处理时，不必要。</li>
</ul>
<blockquote>
<p>特征的构造视模型数量而定</p>
</blockquote>
<p>不同模型对数据集的要求不同，需要分开构造</p>
<blockquote>
<p>特征归一化 / 标准化和数据分桶通常体现在特征构造的过程中</p>
</blockquote>
<h2 id="特征筛选"><a href="#特征筛选" class="headerlink" title="特征筛选"></a><div id="6_title">特征筛选</div></h2><ul>
<li>过滤式（filter）：先对数据进行特征选择，然后在训练学习器，常见的方法有 Relief / 方差选择法 / 相关系数法 / 卡方检验法 / 互信息法；</li>
<li>包裹式（wrapper）：直接把最终将要使用的学习器的性能作为特征子集的评价准则，常见方法有 LVM（Las Vegas Wrapper） ；</li>
<li>嵌入式（embedding）：结合过滤式和包裹式，学习器训练过程中自动进行了特征选择，常见的有 lasso 回归；</li>
</ul>
<p>参见【Reference】：<a href="https://baijiahao.baidu.com/s?id=1604074325918456186&wfr=spider&for=pc" target="_blank" rel="noopener">如何进行特征选择（理论篇）</a></p>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a><div id="7_title">降维</div></h2><blockquote>
<p>采用某种映射方法，将原高维空间中的数据点映射到低维度的空间中</p>
</blockquote>
<p>降维本质上即是学习一个映射函数 $f : x-&gt;y$【其中x是原始数据点的表达，目前最多使用向量表达形式。 y是数据点映射后的低维向量表达，通常y的维度小于x的维度（当然提高维度也是可以的）】</p>
<p>$f$ 可能是显式的或隐式的、线性的或非线性的</p>
<p><strong>目的：</strong></p>
<ul>
<li>减少冗余信息所造成的误差,提高识别（或其他应用）的精度；</li>
<li>希望通过降维算法来寻找数据内部的本质结构特征</li>
</ul>
<p><strong>方法：</strong></p>
<ul>
<li>PCA / LDA / ICA；</li>
<li>特征选择也是一种降维。</li>
</ul>
<p>参见【Reference】:<a href="https://blog.csdn.net/rosenor1/article/details/52278116" target="_blank" rel="noopener">机器学习四大降维方法</a></p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>EDA-数据探索性分析</title>
    <url>/2020/03/24/EDA-%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>EDA (Exploratory Data Analysis)，也就是对数据进行探索性的分析，从而为之后的数据预处理和特征工程提供必要的结论。</p>
<p>针对数据集导入和数据的读取不再赘述，是数据集格式而定。</p>
<blockquote>
<p>格式：.csv / .json / .txt .e.t.c</p>
</blockquote>
<p>参见：<a href="https://blog.csdn.net/weixin_42297855/article/details/97501680" target="_blank" rel="noopener" title="关于 EDA 的 Blog">关于 EDA 的 Blog</a></p>
<p>我想就以下几个问题谈谈我认识的和学到的 EDA。</p>
<ol>
<li><a href="#data_overview">数据总览；</a></li>
<li><a href="#judge">判断数据缺失和异常；</a></li>
<li><a href="#location">了解预测值的分布；</a></li>
<li><a href="#num_analy">数字特征分析；</a></li>
<li><a href="#category_analy">类型特征分析。</a></li>
</ol>
<h2 id="数据总览"><a href="#数据总览" class="headerlink" title="数据总览"></a><div id="data_overview">数据总览</div></h2><p>为了熟悉数据的相关统计量，此前我一直使用的是 describe( )。</p>
<p><strong>优点：</strong> 其统计结果中有每列的统计量【个数 count、平均值 mean、方差 std、最小值 min、中位数25% 50% 75% 、以及最大值】</p>
<blockquote>
<p>方便瞬间掌握数据的大概的范围以及每个值的异常值的判断</p>
</blockquote>
<p><strong>e.g.</strong> 999 9999 -1 等值这些其实都是 nan 的另外一种表达方式</p>
<p>我想说的是还有另外一种数据总览的方法 info( )。</p>
<p><strong>优点：</strong> 了解数据每列的type，有助于了解是否存在除了 nan 以外的特殊符号异常。</p>
<p>describe() 的结果就不展示了，比较直观，针对 info() 的结果展示一个范例。</p>
<center><a  data-fancybox="gallery" href="https://img-blog.csdnimg.cn/2020032323381238.png" target="_blank" rel="noopener">
    <img src="https://img-blog.csdnimg.cn/2020032323381238.png">
</a></center>

<h2 id="判断数据缺失和异常"><a href="#判断数据缺失和异常" class="headerlink" title="判断数据缺失和异常"></a><div id="judge">判断数据缺失和异常</div></h2><blockquote>
<p>说白了还是针对 nan 类型的</p>
</blockquote>
<p>如果 nan 的数量很小一般选择填充，如果使用 lgb 等树模型可以直接空缺，让树自己去优化，但如果 nan 存在的过多、可以考虑删掉</p>
<hr>
<p>补充：</p>
<p>&ensp;&ensp;nan，即非数值，不等于任何数，也不等于 nan 本身</p>
<p>&ensp;&ensp;是否为 nan 类型的判断方式：isnan( )  <strong>or</strong>  isnull( )</p>
<hr>
<p>面对缺省值的三类处理方法：</p>
<ol>
<li>用平均值、中值、分位数、众数、随机值等替代。</li>
</ol>
<blockquote>
<p>效果一般，因为等于人为增加了噪声。</p>
</blockquote>
<ol start="2">
<li>用其他变量做预测模型来算出缺失变量。</li>
</ol>
<blockquote>
<p>效果比方法1略好，<br>但有一个根本缺陷</p>
<blockquote>
<p>如果其他变量和缺失变量无关，则预测的结果无意义。</p>
</blockquote>
<blockquote>
<p>如果预测结果相当准确，则又说明这个变量是没必要加入建模的。一般情况下，介于两者之间。</p>
</blockquote>
</blockquote>
<ol start="3">
<li>最精确的做法，把变量映射到高维空间。</li>
</ol>
<blockquote>
<p>For Example: 性别，有男、女、缺失三种情况，则映射成3个变量：是否男、是否女、是否缺失。</p>
</blockquote>
<p>连续型变量也可以这样处理。比如Google、百度的CTR预估模型，预处理时会把所有变量都这样处理，达到几亿维。</p>
<p>这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值、不用考虑线性不可分之类的问题。缺点是计算量大大提升。</p>
<p>而且只有在样本量非常大的时候效果才好，否则会因为过于稀疏，效果很差。</p>
<h2 id="了解预测值的分布"><a href="#了解预测值的分布" class="headerlink" title="了解预测值的分布"></a><div id="location">了解预测值的分布</div></h2><ol>
<li>总体分布概况（无界约翰逊分布等）</li>
</ol>
<blockquote>
<p>针对预测值的特征寻找最佳拟合曲线，对数 / 正态 / 无界 Johnson SU</p>
</blockquote>
<ol start="2">
<li>查看 skewness and kurtosis</li>
</ol>
<blockquote>
<p>分别对应数据的偏度 .skew( )  &amp;&amp; 峰度  .kurt( )</p>
<blockquote>
<p>参见：<a href="https://www.cnblogs.com/wyy1480/p/10474046.html" target="_blank" rel="noopener">偏度峰度简介及实现</a></p>
</blockquote>
</blockquote>
<ol start="3">
<li>查看预测值的具体频数</li>
</ol>
<blockquote>
<p>通过直方图可视化预测数据，可以得到频数</p>
</blockquote>
<p><strong>处理方法：</strong> 把值较少的部分填充或者删掉</p>
<p>处理过后的数据比较集中，可以在预测之前先进行变换，一般进行 log 变换，使得数据均匀化分布</p>
<h2 id="特征分析"><a href="#特征分析" class="headerlink" title="特征分析"></a>特征分析</h2><ol>
<li>特征 nunique 分布</li>
<li>数据可视化</li>
</ol>
<h3 id="数字特征分析"><a href="#数字特征分析" class="headerlink" title="数字特征分析"></a><div id="data_analy">数字特征分析</div></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">numeric_features = Train_data.select_dtypes(include=[np.number])</span><br><span class="line">numeric_features.columns</span><br></pre></td></tr></table></figure>

<h3 id="类型特征分析"><a href="#类型特征分析" class="headerlink" title="类型特征分析"></a><div id="category_analy">类型特征分析</div></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">categorical_features = Train_data.select_dtypes(include=[np.object])</span><br><span class="line">categorical_features.columns</span><br></pre></td></tr></table></figure>

<h3 id="nunique-分布"><a href="#nunique-分布" class="headerlink" title="nunique 分布"></a>nunique 分布</h3><blockquote>
<p>针对 object 类型字段特征</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> categorical_feas:</span><br><span class="line">    print(feature + <span class="string">"的特征分布如下："</span>)</span><br><span class="line">    print(data_train[feature].value_counts())</span><br><span class="line">    <span class="keyword">if</span> feature != <span class="string">'communityName'</span>:</span><br><span class="line">        plt.hist(data_all[feature], bins=<span class="number">3</span>)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>根据结果找到分类结果的分布特征情况</p>
</blockquote>
<h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><blockquote>
<p>作为一种工具，旨在展现数据或者特征之间的关系</p>
</blockquote>
<p>参见：<a href="https://www.jianshu.com/p/6e18d21a4cad" target="_blank" rel="noopener">数据可视化参数调整</a></p>
<p>数据关系可以分为四种类型：</p>
<ol>
<li><p><strong>比较：</strong> 比较数据间各类别的关系，或者是它们随着时间的变化趋势，比如折线图；</p>
</li>
<li><p><strong>联系：</strong> 查看两个或两个以上变量之间的关系，比如散点图；</p>
</li>
<li><p><strong>构成：</strong> 每个部分占整体的百分比，或者是随着时间的百分比变化，比如饼图；</p>
</li>
<li><p><strong>分布：</strong> 关注单个变量，或者多个变量的分布情况，比如直方图。</p>
</li>
</ol>
<p>参见：<a href="https://www.jianshu.com/p/1b4f351013d3" target="_blank" rel="noopener">数据可视化是一种技能</a></p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>EDA</tag>
      </tags>
  </entry>
  <entry>
    <title>涉猎线程</title>
    <url>/2020/03/19/%E6%B6%89%E7%8C%8E%E7%BA%BF%E7%A8%8B/</url>
    <content><![CDATA[<blockquote>
<p>在Java程序设计语言中，并发程序主要集中于线程</p>
</blockquote>
<blockquote>
<p>而且随着计算机系统拥有多个处理器或带有多个执行内核，线程的系统能力得到了很大的增强</p>
</blockquote>
<blockquote>
<p>其中并发程序设计是指由若干个可在同一时间段执行的程序模块组成程序的程序设计方法。</p>
</blockquote>
<p>这种可并发执行的程序模块称为<strong>进程</strong>，进程由数据和机器指令和堆栈组成，存储在磁盘的程序运行起来就是一个进程，进程在内存中，所以<strong>在一个操作系统中能开启的进程是有限的</strong>。</p>
<h3 id="线程"><a href="#线程" class="headerlink" title="线程"></a><strong>线程</strong></h3><blockquote>
<p>它是进程中的一个独立运行单位，是程序执行流的最小单位。</p>
</blockquote>
<p>线程（Thread）是并发编程的基础，也是程序执行的最小单元，它依托进程而存在。一个进程中可以包含多个线程，多线程可以共享一块内存空间和一组系统资源，因此线程之间的切换更加节省资源、更加轻量化，也因此被称为轻量进程，</p>
<p>线程自己不拥有系统资源，只拥有在运行中必不可少的资源，一个线程可以创建和撤销另一个线程，同一个进程中多个线程之间可以并发执行。</p>
<p>由于线程之间的相互制约，可以说线程有暂停，等待，休眠，停止等状态，从你创建一个线程，到执行任务完毕，这是线程的一个生命周期。</p>
<center id="return"><img src="https://img-blog.csdnimg.cn/20200319094140271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p><a href="#transfer">线程工作模式  详述…</a></p>
<p><strong>线程在CPU上执行</strong>，（CPU多少核多少线程，简单来说意味着在这台电脑上同一时间能执行多少个线程），<strong>线程执行需要消耗CPU（运行），高速缓存（缓存线程运行时所需的数据），内存（存储数据）</strong>，CPU的读取速度是最快的，为达到最大效率，将数据提前提取到高速缓存中便于CPU在运行时能尽量体现其高速的特性。</p>
<h3 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h3><p>线程是程序中一个单一的顺序控制流程，在耽搁程序中同时运行多个线程完成不同的工作，即为多线程。</p>
<p>引入线程是因为<strong>创建一个新线程花费时间少，两个线程（在同一进程中的）的切换时间少,</strong> 线程之间互相通信不必调用内核，线程能独立执行；</p>
<p>而使用多线程就是最大限度的利用硬件资源来提升程序的效率，（这也就是说在一个进程中不能开启N个线程，因为硬件资源会限制执行效率）。</p>
<h3 id="Java实现多线程有两种方式"><a href="#Java实现多线程有两种方式" class="headerlink" title="Java实现多线程有两种方式"></a>Java实现多线程有两种方式</h3><ol>
<li>Runnable接口：此接口中有一个run（）方法，是线程的运行方法（线程要执行的任务），当run结束时，线程也就结束了；</li>
<li>Thread类：（Thread类是Runnable接口的子类）所以run方法仍然保持，此外这个类中还有start( );和sleep(long time); start方法是线程的启动方法，如果想要JVM把你的这个类当作一个线程，就要使用start方法来启动线程，sleep是线程的休眠方法，单位是毫秒</li>
</ol>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>新线程态(New Thread)</td>
<td>可运行态(Runnable)</td>
</tr>
<tr>
<td>非运行态(Not Runnable)</td>
<td>死亡态(Dead)</td>
</tr>
</tbody></table>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Runnable 是接口 其内部有run()方法</span></span><br><span class="line"><span class="comment"> * Thread 是类  继承了Runnable的接口</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span></span>&#123; <span class="comment">//implements Runnable&#123; // extends Thread&#123;</span></span><br><span class="line"> </span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">int</span> num;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="title">Test</span><span class="params">(<span class="keyword">int</span> num)</span></span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.num = num;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  num++;</span><br><span class="line">  System.out.println(Thread.currentThread()+<span class="string">" "</span>+<span class="string">"num ="</span>+num);</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadTest</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line"> </span><br><span class="line"> <span class="keyword">public</span> Test test;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ThreadTest</span><span class="params">(Test test)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.test = test;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(test.num&lt;<span class="number">20</span>) &#123;</span><br><span class="line">  test.print();</span><br><span class="line">  <span class="comment">//异常事件处理</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="comment">//设置休眠时间，以免太快而无法观察</span></span><br><span class="line">   Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">   e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">  Test tes = <span class="keyword">new</span> Test(<span class="number">1</span>);</span><br><span class="line">  <span class="comment">//启动多线程</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=<span class="number">10</span>;i++) &#123;</span><br><span class="line">  ThreadTest thr = <span class="keyword">new</span> ThreadTest(tes);</span><br><span class="line">  <span class="comment">//构造方法转换thr对象类型</span></span><br><span class="line">  Thread t = <span class="keyword">new</span> Thread(thr);</span><br><span class="line">  t.start();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//输出当前的线程</span></span><br><span class="line">  System.out.println(Thread.currentThread());</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Thread[Thread-<span class="number">1</span>,<span class="number">5</span>,main] num =<span class="number">2</span></span><br><span class="line">Thread[Thread-<span class="number">13</span>,<span class="number">5</span>,main] num =<span class="number">3</span></span><br><span class="line">Thread[Thread-<span class="number">9</span>,<span class="number">5</span>,main] num =<span class="number">4</span></span><br><span class="line">Thread[Thread-<span class="number">11</span>,<span class="number">5</span>,main] num =<span class="number">5</span></span><br><span class="line">Thread[main,<span class="number">5</span>,main]</span><br><span class="line">Thread[Thread-<span class="number">7</span>,<span class="number">5</span>,main] num =<span class="number">6</span></span><br><span class="line">Thread[Thread-<span class="number">3</span>,<span class="number">5</span>,main] num =<span class="number">7</span></span><br><span class="line">Thread[Thread-<span class="number">5</span>,<span class="number">5</span>,main] num =<span class="number">8</span></span><br><span class="line">Thread[Thread-<span class="number">21</span>,<span class="number">5</span>,main] num =<span class="number">9</span></span><br><span class="line">Thread[Thread-<span class="number">19</span>,<span class="number">5</span>,main] num =<span class="number">10</span></span><br><span class="line">Thread[Thread-<span class="number">15</span>,<span class="number">5</span>,main] num =<span class="number">11</span></span><br><span class="line">Thread[Thread-<span class="number">17</span>,<span class="number">5</span>,main] num =<span class="number">12</span></span><br><span class="line">Thread[Thread-<span class="number">7</span>,<span class="number">5</span>,main] num =<span class="number">13</span></span><br><span class="line">Thread[Thread-<span class="number">5</span>,<span class="number">5</span>,main] num =<span class="number">14</span></span><br><span class="line">Thread[Thread-<span class="number">15</span>,<span class="number">5</span>,main] num =<span class="number">15</span></span><br><span class="line">Thread[Thread-<span class="number">11</span>,<span class="number">5</span>,main] num =<span class="number">16</span></span><br><span class="line">Thread[Thread-<span class="number">13</span>,<span class="number">5</span>,main] num =<span class="number">17</span></span><br><span class="line">Thread[Thread-<span class="number">1</span>,<span class="number">5</span>,main] num =<span class="number">18</span></span><br><span class="line">Thread[Thread-<span class="number">21</span>,<span class="number">5</span>,main] num =<span class="number">19</span></span><br><span class="line">Thread[Thread-<span class="number">19</span>,<span class="number">5</span>,main] num =<span class="number">20</span></span><br><span class="line">Thread[Thread-<span class="number">17</span>,<span class="number">5</span>,main] num =<span class="number">21</span></span><br><span class="line">Thread[Thread-<span class="number">3</span>,<span class="number">5</span>,main] num =<span class="number">22</span></span><br><span class="line">Thread[Thread-<span class="number">9</span>,<span class="number">5</span>,main] num =<span class="number">23</span></span><br><span class="line"><span class="comment">//在这里发现运行结果并非全部的num都变成了20，这就是多线程控制同一变量变化时出现的问题</span></span><br></pre></td></tr></table></figure>
<p>出现了问题，下边就来解决：</p>
<p>共用代码块是我想到的第一种方法，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span></span>&#123; <span class="comment">//implements Runnable&#123; // extends Thread&#123;</span></span><br><span class="line"> </span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">int</span> num;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="title">Test</span><span class="params">(<span class="keyword">int</span> num)</span></span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.num = num;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  num++;</span><br><span class="line">  System.out.println(Thread.currentThread()+<span class="string">" "</span>+<span class="string">"num ="</span>+num);</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadTest</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line"> </span><br><span class="line"> <span class="keyword">public</span> Test test;</span><br><span class="line"> <span class="comment">//为解决多线程控制同一变量时的问题</span></span><br><span class="line"> <span class="keyword">public</span> Object obj = <span class="keyword">new</span> Object();</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="title">ThreadTest</span><span class="params">(Test test)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.test = test;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">synchronized</span>(obj) &#123;<span class="comment">//共用代码块</span></span><br><span class="line">  <span class="keyword">while</span>(test.num&lt;<span class="number">20</span>) &#123;</span><br><span class="line">  test.print();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">   Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">   e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">  Test tes = <span class="keyword">new</span> Test(<span class="number">1</span>);</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=<span class="number">10</span>;i++) &#123;</span><br><span class="line">  ThreadTest thr = <span class="keyword">new</span> ThreadTest(tes);</span><br><span class="line">  Thread t = <span class="keyword">new</span> Thread(thr);</span><br><span class="line">  t.start();</span><br><span class="line">  &#125;</span><br><span class="line">  System.out.println(Thread.currentThread());</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但貌似并没有起到预想的效果，再接再厉……</p>
<p>之后在实现界面上直线等的动画时再次见到了多线程，这次与之不同的是，我在网上查找到了多线程怎么控制并发数，有两种方法，第一种是用join方法，第二种则是利用线程池</p>
<p>过了好久终于解决了～～～～～</p>
<p>首先明白一个问题，创建一个线程的时间远远小于启动一个线程去运行的时间，所以应该将创建和启动线程分开写</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=<span class="number">10</span>;i++) &#123;</span><br><span class="line">			ThreadTest thr = <span class="keyword">new</span> ThreadTest(tes);</span><br><span class="line">			list.add(thr);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">for</span>(ThreadTest tt : list) &#123;</span><br><span class="line">			tt.start();</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure>
<p>第二点，自加时间足够短，加休眠有点多此一举了，所以run方法就只剩下了一行代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">test.print();</span><br></pre></td></tr></table></figure>
<p>最终的运行结构发现终于没有重复的值了</p>
<p>面对一个问题的时候，从问题的本质入手，简单代码上手，理解多线程同步是一个巨大的工程：</p>
<blockquote>
<p>顺便列举一下了解到的多线程同步的七种方法吧</p>
</blockquote>
<ol>
<li>同步方法</li>
<li>同步代码块（此上两种方法都是涉及到synchronized关键字的）</li>
<li>wait与notify</li>
<li>使用特殊域变量（volatile）实现</li>
<li>重入锁（）java.util.concurrennt包</li>
<li>使用局部变量</li>
<li>使用阻塞队列实现<strong>原子操作</strong></li>
</ol>
<hr>
<p><font size=4><b>补充：</b></font></p>
<div align=center><b>线程状态</b></div><br/>

<p>线程的状态在 JDK 1.5 之后以枚举的方式被定义在 Thread 的源码中，它总共包含以下 6 个状态：</p>
<ul>
<li><strong>NEW</strong>，新建状态，线程被创建出来，但尚未启动时的线程状态；</li>
<li><strong>RUNNABLE</strong>，就绪状态，表示可以运行的线程状态，它可能正在运行，或者是在排队等待操作系统给它分配 CPU 资源；</li>
<li><strong>BLOCKED</strong>，阻塞等待锁的线程状态，表示处于阻塞状态的线程正在等待监视器锁，比如等待执行 synchronized 代码块或者使用 synchronized 标记的方法；  </li>
<li><strong>WAITING</strong>，等待状态，一个处于等待状态的线程正在等待另一个线程执行某个特定的动作，比如，一个线程调用了Object.wait()方法，那它就在等待另一个线程调用Object.notify()…</li>
<li><strong>TIMED_WAITING</strong>，计时等待状态，和等待状态（WAITING）类似，它只是多了超时时间，比如调用了有超时时间设置的方法Object.wait(longtimeout)和Thread.j…</li>
<li><strong>TERMINATED</strong>，终止状态，表示线程已经执行完成。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> State &#123; </span><br><span class="line">	<span class="comment">/** </span></span><br><span class="line"><span class="comment">	* 新建状态，线程被创建出来，但尚未启动时的线程状态 </span></span><br><span class="line"><span class="comment">	*/</span> </span><br><span class="line">	</span><br><span class="line">	NEW,</span><br><span class="line">	 </span><br><span class="line">	<span class="comment">/** </span></span><br><span class="line"><span class="comment">	* 就绪状态，表示可以运行的线程状态，但它在排队等待来自操作系统的 CPU 资源 </span></span><br><span class="line"><span class="comment">	*/</span> </span><br><span class="line">	</span><br><span class="line">	RUNNABLE, </span><br><span class="line">	</span><br><span class="line">	<span class="comment">/** </span></span><br><span class="line"><span class="comment">	* 阻塞等待锁的线程状态，表示正在处于阻塞状态的线程 </span></span><br><span class="line"><span class="comment">	* 正在等待监视器锁，比如等待执行 synchronized 代码块或者 </span></span><br><span class="line"><span class="comment">	* 使用 synchronized 标记的方法 </span></span><br><span class="line"><span class="comment">	*/</span></span><br><span class="line">	</span><br><span class="line">	BLOCKED, </span><br><span class="line">	</span><br><span class="line">	<span class="comment">/** </span></span><br><span class="line"><span class="comment">	* 等待状态，一个处于等待状态的线程正在等待另一个线程执行某个特定的动作。 </span></span><br><span class="line"><span class="comment">	* 例如，一个线程调用了 Object.wait() 它在等待另一个线程调用 </span></span><br><span class="line"><span class="comment">	* Object.notify() 或 Object.notifyAll() </span></span><br><span class="line"><span class="comment">	*/</span> </span><br><span class="line"></span><br><span class="line">	WAITING, </span><br><span class="line">	</span><br><span class="line">	<span class="comment">/** </span></span><br><span class="line"><span class="comment">	* 计时等待状态，和等待状态 (WAITING) 类似，只是多了超时时间，比如 </span></span><br><span class="line"><span class="comment">	* 调用了有超时时间设置的方法 Object.wait(long timeout) 和 </span></span><br><span class="line"><span class="comment">	* Thread.join(long timeout) 就会进入此状态 </span></span><br><span class="line"><span class="comment">	*/</span> </span><br><span class="line"></span><br><span class="line">	TIMED_WAITING, </span><br><span class="line"></span><br><span class="line">	<span class="comment">/** </span></span><br><span class="line"><span class="comment">	* 终止状态，表示线程已经执行完成 </span></span><br><span class="line"><span class="comment">	*/</span></span><br><span class="line"></span><br><span class="line">	TERMINATED</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><a href="#return">返回…</a><div  id="transfer"  align=center><b>线程的工作模式</b></div><br/></p>
<ol>
<li><p>首先先要创建线程并指定线程需要执行的业务方法，然后再调用线程的start()方法，</p>
</li>
<li><p>此时线程就从NEW（新建）状态变成了RUNNABLE（就绪）状态，</p>
</li>
<li><p>此时线程会判断要执行的方法中有没有 synchronized 同步代码块，</p>
</li>
<li><p>如果有并且其他线程也在使用此锁，那么线程就会变为 BLOCKED（阻塞等待）状态，</p>
</li>
<li><p>当其他线程使用完此锁之后，线程会继续执行剩余的方法。</p>
</li>
</ol>
<p>当遇到Object.wait()或Thread.join()方法时，线程会变为WAITING（等待状态）状态，如果是带了超时时间的等待方法，那么线程会进入TIMED_WAITING（计时等待）状态，当有其他线程执行了 notify() 或 notifyAll() 方法之后，线程被唤醒继续执行剩余的业务方法，直到方法执行完成为止，此时整个线程的流程就执行完了。</p>
<hr>
<h3 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h3><p>线程一般会作为并发编程的起始问题，用于引出更多的关于并发编程的面试问题。当然对于线程的掌握程度也决定了你对并发编程的掌握程度。</p>
<ol>
<li>BLOCKED（阻塞等待）和 WAITING（等待）有什么区别？ </li>
<li>start() 方法和 run() 方法有什么区别？ </li>
<li>线程的优先级有什么用？该如何设置？ </li>
<li>线程的常用方法有哪些？</li>
</ol>
<h4 id="BLOCKED-和-WAITING-的区别"><a href="#BLOCKED-和-WAITING-的区别" class="headerlink" title="BLOCKED 和 WAITING 的区别"></a>BLOCKED 和 WAITING 的区别</h4><blockquote>
<p>虽然 BLOCKED 和 WAITING 都有等待的含义，但二者有着本质的区别。</p>
</blockquote>
<p>首先它们状态形成的调用方法不同，</p>
<p>其次 BLOCKED 可以理解为当前线程还处于活跃状态，只是在阻塞等待其他线程使用完某个锁资源；</p>
<p>而 WAITING 则是因为自身调用了 Object.wait() 或着是 Thread.join() 又或者是 LockSupport.park() 而进入等待状态，只能等待其他线程执行某个特定的动作才能被继续唤醒。</p>
<p><strong>For Example：</strong></p>
<p>比如当线程因为调用了 Object.wait() 而进入 WAITING 状态之后，则需要等待另一个线程执行 Object.notify() 或 Object.notifyAll() 才能被唤醒。</p>
<h4 id="start-和-run-的区别"><a href="#start-和-run-的区别" class="headerlink" title="start() 和 run() 的区别"></a>start() 和 run() 的区别</h4><p>首先从 Thread 源码来看，start() 方法属于 Thread 自身的方法，并且使用了 synchronized 来保证线程安全，源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123; </span><br><span class="line">	<span class="comment">// 状态验证，不等于 NEW 的状态会抛出异常 </span></span><br><span class="line">	<span class="keyword">if</span> (threadStatus != <span class="number">0</span>) </span><br><span class="line">		<span class="keyword">throw</span> <span class="keyword">new</span> IllegalThreadStateException(); </span><br><span class="line">	<span class="comment">// 通知线程组，此线程即将启动 </span></span><br><span class="line">	group.add(<span class="keyword">this</span>); </span><br><span class="line">	<span class="keyword">boolean</span> started = <span class="keyword">false</span>; </span><br><span class="line">	<span class="keyword">try</span> &#123; </span><br><span class="line">		start0(); </span><br><span class="line">		started = <span class="keyword">true</span>; </span><br><span class="line">	&#125; <span class="keyword">finally</span> &#123; </span><br><span class="line">		<span class="keyword">try</span> &#123; </span><br><span class="line">			<span class="keyword">if</span> (!started) &#123; </span><br><span class="line">				group.threadStartFailed(<span class="keyword">this</span>); </span><br><span class="line">			&#125; </span><br><span class="line">		&#125; <span class="keyword">catch</span> (Throwable ignore) &#123; </span><br><span class="line">		<span class="comment">// 不处理任何异常，如果 start0 抛出异常，则它将被传递到调用堆栈上 </span></span><br><span class="line">		&#125; </span><br><span class="line"> 	&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>run() 方法为 Runnable 的抽象方法，必须由调用类重写此方法，重写的 run() 方法其实就是此线程要执行的业务方法，源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Thread</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123; </span><br><span class="line">	<span class="comment">// 忽略其他方法...... </span></span><br><span class="line">	<span class="keyword">private</span> Runnable target; </span><br><span class="line">	<span class="meta">@Override</span> </span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123; </span><br><span class="line">		<span class="keyword">if</span> (target != <span class="keyword">null</span>) &#123; </span><br><span class="line">			target.run(); </span><br><span class="line">		&#125; </span><br><span class="line">	&#125; </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="meta">@FunctionalInterface</span> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Runnable</span> </span>&#123; </span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从执行的效果来说，</p>
<p>start() 方法可以开启多线程，让线程从 NEW 状态转换成 RUNNABLE 状态，而 run() 方法只是一个普通的方法。 其次，它们可调用的次数不同，start() 方法不能被多次调用，否则会抛出 java.lang.IllegalStateException；</p>
<p>而 run() 方法可以进行多次调用，因为它只是一个普通的方法而已。</p>
<h4 id="线程优先级"><a href="#线程优先级" class="headerlink" title="线程优先级"></a>线程优先级</h4><p>在 Thread 源码中和线程优先级相关的属性有 3 个：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 线程可以拥有的最小优先级</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> MIN_PRIORITY = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 线程默认优先级</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> NORM_PRIORITY = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 线程可以拥有的最大优先级</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> MAX_PRIORITY = <span class="number">10</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>线程的<strong>优先级</strong>可以理解为<strong>线程抢占 CPU 时间片的概率</strong></p>
</blockquote>
<p>优先级越高的线程优先执行的概率就越大，但并不能保证优先级高的线程一定先执行。</p>
<p>在程序中我们可以通过 Thread.setPriority() 来设置优先级，setPriority() 源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">setPriority</span><span class="params">(<span class="keyword">int</span> newPriority)</span> </span>&#123;</span><br><span class="line">    ThreadGroup g;</span><br><span class="line">    checkAccess();</span><br><span class="line">    <span class="comment">// 先验证优先级的合理性</span></span><br><span class="line">    <span class="keyword">if</span> (newPriority &gt; MAX_PRIORITY || newPriority &lt; MIN_PRIORITY) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>((g = getThreadGroup()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// 优先级如果超过线程组的最高优先级，则把优先级设置为线程组的最高优先级</span></span><br><span class="line">        <span class="keyword">if</span> (newPriority &gt; g.getMaxPriority()) &#123;</span><br><span class="line">            newPriority = g.getMaxPriority();</span><br><span class="line">        &#125;</span><br><span class="line">        setPriority0(priority = newPriority);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="线程的常用方法"><a href="#线程的常用方法" class="headerlink" title="线程的常用方法"></a>线程的常用方法</h4><ol>
<li>join()</li>
</ol>
<p>在一个线程中调用 other.join() ，这时候当前线程会让出执行权给 other 线程，直到 other 线程执行完或者过了超时时间之后再继续执行当前线程，</p>
<p>join() 源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">join</span><span class="params">(<span class="keyword">long</span> millis)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> base = System.currentTimeMillis();</span><br><span class="line">    <span class="keyword">long</span> now = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 超时时间不能小于 0</span></span><br><span class="line">    <span class="keyword">if</span> (millis &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"timeout value is negative"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 等于 0 表示无限等待，直到线程执行完为之</span></span><br><span class="line">    <span class="keyword">if</span> (millis == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// 判断子线程 (其他线程) 为活跃线程，则一直等待</span></span><br><span class="line">        <span class="keyword">while</span> (isAlive()) &#123;</span><br><span class="line">            wait(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 循环判断</span></span><br><span class="line">        <span class="keyword">while</span> (isAlive()) &#123;</span><br><span class="line">            <span class="keyword">long</span> delay = millis - now;</span><br><span class="line">            <span class="keyword">if</span> (delay &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            wait(delay);</span><br><span class="line">            now = System.currentTimeMillis() - base;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从源码中可以看出 join() 方法底层还是通过 wait() 方法来实现的。 </p>
<p>例如，在未使用 join() 时，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread thread = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">6</span>; i++) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(<span class="string">"子线程睡眠："</span> + i + <span class="string">"秒。"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        thread.start(); <span class="comment">// 开启线程</span></span><br><span class="line">        <span class="comment">// 主线程执行</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(<span class="string">"主线程睡眠："</span> + i + <span class="string">"秒。"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">主线程睡眠：<span class="number">1</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">1</span>秒。</span><br><span class="line">主线程睡眠：<span class="number">2</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">2</span>秒。</span><br><span class="line">主线程睡眠：<span class="number">3</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">3</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">4</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">5</span>秒。</span><br></pre></td></tr></table></figure>

<p>从结果可以看出，在未使用 join() 时主子线程会交替执行。</p>
<p>然后我们再把 join() 方法加入到代码中，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread thread = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">6</span>; i++) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(<span class="string">"子线程睡眠："</span> + i + <span class="string">"秒。"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        thread.start(); <span class="comment">// 开启线程</span></span><br><span class="line">        thread.join(<span class="number">2000</span>); <span class="comment">// 等待子线程先执行 2 秒钟</span></span><br><span class="line">        <span class="comment">// 主线程执行</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(<span class="string">"主线程睡眠："</span> + i + <span class="string">"秒。"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">子线程睡眠：<span class="number">1</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">2</span>秒。</span><br><span class="line">主线程睡眠：<span class="number">1</span>秒。 <span class="comment">// thread.join(2000); 等待 2 秒之后，主线程和子线程再交替执行</span></span><br><span class="line">子线程睡眠：<span class="number">3</span>秒。</span><br><span class="line">主线程睡眠：<span class="number">2</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">4</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">5</span>秒。</span><br><span class="line">主线程睡眠：<span class="number">3</span>秒。</span><br></pre></td></tr></table></figure>

<p>从执行结果可以看出，添加 join() 方法之后，主线程会先等子线程执行 2 秒之后才继续执行。</p>
<ol start="2">
<li>yield()</li>
</ol>
<p>看 Thread 的源码可以知道 yield() 为本地方法，也就是说 yield() 是由 C 或 C++ 实现的，源码如下：</p>
<blockquote>
<p>public static native void yield();</p>
</blockquote>
<p>yield() 方法表示给线程调度器一个当前线程愿意出让 CPU 使用权的暗示，但是线程调度器可能会忽略这个暗示。</p>
<p>比如我们执行这段包含了 yield() 方法的代码，如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    Runnable runnable = <span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                System.out.println(<span class="string">"线程："</span> +</span><br><span class="line">                        Thread.currentThread().getName() + <span class="string">" I："</span> + i);</span><br><span class="line">                <span class="keyword">if</span> (i == <span class="number">5</span>) &#123;</span><br><span class="line">                    Thread.yield();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    Thread t1 = <span class="keyword">new</span> Thread(runnable, <span class="string">"T1"</span>);</span><br><span class="line">    Thread t2 = <span class="keyword">new</span> Thread(runnable, <span class="string">"T2"</span>);</span><br><span class="line">    t1.start();</span><br><span class="line">    t2.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当我们把这段代码执行多次之后会发现，每次执行的结果都不相同，这是因为 yield() 执行非常不稳定，线程调度器不一定会采纳 yield() 出让 CPU 使用权的建议，从而导致了这样的结果。</p>
]]></content>
      <categories>
        <category>Operating Systems</category>
      </categories>
      <tags>
        <tag>线程与进程</tag>
      </tags>
  </entry>
  <entry>
    <title>10亿数据找目标，众里寻他之红黑树</title>
    <url>/2020/03/14/10%E4%BA%BF%E6%95%B0%E6%8D%AE%E6%89%BE%E7%9B%AE%E6%A0%87%EF%BC%8C%E4%BC%97%E9%87%8C%E5%AF%BB%E4%BB%96%E4%B9%8B%E7%BA%A2%E9%BB%91%E6%A0%91/</url>
    <content><![CDATA[<p>众里寻他，10亿数据中只需要进行10几次比较就能查找到目标，这便是红黑树的魅力所在。</p>
<p><strong><font color=blue>Q:</font></strong></p>
<ol>
<li>红黑树是一种比较难的数据结构，要完全搞懂非常耗时耗力，红黑树怎么自平衡？</li>
<li>什么时候需要左旋或右旋？</li>
<li>插入和删除破坏了树的平衡后怎么处理？</li>
</ol>
<hr>
<center><b><font size=4>先修知识点</font></b></center>

<ul>
<li>二叉查找树</li>
<li>完美平衡二叉树</li>
</ul>
<hr>
<p>红黑树也是二叉查找树，我们知道，二叉查找树这一数据结构并不难，而红黑树之所以难是难在它是自平衡的二叉查找树，在进行插入和删除等可能会破坏树的平衡的操作时，需要重新自处理达到平衡状态。想想场景就有点烦心。</p>
<h2 id="红黑树定义和性质"><a href="#红黑树定义和性质" class="headerlink" title="红黑树定义和性质"></a>红黑树定义和性质</h2><p>红黑树是一种含有红黑结点并能自平衡的二叉查找树。它必须满足下面性质：</p>
<ul>
<li>性质1：每个节点要么是黑色，要么是红色。</li>
<li>性质2：根节点是黑色。</li>
<li>性质3：每个叶子节点（NIL）是黑色。</li>
<li>性质4：每个红色结点的两个子结点一定都是黑色。</li>
<li>性质5：任意一结点到每个叶子结点的路径都包含数量相同的黑结点。</li>
</ul>
<p>从性质5又可以推出：</p>
<ul>
<li>性质5.1：如果一个结点存在黑子结点，那么该结点肯定有两个子结点</li>
</ul>
<p>图中就是一颗简单的红黑树。其中Nil为叶子结点，并且它是黑色的。(值得提醒注意的是，在Java中，叶子结点是为null的结点。)</p>
<center><img src="https://img-blog.csdnimg.cn/20200314232111332.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>红黑树并不是一个完美平衡二叉查找树，从图中可以看到，根结点 P 的左子树显然比右子树高，但左子树和右子树的黑结点的层数是相等的。</p>
<p>亦即任意一个结点到到每个叶子结点的路径都包含数量相同的黑结点(性质5)。所以我们叫红黑树这种平衡为黑色完美平衡。</p>
<p>接着我们还来约定下红黑树一些结点的叫法，如图所示</p>
<center><img src="https://img-blog.csdnimg.cn/20200314232616270.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>


<p>前面讲到红黑树能自平衡，它靠的是什么？三种操作：左旋、右旋和变色。</p>
<ul>
<li><strong>左旋：</strong> 以某个结点作为支点(旋转结点)，其右子结点变为旋转结点的父结点，右子结点的左子结点变为旋转结点的右子结点，左子结点保持不变。</li>
<li><strong>右旋：</strong> 以某个结点作为支点(旋转结点)，其左子结点变为旋转结点的父结点，左子结点的右子结点变为旋转结点的左子结点，右子结点保持不变。</li>
<li><strong>变色：</strong> 结点的颜色由红变黑或由黑变红。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200314233239573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>$$<br>左旋<br>$$</p>
<p><img src="https://img-blog.csdnimg.cn/20200314233311668.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>$$<br>右旋<br>$$</p>
<p>上面所说的旋转结点也即旋转的支点，也即图中的P结点。</p>
<p>我们先忽略颜色，可以看到旋转操作不会影响旋转结点的父结点，父结点以上的结构还是保持不变的。</p>
<ul>
<li><strong>左旋</strong>只影响旋转结点和<strong>其右子树</strong>的结构，把右子树的结点往左子树挪了。</li>
<li><strong>右旋</strong>只影响旋转结点和<strong>其左子树</strong>的结构，把左子树的结点往右子树挪了。</li>
</ul>
<p>旋转操作是局部的。当一边子树的结点少了，那么向另外一边子树“借”一些结点；当一边子树的结点多了，那么向另外一边子树“租”一些结点。</p>
<p><strong>注意：</strong></p>
<p>但要保持红黑树的性质，结点不能乱挪，还得靠变色了。怎么变？具体情景又不同变法，后面会具体讲到，现在只需要记住<strong>红黑树总是通过旋转和变色达到自平衡</strong>。</p>
<p><strong><font color=blue>Q:</font></strong></p>
<p>&ensp;&ensp;黑结点可以同时包含一个红子结点和一个黑子结点吗？ </p>
<h2 id="红黑树查找"><a href="#红黑树查找" class="headerlink" title="红黑树查找"></a>红黑树查找</h2><p>因为红黑树是一颗二叉平衡树，并且查找不会破坏树的平衡，所以查找跟二叉平衡树的查找无异：</p>
<ol>
<li>从根结点开始查找，把根结点设置为当前结点；</li>
<li>若当前结点为空，返回null；</li>
<li>若当前结点不为空，用当前结点的 key 跟查找 key 作比较；</li>
<li>若当前结点 key 等于查找 key，那么该 key 就是查找目标，返回当前结点；</li>
<li>若当前结点 key 大于查找 key，把当前结点的左子结点设置为当前结点，重复步骤2；</li>
<li>若当前结点 key 小于查找 key，把当前结点的右子结点设置为当前结点，重复步骤2；</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20200314235257779.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>非常简单，但简单不代表它效率不好。</p>
<p>正由于红黑树总保持黑色完美平衡，所以它的查找最坏时间复杂度为 $O(2lgN)$，也即整颗树刚好红黑相隔的时候。能有这么好的查找效率得益于红黑树自平衡的特性。</p>
<h2 id="红黑树插入"><a href="#红黑树插入" class="headerlink" title="红黑树插入"></a>红黑树插入</h2><p>插入操作包括两部分工作</p>
<ul>
<li>一、查找插入的<strong>位置</strong>；</li>
<li>二、插入后<strong>自平衡</strong>。</li>
</ul>
<p>查找插入的父结点很简单，跟查找操作区别不大：</p>
<ol>
<li>从根结点开始查找；</li>
<li>若根结点为空，那么插入结点作为根结点，结束。</li>
<li>若根结点不为空，那么把根结点作为当前结点；</li>
<li>若当前结点为null，返回当前结点的父结点，结束。</li>
<li>若当前结点key等于查找key，那么该key所在结点就是插入结点，更新结点的值，结束。</li>
<li>若当前结点key大于查找key，把当前结点的左子结点设置为当前结点，重复步骤4；</li>
<li>若当前结点key小于查找key，把当前结点的右子结点设置为当前结点，重复步骤4；</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20200314235812841.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>插入位置已经找到，把插入结点放到正确的位置就可以啦。</p>
<p><strong>Q:</strong><br>但插入结点是应该是什么颜色呢？</p>
<p><strong>A:</strong><br>红色。</p>
<p><strong>理由：</strong></p>
<p>红色在父结点（如果存在）为黑色结点时，红黑树的黑色平衡没被破坏，不需要做自平衡操作。但如果插入结点是黑色，那么插入位置所在的子树黑色结点总是多1，必须做自平衡。</p>
<h3 id="插入场景"><a href="#插入场景" class="headerlink" title="插入场景"></a>插入场景</h3><p><img src="https://img-blog.csdnimg.cn/20200315224334183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>根据二叉树的性质，除了情景2，所有插入操作都是在叶子结点进行的。这点应该不难理解，因为查找插入位置时，我们就是在找子结点为空的父结点的。</p>
<p>还是先做个约定：</p>
<center><img src="https://img-blog.csdnimg.cn/20200315224508315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="插入情景1：红黑树为空树"><a href="#插入情景1：红黑树为空树" class="headerlink" title="插入情景1：红黑树为空树"></a>插入情景1：红黑树为空树</h3><p>最简单的一种情景，直接把插入结点作为根结点就行，但注意，根据红黑树性质2：根节点是黑色。还需要把插入结点设为黑色。</p>
<p><strong>处理：</strong> 把插入结点作为根结点，并把结点设置为黑色。</p>
<h3 id="插入情景2：插入结点的Key已存在"><a href="#插入情景2：插入结点的Key已存在" class="headerlink" title="插入情景2：插入结点的Key已存在"></a>插入情景2：插入结点的Key已存在</h3><p>插入结点的Key已存在，既然红黑树总保持平衡，在插入前红黑树已经是平衡的，那么把插入结点设置为将要替代结点的颜色，再把结点的值更新就完成插入。</p>
<p><strong>处理：</strong></p>
<ul>
<li>把I设为当前结点的颜色</li>
<li>更新当前结点的值为插入结点的值</li>
</ul>
<h3 id="插入情景3：插入结点的父结点为黑结点"><a href="#插入情景3：插入结点的父结点为黑结点" class="headerlink" title="插入情景3：插入结点的父结点为黑结点"></a>插入情景3：插入结点的父结点为黑结点</h3><p>由于插入的结点是红色的，当插入结点的黑色时，并不会影响红黑树的平衡，直接插入即可，无需做自平衡。</p>
<p><strong>处理：</strong> 直接插入。</p>
<h3 id="插入情景4：插入结点的父结点为红结点"><a href="#插入情景4：插入结点的父结点为红结点" class="headerlink" title="插入情景4：插入结点的父结点为红结点"></a>插入情景4：插入结点的父结点为红结点</h3><blockquote>
<p>回想下红黑树的性质2：根结点是黑色。</p>
</blockquote>
<p>如果插入的父结点为红结点，那么该父结点不可能为根结点，所以插入结点总是存在祖父结点。这点很重要，因为后续的旋转操作肯定需要祖父结点的参与。<br>情景4又分为很多子情景，下面将进入重点部分，各位看官请留神了。</p>
<h4 id="插入情景4-1：叔叔结点存在并且为红结点"><a href="#插入情景4-1：叔叔结点存在并且为红结点" class="headerlink" title="插入情景4.1：叔叔结点存在并且为红结点"></a>插入情景4.1：叔叔结点存在并且为红结点</h4><p>从红黑树性质4可以，祖父结点肯定为黑结点，因为不可以同时存在两个相连的红结点。那么此时该插入子树的红黑层数的情况是：黑红红。显然最简单的处理方式是把其改为：红黑红。如图所示。</p>
<p><strong>处理：</strong></p>
<ul>
<li>将P和S设置为黑色</li>
<li>将PP设置为红色</li>
<li>把PP设置为当前插入结点</li>
</ul>
<center><img src="https://img-blog.csdnimg.cn/20200316110338633.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<center><img src="https://img-blog.csdnimg.cn/20200316110410528.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>可以看到，我们把PP结点设为红色了，如果PP的父结点是黑色，那么无需再做任何处理；但如果PP的父结点是红色，根据性质4，此时红黑树已不平衡了，所以还需要把PP当作新的插入结点，继续做插入操作自平衡处理，直到平衡为止。</p>
<p>试想下PP刚好为根结点时，那么根据性质2，我们必须把PP重新设为黑色，那么树的红黑结构变为：黑黑红。换句话说，从根结点到叶子结点的路径中，黑色结点增加了。这也是唯一一种会增加红黑树黑色结点层数的插入情景。</p>
<p>我们还可以总结出另外一个经验：红黑树的生长是自底向上的。这点不同于普通的二叉查找树，普通的二叉查找树的生长是自顶向下的。</p>
<h4 id="插入情景4-2：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的左子结点"><a href="#插入情景4-2：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的左子结点" class="headerlink" title="插入情景4.2：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的左子结点"></a>插入情景4.2：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的左子结点</h4><p>单纯从插入前来看，也即不算情景 4.1 自底向上处理时的情况，叔叔结点非红即为叶子结点(Nil)。因为如果叔叔结点为黑结点，而父结点为红结点，那么叔叔结点所在的子树的黑色结点就比父结点所在子树的多了，这不满足红黑树的性质5。后续情景同样如此，不再多做说明了。</p>
<p>前文说了，需要旋转操作时，肯定一边子树的结点多了或少了，需要租或借给另一边。插入显然是多的情况，那么把多的结点租给另一边子树就可以了。</p>
<h5 id="插入情景4-2-1：插入结点是其父结点的左子结点"><a href="#插入情景4-2-1：插入结点是其父结点的左子结点" class="headerlink" title="插入情景4.2.1：插入结点是其父结点的左子结点"></a>插入情景4.2.1：插入结点是其父结点的左子结点</h5><p><strong>处理：</strong></p>
<ul>
<li>将P设为黑色</li>
<li>将PP设为红色</li>
<li>对PP进行右旋</li>
</ul>
<center><img src="https://img-blog.csdnimg.cn/20200316133830860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>左边两个红结点，右边不存在，那么一边一个刚刚好，并且因为为红色，肯定不会破坏树的平衡。</p>
<p><strong>Q:</strong><br>&ensp;可以把P设为红色，I和PP设为黑色吗？</p>
<p><strong>A:</strong><br>&ensp;可以！看过《算法：第4版》的同学可能知道，书中讲解的就是把P设为红色，I和PP设为黑色。但把P设为红色，显然又会出现情景4.1的情况，需要自底向上处理，做多了无谓的操作。</p>
<h5 id="插入情景4-2-2：插入结点是其父结点的右子结点"><a href="#插入情景4-2-2：插入结点是其父结点的右子结点" class="headerlink" title="插入情景4.2.2：插入结点是其父结点的右子结点"></a>插入情景4.2.2：插入结点是其父结点的右子结点</h5><blockquote>
<p>这种情景显然可以转换为情景4.2.1.</p>
</blockquote>
<p><strong>处理：</strong></p>
<ul>
<li>对P进行左旋</li>
<li>把P设置为插入结点，得到情景4.2.1</li>
<li>进行情景4.2.1的处理</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200316134059375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="插入情景4-3：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的右子结点"><a href="#插入情景4-3：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的右子结点" class="headerlink" title="插入情景4.3：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的右子结点"></a>插入情景4.3：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的右子结点</h4><p>该情景对应情景4.2，只是方向反转，不做过多说明了，直接看图。</p>
<h5 id="插入情景4-3-1：插入结点是其父结点的右子结点"><a href="#插入情景4-3-1：插入结点是其父结点的右子结点" class="headerlink" title="插入情景4.3.1：插入结点是其父结点的右子结点"></a>插入情景4.3.1：插入结点是其父结点的右子结点</h5><p><strong>处理：</strong></p>
<ul>
<li>将P设为黑色</li>
<li>将PP设为红色</li>
<li>对PP进行左旋</li>
</ul>
<center><img src="https://img-blog.csdnimg.cn/2020031613421548.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h5 id="插入情景4-3-2：插入结点是其父结点的右子结点"><a href="#插入情景4-3-2：插入结点是其父结点的右子结点" class="headerlink" title="插入情景4.3.2：插入结点是其父结点的右子结点"></a>插入情景4.3.2：插入结点是其父结点的右子结点</h5><p><strong>处理：</strong></p>
<ul>
<li>对P进行右旋</li>
<li>把P设置为插入结点，得到情景4.3.1</li>
<li>进行情景4.3.1的处理</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200316134345104.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>Q:</strong><br>&ensp;上面的情景举例的都是第一次插入而不包含自底向上处理的情况，那么上面所说的情景都适合自底向上的情况吗？</p>
<p><strong>A:</strong><br>&ensp;答案是肯定的。理由很简单，但每棵子树都能自平衡，那么整棵树最终总是平衡的。</p>
]]></content>
      <categories>
        <category>Feedback</category>
      </categories>
      <tags>
        <tag>红黑树</tag>
      </tags>
  </entry>
  <entry>
    <title>我亦未曾饶过岁月</title>
    <url>/2020/03/12/%E6%88%91%E4%BA%A6%E6%9C%AA%E6%9B%BE%E9%A5%B6%E8%BF%87%E5%B2%81%E6%9C%88/</url>
    <content><![CDATA[<p><strong>岁月不饶人，我亦未曾绕过岁月，总结反思，重新来过。</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">后端开发面试总结</span><br></pre></td></tr></table></figure>

<h1 id="哈希"><a href="#哈希" class="headerlink" title="哈希"></a>哈希</h1><p>从效率方面考虑，数组的检索效率较好，但是插入和删除效率低下；对于链表，插入和删除的效率较好，但是检索的效率低下，然而 HashMap 合理的继承了上述两位的所有优点，意味着完美。</p>
<p>针对哈希函数的构造</p>
<ol>
<li>直接定址法</li>
<li>平方取中法</li>
<li>除数余数法</li>
</ol>
<p>为几种主要的方法，具体不展开论述，对于 HashMap 神奇存储展示拙见</p>
<blockquote>
<p>HashMap部分源码分析</p>
<ul>
<li>感触</li>
</ul>
</blockquote>
<p> HashMap 的数据是存储在 Table 数组中的，是一个 Entry 数组，二维处理的话，纵观为数组，横向为链表，每当建立一个 HashMap 的时候，就初始化一个数组</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">transient</span> Entry[] table;</span><br></pre></td></tr></table></figure>

<p>【解释】transient关键字，为了使其修饰的对象不参与序列化<br>【实质】此 table 对象无法持久化</p>
<center><img src="https://img-blog.csdnimg.cn/20200110180103273.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>【总结】对于HashMap的工作原理的简要阐述（可作为面试参考回答）</p>
<p>HashMap类有一个叫做Entry的内部类。这个Entry类包含了key-value作为实例变量。 每当往hashmap        里面存放key-value对的时候，都会为它们实例化一个Entry对象，这个Entry对象就会存储在前面提到    的Entry数组table中。Entry具体存在table的那个位置是 根据key的hashcode()方法计算出来的hash值    （来决定）。</p>
<ul>
<li>定义数组中的链表</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Basic hash bin node, used for most entries.  (See below for</span></span><br><span class="line"><span class="comment">* TreeNode subclass, and in LinkedHashMap for its Entry subclass.)</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Map</span>.<span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123; <span class="comment">//Entry数组中的链表</span></span><br><span class="line">	<span class="keyword">final</span> <span class="keyword">int</span> hash;</span><br><span class="line">    <span class="keyword">final</span> K key;</span><br><span class="line">    V value;</span><br><span class="line">    Node&lt;K,V&gt; next;</span><br><span class="line"></span><br><span class="line">    Node(<span class="keyword">int</span> hash, K key, V value, Node&lt;K,V&gt; next) &#123;</span><br><span class="line">    	<span class="keyword">this</span>.hash = hash;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">        <span class="keyword">this</span>.next = next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>取 Entry ，判断方法为【如果 key==null ，直接取数组的第一个元素，如果不是，先计算出 key 的 hashcode 找到下标，再判断是否相等，如果相等，则返回对应的 entry ，如果不相等，则返回 null 】</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt; e;</span><br><span class="line">    <span class="keyword">return</span> (e = getNode(hash(key), key)) == <span class="keyword">null</span> ? <span class="keyword">null</span> : e.value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>扩容</li>
</ul>
<p>其本质是先新创建一个2倍于原来长度的 table 数组，通过重新遍历将旧的 table 中的数据复制 到新的 table 中，在新的 table 中，索引 hash 值重新计算，并且更新扩容后的阈值。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> ((size &gt;= threshold) &amp;&amp; (<span class="keyword">null</span> != table[bucketIndex])) &#123;</span><br><span class="line"><span class="comment">// 将table表的长度增加到之前的两倍</span></span><br><span class="line">resize(<span class="number">2</span> * table.length);</span><br><span class="line"><span class="comment">// 重新计算哈希值</span></span><br><span class="line">hash = (<span class="keyword">null</span> != key) ? hash(key) : <span class="number">0</span>;</span><br><span class="line"><span class="comment">// 从新计算新增元素在扩容后的table中应该存放的index</span></span><br><span class="line">bucketIndex = indexFor(hash, table.length);</span><br></pre></td></tr></table></figure>
<p>其中 resize 起到了创建一个新的 table 数组的作用</p>
<p>针对HashMap的数据存储无序性及其他特性</p>
<ul>
<li>附上一个较为完善的解析：<a href="https://www.jianshu.com/p/dde9b12343c1" target="_blank" rel="noopener">https://www.jianshu.com/p/dde9b12343c1</a></li>
</ul>
<h1 id="HashCode"><a href="#HashCode" class="headerlink" title="HashCode"></a>HashCode</h1><blockquote>
<ul>
<li>与 equals 的协调合作</li>
</ul>
</blockquote>
<p>hashcode 和 equals 通常是同时出现用来获取值的【在 HashMap 中使用 hashcode() 和 equals() 方法确定键值对的索引】</p>
<p><strong>用法：</strong><br>初步了解了HashMap的工作原理，在此基础上，如果要在 HashMap 中新增加元素的时候保证不重复，只用 equals 显然不如结合 hashcode方便，可以直接利用此方法将新的数据存储而不进行任何比较，这也就是查找效率高的本质，降低了比较次数。</p>
<p><strong>注意点：</strong><br>在Java中相等（相同）的对象必须具有相等的哈希码（或者散列码），但是如果两个对象的hashcode相同，它们并不一定相同。没有正确使用的时候会导致相同 hash 值的结果。</p>
<blockquote>
<ul>
<li>方法详解</li>
</ul>
</blockquote>
<p>先看 hashcode 的定义<code>public native int  hashcode();</code> – 证明 hashcode 是一个本地方法【前提是原生的方法而非经过自定义覆盖过的】</p>
<p>hashcode() 方法给对象返回一个 hash code 值，性质有以下几点：</p>
<ol>
<li>在一个Java应用执行期间，如果一个对象提供给 equals() 做比较的信息没有被修改的话，该对象多次调用hashCode()方法，该方法返回的 integer 必须相同；</li>
<li>如果两个对象依 equals() 方法判断是相等的，分别调用 hashcode() 方法产生的值必须相同；</li>
<li>并不要求依 equals() 方法判断不相等的两个对象，分别调用各自的 hashCode() 方法必须产生不同的 integer 值。然而，对于不同的对象产生不同的integer结果，有可能会提高 hash table 的性能。</li>
</ol>
<h1 id="LinkedHashMap"><a href="#LinkedHashMap" class="headerlink" title="LinkedHashMap"></a>LinkedHashMap</h1><p>我们知道 HashMap 的存储是无序的，那么为了有顺序存储键值对，就需要引入LinedHashMap。<del>【当然检验此句话可以尝试分别输出 HashMap 中存储的值和 LinkedHashMap 中的值】</del> </p>
<p>同样的在 Map 旗下，而且 LinkedHashMap 继承了 HashMap ，在定义的时候稍微变动类名就OK：<code>Map&lt;String, String&gt; = new HashMap&lt;&gt;();</code><br><code>Map&lt;String, String&gt; = new LinkedHashMap&lt;&gt;();</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LinkedHashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">HashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Map</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>所以在用法上，LinkedHashMap 和 HashMap 的用法基本一致，通过 put 和 get 的方法进行 key-value 的存储和读取。</p>
<blockquote>
<ul>
<li>区别点</li>
</ul>
</blockquote>
<p>LinkedHashMap 的有序性到底体现在什么地方？</p>
<p>其本质为 HashMap + 双向链表的组合，即通过 HashMap 的构造函数初始化 Entry 数组【图左】，然后通过LinkedHashMap 自身的 init  的初始化方法初始化了一个只有头节点的双向链表【图右】。</p>
<center><img src="https://img-blog.csdnimg.cn/20200111114222993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>有了双向链表的存在，那么在进行 put /扩容等操作时就需要考虑到双向链表的事。</p>
<ol>
<li>在 put 元素时，不但要把它加入到HashMap中去，还要加入到双向链表中；【记住一点：header 并不能存储数据】</li>
<li>扩容时，数据的再散列和HashMap是不一样。</li>
</ol>
<p>① HashMap 是先遍历旧 table ，再遍历旧 table 中每个元素的单向链表，取得 Entry 以后，重新计算 hash 值，然后存放到新 table 的对应位置；</p>
<p>② LinkedHashMap 是遍历的双向链表，取得每一个 Entry ，然后重新计算hash值并且存放到新 table 的对应位置。</p>
<p>附上一个较为详细的解释：<a href="https://www.jianshu.com/p/8f4f58b4b8ab" target="_blank" rel="noopener">https://www.jianshu.com/p/8f4f58b4b8ab</a></p>
<h1 id="ArrayList"><a href="#ArrayList" class="headerlink" title="ArrayList"></a>ArrayList</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ArrayList</span>&lt;<span class="title">E</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractList</span>&lt;<span class="title">E</span>&gt; <span class="keyword">implements</span> <span class="title">List</span>&lt;<span class="title">E</span>&gt;, <span class="title">RandomAccess</span>, <span class="title">Cloneable</span>, <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span></span></span><br></pre></td></tr></table></figure>
<p>ArrayList 是可以动态增长和缩减的索引序列，它是基于数组实现的List类</p>
<p>ArrayList 的应用在于其的可变长性，本质是类对象内部定义了 capacity 属性，封装了一个动态再分配的 Object[ ] 数组，当数组的元素数量增加时，属性的值会自动增加。</p>
<p>如果想ArrayList中添加大量元素，可使用ensureCapacity方法一次性增加capcacity，可以减少增加重分配的次数提高性能 。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Default initial capacity.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> 	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_CAPACITY = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Shared empty array instance used for empty instances.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Object[] EMPTY_ELEMENTDATA = &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Shared empty array instance used for default sized empty instances. We</span></span><br><span class="line"><span class="comment"> * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when</span></span><br><span class="line"><span class="comment"> * first element is added.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Increases the capacity of this &lt;tt&gt;ArrayList&lt;/tt&gt; instance, if</span></span><br><span class="line"><span class="comment"> * necessary, to ensure that it can hold at least the number of elements</span></span><br><span class="line"><span class="comment"> * specified by the minimum capacity argument.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span>   minCapacity   the desired minimum capacity</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ensureCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA)</span><br><span class="line">            <span class="comment">// any size if not default element table</span></span><br><span class="line">            <span class="comment">// larger than default for default empty table. It's already</span></span><br><span class="line">            <span class="comment">// supposed to be at default size.</span></span><br><span class="line">            <span class="comment">//DEFAULT_CAPACITY;//这里的值为10</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (minCapacity &gt; minExpand) &#123;</span><br><span class="line">            ensureExplicitCapacity(minCapacity);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>ArrayList 和 vector 的区别</p>
<blockquote>
<p>线程</p>
</blockquote>
<p>ArrayList 线程是不安全的, vector 的线程是安全的<br>当多线程访问一个 ArrayList 时,需要手动保持同步性</p>
</blockquote>
<blockquote>
<p>ArrayList 底层实现</p>
</blockquote>
<p>底层的数据结构就是数组，本质是 elementData，数组元素类型为Object类型，即可以存放所有类型数据【包括 null 】，所有的操作都是基于数组的。</p>
<p>ArrayList 继承了 AbstractList，AbstractList 继承了 AbstractCollection</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">grow</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">	<span class="comment">// overflow-conscious code</span></span><br><span class="line">    <span class="comment">//将扩充前的elementData大小给oldCapacity</span></span><br><span class="line">    <span class="keyword">int</span> oldCapacity = elementData.length;</span><br><span class="line">    <span class="comment">//newCapacity就是1.5倍的oldCapacity，因为向右移1位代表除以2</span></span><br><span class="line">    <span class="keyword">int</span> newCapacity = oldCapacity + (oldCapacity &gt;&gt; <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">if</span> (newCapacity - minCapacity &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    *这句话就是适应elementData空数组的时候，length=0</span></span><br><span class="line"><span class="comment">    *那么oldCapacity=0,newCapacity=0,所以这个判断成立</span></span><br><span class="line"><span class="comment">    *在这里就是真正的初始化elementData的大小了，前面的工作都是准备工作。</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">     	newCapacity = minCapacity;</span><br><span class="line">    <span class="keyword">if</span> (newCapacity - MAX_ARRAY_SIZE &gt; <span class="number">0</span>)</span><br><span class="line">    <span class="comment">//如果newCapacity超过了最大的容量的限制，就调用hugeCapacity，也就是能给的最大值给newCapacity</span></span><br><span class="line">        newCapacity = hugeCapacity(minCapacity);</span><br><span class="line">    <span class="comment">// minCapacity is usually close to size, so this is a win:</span></span><br><span class="line">    <span class="comment">//新的容量大小已经确定好了，就copy数组，改变容量大小</span></span><br><span class="line">    elementData = Arrays.copyOf(elementData, newCapacity);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>grow() 方法是 ArrayList 的核心，该方法保障了<strong>变长</strong>特性。</p>
<p>附上一个底层方法个详解：<a href="https://blog.csdn.net/weixin_42036647/article/details/100709820" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42036647/article/details/100709820</a></p>
<blockquote>
<p>ArrayList 和数组的区别</p>
</blockquote>
<p><strong>数组</strong></p>
<p>优点：在内存中的存储时连续的，索引速度快，赋值和修改也较为方便</p>
<p>缺点：数组中插入元素比较繁琐，而且在定义时需要指定元素类型和数组长度</p>
<p><strong>ArrayList</strong></p>
<p>优点：解决了数组的缺点</p>
<p>缺点：在插入元素时，可以插入任意元素，都被处理为 Object 类，但是不保证数据安全，在数据利用的时候会报类型不匹配的错误</p>
<p>从时间复杂度的角度分析 ArrayList：<a href="https://blog.csdn.net/weixin_33939380/article/details/87975097" target="_blank" rel="noopener">https://blog.csdn.net/weixin_33939380/article/details/87975097</a></p>
<h1 id="是如何实现的"><a href="#是如何实现的" class="headerlink" title="+= 是如何实现的"></a>+= 是如何实现的</h1><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">C/C++ code</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt; </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">const</span> <span class="title">T</span> <span class="title">operator</span>+=(<span class="title">const</span> <span class="title">T</span>&amp; <span class="title">rhs</span>) </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span> + rhs;                </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以对于 += 运算符，在使用过程中不会申请新的内存。</p>
<p><strong>由于运算符优先级不同，i = i + 1 的运算速度要低于 i += 1</strong><br>【优先级标准：优先级从上到下依次递减，最上面具有最高的优先级，逗号操作符具有最低的优先级。表达式的结合次序取决于表达式中各种运算符的优先级。优先级高的运算符先结合，优先级低的运算符后结合，同一行中的运算符的优先级相同。】</p>
<h1 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h1><p>在 Java 中字符串 String 是不可改变的，通过操作运算符改变后返回的是新对象，并不能将原有字符串改变</p>
<p><strong>原因：</strong></p>
<p>1)字符串变量是存放栈内存中的，而其所对应的字符串是存放在堆内存中的。</p>
<p>2)某个字符串变量一旦赋值，实际上就是在栈内存中的这个字符串变量通过内存地址指向到堆内存中的某个字符串。</p>
<p>3)而如果这个字符串变量所对应的字符串发生改变，在堆内存中就会新款开辟一块空间出来存放这新字符串，并使得原先的内存地址指向发生改变</p>
<p>对于之前的字符串，如果不再有其他字符串变量所指向，那么将会变成垃圾，交由 Java 中的 GC 机制进行处理。</p>
<p>【注意：无论对字符串变量进行重新赋值、截取、追加等操作其实改变的都不是字符串本身，而是指向该字符串的内存地址。】</p>
<blockquote>
<p>字符串 string 转 Int</p>
</blockquote>
<p>在 Java 中 Integer 类型下有名为 parseInt() 的方法，专门用于将字符串参数作为有符号的十进制整数进行解析【如果方法有两个参数， 使用第二个参数指定的基数，将字符串参数解析为有符号的整数。】</p>
<p>实现 parseInt() 方法：</p>
<ol>
<li>使用 map 字典的形式存储从 0~9 十位数字分别对应的字母，进行字符串的切割组合</li>
<li>直接进行每一位字符的转换组合</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">parseInt</span><span class="params">(String num)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (num.startsWith(<span class="string">"-"</span>)) &#123;</span><br><span class="line">		index++;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">while</span> (index &lt;= num.length() - <span class="number">1</span>) &#123;</span><br><span class="line">		<span class="keyword">int</span> n = num.charAt(index);</span><br><span class="line">		<span class="keyword">if</span> (n &lt;= <span class="number">57</span> &amp;&amp; n &gt;= <span class="number">48</span>) &#123;</span><br><span class="line">			result *= <span class="number">10</span>;</span><br><span class="line">			result += n - <span class="number">48</span>;</span><br><span class="line">		&#125; </span><br><span class="line">		<span class="keyword">else</span> &#123;</span><br><span class="line">			System.err.println(<span class="string">"args is not a interger"</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		index++;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (num.startsWith(<span class="string">"-"</span>)) &#123;</span><br><span class="line">		result = -result;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Feedback</category>
      </categories>
      <tags>
        <tag>面试总结</tag>
      </tags>
  </entry>
  <entry>
    <title>GC in HotSpot JVM</title>
    <url>/2020/03/12/GC-in-HotSpot-JVM/</url>
    <content><![CDATA[<p>今天我们来聊 GC ( Garbage Collection ),  Java 与 C++ 之间有一堵由<strong>内存动态分配</strong>和<strong>垃圾收集技术</strong>所围城的 <font color=blue>“高墙”</font>。</p>
<center>外面的人想进去，里面的人想出来</center>

<h2 id="GC-是什么？"><a href="#GC-是什么？" class="headerlink" title="GC 是什么？"></a>GC 是什么？</h2><blockquote>
<p>Java垃圾回收机制</p>
</blockquote>
<p>目前主流的 JVM（HotSpot）采用的是分代收集算法。与 C++ 不同的是，Java 采用的是类似于树形结构的可达性分析法来判断对象是否还存在引用。</p>
<p>即：从 GC Root 开始，把所有可以搜索得到的对象标记为存活对象。</p>
<h2 id="JVM-内存结构"><a href="#JVM-内存结构" class="headerlink" title="JVM 内存结构"></a>JVM 内存结构</h2><blockquote>
<p>GC主要是针对运行的数据区的</p>
</blockquote>
<p>做应用程序开发，只需要大方向关注 5 块区域</p>
<ol>
<li><a href="#method_area">方法区(Method Area)；</a></li>
<li><a href="#jvm_stack">Java栈(Java stack)；</a></li>
<li><a href="#native_method_stacks">本地方法栈(Native Method Stack)；</a></li>
<li><a href="#heap">堆(Heap)；</a></li>
<li><a href="#program_counter_register">程序计数器(Program Counter Register)</a></li>
</ol>
<center><img src="https://img-blog.csdnimg.cn/20200312115043330.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<div id="method_area"><b>方法区 (Method Area)</b></div><br/>

<p>&ensp;&ensp;&ensp;&ensp;与Heap一样，也是各个线程共享的内存域，这块区域主要是用来存储类加载器加载的类信息，常量，静态变量，通俗的讲就是编译后的class文件信息。</p>
<div id="jvm_stack"><b>Jvm栈</b></div><br/>

<p>&ensp;&ensp;&ensp;&ensp;与程序计数器一样，它是每个线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。</p>
<div id="native_method_stacks"><b>本地方法栈（Native Method Stacks）</b></div><br/>

<p>&ensp;&ensp;&ensp;&ensp;本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。</p>
<div id="heap"><b>堆 (Heap)</b></div><br/>

<p>&ensp;&ensp;&ensp;&ensp;是Jvm管理的内存中最大的一块。程序的主要数据也都是存放在堆内存中的，这一块区域被所有的线程所共享，通常出现线程安全问题的一般都是这个区域的数据出现的问题。</p>
<div id="program_counter_register"><b>程序计数器 (Program Counter Register)</b></div><br/>

<p>&ensp;&ensp;&ensp;&ensp;个人感觉的他就是为多线程准备的，程序计数器是每个线程独有的，所以是线程安全的。它主要用于记录每个线程的执行情况。</p>
<h2 id="GC实现机制-我们为什么要去了解GC和内存分配？"><a href="#GC实现机制-我们为什么要去了解GC和内存分配？" class="headerlink" title="GC实现机制-我们为什么要去了解GC和内存分配？"></a>GC实现机制-我们为什么要去了解GC和内存分配？</h2><p><strong>A:</strong></p>
<ul>
<li>内存溢出</li>
<li>内存泄漏</li>
<li>并发处理</li>
<li>当垃圾收集成为系统达到更高并发量的瓶颈时，我们需要监控和调节</li>
</ul>
<blockquote>
<p>GC 主要是针对 Java Heap 这块区域，其次是方法区</p>
</blockquote>
<p>JVM的内存空间，从大的层面上来分析包含：新生代空间( Young )和老年代空间（ Old ）。新生代空间（Young）又被分为2个部分（Eden区域、Survivous区域）和3个板块（1个Eden区域和2个Survivous区域）</p>
<center><img src="https://img-blog.csdnimg.cn/2020031212013163.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p><b>Eden(伊甸园)区域</b>：用来存放使用new或者newInstance等方式创建的对象，默认这些对象都是存放在Eden区，除非这个对象太大，或者超出了设定的阈值-XX:PretenureSizeThresold，这样的对象会被直接分配到Old区域。</p>
<p>2个<b>Survivous(幸存)区域</b>：一般称为S0，S1，理论上他们一样大。</p>
<p><strong>Q:</strong><br><strong>S0和S1一般多大，靠什么参数来控制，有什么变化？</strong></p>
<p><strong>A:</strong></p>
<p>一般来说很小，我们大概知道它与Young差不多相差一倍的比例，设置的参数主要有两个：</p>
<blockquote>
<p>-XX:SurvivorRatio=8<br>-XX:InitialSurvivorRatio=8</p>
</blockquote>
<p><strong>第一个参数</strong> :是 Eden 和 Survivous 区域比重（注意 Survivous 一般包含两个区域 $S_{0}$ 和 $S_{1}$ ，这里是一个Survivous的大小）。如果将 -XX:SurvivorRatio=8 设置为8，则说明Eden区域是一个Survivous区的8倍，换句话说 $S_{0}$ 或 $S_{1}$ 空间是整个Young空间的1/10，剩余的8/10由Eden区域来使用。</p>
<p><strong>第二个参数</strong> :是 $Young/S0$ 的比值，当其设置为 8 时，表示 $S_{0}$ 或 $S_{1}$ 占整个 Young 空间的1/8（或12.5%）</p>
<h2 id="GC的实现"><a href="#GC的实现" class="headerlink" title="GC的实现"></a>GC的实现</h2><ol>
<li>Minor GC</li>
<li>Full GC</li>
</ol>
<p><font color=red>通过 Minor GC 后进入旧生代的平均大小大于旧生代的可用内存。如果发现统计数据说之前 Minor GC 的平均晋升大小比目前旧生代剩余的空间大，则不会触发 Minor GC 而是转为触发 Full GC。</font></p>
<h3 id="Minor-GC"><a href="#Minor-GC" class="headerlink" title="Minor GC"></a>Minor GC</h3><h4 id="First-GC"><a href="#First-GC" class="headerlink" title="First GC"></a>First GC</h4><p>在不断创建对象的过程中，当Eden区域被占满，此时会开始做Young GC也叫Minor GC</p>
<ol>
<li><p>第一次GC时Survivous中S0区和S1区都为空，将其中一个作为To Survivous(用来存储Eden区域执行GC后不能被回收的对象)。比如：将S0作为To Survivous，则S1为From Survivous。</p>
</li>
<li><p>将Eden区域经过GC不能被回收的对象存储到To Survivous（S0）区域（此时Eden区域的内存会在垃圾回收的过程中全部释放），但如果To Survivous（S0）被占满了，Eden中剩下不能被回收对象只能存放到Old区域。</p>
</li>
<li><p>将Eden区域空间清空，此时From Survivous区域（S1）也是空的。</p>
</li>
<li><p>S0与S1互相切换标签，S0为From Survivous，S1为To Survivous。</p>
</li>
</ol>
<center><img src="https://img-blog.csdnimg.cn/20200312121238749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h4 id="Second-GC"><a href="#Second-GC" class="headerlink" title="Second GC"></a>Second GC</h4><p>当第二次Eden区域被占满时，此时开始做GC</p>
<ol>
<li><p>将Eden和From Survivous(S0)中经过GC未被回收的对象迁移到To Survivous(S1)，如果To Survious(S1)区放不下，将剩下的不能回收对象放入Old区域；</p>
</li>
<li><p>将Eden区域空间和From Survivous（S0）区域空间清空；</p>
</li>
<li><p>S0与S1互相切换标签，S0为To Survivous，S1为From Survivous。</p>
</li>
</ol>
<center><img src="https://img-blog.csdnimg.cn/2020031213042674.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>。。。以此类推</p>
<h4 id="第三次、第四次…"><a href="#第三次、第四次…" class="headerlink" title="第三次、第四次…"></a>第三次、第四次…</h4><blockquote>
<p>反反复复多次没有被淘汰的对象，将会被放入Old区域中</p>
</blockquote>
<h3 id="究竟会经过多少次？"><a href="#究竟会经过多少次？" class="headerlink" title="究竟会经过多少次？"></a>究竟会经过多少次？</h3><p>设置次数的参数：由计数器记录【计数器会在对象的头部记录它的交换次数】 </p>
<blockquote>
<p>–XX:MaxTenuringThreshold=15</p>
</blockquote>
<h3 id="Full-GC"><a href="#Full-GC" class="headerlink" title="Full GC"></a>Full GC</h3><p>我们来捋捋 Full GC 的触发条件：</p>
<ol>
<li>System.gc()方法的调用；<br>此方法的调用是建议JVM进行Full GC,虽然只是建议而非一定，但很多情况下它会触发 Full GC,从而增加Full GC的频率，也即增加了间歇性停顿的次数。强烈影响系建议能不使用此方法就别使用，让虚拟机自己去管理它的内存，可通过通过-XX:+ DisableExplicitGC来禁止RMI（Java远程方法调用）调用System.gc。</li>
<li>旧生代空间不足;<br>旧生代空间只有在新生代对象转入及创建为大对象、大数组时才会出现不足的现象，当执行Full GC后空间仍然不足，则抛出错误：java.lang.OutOfMemoryError: Java heap space 。为避免以上两种状况引起的FullGC，调优时应尽量做到让对象在Minor GC阶段被回收、让对象在新生代多存活一段时间及不要创建过大的对象及数组。</li>
<li>由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小</li>
</ol>
<h2 id="GC实现机制-Java虚拟机如何实现垃圾回收机制"><a href="#GC实现机制-Java虚拟机如何实现垃圾回收机制" class="headerlink" title="GC实现机制-Java虚拟机如何实现垃圾回收机制"></a>GC实现机制-Java虚拟机如何实现垃圾回收机制</h2><ol>
<li><a href="#reference_counting">引用计数算法（Reference Counting）</a></li>
<li><a href="#reachability_analysis">可达性分析算法（Reachability Analysis）</a></li>
</ol>
<h3 id="引用计数算法（Reference-Counting）"><a href="#引用计数算法（Reference-Counting）" class="headerlink" title="引用计数算法（Reference Counting）"></a><div id="reference_counting">引用计数算法（Reference Counting）</div></h3><p>给对象添加一个引用计数器，每当有一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的，这就是引用计数算法的核心。</p>
<p>客观来讲，引用计数算法实现简单，判定效率也很高，在大部分情况下都是一个不错的算法。但是Java虚拟机并没有采用这个算法来判断何种对象为死亡对象，因为它很难解决对象之间相互循环引用的问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReferenceCountingGC</span> </span>&#123;</span><br><span class="line">   <span class="keyword">public</span> Object object = <span class="keyword">null</span>;</span><br><span class="line">  </span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> OenM = <span class="number">1024</span> * <span class="number">1024</span>;</span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">byte</span>[] bigSize = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">2</span> * OneM];</span><br><span class="line"> </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">testCG</span><span class="params">()</span></span>&#123;</span><br><span class="line">      ReferenceCountingGC objA = <span class="keyword">new</span> ReferenceCountingGC(); </span><br><span class="line">       ReferenceCountingGC objB = <span class="keyword">new</span> ReferenceCountingGC(); </span><br><span class="line">      </span><br><span class="line">       objA.object = <span class="keyword">null</span>;</span><br><span class="line">       objB.object = <span class="keyword">null</span>;</span><br><span class="line"> </span><br><span class="line">      System.gc();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上述代码段中，objA与objB互相循环引用，没有结束循环的判断条件，运行结果显示Full GC，就说明当Java虚拟机并不是使用引用计数算法来判断对象是否存活的。</p>
<h3 id="可达性分析算法（Reachability-Analysis）"><a href="#可达性分析算法（Reachability-Analysis）" class="headerlink" title="可达性分析算法（Reachability Analysis）"></a><div id="reachability_analysis">可达性分析算法（Reachability Analysis）</div></h3><blockquote>
<p>这是Java虚拟机采用的判定对象是否存活的算法。</p>
</blockquote>
<p>通过一系列的称为“GC Roots”的对象作为起始点，从这些结点开始向下搜索，搜索所走过的路径称为饮用链（Reference Chain），当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。</p>
<p>可作为GC Roots的对象包括：</p>
<ol>
<li>虚拟机栈中引用的对象;</li>
<li>方法区中类静态属性引用的对象;</li>
<li>方法区中常量引用的对象;</li>
<li>本地方法栈JNI引用的对象</li>
</ol>
<h2 id="GC实现机制-何为死亡对象？"><a href="#GC实现机制-何为死亡对象？" class="headerlink" title="GC实现机制-何为死亡对象？"></a>GC实现机制-何为死亡对象？</h2><blockquote>
<p>从根引用开始，对象的内部属性可能也是引用，只要能级联到的都被认为是活着的对象。</p>
</blockquote>
<blockquote>
<p>Java虚拟机在进行死亡对象判定时，会经历两个过程。</p>
</blockquote>
<p>如果对象在进行可达性分析后没有与 GC Roots 相关联的引用链，则该对象会被 JVM 进行第一次标记并且进行一次筛选</p>
<blockquote>
<p>筛选的条件是此对象是否有必要执行 finalize() 方法，</p>
</blockquote>
<ul>
<li><p>如果当前对象没有覆盖该方法，或者 finalize() 方法已经被 JVM 调用过都会被虚拟机判定为“没有必要执行”。</p>
</li>
<li><p>如果该对象被判定为没有必要执行，那么该对象将会被放置在一个叫做 F-Queue 的队列当中，并在稍后由一个虚拟机自动建立的、低优先级的 Finalizer 线程去执行它，在执行过程中JVM可能不会等待该线程执行完毕，因为如果一个对象在 finalize() 方法中执行缓慢，或者发生死循环，将很有可能导致 F-Queue 队列中其他对象永久处于等待状态，甚至导致整个内存回收系统崩溃。</p>
</li>
<li><p>如果在 finalize() 方法中该对象重新与引用链上的任何一个对象建立了关联，即该对象连上了任何一个对象的引用链，例如 this 关键字，那么该对象就会逃脱垃圾回收系统；</p>
</li>
<li><p>如果该对象在 finalize() 方法中没有与任何一个对象进行关联操作，那么该对象会被虚拟机进行第二次标记，该对象就会被垃圾回收系统回收。值得注意的是 finaliza() 方法JVM系统只会自动调用一次.</p>
</li>
<li><p>如果对象面临下一次回收，它的 finalize() 方法不会被再次执行。</p>
</li>
</ul>
<h2 id="GC实现机制-垃圾收集算法"><a href="#GC实现机制-垃圾收集算法" class="headerlink" title="GC实现机制-垃圾收集算法"></a>GC实现机制-垃圾收集算法</h2><ol>
<li>标记-清除算法（Mark-Sweep）</li>
<li>复制算法（Copying）</li>
<li>分代收集(Generational Collection)算法</li>
<li>标记-整理(Mark-Compat)算法</li>
</ol>
<p>新生代中，每次垃圾收集时都有大批对象死去，只有少量存活，就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集；</p>
<p>老年代中，其存活率较高、没有额外空间对它进行分配担保，就应该使用“标记-整理”或“标记-清理”算法进行回收。</p>
<h2 id="常用的垃圾回收器"><a href="#常用的垃圾回收器" class="headerlink" title="常用的垃圾回收器"></a>常用的垃圾回收器</h2><ol>
<li>Serial收集器</li>
<li>ParNew收集器</li>
<li>Parallel Scavenge收集器</li>
<li>CMS(Concurrent Mark Sweep) 收集器</li>
<li>G1(Garbage First) 收集器</li>
</ol>
<p><strong>Q:</strong><br><strong>对象进入Old区域有什么坏处？</strong></p>
<p><strong>A:</strong><br>&ensp;&ensp;&ensp;&ensp;&ensp;Old区域一般称为老年代，老年代与新生代不一样。新生代，我们可以认为存活下来的对象很少，而老年代则相反，存活下来的对象很多，所以JVM的堆内存，才是我们通常关注的主战场，因为这里面活着的对象非常多，所以发生一次FULL GC，来找出来所有存活的对象是非常耗时的，因此，我们应该避免FULL GC的发生。</p>
<p><strong>Q:</strong><br><strong>为什么发生FULL GC会带来很大的危害？</strong></p>
<p><strong>A:</strong><br>&ensp;&ensp;&ensp;&ensp;在发生FULL GC的时候，意味着JVM会安全的暂停所有正在执行的线程（Stop The World），来回收内存空间，在这个时间内，所有除了回收垃圾的线程外，其他有关JAVA的程序，代码都会静止，反映到系统上，就会出现系统响应大幅度变慢，卡机等状态。</p>
]]></content>
      <categories>
        <category>Feedback</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title>人=波,颠倒因果</title>
    <url>/2020/03/05/%E4%BA%BA=%E6%B3%A2%EF%BC%8C%E9%A2%A0%E5%80%92%E5%9B%A0%E6%9E%9C/</url>
    <content><![CDATA[<p>忆中学时期的物理课，物理科学使我受益匪浅，特别是那一假设：</p>
<hr>
<p>人说可能也是以一种波的形式存在，当你观察的时候就以实物的形式存在，当你不观察的时候就以波的形式存在，因为每当我们闭上眼睛根本不知道周围真正发生了什么。</p>
<hr>
<p>这个假设深埋心底，迟迟未能解决，到了大学，有幸在物理实验中接触到双缝干涉实验，并受一篇物理学文章的启发，终于算是解决了这个埋藏已久的问题。</p>
<p><strong>引入：</strong></p>
<blockquote>
<p>双缝干涉实验：颠倒因果</p>
</blockquote>
<ul>
<li><p>牛顿的经典力学曾被认为适用于宇宙中所有物体，但自从波尔的量子力学出现，经典力学总会失效，微观世界与宏观世界并不同。</p>
</li>
<li><p>双缝干涉实验中，让光源的光线先通过第一个圆孔，这样产生的光源变成相干光源。</p>
</li>
<li><p>再让相干光源通过两条细缝的纸板，最后用一块屏幕承接光源，发现屏幕上出现了明暗相间的条纹。</p>
</li>
</ul>
<blockquote>
<p>这个实验证明了光不仅是一种粒子还具有波的性质</p>
</blockquote>
<p><strong>双缝干涉中粒子是光子，不知对于其他粒子可否能行呢？</strong></p>
<blockquote>
<p>电子</p>
</blockquote>
<p>实验：</p>
<p>&ensp;&ensp;当科学家用电子枪做电子的双缝干涉实验时，电子枪连续向双缝中的其中一条缝发射电子，屏幕上也出现了明暗相间的条纹，这就说明电子和光子一样，具有波粒二象性。后来逐渐证明了质子中子都具有波粒二象性。</p>
<p>测量：</p>
<p>&ensp;&ensp;为了弄清电子穿过双缝时到底是粒子还是波的形态，科学家就在双缝后摆了量子观显示察仪</p>
<p>结果：<br>&ensp;&ensp;当电子通过左缝时显示1，当电子通过右缝时显示2，除此以外的状态，仪器显示0。当再次对电子进行观测时，干涉条纹消失了，屏幕上出现了明亮的电子亮斑，仪器显示此时电子只经过了一条缝隙，而当关闭观察仪的时候再次出现了干涉条纹。</p>
<blockquote>
<p>这就说明了<strong>观察</strong>这个动作能对粒子的形态产生影响</p>
<blockquote>
<p>进一步猜测<strong>不观察电子就是波，观察电子就是粒子</strong></p>
</blockquote>
</blockquote>
<p><strong>这点正是和我刚开始的问题一致</strong>，物理学家认为这或许只是观察仪器发出的光子影响了电子运行中的状态，使电子吸收了光的能量从而产生变化。</p>
<p>其实，这种现象类似于<code>海森堡不确定性原理</code>，微观世界中你无法同时精准得知道粒子的动量和位置，动量越精确位置就越不准。</p>
<p>【注意：涉及到相近于原子内部电子绕原子核转动的问题，电子实际并非绕原子核做规则的圆周运动，而是以一种概率性的出现，一定概率一定时间点出现在某个位置，下一刻的位置并不确定也没有规律可言】</p>
<hr>
<p><strong>补充：</strong></p>
<center><b>量子延迟选择实验</b></center>

<p>$$<br>这是双缝干涉实验的另一个版本<br>$$</p>
<p>当电子枪向双缝发射电子时，这时观察仪没有开启，电子以波的形式同时通过两条细缝，</p>
<p>但当电子通过细缝之后立刻开启观察仪，记录电子的运行轨迹时，<strong>发现电子竟然是从其中的一条细缝通过</strong>,在微观世界中这个动作在发生之前已经对电子的形态产生了影响</p>
<p>明确说明这种现象在现实世界或者宏观条件下时不可能存在的，既说明了宏观与微观的区别，更加说明了微观世界中可以颠倒因果。</p>
<hr>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>不确定性</tag>
      </tags>
  </entry>
  <entry>
    <title>CPU Sched</title>
    <url>/2020/03/05/CPU-Sched/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文将介绍 CPU 高级策略之 CPU 调度，针对一系列调度策略展开。</p>
<center><b>THE CRUX</b></center>

<p><strong>Q：</strong><br>&ensp;如何制定调度策略？</p>
<ol>
<li>如何开发一个基本框架考虑调度策略？</li>
<li>需要做哪些关键的假设？</li>
<li>哪些指标是最重要的？</li>
<li>最早的计算机使用的基本方法？</li>
</ol>
<h2 id="工作量假设-Workload-Assumptions"><a href="#工作量假设-Workload-Assumptions" class="headerlink" title="工作量假设 [Workload Assumptions]"></a>工作量假设 [Workload Assumptions]</h2><p>在开始介绍策略之前，首先对系统中运行的进程 (或称为工作量) 做一些假设。</p>
<blockquote>
<p>确定工作量是建立策略的步骤中最重要的部分</p>
</blockquote>
<div id="presume"><font size=4><b>假设如下：</b></font></div><br/>

<p>&ensp;<strong>Hint：</strong> 这些看似不可行的假设，在之后的介绍中会放宽要求，简化了全业务调度的开发</p>
<ol>
<li>所有进程的运行时间相同;</li>
<li>所有进程同时到达等待启动；</li>
<li>一旦开始，所有进程终将进行完整的运行流程；</li>
<li>所有的进程均只利用 CPU（不执行任何 I/O）</li>
<li>每个进程的运行时间已知</li>
</ol>
<h3 id="调度指标-Scheduling-Metrics"><a href="#调度指标-Scheduling-Metrics" class="headerlink" title="调度指标 [Scheduling Metrics]"></a>调度指标 [Scheduling Metrics]</h3><blockquote>
<p>我们需要一样东西来衡量不同的调度策略，即调度指标</p>
</blockquote>
<p>设定一个简单的指标：Turnaround Time (理解为 “周转时间” )， 其定义为进程完成的时刻减去到达等待开始的时刻：</p>
<p>$$<br>T_{turnaround } = T_{completion} - T_{arrival}<br>$$</p>
<p>在工作量假设中有一条假设：所有进程同时等待启动，所以  $T_{arrival} = 0$ ，进一步，$T_{turnaround } = T_{completion}$。也就是说放宽假设的要求，这个观点就会改变。</p>
<blockquote>
<p>生活不总是完美的</p>
</blockquote>
<p><strong>注意到</strong>，周转时间是一个性能指标，由此不得不再设定一个指标，Fairness (理解为公平性)</p>
<hr>
<p>在调度中，<strong>性能</strong>和<strong>公平性</strong>常常是<strong>不一致</strong>的</p>
<p>For Example：调度程序可以优化性能，但代价是阻止一些作业运行，从而降低公平性。</p>
<hr>
<h3 id="先进先出（FIFO）"><a href="#先进先出（FIFO）" class="headerlink" title="先进先出（FIFO）"></a>先进先出（FIFO）</h3><blockquote>
<p>FIFO 的优点在于简单易实现</p>
</blockquote>
<p>我们可以实现的最基本的算法是<strong>先进先出调度</strong> ( 亦被称为 <strong>First Come, First Served</strong> - FCFS )</p>
<p><strong>For Example:</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;假设系统中有三个进程，A、B和C，大致同时到达（$T_{arrival}= 0$）。因为 FIFO 必须把某个进程放在第一位执行。假设当它们同时到达时，A在B之前到达，而B在C之前到达。同时假设每个进程运行10秒。这些工作的<strong>平均周转时间</strong>将是多少？</p>
<!-- <center><img src="https://img-blog.csdnimg.cn/20200304141601408.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center> -->

<a data-fancybox="gallery" href="https://img-blog.csdnimg.cn/20200304141601408.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener">
    <img src="https://img-blog.csdnimg.cn/20200304141601408.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70">
</a>

<p>图中模拟了进程执行时间，A 在10 的位置完成，B 在20 的位置完成，C 在30 的位置完成</p>
<p>那么，平均周转时间计算为：</p>
<p>$$<br>{\frac{10+20+30}{3}} = 20<br>$$</p>
<p><strong>问题来了：</strong></p>
<p>如果我们放宽第一个假设的要求，不再假设每个进程的运行时间相同。</p>
<ol>
<li>这种情形下，FIFO 如何运行？</li>
<li>如何构建工作量，会导致 FIFO 性能崩溃？</li>
</ol>
<p><strong>我们继续通过例子模拟进程的运行时间，以及不同运行时间的进程如何搞垮 FIFO</strong></p>
<p><strong>For Example：</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;接着之前的三个进程 A、B、C，与上次不同的是 A 运行时间为100，B、C均为10</p>
<p><strong>模拟结果：</strong></p>
<center><img src="https://img-blog.csdnimg.cn/20200304143751298.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>B、C 在运行之前先等待 A 运行 100，这样，平均周转时间计算为：</p>
<p>$$<br>{\frac{100+110+120}{3}} = 110<br>$$</p>
<p>一些资源较短的潜在消费者排在一些重量级资源消费者的后面，这便是经典的<strong>护航效应 (convoy effect)</strong>。</p>
<p><strong>Q:</strong></p>
<p>&ensp;&ensp;怎样才能开发出一个更好的算法来处理运行时间不同的进程呢？</p>
<h3 id="Shortest-Job-First-SJF"><a href="#Shortest-Job-First-SJF" class="headerlink" title="Shortest Job First - SJF"></a>Shortest Job First - SJF</h3><blockquote>
<p>是一个通用的调度原则，它可以应用于任何系统</p>
<blockquote>
<p>特点是：每个进程的周转时间平等重要</p>
</blockquote>
</blockquote>
<p>忆上个学期<strong>运筹学</strong>中的<strong>运输问题</strong>解决办法$-$&gt;<strong>最短作业法</strong>事实上，这个方法可以应用于计算机系统中的工作调度。</p>
<p>这个新的调度规则称为最短作业优先 (最短作业优先)，其名称应该很容易记住，因为它非常完整地描述了策略: 首先运行最短作业，然后运行下一个最短作业，以此类推。</p>
<center><img src="https://img-blog.csdnimg.cn/20200304150420976.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>接着前面的例子重新模拟，平均周转时间计算为：</p>
<p>$$<br>{\frac{10+20+120}{3}} = 50<br>$$</p>
<p>$$<br>[110-&gt;50 ]<br>$$</p>
<p>理论上，如果假设进程同时到达成立，那么可以证明 SJF 是一个最优调度算法【实际中，非也】</p>
<p><strong>问题来了：</strong></p>
<p>&ensp;&ensp;针对假设2，对其要求再进一步放宽，让进程随机到达，而不是同时，会出现什么状况？</p>
<p><strong>For Example:</strong></p>
<p>&ensp;&ensp;假设 A 在 t = 0 时刻到达，需要运行 100 ，而 B 和 C 在 t = 10 到达，每个需要运行 10 。</p>
<p><strong>模拟结果：</strong></p>
<center><img src="https://img-blog.csdnimg.cn/20200304152513501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>B、C 在 A 开始运行后到达，出现了和之前相同的护航效应，此时，平均周转时间计算为：</p>
<p>$$<br>{\frac{100+(110-10)+(120-10)}{3}} = 103.33…<br>$$</p>
<p><strong>Q:</strong><br>&ensp;&ensp;调度究竟能做什么？<br><strong>A:</strong><br>&ensp;&ensp;为了解决新出现的问题，接下来考虑放宽假设 3 的要求</p>
<hr>
<p><b><font size=4>补充：</font></b></p>
<center><b>抢占式调度 Preemptive Schedulers</b></center><br/>

<p>&ensp;&ensp;&ensp;&ensp;在以前的批量计算时代，开发了许多非抢占式调度程序，这种系统将正在运行的进程运行完毕才考虑要不要运行下一个新的进程，几乎所有的现代调度程序都是抢占式的，并且会为了运行另一个进程而停止一个进程的运行。</p>
<p>&ensp;&ensp;&ensp;&ensp;这意味着调度程序采用了我们之前了解到的机制；特别是，调度程序可以执行上下切换，暂时停止一个正在运行的进程，并恢复（或启动）另一个进程。</p>
<hr>
<p>根据调度的本身机制，考虑到计时器中断和上下切换，当 B 和C到达时，调度器会做一些其他的事情：抢占 A 的运行时间并运行其他进程，或者决定先暂停稍后再运行。而 <strong>SJF 是一个非抢占式调度程序</strong>，因此<strong>无法改变平均周转时间</strong>仍然很<strong>长</strong>的现状。</p>
<p>那么可否将抢占机制加到 SJF 中呢，这样就可以完善了？答案是可以的，添加后的结果称为最短完成时间优先（STCF）或抢占最短作业优先（PSJF）调度程序</p>
<center>SJF</center>
<center>+</center>
<center>抢占机制</center>
<center>||</center>
<center>STCF</center>

<h3 id="Shortest-Time-to-Completion-First-STCF"><a href="#Shortest-Time-to-Completion-First-STCF" class="headerlink" title="Shortest Time-to-Completion First (STCF)"></a>Shortest Time-to-Completion First (STCF)</h3><blockquote>
<p>放宽假设3的要求，进程不必要一旦开始就定要执行完毕</p>
</blockquote>
<p>每当新进程进入系统时，STCF调度器确定剩余进程 (包括新进程) 中剩余时间最少的进程，并调度该进程。</p>
<center><img src="https://img-blog.csdnimg.cn/20200304160421216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>STCF将抢占 A 并运行 B 和 C 直到完成；只有当它们完成时，才会安排 A 的剩余时间。如此，平均周转时间计算为：</p>
<p>$$<br>{\frac{(120-0)+(20-10)+(30-10)}{3}} = 50<br>$$</p>
<p><strong>和之前一样，考虑到我们的新假设，STCF是可证明最优的；考虑到如果所有工作同时到达，SJF是最优的。</strong></p>
<p><strong>However：</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;如果我们知道进程运行时间长度、进程仅需要 CPU，并且<strong>唯一的度量指标</strong>是周转时间，可以确定 STCF 是最优的调度策略。但是随着<strong>分时机</strong>的引入，用户在终端中也可以与系统进行交互。</p>
<p>&ensp;&ensp;&ensp;&ensp;出现了<strong>交互</strong>，我们需要引入一个<strong>新的度量指标</strong>：响应时间。</p>
<h3 id="响应时间-A-New-Metric-Response-Time"><a href="#响应时间-A-New-Metric-Response-Time" class="headerlink" title="响应时间 [A New Metric: Response Time]"></a>响应时间 [A New Metric: Response Time]</h3><blockquote>
<p>从进程到达系统到它第一次被调度的时间</p>
</blockquote>
<p>$$<br>T_{response} = T_{firstrun} - T_{arrival}<br>$$</p>
<p><strong>利用之前的调度例子：</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;不同的是 A 在时间 0 时到达，B 和 C 在时间 10 时到达，每个进程的响应时间如下 : A为0，B为0，C为10，响应时间平均值为 3.33</p>
<blockquote>
<p>可以看出：STCF及其相关并不利于优化响应时间。</p>
</blockquote>
<p><strong>For Example：</strong></p>
<p>如果三个作业同时到达，那么第三个作业必须等待前两个作业全部运行，然后才进行一次调度。虽然这种方法对于周转时间很好，但是对于响应时间和交互性却很差。</p>
<p>实际上，现实生活中，终端前打字，不得不等上 10 秒钟才能看到系统的响应，因为在这之前有其他的进程已经被调度。</p>
<p><strong>Q:</strong></p>
<p>那么我们该如何构建一个对响应时间敏感的调度器？</p>
<p><strong>A:</strong></p>
<p>引入新的调度算法$-$&gt; Round Robin</p>
<h3 id="轮询法-Round-Robin"><a href="#轮询法-Round-Robin" class="headerlink" title="轮询法 [Round Robin]"></a>轮询法 [Round Robin]</h3><blockquote>
<p>负载均衡算法【简称 RR】</p>
</blockquote>
<p>RR 不再是一旦运行开始就必须运行完毕，而是运行一个时间片( 亦称为调度量 )，接着切换到运行队列中的下一个进程，反复这样做，直到所有工作完成。所以有时候 RR 冠之以时间切片的称号。</p>
<p><font color=skyblue><strong>注意：</strong> 时间片的长度必须是计时器中断周期的倍数；因此，如果计时器每10毫秒中断一次，则时间片可以是10、20或任何其他10毫秒的倍数。</font></p>
<p>还是通过例子以达到对 RR 的彻底理解：</p>
<p><strong><font size=4>For Example：</font></strong></p>
<p>假设A、B、C三个进程同时到达，并且每一个进程都希望运行 5 个时间单位。</p>
<blockquote>
<p>一个 SJF 调度器就像之前介绍的一样在运行一个新的进程之前保证正在运行的进程运行完毕。</p>
<center><img src="https://img-blog.csdnimg.cn/20200304170743309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>
平均响应时间：(0+5+10) / 3 = 5
</blockquote>
<blockquote>
<p>相比之下，RR 按照 1 个时间单位的时间片可以很高效的完成所有任务。</p>
<center><img src="https://img-blog.csdnimg.cn/20200304170827977.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>
平均响应时间：(0+1+2) / 3 = 1
</blockquote>
<ul>
<li><p><font size=4><b>如上对比:</b></font> 时间片的长度对于 RR 来说是至关重要的。它<strong>越短</strong>，在响应时间度量下 RR 的<strong>性能越好</strong>。</p>
</li>
<li><p><font size=4><b>但是</b></font> ，盲目得让时间片太短是有问题的，突发的上下文切换带来的开销将主导整体性能。</p>
</li>
<li><p><font size=4><b>因此</b></font>，<strong>决定时间片的长度</strong>对系统设计人员来说是一种<strong>权衡</strong>。目的是<font color=red>使其有足够长的时间来摊销切换的成本，同时又不会使其太长而导致系统不再响应</font></p>
</li>
</ul>
<p>【<strong>注意：</strong> 上下文切换的开销并不仅仅来自于OS保存和恢复几个寄存器的操作。当程序运行时，它们会在CPU缓存、缓存器、分支预测器和其他片上硬件中建立大量的状态。切换到另一个作业会导致刷新此状态并引入与当前正在运行的进程相关的新状态，这可能会导致显著的性能损失】</p>
<hr>
<p><strong>补充：</strong></p>
<center><b>摊销可以降低成本 [Amortization can Reduce Costs]</b></center><br/>
&ensp;&ensp;&ensp;&ensp;某项固定的开销产生时，往往伴随着系统中的摊销技术的体现。通过减少成本（即减少操作次数），降低了系统的总成本。<br/>

<p><strong>For Example:</strong> </p>
<ul>
<li><p>如果时间片设置为10毫秒，而上下文切换成本为1毫秒，则大约10%的时间用于上下文切换，均被浪费；</p>
</li>
<li><p>如果我们想分摊这个成本，可以增加时间片到 100 毫秒。在这种情况下，不到 1% 的时间用于上下文切换，这样一来，时间成本得到了摊销。</p>
</li>
</ul>
<hr>
<center><b><font size=4>没有对比就没有伤害</b></center><br/>

<ul>
<li><p>如果响应时间是我们唯一的度量，那么具有合理时间片的RR就是一个非常好的调度器。</p>
</li>
<li><p>还记得之前我们以周转时间为唯一度量，那么在 RR 的例子的基础上，运行时间分别为 5 秒的 A、B和 C 同时到达，RR 是具有 1 个单位长时间片的调度器。从上面的图中我们可以看到，A结束于13,B结束于14,C结束于15，<strong>平均周转时间</strong>为14。</p>
</li>
</ul>
<p><strong>因此</strong>，如果周转时间是我们的度量标准，那么 RR 确实是最糟糕的策略之一也就不足为奇了。</p>
<p><strong>从直觉上看：</strong> RR所做的就是尽可能地延长每个作业的执行时间，在执行下一个作业之前，只运行一小段时间。因为周转时间只关心作业何时完成，所以 RR 在特定情况下比 FIFO 的效果都差。</p>
<center><b><font size=4>鱼与熊掌不可兼得</b></center><br/>

<blockquote>
<p>说白了就是：权衡</p>
</blockquote>
<p>任何公平的策略（如 RR），即在小时间尺度上将 CPU 平均分配给活动进程的策略，相应的在周转时间等指标上都会表现不佳。</p>
<p><strong>事实上，这是一种内在的权衡</strong> 。当然，你也可以运行更短的作业到完成，但代价是响应时间；如果你重视公平，则响应时间会降低，但代价是周转时间。</p>
<hr>
<p><strong>补充：</strong></p>
<center><b>重叠操作可提高系统的利用率</b></center><br/>

<p>&ensp;&ensp;&ensp;&ensp;在尽可能的情况下，重叠操作得以最大限度地利用系统。重叠在许多不同的域中都很有用，包括在执行磁盘 I/O 或向远程计算机发送消息时；</p>
<p>&ensp;&ensp;&ensp;&ensp;在这两种情况下，启动操作然后切换到另一个工作空间，可以提高系统的总体利用率和效率。</p>
<hr>
<p>至此，需要进行总结…</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>我们认知了两种类型的调度策略：【就两个度量指标进行对比】</p>
<ul>
<li><p>第一种类型（SJF，STCF）优化了周转时间，但对响应时间不利。</p>
</li>
<li><p>第二种类型（RR）优化了响应时间，但不利于周转。</p>
</li>
</ul>
<p>试回想 <a href="#presume">5 个假设</a>，我们已经涉及到了前三个假设，并且都进行了需求更改。</p>
<p>接下来，需要进一步面对剩余两个假设…</p>
<h3 id="Incorporating-I-O"><a href="#Incorporating-I-O" class="headerlink" title="Incorporating I/O"></a>Incorporating I/O</h3><blockquote>
<p>针对假设 4 ：所有的进程均只利用 CPU（不执行任何 I/O）</p>
</blockquote>
<p>对于假设 4 ，当然正常情况下所有程序都执行I/O。</p>
<ul>
<li><p>设想一个不接受任何输入的程序，它每次都将会产生相同的输出；</p>
</li>
<li><p>再想象一个没有输出的程序，那么它有没有在运行似乎已经变得无关紧要。</p>
</li>
</ul>
<p>当一个进程启动一个I/O请求时，调度器显然具有决策权。因为当前运行的进程在 I/O 期间不会使用CPU，它在等待 I/O 完成时被阻塞。</p>
<p>如果 I/O 是输出到硬盘驱动器，进程可能会被阻塞几毫秒甚至更长时间，具体取决于驱动器的当前 I/O 负载。从而，调度器可能在 CPU 上调度另一个进程。</p>
<p>具有决策权的调度器还必须在 I/O 完成时做出决定。做决定时会引发中断，操作系统运行并将请求 I/O 的进程从阻塞状态移回就绪状态。当然，它甚至也可以决定在那时进行这个任务。</p>
<p><strong>Q:</strong><br><strong>那么，对待每一个进程，可以重新开始也可以继续进行，操作系统应该如何对待每项工作？</strong><br><strong>A:</strong><br><strong>e.g.</strong> 假设有两个进程，A 和 B，每个进程都需要50毫秒的计算时间。区别在于，A 运行10毫秒，然后发出一个I/O请求（ 这里假设每个I/O需要10毫秒 ），而 B 只使用50毫秒计算时间，不执行I/O。这种情形下，调度器会先运行A，然后运行B。</p>
<center><img src="https://img-blog.csdnimg.cn/20200305092834284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p><strong>Q:</strong><br>&ensp;假设我们用的调度器是STCF调度程序。那么应该如何解释？<br><strong>A:</strong><br>&ensp;a 被分解成5个10毫秒的子作业，而B只是一个50毫秒的 CPU 需求，很显然，只需要运行一个进程，对于另一个没有把 I/O 考虑在内的进程是没有意义的。</p>
<blockquote>
<p>另一种常见的方法是将 A 的每个 10ms 子作业视为独立作业。</p>
</blockquote>
<p>因此，当系统启动时，其选择是调度 10 ms A还是 50 ms B。对于STCF，显然是选择较短的一个。当 Ａ 的第一个子作业完成时，只剩下 B，它开始运行。然后提交一个新的 A 子作业，它抢占 B 并运行 10 ms。这样做允许重叠，一个进程使用CPU，同时等待另一个进程的 I/O 完成；<strong>高效的利用</strong>了 CPU。</p>
<center><img src="https://img-blog.csdnimg.cn/20200305093935132.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p><strong>Summary：</strong></p>
<p>&ensp;&ensp;&ensp;通过这种新的方法，我们可以看到调度器如何合并 I/O。将每个 CPU 突发当作一个作业来处理，调度器确保 <strong>“交互式”</strong> 运行。当这些交互作业执行 I/O 时，其他CPU密集型作业也会运行，从而更好地利用处理器。</p>
<h3 id="No-More-Oracle"><a href="#No-More-Oracle" class="headerlink" title="No More Oracle"></a>No More Oracle</h3><p>基于 合并 I/O 方法，我们就可以得到最后的假设，调度器知道每个作业的长度。正如我们之前所说，这可能是我们能做出的最不切实际的假设。事实上，在一个通用操作系统（就像我们关心的那些操作系统）中，操作系统通常对每个作业的长度知之甚少。</p>
<h1 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h1><p>此文介绍了调度背后的基本思想，并开发了两种方法。第一个运行剩余的最短作业，从而优化周转时间；第二个在所有作业之间交替，从而优化响应时间。内在均衡问题导致无法判别某一种方法的绝对好坏。</p>
<p>我们也看到了如何将I/O集成到图片中，但仍然没有解决操作系统无法预见未来的问题。</p>
<p><strong>经典 Q:</strong><br>&ensp;&ensp;&ensp;&ensp;因此，如果没有这样的先验知识，我们怎么能建立一个像SJF/STCF这样的方法呢？此外，我们如何将我们看到的一些想法与RR调度器结合起来，从而使响应时间也相当好？</p>
<p><strong>A:</strong><br>&ensp;&ensp;&ensp;&ensp;通过构建一个调度程序，使用最近的过去来预测未来。这个调度程序称为<strong>多级反馈队列</strong>，见下回分晓…</p>
]]></content>
      <categories>
        <category>Operating Systems</category>
      </categories>
      <tags>
        <tag>调度策略</tag>
      </tags>
  </entry>
  <entry>
    <title>自增主键的前世今生</title>
    <url>/2020/02/29/%E8%87%AA%E5%A2%9E%E4%B8%BB%E9%94%AE%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/</url>
    <content><![CDATA[<p>以MySQL(Innodb存储)为例介绍自增主键，介入场景分析主键的目的，从面试题下手，深入理解主键机制</p>
<a id="more"></a>

<p><font size=5><strong>引入：</strong></font></p>
<hr>
<p>使用MySQL建表时，我们通常会创建一个自增字段(AUTO_INCREMENT)，并以此字段作为主键</p>
<hr>
<p>本文将分三点阐述：</p>
<ol>
<li><a href="#title1">你可能不知道的自增主键</a></li>
<li><a href="#title2">应对变化的自增主键</a></li>
<li><a href="#title3"><font color=red>[坑]</font>如果自增主键用完怎么办</a></li>
</ol>
<h2 id="1-你可能不知道的自增主键"><a href="#1-你可能不知道的自增主键" class="headerlink" title="1.你可能不知道的自增主键"></a><div id="title1">1.你可能不知道的自增主键</div></h2><blockquote>
<p>使用自增主键可以提高数据存储效率</p>
</blockquote>
<p>在MySQL中(Innodb 存储引擎)，数据记录本身被存于主索引（B+Tree）的叶子节点上<br><font color=blue>*补充：【要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放】</font></p>
<hr>
<p>针对索引，</p>
<ul>
<li>如果我们<strong>定义了主键</strong>(PRIMARY KEY)，那么InnoDB会选择主键作为聚集索引</li>
<li>如果<strong>没有显式定义</strong>主键，则InnoDB会选择第一个<strong>不包含有NULL值的唯一索引</strong>作为主键索引</li>
<li>如果也<strong>没有这样的唯一索引</strong>，则InnoDB会选择内置6字节长的<strong>ROWID</strong>作为隐含的聚集索</li>
</ul>
<p><font color=blue>*补充：【ROWID随着行记录的写入而主键递增，这个ROWID不像ORACLE的ROWID那样可引用，是隐含的】</font></p>
<hr>
<p><strong>Q：</strong></p>
<p>每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）。</p>
<p><strong>A：</strong></p>
<ul>
<li><p>如果表<strong>使用自增主键</strong>。每次插入新的记录，会顺序添加到当前索引节点的后续位置，一页写满，自动开辟一个新的页</p>
</li>
<li><p>如果<strong>使用非自增主键</strong>（For Example:身份证号或学号等）【每次插入主键的值近似于随机】，每次新纪录都要被插到现有索引页中间某个位置，MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来<br/><br>这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。</p>
</li>
</ul>
<blockquote>
<p>自增id是增长的，不一定连续</p>
</blockquote>
<p><strong>原因有以下几点：</strong></p>
<ul>
<li>唯一键冲突</li>
<li>事务回滚</li>
<li>insert … select语句批量申请</li>
</ul>
<p><strong>针对自增值的保存策略：</strong></p>
<p>InnoDB 引擎中，自增值保存在了内存中，继 MySQL 8.0 之后，出现了<strong>自增值持久化</strong>，自增值的变更记录存储在 redo log 中，重启时可以依靠其恢复之前的值</p>
<p><font color=blue>*补充：【自增值持久化：如果发生重启，表的自增值可以恢复为 MySQL 重启前的值】</font></p>
<h2 id="2-应对变化的自增主键"><a href="#2-应对变化的自增主键" class="headerlink" title="2.应对变化的自增主键"></a><div id="title2">2.应对变化的自增主键</div></h2><p><strong>导入：</strong></p>
<blockquote>
<p>在设计数据库时不需要费尽心思去考虑设置哪个字段为主键</p>
</blockquote>
<p> 但是应用到实际场景，自增主键的主要目的还是应对变化。</p>
<p>设计一个场景：</p>
<p>&ensp;&ensp;&ensp;&ensp;<a href="https://mp.weixin.qq.com/s/rh0tg4L9Ffj1fy32rKCkHw" target="_blank" rel="noopener">维护商业账号的资质相关信息</a></p>
<p><strong>最初设计：</strong> 账号是由全局唯一且自增的分布式ID生成器生成的，很显然这个时候我们把账号作为主键这就天然合理</p>
<p><strong>业务迭代一定时间:</strong>  提出了新的需求，一个账号，在不同业务线，需要享有不同资质</p>
<p><strong>比较：</strong> accountId 较之前不唯一，因为，同一个账号，不同业务线，资质是不一样的【无法像最初那样作为主键】</p>
<p><strong>解决方式：</strong> <a href="https://mp.weixin.qq.com/s/rh0tg4L9Ffj1fy32rKCkHw" target="_blank" rel="noopener">见场景中</a></p>
<h2 id="3-如果自增主键用完怎么办"><a href="#3-如果自增主键用完怎么办" class="headerlink" title="3.如果自增主键用完怎么办"></a><div id="title3">3.如果自增主键用完怎么办</div></h2><p><strong>老掉牙但经典：</strong>【面试题】</p>
<blockquote>
<p>面试官:”用过mysql吧，你们是用自增主键还是UUID？”<br>你:”用的是自增主键”<br>面试官:”为什么是自增主键？”<br>你:”因为采用自增主键，数据在物理结构上是顺序存储，性能最好，blabla…”<br>面试官:”那自增主键达到最大值了，用完了怎么办？”<br>你:”what，没复习啊！！”<br>( 然后，你就可以回去等通知了！)</p>
</blockquote>
<p><strong>说明：</strong> 自增 id 是整型字段，常用 int 类型来定义增长 id ，而 int 类型有上限 即增长 id 也是有上限的。</p>
<p>既然 int 不够了，首先想到的是改为 BigInt 类型【做个对比】</p>
<center><img src="https://img-blog.csdnimg.cn/20200229113709135.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<center><img src="https://img-blog.csdnimg.cn/20200229113551427.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<p>如果回答：把自增主键的类型改为BigInt类型就好了<br/><br>面试官:”你在线上怎么修改列的数据类型的？”</p>
</blockquote>
<p>修改方法：</p>
<ul>
<li>使用mysql5.6+提供的在线修改功能</li>
<li>借助第三方工具</li>
<li>改从库表结构，然后主从切换</li>
</ul>
<p><strong>差不多就算结束了这个问题了。但是回过头想一想，是不是一条路走到黑了，或许从头开始就错了呢！！！</strong></p>
<hr>
<p><strong>插入一条生存力测试，形象生动：</strong></p>
<p>&ensp;&ensp;假如女朋友问：我刚才吃药时看窗外，你猜我看到了什么？</p>
<p>&ensp;&ensp;&ensp;&ensp;<strong>歧途：</strong> 白云？你为什么不看我呢。</p>
<p>&ensp;&ensp;&ensp;&ensp;<strong>正解：</strong> 你怎么要吃药呢</p>
<hr>
<p>那么<strong>正解</strong>应该是什么呢？</p>
<blockquote>
<p>这问题没遇到过，因为自增主键一般用int类型，一般达不到最大值，我们就分库分表了，所以不曾遇见过！</p>
</blockquote>
<p><a href="https://mp.weixin.qq.com/s/kVqj4VdZewuvR_OsXgY_RQ" target="_blank" rel="noopener">具体介绍…</a></p>
]]></content>
      <categories>
        <category>Database Systems</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Fundamentals of Recurrent Neural Network</title>
    <url>/2020/02/29/Fundamentals-of-Recurrent-Neural-Network/</url>
    <content><![CDATA[<h2 id="基于循环神经网络实现语言模型。"><a href="#基于循环神经网络实现语言模型。" class="headerlink" title="基于循环神经网络实现语言模型。"></a>基于循环神经网络实现语言模型。</h2><blockquote>
<p>对于语言模型的介绍</p>
<blockquote>
<p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104303197" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104303197</a></p>
</blockquote>
</blockquote>
<p>我们的目的是基于当前的输入与过去的输入序列，预测序列的下一个字符。循环神经网络引入一个隐藏变量$H$，用$H_{t}$表示$H$在时间步$t$的值。$H_{t}$的计算基于$X_{t}$和$H_{t-1}$，可以认为$H_{t}$记录了到当前字符为止的序列信息，利用$H_{t}$对序列的下一个字符进行预测。</p>
<div align=center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWprbTB2NDRpLnBuZw?x-oss-process=image/format,png" /></div>

<h2 id="构造-Structure"><a href="#构造-Structure" class="headerlink" title="构造(Structure)"></a>构造(Structure)</h2><p>我们先看循环神经网络的具体构造。假设 $\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$ 是时间步 $t$ 的小批量输入，$\boldsymbol{H}_t  \in \mathbb{R}^{n \times h}$ 是该时间步的隐藏变量，则：</p>
<center>【广播机制】</center>

<p>$$<br>\boldsymbol{H}_t = \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}  + \boldsymbol{b}_h).<br>$$</p>
<p>其中，$\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}$，$\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}$, $\boldsymbol{b}_{h} \in \mathbb{R}^{1 \times h}$, $\phi$ 函数是非线性激活函数。</p>
<p>由于引入了 $\boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}$，$H_{t}$ 能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。</p>
<p>由于$H_{t}$的计算基于$H_{t-1}$，上式的计算是循环的，使用循环计算的网络即循环神经网络（recurrent neural network）。</p>
<p>在时间步$t$，输出层的输出为：</p>
<p>$$<br>\boldsymbol{O}_t = \boldsymbol{H}_t \boldsymbol{W}_{hq} + \boldsymbol{b}_q.<br>$$<br>其中$\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}$，$\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}$。</p>
<h2 id="手动实现"><a href="#手动实现" class="headerlink" title="手动实现"></a>手动实现</h2><blockquote>
<p>实现一个基于字符级循环神经网络的语言模型，仍然使用周杰伦的歌词作为语料</p>
<blockquote>
<p>下载地址：<a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/data/jaychou_lyrics.txt.zip?raw=true" target="_blank" rel="noopener">见语言模型一章</a>【点击可直接下载】</p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2l_jay9460 <span class="keyword">as</span> d2l</span><br><span class="line">(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="one-hot向量"><a href="#one-hot向量" class="headerlink" title="one-hot向量"></a>one-hot向量</h3><blockquote>
<p>在此采用one-hot向量将字符表示成向量</p>
</blockquote>
<p>假设词典大小是$N$，每次字符对应一个从$0$到$N-1$的唯一的索引，则该字符的向量是一个长度为$N$的向量，若字符的索引是$i$，则该向量的第$i$个位置为$1$，其他位置为$0$。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span><span class="params">(x, n_class, dtype=torch.float32)</span>:</span></span><br><span class="line">    result = torch.zeros(x.shape[<span class="number">0</span>], n_class, dtype=dtype, device=x.device)  <span class="comment"># shape: (n, n_class)</span></span><br><span class="line">    result.scatter_(<span class="number">1</span>, x.long().view(<span class="number">-1</span>, <span class="number">1</span>), <span class="number">1</span>)  <span class="comment"># result[i, x[i, 0]] = 1</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">    </span><br><span class="line">x = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">x_one_hot = one_hot(x, vocab_size)</span><br><span class="line">print(x_one_hot)</span><br><span class="line">print(x_one_hot.shape)</span><br><span class="line">print(x_one_hot.sum(axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>每次采样的小批量的形状是（批量大小, 时间步数）。我们将其变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。也就是说，时间步$t$的输入为</p>
<p>$$<br>\boldsymbol{X}_t \in \mathbb{R}^{n \times d}<br>$$</p>
<p>其中$n$为批量大小，$d$为词向量大小，即one-hot向量长度（词典大小)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_onehot</span><span class="params">(X, n_class)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [one_hot(X[:, i], n_class) <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>])]</span><br><span class="line"></span><br><span class="line">X = torch.arange(<span class="number">10</span>).view(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">inputs = to_onehot(X, vocab_size)</span><br><span class="line">print(len(inputs), inputs[<span class="number">0</span>].shape)</span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init module param</span></span><br><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line"><span class="comment"># num_inputs: d</span></span><br><span class="line"><span class="comment"># num_hiddens: h, 隐藏单元的个数是超参数</span></span><br><span class="line"><span class="comment"># num_outputs: q</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span> <span class="comment"># 随机初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        param = torch.zeros(shape, device=device, dtype=torch.float32)</span><br><span class="line">        nn.init.normal_(param, <span class="number">0</span>, <span class="number">0.01</span>) <span class="comment"># 随机体现</span></span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(param)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = _one((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = _one((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device))<span class="comment"># 偏置参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device))<span class="comment"># 偏置参数</span></span><br><span class="line">    <span class="keyword">return</span> (W_xh, W_hh, b_h, W_hq, b_q)</span><br></pre></td></tr></table></figure>

<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>函数<code>rnn</code>用循环的方式依次完成循环神经网络每个时间步的计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn</span><span class="params">(inputs, state, params)</span>:</span> <span class="comment"># 前向计算</span></span><br><span class="line">    <span class="comment"># inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state <span class="comment"># 提供了需要维护的状态的初始值 state定义成了元组</span></span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H,) <span class="comment"># 返回新的状态Ｈ，以便于相邻采样</span></span><br></pre></td></tr></table></figure>
<p>函数init_rnn_state初始化隐藏变量，这里的返回值是一个元组。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rnn_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>
<h3 id="裁剪梯度-clip-gradient"><a href="#裁剪梯度-clip-gradient" class="headerlink" title="裁剪梯度(clip gradient)"></a>裁剪梯度(clip gradient)</h3><blockquote>
<p>针对梯度爆炸问题</p>
</blockquote>
<p>循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。假设我们把所有模型参数的梯度拼接成一个向量 $\boldsymbol{g}$，并设裁剪的阈值是$\theta$。裁剪后的梯度</p>
<p>$$<br> \min\left(\frac{\theta}{|\boldsymbol{g}|}, 1\right)\boldsymbol{g}<br>$$</p>
<p>的$L_2$范数不超过$\theta$。</p>
<p>反向传播方式：时间反向传播【DPTT】</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, device)</span>:</span> <span class="comment"># theta 预设的阈值</span></span><br><span class="line">    norm = torch.tensor([<span class="number">0.0</span>], device=device)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad.data ** <span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().item()</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad.data *= (theta / norm)</span><br></pre></td></tr></table></figure>
<h3 id="定义预测函数"><a href="#定义预测函数" class="headerlink" title="定义预测函数"></a>定义预测函数</h3><blockquote>
<p>基于前缀 <code>prefix</code>（含有数个字符的字符串）来预测接下来的 <code>num_chars</code> 个字符。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn</span><span class="params">(prefix, num_chars, rnn, params, init_rnn_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                num_hiddens, vocab_size, device, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    <span class="comment"># 模型处理前缀prefix，隐藏状态H就记录了相关信息，模型在处理prefix 最后一个字符时，就已经预测出了下一个字符，所以可以作为之后的输入</span></span><br><span class="line">    state = init_rnn_state(<span class="number">1</span>, num_hiddens, device) <span class="comment">#　构造并且初始化状态</span></span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]   <span class="comment"># output记录prefix加上预测的num_chars个字符</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一时间步的输出作为当前时间步的输入</span></span><br><span class="line">        X = to_onehot(torch.tensor([[output[<span class="number">-1</span>]]], device=device), vocab_size)</span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        (Y, state) = rnn(X, state, params)</span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(Y[<span class="number">0</span>].argmax(dim=<span class="number">1</span>).item())　<span class="comment"># 最大的一列</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br></pre></td></tr></table></figure>
<h3 id="困惑度"><a href="#困惑度" class="headerlink" title="困惑度"></a>困惑度</h3><p>我们通常使用困惑度（perplexity）来评价语言模型的好坏。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，</p>
<blockquote>
<p>交叉熵损失函数</p>
<blockquote>
<p>损失函数详解：<a href="https://zhuanlan.zhihu.com/p/35709485" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35709485</a></p>
</blockquote>
</blockquote>
<ul>
<li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；</li>
<li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li>
<li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li>
</ul>
<p>显然，任何一个有效模型的困惑度必须小于类别个数。此处困惑度必须小于词典大小<code>vocab_size</code>。</p>
<h3 id="定义模型训练函数"><a href="#定义模型训练函数" class="headerlink" title="定义模型训练函数"></a>定义模型训练函数</h3><p>跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：</p>
<ol>
<li>使用困惑度评价模型。</li>
<li>在迭代模型参数前裁剪梯度。</li>
<li>对时序数据采用不同采样方法将导致隐藏状态初始化的不同。</li>
</ol>
<p>相邻采样，开始的时候初始化隐藏状态，容易引起开销过大，通常将隐藏状态分离</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn</span><span class="params">(rnn, get_params, init_rnn_state, num_hiddens,</span></span></span><br><span class="line"><span class="function"><span class="params">                          vocab_size, device, corpus_indices, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                          char_to_idx, is_random_iter, num_epochs, num_steps,</span></span></span><br><span class="line"><span class="function"><span class="params">                          lr, clipping_theta, batch_size, pred_period,</span></span></span><br><span class="line"><span class="function"><span class="params">                          pred_len, prefixes)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_random_iter:</span><br><span class="line">        data_iter_fn = d2l.data_iter_random <span class="comment"># 随机采样</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data_iter_fn = d2l.data_iter_consecutive <span class="comment">#相邻采样</span></span><br><span class="line">    params = get_params()</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_random_iter:  <span class="comment"># 如使用相邻采样，在epoch开始时初始化隐藏状态</span></span><br><span class="line">            state = init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> is_random_iter:  <span class="comment"># 如使用随机采样，在每个小批量更新前初始化隐藏状态</span></span><br><span class="line">                state = init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 否则需要使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">            <span class="comment"># inputs是num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">            inputs = to_onehot(X, vocab_size)</span><br><span class="line">            <span class="comment"># outputs有num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">            (outputs, state) = rnn(inputs, state, params) <span class="comment">#循环神经网路的前向计算</span></span><br><span class="line">            <span class="comment"># 拼接之后形状为(num_steps * batch_size, vocab_size)</span></span><br><span class="line">            outputs = torch.cat(outputs, dim=<span class="number">0</span>) <span class="comment"># 拼接</span></span><br><span class="line">            <span class="comment"># Y的形状是(batch_size, num_steps)，转置后再变成形状为</span></span><br><span class="line">            <span class="comment"># (num_steps * batch_size,)的向量，这样跟输出的行一一对应</span></span><br><span class="line">            y = torch.flatten(Y.T)</span><br><span class="line">            <span class="comment"># 使用交叉熵损失计算平均分类误差</span></span><br><span class="line">            l = loss(outputs, y.long())</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 梯度清0</span></span><br><span class="line">            <span class="keyword">if</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(params, clipping_theta, device)  <span class="comment"># 裁剪梯度</span></span><br><span class="line">            d2l.sgd(params, lr, <span class="number">1</span>)  <span class="comment"># 因为误差已经取过均值，梯度不用再做平均</span></span><br><span class="line">            l_sum += l.item() * y.shape[<span class="number">0</span>]</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_period == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,</span><br><span class="line">                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))</span><br></pre></td></tr></table></figure>
<h3 id="训练模型并创作歌词"><a href="#训练模型并创作歌词" class="headerlink" title="训练模型并创作歌词"></a>训练模型并创作歌词</h3><blockquote>
<ul>
<li>设置超参数</li>
<li>前缀：“分开”和“不分开”</li>
<li>歌词长度：50个字符（不考虑前缀长度）</li>
<li>周期：50</li>
<li>采样方式：随机采样 &amp;&amp; 相邻采样</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set super param</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line"><span class="comment"># set prefix and recurrent </span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"><span class="comment"># training by random sampling</span></span><br><span class="line">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, <span class="literal">True</span>, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                      prefixes)</span><br><span class="line"><span class="comment"># training by adjacent sampling</span></span><br><span class="line">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, <span class="literal">False</span>, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                      prefixes)</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="简化实现"><a href="#简化实现" class="headerlink" title="简化实现"></a>简化实现</h2><h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><blockquote>
<p>使用 Pytorch 中的 nn.RNN 构造神经网络</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个基于循环神经网络的语言模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_layer, vocab_size)</span>:</span> <span class="comment">#rnn_layer 是pytorch中的一个类</span></span><br><span class="line">        super(RNNModel, self).__init__()</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.hidden_size = rnn_layer.hidden_size * (<span class="number">2</span> <span class="keyword">if</span> rnn_layer.bidirectional <span class="keyword">else</span> <span class="number">1</span>) </span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.dense = nn.Linear(self.hidden_size, vocab_size) <span class="comment">#定义一个线性层作为输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">        <span class="comment"># inputs.shape: (batch_size, num_steps)</span></span><br><span class="line">        X = to_onehot(inputs, vocab_size)</span><br><span class="line">        X = torch.stack(X)  <span class="comment"># X.shape: (num_steps, batch_size, vocab_size)</span></span><br><span class="line">        hiddens, state = self.rnn(X, state)</span><br><span class="line">        hiddens = hiddens.view(<span class="number">-1</span>, hiddens.shape[<span class="number">-1</span>])  <span class="comment"># hiddens.shape: (num_steps * batch_size, hidden_size)</span></span><br><span class="line">        output = self.dense(hiddens)</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure>

<h3 id="预测函数"><a href="#预测函数" class="headerlink" title="预测函数"></a>预测函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn_pytorch</span><span class="params">(prefix, num_chars, model, vocab_size, device, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                      char_to_idx)</span>:</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]  <span class="comment"># output记录prefix加上预测的num_chars个字符</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        X = torch.tensor([output[<span class="number">-1</span>]], device=device).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        (Y, state) = model(X, state)  <span class="comment"># 前向计算不需要传入模型参数</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(Y.argmax(dim=<span class="number">1</span>).item())</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><blockquote>
<p>采用相邻采样</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># training function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn_pytorch</span><span class="params">(model, num_hiddens, vocab_size, device,</span></span></span><br><span class="line"><span class="function"><span class="params">                                corpus_indices, idx_to_char, char_to_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                num_epochs, num_steps, lr, clipping_theta,</span></span></span><br><span class="line"><span class="function"><span class="params">                                batch_size, pred_period, pred_len, prefixes)</span>:</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=lr) <span class="comment">#优化模型参数</span></span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) <span class="comment"># 相邻采样</span></span><br><span class="line">        state = <span class="literal">None</span> <span class="comment">#构造 并初始化</span></span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">if</span> isinstance (state, tuple): <span class="comment"># LSTM, state:(h, c)  </span></span><br><span class="line">                    state[<span class="number">0</span>].detach_()</span><br><span class="line">                    state[<span class="number">1</span>].detach_()</span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    state.detach_()</span><br><span class="line">            (output, state) = model(X, state) <span class="comment"># output.shape: (num_steps * batch_size, vocab_size)</span></span><br><span class="line">            y = torch.flatten(Y.T)</span><br><span class="line">            l = loss(output, y.long())</span><br><span class="line">            </span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(model.parameters(), clipping_theta, device)</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.item() * y.shape[<span class="number">0</span>]</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_period == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn_pytorch(</span><br><span class="line">                    prefix, pred_len, model, vocab_size, device, idx_to_char,</span><br><span class="line">                    char_to_idx))</span><br><span class="line">       </span><br><span class="line">num_epochs, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">32</span>, <span class="number">1e-3</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                            corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                            num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                            batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>梯度问题</tag>
      </tags>
  </entry>
  <entry>
    <title>词嵌入之 Word2Vec</title>
    <url>/2020/02/28/%E8%AF%8D%E5%B5%8C%E5%85%A5%E4%B9%8B%20Word2Vec/</url>
    <content><![CDATA[<h1 id="词嵌入基础"><a href="#词嵌入基础" class="headerlink" title="词嵌入基础"></a>词嵌入基础</h1><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">循环神经网络的从零开始实现</a>中使用 one-hot 向量表示单词，虽然它们构造起来很容易，但通常并不是一个好选择。</p>
<blockquote>
<p><strong>原因：</strong></p>
<blockquote>
<p>one-hot 词向量无法准确表达不同词之间的相似度，如我们常常使用的余弦相似度<br/><br>任意单词间的余弦相似度都为零。</p>
</blockquote>
</blockquote>
<p>Word2Vec 词嵌入工具的提出正是为了解决上面这个问题，它将每个词表示成一个定长的向量，并通过在语料库上的预训练使得这些向量能较好地表达不同词之间的相似和类比关系，以引入一定的语义信息。</p>
<p>基于两种概率模型的假设，介绍以下两种 Word2Vec 模型：</p>
<ol>
<li>Skip-Gram 跳字模型：假设背景词由中心词生成，即建模 $P(w_o\mid w_c)$，其中 $w_c$ 为中心词，$w_o$ 为任一背景词；</li>
</ol>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW1qc3E4NG85LnBuZw?x-oss-process=image/format,png" /></center>

<ol start="2">
<li>CBOW (continuous bag-of-words) 连续词袋模型：假设中心词由背景词生成，即建模 $P(w_c\mid \mathcal{W}_o)$，其中 $\mathcal{W}_o$ 为背景词的集合。</li>
</ol>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW1qdDRyMDJuLnBuZw?x-oss-process=image/format,png" /></center>

<p>在这里我们主要介绍 Skip-Gram 模型的实现，CBOW 实现与其类似，读者可之后自己尝试实现。后续的内容将大致从以下四个部分展开：</p>
<ol>
<li><a href="#ptb">PTB 数据集</a></li>
<li><a href="#skip-gram">Skip-Gram 跳字模型</a></li>
<li><a href="#nsa">负采样近似</a></li>
<li><a href="#train">训练模型</a></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br></pre></td></tr></table></figure>
<h2 id="PTB-数据集"><a href="#PTB-数据集" class="headerlink" title="PTB 数据集"></a><div id="ptb">PTB 数据集</div></h2><p>简单来说，Word2Vec 能从语料中学到如何将离散的词映射为连续空间中的向量，并保留其语义上的相似关系。</p>
<p>为了训练 Word2Vec 模型，我们就需要准备一个自然语言语料库，模型将从中学习各个单词间的关系，这里我们使用经典的 PTB 语料库进行训练。</p>
<h3 id="载入数据集"><a href="#载入数据集" class="headerlink" title="载入数据集"></a>载入数据集</h3><p>数据集训练文件 <code>ptb.train.txt</code> 示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">aer banknote berlitz calloway centrust cluett fromstein gitano guterman ...</span><br><span class="line">pierre  N years old will join the board as a nonexecutive director nov. N </span><br><span class="line">mr.  is chairman of  n.v. the dutch publishing group </span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'path to ptb.train.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    lines = f.readlines() <span class="comment"># 该数据集中句子以换行符为分割</span></span><br><span class="line">    raw_dataset = [st.split() <span class="keyword">for</span> st <span class="keyword">in</span> lines] <span class="comment"># st是sentence的缩写，单词以空格为分割</span></span><br><span class="line">print(<span class="string">'# sentences: %d'</span> % len(raw_dataset))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于数据集的前3个句子，打印每个句子的词数和前5个词</span></span><br><span class="line"><span class="comment"># 句尾符为 '' ，生僻词全用 '' 表示，数字则被替换成了 'N'</span></span><br><span class="line"><span class="keyword">for</span> st <span class="keyword">in</span> raw_dataset[:<span class="number">3</span>]:</span><br><span class="line">    print(<span class="string">'# tokens:'</span>, len(st), st[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line"><span class="comment"># sentences: 42068</span></span><br><span class="line"><span class="comment"># tokens: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']</span></span><br><span class="line"><span class="comment"># tokens: 15 ['pierre', '&lt;unk&gt;', 'N', 'years', 'old']</span></span><br><span class="line"><span class="comment"># tokens: 11 ['mr.', '&lt;unk&gt;', 'is', 'chairman', 'of']</span></span><br></pre></td></tr></table></figure>
<h3 id="建立词语索引"><a href="#建立词语索引" class="headerlink" title="建立词语索引"></a>建立词语索引</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">counter = collections.Counter([tk <span class="keyword">for</span> st <span class="keyword">in</span> raw_dataset <span class="keyword">for</span> tk <span class="keyword">in</span> st]) <span class="comment"># tk是token的缩写</span></span><br><span class="line">counter = dict(filter(<span class="keyword">lambda</span> x: x[<span class="number">1</span>] &gt;= <span class="number">5</span>, counter.items())) <span class="comment"># 只保留在数据集中至少出现5次的词</span></span><br><span class="line"></span><br><span class="line">idx_to_token = [tk <span class="keyword">for</span> tk, _ <span class="keyword">in</span> counter.items()]</span><br><span class="line">token_to_idx = &#123;tk: idx <span class="keyword">for</span> idx, tk <span class="keyword">in</span> enumerate(idx_to_token)&#125;</span><br><span class="line">dataset = [[token_to_idx[tk] <span class="keyword">for</span> tk <span class="keyword">in</span> st <span class="keyword">if</span> tk <span class="keyword">in</span> token_to_idx]</span><br><span class="line">           <span class="keyword">for</span> st <span class="keyword">in</span> raw_dataset] <span class="comment"># raw_dataset中的单词在这一步被转换为对应的idx</span></span><br><span class="line">num_tokens = sum([len(st) <span class="keyword">for</span> st <span class="keyword">in</span> dataset])</span><br><span class="line"><span class="string">'# tokens: %d'</span> % num_tokens</span><br></pre></td></tr></table></figure>
<h3 id="二次采样"><a href="#二次采样" class="headerlink" title="二次采样"></a>二次采样</h3><p>文本数据中一般会出现一些高频词，如英文中的“the”“a”和“in”。通常来说</p>
<p>在一个背景窗口中，一个词（如“chip”）和较低频词（如“microprocessor”）同时出现比和较高频词（如“the”）同时出现对训练词嵌入模型更有益。</p>
<p>因此，训练词嵌入模型时可以对词进行二次采样。 具体来说，数据集中每个被索引词 $w_i$ 将有一定概率被丢弃，该丢弃概率为</p>
<p>$$<br>P(w_i)=\max(1-\sqrt{\frac{t}{f(w_i)}},0)<br>$$</p>
<p>其中  $f(w_i)$  是数据集中词 $w_i$ 的个数与总词数之比，常数 $t$ 是一个超参数（实验中设为 $10^{−4}$）。</p>
<p>可见，只有当 $f(w_i)&gt;t$ 时，我们才有可能在二次采样中丢弃词 $w_i$，并且越高频的词被丢弃的概率越大。具体的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discard</span><span class="params">(idx)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        idx: 单词的下标</span></span><br><span class="line"><span class="string">    @return: True/False 表示是否丢弃该单词</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> random.uniform(<span class="number">0</span>, <span class="number">1</span>) &lt; <span class="number">1</span> - math.sqrt(</span><br><span class="line">        <span class="number">1e-4</span> / counter[idx_to_token[idx]] * num_tokens)</span><br><span class="line"></span><br><span class="line">subsampled_dataset = [[tk <span class="keyword">for</span> tk <span class="keyword">in</span> st <span class="keyword">if</span> <span class="keyword">not</span> discard(tk)] <span class="keyword">for</span> st <span class="keyword">in</span> dataset]</span><br><span class="line">print(<span class="string">'# tokens: %d'</span> % sum([len(st) <span class="keyword">for</span> st <span class="keyword">in</span> subsampled_dataset]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compare_counts</span><span class="params">(token)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'# %s: before=%d, after=%d'</span> % (token, sum(</span><br><span class="line">        [st.count(token_to_idx[token]) <span class="keyword">for</span> st <span class="keyword">in</span> dataset]), sum(</span><br><span class="line">        [st.count(token_to_idx[token]) <span class="keyword">for</span> st <span class="keyword">in</span> subsampled_dataset]))</span><br><span class="line"></span><br><span class="line">print(compare_counts(<span class="string">'the'</span>))</span><br><span class="line">print(compare_counts(<span class="string">'join'</span>))</span><br></pre></td></tr></table></figure>
<h3 id="提取中心词和背景词"><a href="#提取中心词和背景词" class="headerlink" title="提取中心词和背景词"></a>提取中心词和背景词</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_centers_and_contexts</span><span class="params">(dataset, max_window_size)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        dataset: 数据集为句子的集合，每个句子则为单词的集合，此时单词已经被转换为相应数字下标</span></span><br><span class="line"><span class="string">        max_window_size: 背景词的词窗大小的最大值</span></span><br><span class="line"><span class="string">    @return:</span></span><br><span class="line"><span class="string">        centers: 中心词的集合</span></span><br><span class="line"><span class="string">        contexts: 背景词窗的集合，与中心词对应，每个背景词窗则为背景词的集合</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    centers, contexts = [], []</span><br><span class="line">    <span class="keyword">for</span> st <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="keyword">if</span> len(st) &lt; <span class="number">2</span>:  <span class="comment"># 每个句子至少要有2个词才可能组成一对“中心词-背景词”</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        centers += st</span><br><span class="line">        <span class="keyword">for</span> center_i <span class="keyword">in</span> range(len(st)):</span><br><span class="line">            window_size = random.randint(<span class="number">1</span>, max_window_size) <span class="comment"># 随机选取背景词窗大小</span></span><br><span class="line">            indices = list(range(max(<span class="number">0</span>, center_i - window_size),</span><br><span class="line">                                 min(len(st), center_i + <span class="number">1</span> + window_size)))</span><br><span class="line">            indices.remove(center_i)  <span class="comment"># 将中心词排除在背景词之外</span></span><br><span class="line">            contexts.append([st[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> indices])</span><br><span class="line">    <span class="keyword">return</span> centers, contexts</span><br><span class="line"></span><br><span class="line">all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">tiny_dataset = [list(range(<span class="number">7</span>)), list(range(<span class="number">7</span>, <span class="number">10</span>))]</span><br><span class="line">print(<span class="string">'dataset'</span>, tiny_dataset)</span><br><span class="line"><span class="keyword">for</span> center, context <span class="keyword">in</span> zip(*get_centers_and_contexts(tiny_dataset, <span class="number">2</span>)):</span><br><span class="line">    print(<span class="string">'center'</span>, center, <span class="string">'has contexts'</span>, context)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><em>注：数据批量读取的实现需要依赖负采样近似的实现，故放于负采样近似部分进行讲解。</em></p>
</blockquote>
<h2 id="Skip-Gram-跳字模型"><a href="#Skip-Gram-跳字模型" class="headerlink" title="Skip-Gram 跳字模型"></a><div id="skip-gram">Skip-Gram 跳字模型</div></h2><p>在跳字模型中，每个词被表示成两个 $d$ 维向量，用来计算条件概率。</p>
<p>假设这个词在词典中索引为 $i$ ，当它为中心词时向量表示为 $\boldsymbol{v}_i\in\mathbb{R}^d$，而为背景词时向量表示为 $\boldsymbol{u}_i\in\mathbb{R}^d$ 。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_o$ 在词典中索引为 $o$</p>
<p>我们假设给定中心词生成背景词的条件概率满足下式：</p>
<p>$$<br>P(w_o\mid w_c)=\frac{\exp(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{\sum_{i\in\mathcal{V}}\exp(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}<br>$$</p>
<h3 id="PyTorch-预置的-Embedding-层"><a href="#PyTorch-预置的-Embedding-层" class="headerlink" title="PyTorch 预置的 Embedding 层"></a>PyTorch 预置的 Embedding 层</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed = nn.Embedding(num_embeddings=<span class="number">10</span>, embedding_dim=<span class="number">4</span>)</span><br><span class="line">print(embed.weight)</span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.long)</span><br><span class="line">print(embed(x))</span><br></pre></td></tr></table></figure>
<h3 id="PyTorch-预置的批量乘法"><a href="#PyTorch-预置的批量乘法" class="headerlink" title="PyTorch 预置的批量乘法"></a>PyTorch 预置的批量乘法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">print(torch.bmm(X, Y).shape)</span><br></pre></td></tr></table></figure>
<h3 id="Skip-Gram-模型的前向计算"><a href="#Skip-Gram-模型的前向计算" class="headerlink" title="Skip-Gram 模型的前向计算"></a>Skip-Gram 模型的前向计算</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">skip_gram</span><span class="params">(center, contexts_and_negatives, embed_v, embed_u)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        center: 中心词下标，形状为 (n, 1) 的整数张量</span></span><br><span class="line"><span class="string">        contexts_and_negatives: 背景词和噪音词下标，形状为 (n, m) 的整数张量</span></span><br><span class="line"><span class="string">        embed_v: 中心词的 embedding 层</span></span><br><span class="line"><span class="string">        embed_u: 背景词的 embedding 层</span></span><br><span class="line"><span class="string">    @return:</span></span><br><span class="line"><span class="string">        pred: 中心词与背景词（或噪音词）的内积，之后可用于计算概率 p(w_o|w_c)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    v = embed_v(center) <span class="comment"># shape of (n, 1, d)</span></span><br><span class="line">    u = embed_u(contexts_and_negatives) <span class="comment"># shape of (n, m, d)</span></span><br><span class="line">    pred = torch.bmm(v, u.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)) <span class="comment"># bmm((n, 1, d), (n, d, m)) =&gt; shape of (n, 1, m)</span></span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure>
<h2 id="负采样近似"><a href="#负采样近似" class="headerlink" title="负采样近似"></a><div id="nsa">负采样近似</div></h2><p><strong>问题：</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;由于 softmax 运算考虑了背景词可能是词典 $\mathcal{V}$ 中的任一词，对于含几十万或上百万词的较大词典，就可能导致计算的开销过大。</p>
<p>我们将<strong>以 skip-gram 模型为例</strong>，介绍负采样 (negative sampling) 的实现来尝试解决这个问题。</p>
<p>负采样方法用以下公式来近似条件概率 $P(w_o\mid w_c)=\frac{\exp(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{\sum_{i\in\mathcal{V}}\exp(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}$：</p>
<p>$$<br>P(w_o\mid w_c)=P(D=1\mid w_c,w_o)\prod_{k=1,w_k\sim P(w)}^K P(D=0\mid w_c,w_k)<br>$$</p>
<p>其中 $P(D=1\mid w_c,w_o)=\sigma(\boldsymbol{u}_o^\top\boldsymbol{v}_c)$，$\sigma(\cdot)$ 为 sigmoid 函数。对于一对中心词和背景词，我们从词典中随机采样 $K$ 个噪声词（实验中设 $K=5$）。</p>
<hr>
<p>根据 Word2Vec 论文的建议，噪声词采样概率 $P(w)$ 设为 $w$ 词频与总词频之比的 $0.75$ 次方。</p>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_negatives</span><span class="params">(all_contexts, sampling_weights, K)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        all_contexts: [[w_o1, w_o2, ...], [...], ... ]</span></span><br><span class="line"><span class="string">        sampling_weights: 每个单词的噪声词采样概率</span></span><br><span class="line"><span class="string">        K: 随机采样个数</span></span><br><span class="line"><span class="string">    @return:</span></span><br><span class="line"><span class="string">        all_negatives: [[w_n1, w_n2, ...], [...], ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    all_negatives, neg_candidates, i = [], [], <span class="number">0</span></span><br><span class="line">    population = list(range(len(sampling_weights)))</span><br><span class="line">    <span class="keyword">for</span> contexts <span class="keyword">in</span> all_contexts:</span><br><span class="line">        negatives = []</span><br><span class="line">        <span class="keyword">while</span> len(negatives) &lt; len(contexts) * K:</span><br><span class="line">            <span class="keyword">if</span> i == len(neg_candidates):</span><br><span class="line">                <span class="comment"># 根据每个词的权重（sampling_weights）随机生成k个词的索引作为噪声词。</span></span><br><span class="line">                <span class="comment"># 为了高效计算，可以将k设得稍大一点</span></span><br><span class="line">                i, neg_candidates = <span class="number">0</span>, random.choices(</span><br><span class="line">                    population, sampling_weights, k=int(<span class="number">1e5</span>))</span><br><span class="line">            neg, i = neg_candidates[i], i + <span class="number">1</span></span><br><span class="line">            <span class="comment"># 噪声词不能是背景词</span></span><br><span class="line">            <span class="keyword">if</span> neg <span class="keyword">not</span> <span class="keyword">in</span> set(contexts):</span><br><span class="line">                negatives.append(neg)</span><br><span class="line">        all_negatives.append(negatives)</span><br><span class="line">    <span class="keyword">return</span> all_negatives</span><br><span class="line"></span><br><span class="line">sampling_weights = [counter[w]**<span class="number">0.75</span> <span class="keyword">for</span> w <span class="keyword">in</span> idx_to_token]</span><br><span class="line">all_negatives = get_negatives(all_contexts, sampling_weights, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>*注：除负采样方法外，还有层序 softmax (hiererarchical softmax) 方法也可以用来解决计算量过大的问题</p>
</blockquote>
<h3 id="批量读取数据"><a href="#批量读取数据" class="headerlink" title="批量读取数据"></a>批量读取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, centers, contexts, negatives)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> len(centers) == len(contexts) == len(negatives)</span><br><span class="line">        self.centers = centers</span><br><span class="line">        self.contexts = contexts</span><br><span class="line">        self.negatives = negatives</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (self.centers[index], self.contexts[index], self.negatives[index])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.centers)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    用作DataLoader的参数collate_fn</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        data: 长为batch_size的列表，列表中的每个元素都是__getitem__得到的结果</span></span><br><span class="line"><span class="string">    @outputs:</span></span><br><span class="line"><span class="string">        batch: 批量化后得到 (centers, contexts_negatives, masks, labels) 元组</span></span><br><span class="line"><span class="string">            centers: 中心词下标，形状为 (n, 1) 的整数张量</span></span><br><span class="line"><span class="string">            contexts_negatives: 背景词和噪声词的下标，形状为 (n, m) 的整数张量</span></span><br><span class="line"><span class="string">            masks: 与补齐相对应的掩码，形状为 (n, m) 的0/1整数张量</span></span><br><span class="line"><span class="string">            labels: 指示中心词的标签，形状为 (n, m) 的0/1整数张量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    max_len = max(len(c) + len(n) <span class="keyword">for</span> _, c, n <span class="keyword">in</span> data)</span><br><span class="line">    centers, contexts_negatives, masks, labels = [], [], [], []</span><br><span class="line">    <span class="keyword">for</span> center, context, negative <span class="keyword">in</span> data:</span><br><span class="line">        cur_len = len(context) + len(negative)</span><br><span class="line">        centers += [center]</span><br><span class="line">        contexts_negatives += [context + negative + [<span class="number">0</span>] * (max_len - cur_len)]</span><br><span class="line">        masks += [[<span class="number">1</span>] * cur_len + [<span class="number">0</span>] * (max_len - cur_len)] <span class="comment"># 使用掩码变量mask来避免填充项对损失函数计算的影响</span></span><br><span class="line">        labels += [[<span class="number">1</span>] * len(context) + [<span class="number">0</span>] * (max_len - len(context))]</span><br><span class="line">        batch = (torch.tensor(centers).view(<span class="number">-1</span>, <span class="number">1</span>), torch.tensor(contexts_negatives),</span><br><span class="line">            torch.tensor(masks), torch.tensor(labels))</span><br><span class="line">    <span class="keyword">return</span> batch</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">512</span></span><br><span class="line">num_workers = <span class="number">0</span> <span class="keyword">if</span> sys.platform.startswith(<span class="string">'win32'</span>) <span class="keyword">else</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">dataset = MyDataset(all_centers, all_contexts, all_negatives)</span><br><span class="line">data_iter = Data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            collate_fn=batchify, </span><br><span class="line">                            num_workers=num_workers)</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">    <span class="keyword">for</span> name, data <span class="keyword">in</span> zip([<span class="string">'centers'</span>, <span class="string">'contexts_negatives'</span>, <span class="string">'masks'</span>,</span><br><span class="line">                           <span class="string">'labels'</span>], batch):</span><br><span class="line">        print(name, <span class="string">'shape:'</span>, data.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a><div id="train">训练模型</div></h2><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>应用负采样方法后，我们可利用最大似然估计的对数等价形式将损失函数定义为如下</p>
<p>$$<br>\sum_{t=1}^T\sum_{-m\le j\le m,j\ne 0} [-\log P(D=1\mid w^{(t)},w^{(t+j)})-\sum_{k=1,w_k\sim P(w)^K}\log P(D=0\mid w^{(t)},w_k)]<br>$$</p>
<p>根据这个损失函数的定义，我们可以直接使用<strong>二元交叉熵损失函数</strong>进行计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SigmoidBinaryCrossEntropyLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(SigmoidBinaryCrossEntropyLoss, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets, mask=None)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        @params:</span></span><br><span class="line"><span class="string">            inputs: 经过sigmoid层后为预测D=1的概率</span></span><br><span class="line"><span class="string">            targets: 0/1向量，1代表背景词，0代表噪音词</span></span><br><span class="line"><span class="string">        @return:</span></span><br><span class="line"><span class="string">            res: 平均到每个label的loss</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        inputs, targets, mask = inputs.float(), targets.float(), mask.float()</span><br><span class="line">        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=<span class="string">"none"</span>, weight=mask)</span><br><span class="line">        res = res.sum(dim=<span class="number">1</span>) / mask.float().sum(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">loss = SigmoidBinaryCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">pred = torch.tensor([[<span class="number">1.5</span>, <span class="number">0.3</span>, <span class="number">-1</span>, <span class="number">2</span>], [<span class="number">1.1</span>, <span class="number">-0.6</span>, <span class="number">2.2</span>, <span class="number">0.4</span>]])</span><br><span class="line">label = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]]) <span class="comment"># 标签变量label中的1和0分别代表背景词和噪声词</span></span><br><span class="line">mask = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]])  <span class="comment"># 掩码变量</span></span><br><span class="line">print(loss(pred, label, mask))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmd</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> - math.log(<span class="number">1</span> / (<span class="number">1</span> + math.exp(-x)))</span><br><span class="line">print(<span class="string">'%.4f'</span> % ((sigmd(<span class="number">1.5</span>) + sigmd(<span class="number">-0.3</span>) + sigmd(<span class="number">1</span>) + sigmd(<span class="number">-2</span>)) / <span class="number">4</span>)) <span class="comment"># 注意1-sigmoid(x) = sigmoid(-x)</span></span><br><span class="line">print(<span class="string">'%.4f'</span> % ((sigmd(<span class="number">1.1</span>) + sigmd(<span class="number">-0.6</span>) + sigmd(<span class="number">-2.2</span>)) / <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<h3 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed_size = <span class="number">100</span></span><br><span class="line">net = nn.Sequential(nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),</span><br><span class="line">                    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size))</span><br></pre></td></tr></table></figure>
<h3 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(net, lr, num_epochs)</span>:</span></span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">    print(<span class="string">"train on"</span>, device)</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        start, l_sum, n = time.time(), <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            center, context_negative, mask, label = [d.to(device) <span class="keyword">for</span> d <span class="keyword">in</span> batch]</span><br><span class="line">            </span><br><span class="line">            pred = skip_gram(center, context_negative, net[<span class="number">0</span>], net[<span class="number">1</span>])</span><br><span class="line">            </span><br><span class="line">            l = loss(pred.view(label.shape), label, mask).mean() <span class="comment"># 一个batch的平均loss</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.cpu().item()</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">        print(<span class="string">'epoch %d, loss %.2f, time %.2fs'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, l_sum / n, time.time() - start))</span><br><span class="line"></span><br><span class="line">train(net, <span class="number">0.01</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>*注：最好在ＧＰＵ上运行</p>
</blockquote>
<h3 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_similar_tokens</span><span class="params">(query_token, k, embed)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        query_token: 给定的词语</span></span><br><span class="line"><span class="string">        k: 近义词的个数</span></span><br><span class="line"><span class="string">        embed: 预训练词向量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    W = embed.weight.data</span><br><span class="line">    x = W[token_to_idx[query_token]]</span><br><span class="line">    <span class="comment"># 添加的1e-9是为了数值稳定性</span></span><br><span class="line">    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=<span class="number">1</span>) * torch.sum(x * x) + <span class="number">1e-9</span>).sqrt()</span><br><span class="line">    _, topk = torch.topk(cos, k=k+<span class="number">1</span>)</span><br><span class="line">    topk = topk.cpu().numpy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> topk[<span class="number">1</span>:]:  <span class="comment"># 除去输入词</span></span><br><span class="line">        print(<span class="string">'cosine sim=%.3f: %s'</span> % (cos[i], (idx_to_token[i])))</span><br><span class="line">        </span><br><span class="line">get_similar_tokens(<span class="string">'chip'</span>, <span class="number">3</span>, net[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>负近似采样</tag>
      </tags>
  </entry>
  <entry>
    <title>Advanced Optimization</title>
    <url>/2020/02/28/Advanced-Optimization/</url>
    <content><![CDATA[<p>基于<a href="https://blog.csdn.net/RokoBasilisk/article/details/104413638" target="_blank" rel="noopener">凸优化和梯度下降优化算法</a>，进一步展开阐述 :</p>
<ol>
<li><a href="#momentum">Momentum;</a></li>
<li><a href="#adagrad">AdaGrad;</a></li>
<li><a href="#rmsprop">RMSProp;</a></li>
<li><a href="#adadelta">AdaDelta;</a></li>
<li><a href="#adam">Adam</a></li>
</ol>
<h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a><div id="momentum">Momentum</div></h1><p>在<a href="https://blog.csdn.net/RokoBasilisk/article/details/104413638" target="_blank" rel="noopener">随机梯度下降</a>中，我们提到，目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。对于noisy gradient,我们需要谨慎的选取学习率和batch size, 来控制梯度方差和收敛的结果。</p>
<p>$$<br>\mathbf{g}<em>t = \partial</em>{\mathbf{w}} \frac{1}{|\mathcal{B}<em>t|} \sum</em>{i \in \mathcal{B}<em>t} f(\mathbf{x}</em>{i}, \mathbf{w}<em>{t-1}) = \frac{1}{|\mathcal{B}_t|} \sum</em>{i \in \mathcal{B}<em>t} \mathbf{g}</em>{i, t-1}.<br>$$</p>
<h2 id="An-ill-conditioned-Problem"><a href="#An-ill-conditioned-Problem" class="headerlink" title="An ill-conditioned Problem"></a>An ill-conditioned Problem</h2><p>Condition Number of Hessian Matrix:</p>
<p>$$<br> cond_{H} = \frac{\lambda_{max}}{\lambda_{min}}<br>$$</p>
<p>where $\lambda_{max}, \lambda_{min}$ is the maximum amd minimum eignvalue of Hessian matrix.</p>
<p>让我们考虑一个输入和输出分别为二维向量$\boldsymbol{x} = [x_1, x_2]^\top$和标量的目标函数:</p>
<p>$$<br> f(\boldsymbol{x})=0.1x_1^2+2x_2^2<br>$$</p>
<p>$$<br> cond_{H} = \frac{4}{0.2} = 20 \quad \rightarrow \quad \text{ill-conditioned}<br>$$</p>
<h2 id="Supp-Preconditioning"><a href="#Supp-Preconditioning" class="headerlink" title="Supp: Preconditioning"></a>Supp: Preconditioning</h2><center><img src="https://img-blog.csdnimg.cn/2020022114561129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>在二阶优化中，我们使用Hessian matrix的逆矩阵(或者pseudo inverse)来左乘梯度向量 $i.e. \Delta_{x} = H^{-1}\mathbf{g}$，这样的做法称为precondition，相当于将 $H$ 映射为一个单位矩阵，拥有分布均匀的Spectrum，也即我们去优化的等价标函数的Hessian matrix为良好的identity matrix。</p>
<p>将$x_1^2$系数从$1$减小到了$0.1$。下面实现基于这个目标函数的梯度下降，并展示使用学习率为$0.4$时自变量的迭代轨迹。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.4</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * <span class="number">0.2</span> * x1, x2 - eta * <span class="number">4</span> * x2, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223165505355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>可以看到，同一位置上，目标函数在竖直方向（$x_2$轴方向）比在水平方向（$x_1$轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。</p>
<p>接下来将学习率 $eta$ 加大+<font size=6>+</font><font size=10>+</font></p>
<p>此时自变量在竖直方向不断越过最优解并逐渐发散。</p>
<h3 id="Solution-to-ill-condition"><a href="#Solution-to-ill-condition" class="headerlink" title="Solution to ill-condition"></a>Solution to ill-condition</h3><ul>
<li><strong>Preconditioning gradient vector</strong>: applied in Adam, RMSProp, AdaGrad, Adelta, KFC, Natural gradient and other secord-order optimization algorithms.</li>
<li><strong>Averaging history gradient</strong>: like momentum, which allows larger learning rates to accelerate convergence; applied in Adam, RMSProp, SGD momentum.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.6</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223165906709.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Momentum-Algorithm"><a href="#Momentum-Algorithm" class="headerlink" title="Momentum Algorithm"></a>Momentum Algorithm</h2><p>动量法的提出是为了解决梯度下降的上述问题。设时间步 $t$ 的自变量为 $\boldsymbol{x}_t$，学习率为 $\eta_t$。<br>在时间步 $t=0$，动量法创建速度变量 $\boldsymbol{m}_0$，并将其元素初始化成 0。在时间步 $t&gt;0$，动量法对每次迭代的步骤做如下修改：</p>
<p>$$<br>\begin{aligned}<br>\boldsymbol{m}_t &amp;\leftarrow \beta \boldsymbol{m}_{t-1} + \eta_t \boldsymbol{g}_t, \<br>\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{m}_t,<br>\end{aligned}<br>$$</p>
<p>Another version:</p>
<p>$$<br>\begin{aligned}<br>\boldsymbol{m}_t &amp;\leftarrow \beta \boldsymbol{m}_{t-1} + (1-\beta) \boldsymbol{g}_t, \<br>\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}_{t-1} - \alpha_t \boldsymbol{m}_t,<br>\end{aligned}<br>$$</p>
<p>$$<br>\alpha_t = \frac{\eta_t}{1-\beta}<br>$$</p>
<p>其中，动量超参数 $\beta$满足 $0 \leq \beta &lt; 1$。当 $\beta=0$ 时，动量法等价于小批量随机梯度下降。</p>
<p>利用梯度下降在使用动量法后的迭代轨迹更加方便理解数学推导</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">momentum_2d</span><span class="params">(x1, x2, v1, v2)</span>:</span></span><br><span class="line">    v1 = beta * v1 + eta * <span class="number">0.2</span> * x1</span><br><span class="line">    v2 = beta * v2 + eta * <span class="number">4</span> * x2</span><br><span class="line">    <span class="keyword">return</span> x1 - v1, x2 - v2, v1, v2</span><br><span class="line"></span><br><span class="line">eta, beta = <span class="number">0.4</span>, <span class="number">0.5</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223170104740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>可以看到使用较小的学习率 $\eta=0.4$ 和动量超参数 $\beta=0.5$ 时，动量法在竖直方向上的移动更加平滑，且在水平方向上更快逼近最优解。下面使用较大的学习率 $\eta=0.6$，此时自变量也不再发散。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.6</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223170204913.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="Exponential-Moving-Average"><a href="#Exponential-Moving-Average" class="headerlink" title="Exponential Moving Average"></a>Exponential Moving Average</h3><blockquote>
<p>从数学上理解动量法</p>
</blockquote>
<hr>
<p>指数加权移动平均（exponential moving average）</p>
<p>给定超参数 $0 \leq \beta &lt; 1$，当前时间步 $t$ 的变量 $y_t$ 是上一时间步 $t-1$ 的变量 $y_{t-1}$ 和当前时间步另一变量 $x_t$ 的线性组合：</p>
<p>$$<br>y_t = \beta y_{t-1} + (1-\beta) x_t.<br>$$</p>
<p>我们可以对 $y_t$ 展开：</p>
<p>$$<br>\begin{aligned}<br>y_t  &amp;= (1-\beta) x_t + \beta y_{t-1}\<br>         &amp;= (1-\beta)x_t + (1-\beta) \cdot \beta x_{t-1} + \beta^2y_{t-2}\<br>         &amp;= (1-\beta)x_t + (1-\beta) \cdot \beta x_{t-1} + (1-\beta) \cdot \beta^2x_{t-2} + \beta^3y_{t-3}\<br>         &amp;= (1-\beta) \sum_{i=0}^{t} \beta^{i}x_{t-i}<br>\end{aligned}<br>$$</p>
<p>$$<br>(1-\beta)\sum_{i=0}^{t} \beta^{i} = \frac{1-\beta^{t}}{1-\beta} (1-\beta) = (1-\beta^{t})<br>$$</p>
<h3 id="Supp-Approximate"><a href="#Supp-Approximate" class="headerlink" title="Supp Approximate"></a>Supp Approximate</h3><p>Average of $\frac{1}{1-\beta}$ Steps</p>
<p>令 $n = 1/(1-\beta)$，那么 $\left(1-1/n\right)^n = \beta^{1/(1-\beta)}$。因为</p>
<p>$$<br> \lim_{n \rightarrow \infty}  \left(1-\frac{1}{n}\right)^n = \exp(-1) \approx 0.3679,<br>$$</p>
<p>所以当 $\beta \rightarrow 1$时，$\beta^{1/(1-\beta)}=\exp(-1)$，如 $0.95^{20} \approx \exp(-1)$。如果把 $\exp(-1)$ 当作一个比较小的数，我们可以在近似中忽略所有含 $\beta^{1/(1-\beta)}$ 和比 $\beta^{1/(1-\beta)}$ 更高阶的系数的项。例如，当 $\beta=0.95$ 时，</p>
<p>$$<br>y_t \approx 0.05 \sum_{i=0}^{19} 0.95^i x_{t-i}.<br>$$</p>
<p>因此，在实际中，我们常常将 $y_t$ 看作是对最近 $1/(1-\beta)$ 个时间步的 $x_t$ 值的加权平均。例如，当 $\gamma = 0.95$ 时，$y_t$ 可以被看作对最近20个时间步的 $x_t$ 值的加权平均；当 $\beta = 0.9$ 时，$y_t$ 可以看作是对最近10个时间步的 $x_t$ 值的加权平均。而且，离当前时间步 $t$ 越近的 $x_t$ 值获得的权重越大（越接近1）。</p>
<hr>
<h3 id="由指数加权移动平均理解动量法"><a href="#由指数加权移动平均理解动量法" class="headerlink" title="由指数加权移动平均理解动量法"></a>由指数加权移动平均理解动量法</h3><p>现在，我们对动量法的速度变量做变形：</p>
<p>$$<br>\boldsymbol{m}_t \leftarrow \beta \boldsymbol{m}_{t-1} + (1 - \beta) \left(\frac{\eta_t}{1 - \beta} \boldsymbol{g}_t\right).<br>$$</p>
<p>Another version:</p>
<p>$$<br>\boldsymbol{m}_t \leftarrow \beta \boldsymbol{m}_{t-1} + (1 - \beta) \boldsymbol{g}_t.<br>$$</p>
<p>$$<br>\begin{aligned}<br>\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}_{t-1} - \alpha_t \boldsymbol{m}_t,<br>\end{aligned}<br>$$</p>
<p>$$<br>\alpha_t = \frac{\eta_t}{1-\beta}<br>$$</p>
<p>由指数加权移动平均的形式可得，速度变量 $\boldsymbol{v}_t$ 实际上对序列 $\{\eta_{t-i}\boldsymbol{g}_{t-i} /(1-\beta):i=0,\ldots,1/(1-\beta)-1\}$ 做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，动量法在每个时间步的自变量更新量近似于将前者对应的最近 $1/(1-\beta)$ 个时间步的更新量做了指数加权移动平均后再除以 $1-\beta$。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。在本节之前示例的优化问题中，所有梯度在水平方向上为正（向右），而在竖直方向上时正（向上）时负（向下）。这样，我们就可以使用较大的学习率，从而使自变量向最优解更快移动。</p>
<h2 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h2><p>相对于小批量随机梯度下降，动量法需要对每一个自变量维护一个同它一样形状的速度变量，且超参数里多了动量超参数。实现中，我们将速度变量用更广义的状态变量<code>states</code>表示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span>  </span><br><span class="line">    data = np.genfromtxt(<span class="string">'/home/kesci/input/airfoil4755/airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), \</span><br><span class="line">        torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_momentum_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32)</span><br><span class="line">    v_b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> p, v <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v.data = hyperparams[<span class="string">'momentum'</span>] * v.data + hyperparams[<span class="string">'lr'</span>] * p.grad.data</span><br><span class="line">        p.data -= v.data</span><br></pre></td></tr></table></figure>
<blockquote>
<p>令 momentum = 0.5</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(sgd_momentum, init_momentum_states(),</span><br><span class="line">              &#123;<span class="string">'lr'</span>: <span class="number">0.02</span>, <span class="string">'momentum'</span>: <span class="number">0.5</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223170543454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<p>令 momentum = 0.9</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(sgd_momentum, init_momentum_states(),</span><br><span class="line">              &#123;<span class="string">'lr'</span>: <span class="number">0.02</span>, <span class="string">'momentum'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223170642172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>可见目标函数值在后期迭代过程中的变化不够平滑。直觉上，10倍小批量梯度比2倍小批量梯度大了5倍，我们可以试着将学习率减小到原来的1/5。此时目标函数值在下降了一段时间后变化更加平滑。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(sgd_momentum, init_momentum_states(),</span><br><span class="line">              &#123;<span class="string">'lr'</span>: <span class="number">0.004</span>, <span class="string">'momentum'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223170904126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class"><a href="#Pytorch-Class" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><blockquote>
<p>在Pytorch中，torch.optim.SGD 已实现了Momentum</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.SGD, &#123;<span class="string">'lr'</span>: <span class="number">0.004</span>, <span class="string">'momentum'</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">                    features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/202002231714533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h1 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a><div id="adagrad">AdaGrad</div></h1><p>在之前介绍过的优化算法中，目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。举个例子，假设目标函数为$f$，自变量为一个二维向量$[x_1, x_2]^\top$，该向量中每一个元素在迭代时都使用相同的学习率。例如，在学习率为$\eta$的梯度下降中，元素$x_1$和$x_2$都使用相同的学习率$\eta$来自我迭代：</p>
<p>$$<br>x_1 \leftarrow x_1 - \eta \frac{\partial{f}}{\partial{x_1}}, \quad<br>x_2 \leftarrow x_2 - \eta \frac{\partial{f}}{\partial{x_2}}.<br>$$</p>
<p><a href="#momentum">动量法</a>中当 $x_1$ 和 $x_2$ 的梯度值有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。</p>
<p>动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。</p>
<p>AdaGrad 算法，它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题 。</p>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>AdaGrad算法会使用一个小批量随机梯度 $\boldsymbol{g}_t$ 按元素平方的累加变量$\boldsymbol{s}_t$。在时间步0，AdaGrad将 $\boldsymbol{s}_0$ 中每个元素初始化为0。在时间步$t$，首先将小批量随机梯度$\boldsymbol{g}_t$按元素平方后累加到变量$\boldsymbol{s}_t$：</p>
<p>$$<br>\boldsymbol{s}_t \leftarrow \boldsymbol{s}_{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t,<br>$$</p>
<p>其中$\odot$是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：</p>
<p>$$<br>\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t,<br>$$</p>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。</p>
<h2 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h2><p>需要强调的是，小批量随机梯度按元素平方的累加变量$\boldsymbol{s}_t$出现在学习率的分母项中。</p>
<ul>
<li><p>因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；</p>
</li>
<li><p>反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。</p>
</li>
<li><p>然而，由于$\boldsymbol{s}_t$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。</p>
</li>
<li><p>所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。</p>
</li>
</ul>
<p>下面我们仍然以目标函数$f(\boldsymbol{x})=0.1x_1^2+2x_2^2$为例观察AdaGrad算法对自变量的迭代轨迹。我们实现AdaGrad算法并使用和上一节实验中相同的学习率0.4。可以看到，自变量的迭代轨迹较平滑。但由于$\boldsymbol{s}_t$的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span>  <span class="comment"># 前两项为自变量梯度</span></span><br><span class="line">    s1 += g1 ** <span class="number">2</span></span><br><span class="line">    s2 += g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.4</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223171954469.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<p>下面将学习率增大到2。可以看到自变量更为迅速地逼近了最优解</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">2</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223172353999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Implement-1"><a href="#Implement-1" class="headerlink" title="Implement"></a>Implement</h2><p>同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。我们根据AdaGrad算法中的公式实现该算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span>  </span><br><span class="line">    data = np.genfromtxt(<span class="string">'path to airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), </span><br><span class="line">        torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32)</span><br><span class="line">        </span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adagrad_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32)</span><br><span class="line">    s_b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    eps = <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s.data += (p.grad.data**<span class="number">2</span>)</span><br><span class="line">        p.data -= hyperparams[<span class="string">'lr'</span>] * p.grad.data / torch.sqrt(s + eps)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>增大学习率</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(adagrad, init_adagrad_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223173629947.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class-1"><a href="#Pytorch-Class-1" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><p>通过名称为 “adagrad” 的 Trainer 实例，我们便可使用 Pytorch 提供的 AdaGrad 算法来训练模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adagrad, &#123;<span class="string">'lr'</span>: <span class="number">0.1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223173755189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h1 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a><div id="rmsprop">RMSProp</div></h1><p><a href="#adagrad">“AdaGrad算法”</a>中因为调整学习率时分母上的变量 $\boldsymbol{s}_t$ 一直在累加按元素平方的小批量随机梯度，所以目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。因此，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp算法对AdaGrad算法做了修改。该算法源自Coursera上的一门课程，即“机器学习的神经网络”。</p>
<h2 id="Algorithm-1"><a href="#Algorithm-1" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>我们在<a href="#momentum">“动量法”</a>一节里介绍过指数加权移动平均。不同于AdaGrad算法里状态变量$\boldsymbol{s}_t$是截至时间步$t$所有小批量随机梯度$\boldsymbol{g}_t$按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数$0 \leq \gamma 0$计算</p>
<p>$$<br>\boldsymbol{v}_t \leftarrow \beta \boldsymbol{v}_{t-1} + (1 - \beta) \boldsymbol{g}_t \odot \boldsymbol{g}_t.<br>$$</p>
<p>和AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量</p>
<p>$$<br>\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\alpha}{\sqrt{\boldsymbol{v}_t + \epsilon}} \odot \boldsymbol{g}_t,<br>$$</p>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。因为RMSProp算法的状态变量$\boldsymbol{s}_t$是对平方项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$的指数加权移动平均，所以可以看作是最近$1/(1-\beta)$个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。</p>
<p>照例，让我们先观察RMSProp算法对目标函数$f(\boldsymbol{x})=0.1x_1^2+2x_2^2$中自变量的迭代轨迹。回忆在<a href="#adagrad">“AdaGrad算法”</a>中使用的学习率为0.4的AdaGrad算法，自变量在迭代后期的移动幅度较小。但在同样的学习率下，RMSProp算法可以更快逼近最优解。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span></span><br><span class="line">    s1 = beta * s1 + (<span class="number">1</span> - beta) * g1 ** <span class="number">2</span></span><br><span class="line">    s2 = beta * s2 + (<span class="number">1</span> - beta) * g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= alpha / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= alpha / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">alpha, beta = <span class="number">0.4</span>, <span class="number">0.9</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/2020022317423677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Implement-2"><a href="#Implement-2" class="headerlink" title="Implement"></a>Implement</h2><blockquote>
<p>接下来按照RMSProp算法中的公式实现该算法。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span>  </span><br><span class="line">    data = np.genfromtxt(<span class="string">'path to airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), \</span><br><span class="line">        torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32)</span><br><span class="line">        </span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rmsprop_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32)</span><br><span class="line">    s_b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    gamma, eps = hyperparams[<span class="string">'beta'</span>], <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s.data = gamma * s.data + (<span class="number">1</span> - gamma) * (p.grad.data)**<span class="number">2</span></span><br><span class="line">        p.data -= hyperparams[<span class="string">'lr'</span>] * p.grad.data / torch.sqrt(s + eps)</span><br></pre></td></tr></table></figure>
<p>我们将初始学习率设为0.01，并将超参数$\gamma$设为0.9。此时，变量$\boldsymbol{s}_t$可看作是最近$1/(1-0.9) = 10$个时间步的平方项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$的加权平均。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(rmsprop, init_rmsprop_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'beta'</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">              features, labels)</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/2020022317441345.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class-2"><a href="#Pytorch-Class-2" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><p>通过名称为“rmsprop”的Trainer实例，我们便可使用Gluon提供的RMSProp算法来训练模型。注意，超参数$\gamma$通过gamma1指定。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.RMSprop, &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'alpha'</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">                    features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223174542606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h1 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a><div id="adadelta">AdaDelta</div></h1><p>除了<a href="#rmsprop">RMSProp算法</a>以外，另一个常用优化算法AdaDelta算法也针对<a href="#adagrad">AdaGrad算法</a>在迭代后期可能较难找到有用解的问题做了改进 .</p>
<blockquote>
<p>AdaDelta算法没有学习率这一超参数。</p>
</blockquote>
<h2 id="Algorithm-2"><a href="#Algorithm-2" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>AdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度$\boldsymbol{g}_t$按元素平方的指数加权移动平均变量$\boldsymbol{s}_t$。在时间步0，它的所有元素被初始化为0。给定超参数$0 \leq \rho 0$，同RMSProp算法一样计算</p>
<p>$$<br>\boldsymbol{s}_t \leftarrow \rho \boldsymbol{s}_{t-1} + (1 - \rho) \boldsymbol{g}_t \odot \boldsymbol{g}_t.<br>$$</p>
<p>与RMSProp算法不同的是，AdaDelta算法还维护一个额外的状态变量$\Delta\boldsymbol{x}<em>t$，其元素同样在时间步0时被初始化为0。我们使用$\Delta\boldsymbol{x}</em>{t-1}$来计算自变量的变化量：</p>
<p>$$<br> \boldsymbol{g}_t’ \leftarrow \sqrt{\frac{\Delta\boldsymbol{x}_{t-1} + \epsilon}{\boldsymbol{s}_t + \epsilon}}   \odot \boldsymbol{g}_t,<br>$$</p>
<p>其中$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-5}$。接着更新自变量：</p>
<p>$$<br>\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}’_t.<br>$$</p>
<p>最后，我们使用$\Delta\boldsymbol{x}_t$来记录自变量变化量$\boldsymbol{g}’_t$按元素平方的指数加权移动平均：</p>
<p>$$<br>\Delta\boldsymbol{x}_t \leftarrow \rho \Delta\boldsymbol{x}_{t-1} + (1 - \rho) \boldsymbol{g}’_t \odot \boldsymbol{g}’_t.<br>$$</p>
<p>可以看到，如不考虑$\epsilon$的影响，AdaDelta算法与RMSProp算法的不同之处在于使用$\sqrt{\Delta\boldsymbol{x}_{t-1}}$来替代超参数$\eta$。</p>
<h2 id="Implement-3"><a href="#Implement-3" class="headerlink" title="Implement"></a>Implement</h2><p>AdaDelta算法需要对每个自变量维护两个状态变量，即$\boldsymbol{s}_t$和$\Delta\boldsymbol{x}_t$。我们按AdaDelta算法中的公式实现该算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adadelta_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w, s_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    delta_w, delta_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> ((s_w, delta_w), (s_b, delta_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adadelta</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    rho, eps = hyperparams[<span class="string">'rho'</span>], <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> p, (s, delta) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s[:] = rho * s + (<span class="number">1</span> - rho) * (p.grad.data**<span class="number">2</span>)</span><br><span class="line">        g =  p.grad.data * torch.sqrt((delta + eps) / (s + eps))</span><br><span class="line">        p.data -= g</span><br><span class="line">        delta[:] = rho * delta + (<span class="number">1</span> - rho) * g * g</span><br><span class="line">d2l.train_ch7(adadelta, init_adadelta_states(), &#123;<span class="string">'rho'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223174834831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class-3"><a href="#Pytorch-Class-3" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><p>通过名称为 “adadelta” 的 Traine r实例，我们便可使用 pytorch 提供的 AdaDelta 算法。它的超参数可以通过 rho 来指定。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adadelta, &#123;<span class="string">'rho'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/2020022317495777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a><div id="adam">Adam</div></h1><blockquote>
<p>相当于是<a href="#rmsprop">RMSProp算法</a>和<a href="#momentum">动量算法</a>的结合</p>
</blockquote>
<center>RMSProp算法</center>
<center>+</center>
<center>对小批量随机梯度也做了指数加权移动平均</center>
<center>||</center>
<center>Adam</center>

<h2 id="Algorithm-3"><a href="#Algorithm-3" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Adam算法使用了动量变量$\boldsymbol{m}_t$和RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量$\boldsymbol{v}_t$，并在时间步0将它们中每个元素初始化为0。给定超参数$0 \leq \beta_1 &lt; 1$（算法作者建议设为0.9），时间步$t$的动量变量$\boldsymbol{m}_t$即小批量随机梯度$\boldsymbol{g}_t$的指数加权移动平均：</p>
<p>$$<br>\boldsymbol{m}_t \leftarrow \beta_1 \boldsymbol{m}_{t-1} + (1 - \beta_1) \boldsymbol{g}_t.<br>$$</p>
<p>和RMSProp算法中一样，给定超参数$0 \leq \beta_2 &lt; 1$（算法作者建议设为0.999），<br>将小批量随机梯度按元素平方后的项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$做指数加权移动平均得到$\boldsymbol{v}_t$：</p>
<p>$$<br>\boldsymbol{v}_t \leftarrow \beta_2 \boldsymbol{v}_{t-1} + (1 - \beta_2) \boldsymbol{g}_t \odot \boldsymbol{g}_t.<br>$$</p>
<p>由于我们将$\boldsymbol{m}_0$和$\boldsymbol{s}_0$中的元素都初始化为0，<br>在时间步$t$我们得到$\boldsymbol{m}_t =  (1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} \boldsymbol{g}_i$。将过去各时间步小批量随机梯度的权值相加，得到 $(1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} = 1 - \beta_1^t$。需要注意的是，当$t$较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当$\beta_1 = 0.9$时，$\boldsymbol{m}_1 = 0.1\boldsymbol{g}_1$。为了消除这样的影响，对于任意时间步$t$，我们可以将$\boldsymbol{m}_t$再除以$1 - \beta_1^t$，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，我们对变量$\boldsymbol{m}_t$和$\boldsymbol{v}_t$均作偏差修正：</p>
<p>$$<br>\hat{\boldsymbol{m}}_t \leftarrow \frac{\boldsymbol{m}_t}{1 - \beta_1^t},<br>$$</p>
<p>$$<br>\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_2^t}.<br>$$</p>
<p>接下来，Adam算法使用以上偏差修正后的变量$\hat{\boldsymbol{m}}_t$和$\hat{\boldsymbol{m}}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整：</p>
<p>$$<br>\boldsymbol{g}_t’ \leftarrow \frac{\eta \hat{\boldsymbol{m}}_t}{\sqrt{\hat{\boldsymbol{v}}_t} + \epsilon},<br>$$</p>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-8}$。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用$\boldsymbol{g}_t’$迭代自变量：</p>
<p>$$<br>\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}_t’.<br>$$</p>
<h2 id="Implement-4"><a href="#Implement-4" class="headerlink" title="Implement"></a>Implement</h2><p>我们按照Adam算法中的公式实现该算法。其中时间步$t$通过 hyperparams 参数传入 adam 函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span>  </span><br><span class="line">    data = np.genfromtxt(<span class="string">'path to airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), \</span><br><span class="line">        torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32)</span><br><span class="line">        </span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adam_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w, v_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    s_w, s_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad.data</span><br><span class="line">        s[:] = beta2 * s + (<span class="number">1</span> - beta2) * p.grad.data**<span class="number">2</span></span><br><span class="line">        v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">'t'</span>])</span><br><span class="line">        s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">'t'</span>])</span><br><span class="line">        p.data -= hyperparams[<span class="string">'lr'</span>] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)</span><br><span class="line">    hyperparams[<span class="string">'t'</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(adam, init_adam_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'t'</span>: <span class="number">1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223180150289.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class-4"><a href="#Pytorch-Class-4" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adam, &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>


<center><img src="https://img-blog.csdnimg.cn/20200223180238893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>梯度问题</tag>
      </tags>
  </entry>
  <entry>
    <title>Optimization including Convex Optimization and Gradient Descent</title>
    <url>/2020/02/28/Optimization-including-Convex-Optimization-and-Gradient-Descent/</url>
    <content><![CDATA[<p><font color=red>温馨提示：</font></p>
<p>&ensp;&ensp;&ensp;&ensp;<font color=red>本文将介绍统计学中的优化知识，凸优化和梯度下降，多为公式推导和图形化展示，较为硬核</font></p>
<h1 id="优化与深度学习"><a href="#优化与深度学习" class="headerlink" title="优化与深度学习"></a>优化与深度学习</h1><h2 id="优化与估计"><a href="#优化与估计" class="headerlink" title="优化与估计"></a>优化与估计</h2><p>尽管优化方法可以最小化深度学习中的损失函数值，但本质上优化方法达到的目标与深度学习的目标并不相同。</p>
<ul>
<li><p><strong>优化方法目标</strong>：训练集损失函数值</p>
</li>
<li><p><strong>深度学习目标</strong>：测试集损失函数值（泛化性）</p>
</li>
<li><p><strong>借助图形直观比较</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'path to file storge d2lzh1981'</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d <span class="comment"># 三维画图</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span> <span class="keyword">return</span> x * np.cos(np.pi * x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(x)</span>:</span> <span class="keyword">return</span> f(x) + <span class="number">0.2</span> * np.cos(<span class="number">5</span> * np.pi * x)</span><br><span class="line"></span><br><span class="line">d2l.set_figsize((<span class="number">5</span>, <span class="number">3</span>))</span><br><span class="line">x = np.arange(<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">0.01</span>)</span><br><span class="line">fig_f, = d2l.plt.plot(x, f(x),label=<span class="string">"train error"</span>)</span><br><span class="line">fig_g, = d2l.plt.plot(x, g(x),<span class="string">'--'</span>, c=<span class="string">'purple'</span>, label=<span class="string">"test error"</span>)</span><br><span class="line">fig_f.axes.annotate(<span class="string">'empirical risk'</span>, (<span class="number">1.0</span>, <span class="number">-1.2</span>), (<span class="number">0.5</span>, <span class="number">-1.1</span>),arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">fig_g.axes.annotate(<span class="string">'expected risk'</span>, (<span class="number">1.1</span>, <span class="number">-1.05</span>), (<span class="number">0.95</span>, <span class="number">-0.5</span>),arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'risk'</span>)</span><br><span class="line">d2l.plt.legend(loc=<span class="string">"upper right"</span>)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220165647572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="优化在深度学习中的挑战"><a href="#优化在深度学习中的挑战" class="headerlink" title="优化在深度学习中的挑战"></a>优化在深度学习中的挑战</h2><ol>
<li><a href="#Local_minimum"><strong>局部最小值</strong></a></li>
<li><a href="#saddle_point"><strong>鞍点</strong></a></li>
<li><a href="#vanishing_gradient"><strong>梯度消失</strong></a></li>
</ol>
<div id="Local_minimum"><b>局部最小值</b></div>

<p>$$<br>f(x) = x\cos \pi x<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * np.cos(np.pi * x)</span><br><span class="line"></span><br><span class="line">d2l.set_figsize((<span class="number">4.5</span>, <span class="number">2.5</span>))</span><br><span class="line">x = np.arange(<span class="number">-1.0</span>, <span class="number">2.0</span>, <span class="number">0.1</span>)</span><br><span class="line">fig,  = d2l.plt.plot(x, f(x))</span><br><span class="line">fig.axes.annotate(<span class="string">'local minimum'</span>, xy=(<span class="number">-0.3</span>, <span class="number">-0.25</span>), xytext=(<span class="number">-0.77</span>, <span class="number">-1.0</span>),</span><br><span class="line">                  arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">fig.axes.annotate(<span class="string">'global minimum'</span>, xy=(<span class="number">1.1</span>, <span class="number">-0.95</span>), xytext=(<span class="number">0.6</span>, <span class="number">0.8</span>),</span><br><span class="line">                  arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'f(x)'</span>);</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220170158309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<div id="saddle_point"><b>鞍点</b></div><br/>

<blockquote>
<p>函数在一阶导数为零处（驻点）的黑塞矩阵为不定矩阵。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">-2.0</span>, <span class="number">2.0</span>, <span class="number">0.1</span>)</span><br><span class="line">fig, = d2l.plt.plot(x, x**<span class="number">3</span>)</span><br><span class="line">fig.axes.annotate(<span class="string">'saddle point'</span>, xy=(<span class="number">0</span>, <span class="number">-0.2</span>), xytext=(<span class="number">-0.52</span>, <span class="number">-5.0</span>),</span><br><span class="line">                  arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'f(x)'</span>);</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220170416636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<hr>
<p><strong>海森矩阵</strong></p>
<p>$$<br>A=\left[\begin{array}{cccc}{\frac{\partial^{2} f}{\partial x_{1}^{2}}} &amp; {\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}}} \\ {\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}} &amp; {\frac{\partial^{2} f}{\partial x_{2}^{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{2} \partial x_{n}}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} &amp; {\frac{\partial^{2} f}{\partial x_{n} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{n}^{2}}}\end{array}\right]<br>$$</p>
<blockquote>
<p>海森矩阵特征值和鞍点还有局部极小值的点的关系</p>
<blockquote>
<p>偏导数为零的点</p>
<ul>
<li>特征值都大于零是局部极小值点</li>
<li>都为负数是局部极大指点</li>
<li>有正有负就是鞍点</li>
</ul>
</blockquote>
</blockquote>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x, y = np.mgrid[<span class="number">-1</span>: <span class="number">1</span>: <span class="number">31j</span>, <span class="number">-1</span>: <span class="number">1</span>: <span class="number">31j</span>]</span><br><span class="line">z = x**<span class="number">2</span> - y**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">d2l.set_figsize((<span class="number">6</span>, <span class="number">4</span>))</span><br><span class="line">ax = d2l.plt.figure().add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.plot_wireframe(x, y, z, **&#123;<span class="string">'rstride'</span>: <span class="number">2</span>, <span class="string">'cstride'</span>: <span class="number">2</span>&#125;)</span><br><span class="line">ax.plot([<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">0</span>], <span class="string">'ro'</span>, markersize=<span class="number">10</span>)</span><br><span class="line">ticks = [<span class="number">-1</span>,  <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">d2l.plt.xticks(ticks)</span><br><span class="line">d2l.plt.yticks(ticks)</span><br><span class="line">ax.set_zticks(ticks)</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'y'</span>);</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220171025698.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<div id="vanishing_gradient"><b>梯度消失</b></div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">-2.0</span>, <span class="number">5.0</span>, <span class="number">0.01</span>)</span><br><span class="line">fig, = d2l.plt.plot(x, np.tanh(x))</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'f(x)'</span>)</span><br><span class="line">fig.axes.annotate(<span class="string">'vanishing gradient'</span>, (<span class="number">4</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">0.0</span>) ,arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220171233420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'path to file storge d2lzh1981'</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h3 id="一维梯度下降"><a href="#一维梯度下降" class="headerlink" title="一维梯度下降"></a>一维梯度下降</h3><hr>
<p><strong>证明：沿梯度反方向移动自变量可以减小函数值</strong></p>
<p>泰勒展开：</p>
<p>$$<br>f(x+\epsilon)=f(x)+\epsilon f^{\prime}(x)+\mathcal{O}\left(\epsilon^{2}\right)<br>$$</p>
<p>代入沿梯度方向的移动量 $\eta f^{\prime}(x)$：</p>
<p>$$<br>f\left(x-\eta f^{\prime}(x)\right)=f(x)-\eta f^{\prime 2}(x)+\mathcal{O}\left(\eta^{2} f^{\prime 2}(x)\right)<br>$$</p>
<p>$$<br>f\left(x-\eta f^{\prime}(x)\right) \lesssim f(x)<br>$$</p>
<p>$$<br>x \leftarrow x-\eta f^{\prime}(x)<br>$$</p>
<hr>
<p>e.g.</p>
<p>$$<br>f(x) = x^2<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span>  <span class="comment"># Objective function</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x  <span class="comment"># Its derivative</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd</span><span class="params">(eta)</span>:</span></span><br><span class="line">    x = <span class="number">10</span></span><br><span class="line">    results = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        <span class="comment"># eta 学习率</span></span><br><span class="line">        x -= eta * gradf(x)</span><br><span class="line">        results.append(x)</span><br><span class="line">    print(<span class="string">'epoch 20, x:'</span>, x)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">res = gd(<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<p><strong>梯度下降轨迹</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_trace</span><span class="params">(res)</span>:</span></span><br><span class="line">    n = max(abs(min(res)), abs(max(res)))</span><br><span class="line">    f_line = np.arange(-n, n, <span class="number">0.01</span>)</span><br><span class="line">    d2l.set_figsize((<span class="number">3.5</span>, <span class="number">2.5</span>))</span><br><span class="line">    d2l.plt.plot(f_line, [f(x) <span class="keyword">for</span> x <span class="keyword">in</span> f_line],<span class="string">'-'</span>)</span><br><span class="line">    d2l.plt.plot(res, [f(x) <span class="keyword">for</span> x <span class="keyword">in</span> res],<span class="string">'-o'</span>)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'f(x)'</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">show_trace(res)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220171815966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<center><font size=5>学习率</font></center>

<blockquote>
<p>学习率过小 Code：show_trace(gd(0.05))<br><img src="https://img-blog.csdnimg.cn/2020022017211762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
<blockquote>
<p>学习率过大  Code：show_trace(gd(1.1))<br><img src="https://img-blog.csdnimg.cn/20200220172154883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
</blockquote>
<blockquote>
<center><font size=4>局部极小值</font></center>
</blockquote>
<p>$$<br>f(x) = x\cos cx<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = <span class="number">0.15</span> * np.pi</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * np.cos(c * x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.cos(c * x) - c * x * np.sin(c * x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率不合适容易导致</span></span><br><span class="line">show_trace(gd(<span class="number">2</span>))</span><br><span class="line">show_trace(gd(<span class="number">0.5</span>))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220172910884.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="多维梯度下降"><a href="#多维梯度下降" class="headerlink" title="多维梯度下降"></a>多维梯度下降</h3><p>$$<br>\nabla f(\mathbf{x})=\left[\frac{\partial f(\mathbf{x})}{\partial x_{1}}, \frac{\partial f(\mathbf{x})}{\partial x_{2}}, \dots, \frac{\partial f(\mathbf{x})}{\partial x_{d}}\right]^{\top}<br>$$</p>
<p>$$<br>f(\mathbf{x}+\epsilon)=f(\mathbf{x})+\epsilon^{\top} \nabla f(\mathbf{x})+\mathcal{O}\left(|\epsilon|^{2}\right)<br>$$</p>
<p>$$<br>\mathbf{x} \leftarrow \mathbf{x}-\eta \nabla f(\mathbf{x})<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练 trainer展示x如何更新</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_2d</span><span class="params">(trainer, steps=<span class="number">20</span>)</span>:</span></span><br><span class="line">    x1, x2 = <span class="number">-5</span>, <span class="number">-2</span></span><br><span class="line">    results = [(x1, x2)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(steps):</span><br><span class="line">        x1, x2 = trainer(x1, x2)</span><br><span class="line">        results.append((x1, x2))</span><br><span class="line">    print(<span class="string">'epoch %d, x1 %f, x2 %f'</span> % (i + <span class="number">1</span>, x1, x2))</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"><span class="comment"># 垂直于等高线梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_trace_2d</span><span class="params">(f, results)</span>:</span> </span><br><span class="line">    d2l.plt.plot(*zip(*results), <span class="string">'-o'</span>, color=<span class="string">'#ff7f0e'</span>)</span><br><span class="line">    x1, x2 = np.meshgrid(np.arange(<span class="number">-5.5</span>, <span class="number">1.0</span>, <span class="number">0.1</span>), np.arange(<span class="number">-3.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>))</span><br><span class="line">    d2l.plt.contour(x1, x2, f(x1, x2), colors=<span class="string">'#1f77b4'</span>)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'x2'</span>)</span><br></pre></td></tr></table></figure>
<p>e.g.</p>
<p>$$<br>f(x) = x_1^2 + 2x_2^2<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span>  <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="keyword">return</span> x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * <span class="number">2</span> * x1, x2 - eta * <span class="number">4</span> * x2)</span><br><span class="line"></span><br><span class="line">show_trace_2d(f_2d, train_2d(gd_2d))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220214102821.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="自适应方法"><a href="#自适应方法" class="headerlink" title="自适应方法"></a>自适应方法</h2><h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><blockquote>
<p><strong>优势 :</strong><br/><br/></p>
<blockquote>
<p>梯度下降“步幅”的确定比较困难<br/><br>而牛顿法相当于可以通过Hessian矩阵来调整“步幅”。</p>
</blockquote>
<p>在牛顿法中，局部极小值也可以通过调整学习率来解决。</p>
</blockquote>
<p>在 $x + \epsilon$ 处泰勒展开：</p>
<p>$$<br>f(\mathbf{x}+\epsilon)=f(\mathbf{x})+\epsilon^{\top} \nabla f(\mathbf{x})+\frac{1}{2} \epsilon^{\top} \nabla \nabla^{\top} f(\mathbf{x}) \epsilon+\mathcal{O}\left(|\epsilon|^{3}\right)<br>$$</p>
<p>最小值点处满足: $\nabla f(\mathbf{x})=0$, 即我们希望 $\nabla f(\mathbf{x} + \epsilon)=0$, 对上式关于 $\epsilon$ 求导，忽略高阶无穷小，有：</p>
<p>$$<br>\nabla f(\mathbf{x})+\boldsymbol{H}<em>{f} \boldsymbol{\epsilon}=0 \text { and hence } \epsilon=-\boldsymbol{H}</em>{f}^{-1} \nabla f(\mathbf{x})<br>$$</p>
<blockquote>
<p>牛顿法需要计算Hessian矩阵的逆，计算量比较大。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.cosh(c * x)  <span class="comment"># Objective</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> c * np.sinh(c * x)  <span class="comment"># Derivative</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hessf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> c**<span class="number">2</span> * np.cosh(c * x)  <span class="comment"># Hessian</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hide learning rate for now</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newton</span><span class="params">(eta=<span class="number">1</span>)</span>:</span></span><br><span class="line">    x = <span class="number">10</span></span><br><span class="line">    results = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        x -= eta * gradf(x) / hessf(x)</span><br><span class="line">        results.append(x)</span><br><span class="line">    print(<span class="string">'epoch 10, x:'</span>, x)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">show_trace(newton())</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220214418174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 牛顿法对于有局部极小值的情况</span></span><br><span class="line"><span class="comment"># 和梯度下降的方法有一样的效果</span></span><br><span class="line"><span class="comment"># 正确的方法还是降低学习率</span></span><br><span class="line">c = <span class="number">0.15</span> * np.pi</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * np.cos(c * x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.cos(c * x) - c * x * np.sin(c * x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hessf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> - <span class="number">2</span> * c * np.sin(c * x) - x * c**<span class="number">2</span> * np.cos(c * x)</span><br><span class="line"></span><br><span class="line">show_trace(newton())</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220214459356.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<p>show_trace(newton(0.5))<br><img src="https://img-blog.csdnimg.cn/20200220214557927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
<h3 id="收敛性分析"><a href="#收敛性分析" class="headerlink" title="收敛性分析"></a>收敛性分析</h3><p>只考虑在函数为凸函数, 且最小值点上 $f’’(x^*) &gt;0$ 时的收敛速度：</p>
<p>令 $x_k$ 为第 $k$ 次迭代后 $x$ 的值， $e_{k}:=x_{k}-x^{*}$ 表示 $x_k$ 到最小值点 $x^{*}$ 的距离，由 $f’(x^{*}) = 0$:</p>
<p>$$<br>0=f^{\prime}\left(x_{k}-e_{k}\right)=f^{\prime}\left(x_{k}\right)-e_{k} f^{\prime \prime}\left(x_{k}\right)+\frac{1}{2} e_{k}^{2} f^{\prime \prime \prime}\left(\xi_{k}\right) \text{for some } \xi_{k} \in\left[x_{k}-e_{k}, x_{k}\right]<br>$$</p>
<p>两边除以 $f’’(x_k)$, 有：</p>
<p>$$<br>e_{k}-f^{\prime}\left(x_{k}\right) / f^{\prime \prime}\left(x_{k}\right)=\frac{1}{2} e_{k}^{2} f^{\prime \prime \prime}\left(\xi_{k}\right) / f^{\prime \prime}\left(x_{k}\right)<br>$$</p>
<p>代入更新方程 $x_{k+1} = x_{k} - f^{\prime}\left(x_{k}\right) / f^{\prime \prime}\left(x_{k}\right)$, 得到：</p>
<p>$$<br>x_k - x^{*} - f^{\prime}\left(x_{k}\right) / f^{\prime \prime}\left(x_{k}\right) =\frac{1}{2} e_{k}^{2} f^{\prime \prime \prime}\left(\xi_{k}\right) / f^{\prime \prime}\left(x_{k}\right)<br>$$</p>
<p>$$<br>x_{k+1} - x^{*} = e_{k+1} = \frac{1}{2} e_{k}^{2} f^{\prime \prime \prime}\left(\xi_{k}\right) / f^{\prime \prime}\left(x_{k}\right)<br>$$</p>
<p>当 $\frac{1}{2} f^{\prime \prime \prime}\left(\xi_{k}\right) / f^{\prime \prime}\left(x_{k}\right) \leq c$ 时，有:</p>
<p>$$<br>e_{k+1} \leq c e_{k}^{2}<br>$$</p>
<h3 id="预处理-（Heissan阵辅助梯度下降）"><a href="#预处理-（Heissan阵辅助梯度下降）" class="headerlink" title="预处理 （Heissan阵辅助梯度下降）"></a>预处理 （Heissan阵辅助梯度下降）</h3><p>$$<br>\mathbf{x} \leftarrow \mathbf{x}-\eta \operatorname{diag}\left(H_{f}\right)^{-1} \nabla \mathbf{x}<br>$$</p>
<h3 id="梯度下降与线性搜索（共轭梯度法）"><a href="#梯度下降与线性搜索（共轭梯度法）" class="headerlink" title="梯度下降与线性搜索（共轭梯度法）"></a>梯度下降与线性搜索（共轭梯度法）</h3><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><h3 id="随机梯度下降参数更新"><a href="#随机梯度下降参数更新" class="headerlink" title="随机梯度下降参数更新"></a>随机梯度下降参数更新</h3><p>对于有 $n$ 个样本对训练数据集，设 $f_i(x)$ 是第 $i$ 个样本的损失函数, 则目标函数为:</p>
<p>$$<br>f(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} f_{i}(\mathbf{x})<br>$$</p>
<p>其梯度为:</p>
<p>$$<br>\nabla f(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} \nabla f_{i}(\mathbf{x})<br>$$</p>
<p>每一个样本的梯度是对整体的梯度的无偏估计</p>
<p>使用该梯度的一次更新的时间复杂度为 $\mathcal{O}(n)$</p>
<p>随机梯度下降更新公式 $\mathcal{O}(1)$:</p>
<p>$$<br>\mathbf{x} \leftarrow \mathbf{x}-\eta \nabla f_{i}(\mathbf{x})<br>$$</p>
<p>且有：</p>
<p>$$<br>\mathbb{E}<em>{i} \nabla f</em>{i}(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} \nabla f_{i}(\mathbf{x})=\nabla f(\mathbf{x})<br>$$<br>e.g. </p>
<p>$$<br>f(x_1, x_2) = x_1^2 + 2 x_2^2<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span>  <span class="comment"># Objective</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">2</span> * x1, <span class="number">4</span> * x2)  <span class="comment"># Gradient</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(x1, x2)</span>:</span>  <span class="comment"># Simulate noisy gradient</span></span><br><span class="line">    <span class="keyword">global</span> lr  <span class="comment"># Learning rate scheduler</span></span><br><span class="line">    (g1, g2) = gradf(x1, x2)  <span class="comment"># Compute gradient</span></span><br><span class="line">    (g1, g2) = (g1 + np.random.normal(<span class="number">0.1</span>), g2 + np.random.normal(<span class="number">0.1</span>))</span><br><span class="line">    eta_t = eta * lr()  <span class="comment"># Learning rate at time t</span></span><br><span class="line">    <span class="keyword">return</span> (x1 - eta_t * g1, x2 - eta_t * g2)  <span class="comment"># Update variables</span></span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.1</span></span><br><span class="line">lr = (<span class="keyword">lambda</span>: <span class="number">1</span>)  <span class="comment"># Constant learning rate</span></span><br><span class="line">show_trace_2d(f, train_2d(sgd, steps=<span class="number">50</span>))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/2020022021481780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="动态学习率"><a href="#动态学习率" class="headerlink" title="动态学习率"></a>动态学习率</h3><ul>
<li>在最开始学习率设计比较大，加速收敛</li>
<li>学习率可以设计为指数衰减或多项式衰减</li>
<li>在优化进行一段时间后可以适当减小学习率来避免振荡</li>
</ul>
<p>$$<br>\begin{array}{ll}{\eta(t)=\eta_{i} \text { if } t_{i} \leq t \leq t_{i+1}} &amp; {\text { piecewise constant }} \\ {\eta(t)=\eta_{0} \cdot e^{-\lambda t}} &amp; {\text { exponential }} \\ {\eta(t)=\eta_{0} \cdot(\beta t+1)^{-\alpha}} &amp; {\text { polynomial }}\end{array}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exponential</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ctr</span><br><span class="line">    ctr += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> math.exp(<span class="number">-0.1</span> * ctr)</span><br><span class="line"></span><br><span class="line">ctr = <span class="number">1</span></span><br><span class="line">lr = exponential  <span class="comment"># Set up learning rate</span></span><br><span class="line">show_trace_2d(f, train_2d(sgd, steps=<span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220215551658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polynomial</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 迭代次数</span></span><br><span class="line">    <span class="keyword">global</span> ctr</span><br><span class="line">    ctr += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> + <span class="number">0.1</span> * ctr)**(<span class="number">-0.5</span>)</span><br><span class="line"></span><br><span class="line">ctr = <span class="number">1</span></span><br><span class="line">lr = polynomial  <span class="comment"># Set up learning rate</span></span><br><span class="line">show_trace_2d(f, train_2d(sgd, steps=<span class="number">50</span>))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200220215558471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h2><h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><p><a href="https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise" target="_blank" rel="noopener">读取数据</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span></span><br><span class="line">    data = np.genfromtxt(<span class="string">'/home/kesci/input/airfoil4755/airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>) <span class="comment"># 标准化</span></span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), \</span><br><span class="line">           torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32) <span class="comment"># 前1500个样本(每个样本5个特征)</span></span><br><span class="line"></span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line">features.shape</span><br></pre></td></tr></table></figure>
<p><strong>数据可视化</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">'path to airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Stochastic Gradient Descent (SGD)函数</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        p.data -= hyperparams[<span class="string">'lr'</span>] * p.grad.data</span><br></pre></td></tr></table></figure>
<p><strong>训练</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch7</span><span class="params">(optimizer_fn, states, hyperparams, features, labels,</span></span></span><br><span class="line"><span class="function"><span class="params">              batch_size=<span class="number">10</span>, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    net, loss = d2l.linreg, d2l.squared_loss</span><br><span class="line">    </span><br><span class="line">    w = torch.nn.Parameter(torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(features.shape[<span class="number">1</span>], <span class="number">1</span>)), dtype=torch.float32),</span><br><span class="line">                           requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.nn.Parameter(torch.zeros(<span class="number">1</span>, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_loss</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> loss(net(features, w, b), labels).mean().item()</span><br><span class="line"></span><br><span class="line">    ls = [eval_loss()]</span><br><span class="line">    data_iter = torch.utils.data.DataLoader(</span><br><span class="line">        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">for</span> batch_i, (X, y) <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">            l = loss(net(X, w, b), y).mean()  <span class="comment"># 使用平均损失</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> w.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                w.grad.data.zero_()</span><br><span class="line">                b.grad.data.zero_()</span><br><span class="line">                </span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer_fn([w, b], states, hyperparams)  <span class="comment"># 迭代模型参数</span></span><br><span class="line">            <span class="keyword">if</span> (batch_i + <span class="number">1</span>) * batch_size % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                ls.append(eval_loss())  <span class="comment"># 每100个样本记录下当前训练误差</span></span><br><span class="line">    <span class="comment"># 打印结果和作图</span></span><br><span class="line">    print(<span class="string">'loss: %f, %f sec per epoch'</span> % (ls[<span class="number">-1</span>], time.time() - start))</span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    d2l.plt.plot(np.linspace(<span class="number">0</span>, num_epochs, len(ls)), ls)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'loss'</span>)</span><br></pre></td></tr></table></figure>
<p><strong>测试</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_sgd</span><span class="params">(lr, batch_size, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    train_ch7(sgd, <span class="literal">None</span>, &#123;<span class="string">'lr'</span>: lr&#125;, features, labels, batch_size, num_epochs)</span><br></pre></td></tr></table></figure>
<p><strong>Result</strong></p>
<blockquote>
<ul>
<li>train_sgd(1, 1500, 6)<br><img src="https://img-blog.csdnimg.cn/20200220220207217.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li>train_sgd(0.005, 1)<br><img src="https://img-blog.csdnimg.cn/20200220220222233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li>train_sgd(0.05, 10)<br><img src="https://img-blog.csdnimg.cn/20200220220233911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
</blockquote>
<p><strong>简化模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_pytorch_ch7</span><span class="params">(optimizer_fn, optimizer_hyperparams, features, labels,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_size=<span class="number">10</span>, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Linear(features.shape[<span class="number">-1</span>], <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    loss = nn.MSELoss()</span><br><span class="line">    optimizer = optimizer_fn(net.parameters(), **optimizer_hyperparams)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_loss</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> loss(net(features).view(<span class="number">-1</span>), labels).item() / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    ls = [eval_loss()]</span><br><span class="line">    data_iter = torch.utils.data.DataLoader(</span><br><span class="line">        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">for</span> batch_i, (X, y) <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">            <span class="comment"># 除以2是为了和train_ch7保持一致, 因为squared_loss中除了2</span></span><br><span class="line">            l = loss(net(X).view(<span class="number">-1</span>), y) / <span class="number">2</span> </span><br><span class="line">            </span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">if</span> (batch_i + <span class="number">1</span>) * batch_size % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                ls.append(eval_loss())</span><br><span class="line">    <span class="comment"># 打印结果和作图</span></span><br><span class="line">    print(<span class="string">'loss: %f, %f sec per epoch'</span> % (ls[<span class="number">-1</span>], time.time() - start))</span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    d2l.plt.plot(np.linspace(<span class="number">0</span>, num_epochs, len(ls)), ls)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'loss'</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>train_pytorch_ch7(optim.SGD, {“lr”: 0.05}, features, labels, 10)<br><img src="https://img-blog.csdnimg.cn/20200220220400842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>梯度问题</tag>
      </tags>
  </entry>
  <entry>
    <title>批量归一化 &amp;&amp; 残差网络</title>
    <url>/2020/02/28/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%20&amp;&amp;%20%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>基于此前对于CNN的介绍</p>
<ul>
<li><a href="https://blog.csdn.net/RokoBasilisk/article/details/104380762" target="_blank" rel="noopener">Fundamentals of Convolutional Neural Networks</a></li>
<li><a href="https://blog.csdn.net/RokoBasilisk/article/details/104381637" target="_blank" rel="noopener">LeNet &amp;&amp; ModernCNN</a></li>
</ul>
<p>就深层次 CNN 的结构进一步探讨归一化和残差网络。</p>
<h1 id="批量归一化（BatchNormalization）"><a href="#批量归一化（BatchNormalization）" class="headerlink" title="批量归一化（BatchNormalization）"></a>批量归一化（BatchNormalization）</h1><blockquote>
<p>让网络训练归一化变得更加容易，本质是一种对数据的标准化处理</p>
</blockquote>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><ul>
<li><strong>对输入的标准化（浅层模型）</strong></li>
</ul>
<p>处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。  标准化处理输入数据使各个特征的分布相近</p>
<ul>
<li><strong>批量归一化（深度模型）随着模型参数的迭代更新，靠近输出层的数据剧烈变化</strong></li>
</ul>
<p>利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li><strong>对全连接层做批量归一化</strong><blockquote>
<p>位置：全连接层中的仿射变换和激活函数之间。  </p>
</blockquote>
</li>
</ul>
<p><strong>全连接：</strong><br>$$<br>\boldsymbol{x} = \boldsymbol{W\boldsymbol{u} + \boldsymbol{b}} \\<br> output =\phi(\boldsymbol{x})<br>$$</p>
<p>输入是u，经过仿射变化得到x，经过激活函数得到output，size=(batch_size，输出神经元的个数)</p>
<p><strong>批量归一化：</strong><br>$$<br>output=\phi(\text{BN}(\boldsymbol{x}))$$</p>
<p>$$<br>\boldsymbol{y}^{(i)} = \text{BN}(\boldsymbol{x}^{(i)})<br>$$</p>
<p>$$<br>\boldsymbol{\mu}<em>\mathcal{B} \leftarrow \frac{1}{m}\sum</em>{i = 1}^{m} \boldsymbol{x}^{(i)},<br>$$</p>
<p>$$<br>\boldsymbol{\sigma}_\mathcal{B}^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m}(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B})^2,<br>$$</p>
<p>$$<br>\hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B}}{\sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}},<br>$$</p>
<p>$$<br>标准化处理<br>$$<br>这⾥ϵ &gt; 0是个很小的常数，保证分母大于0</p>
<p>$$<br>{\boldsymbol{y}}^{(i)} \leftarrow \boldsymbol{\gamma} \odot<br>\hat{\boldsymbol{x}}^{(i)} + \boldsymbol{\beta}.<br>$$</p>
<p>引入可学习参数：拉伸参数γ和偏移参数β。若$\boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}$和$\boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}$，批量归一化无效。</p>
<ul>
<li><strong>对卷积层做批量归⼀化</strong><blockquote>
<p>位置：卷积计算之后、应⽤激活函数之前</p>
</blockquote>
</li>
</ul>
<p>如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数。</p>
<p>计算：对单通道，$batchsize = m,卷积计算输出 = p \times q$</p>
<p>对该通道中 $m\times p\times q$ 个元素同时做批量归一化,使用相同的均值和方差。</p>
<ul>
<li><strong>预测时的批量归⼀化</strong></li>
</ul>
<p>训练：以 batch 为单位, 对每个 batch 计算均值和方差。  </p>
<p>预测：用移动平均估算整个训练数据集的样本均值和方差。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="batch-norm-function"><a href="#batch-norm-function" class="headerlink" title="batch_norm function"></a>batch_norm function</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 放到了BatchNorm类中，使用时直接调用forward函数，此函数将成为cell函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_norm</span><span class="params">(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum)</span>:</span></span><br><span class="line">    <span class="comment"># 判断当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">        <span class="comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> len(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> len(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>) <span class="comment">#是d维的值</span></span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持</span></span><br><span class="line">            <span class="comment"># X的形状以便后面可以做广播运算</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">3</span>, keepdim=<span class="literal">True</span>) <span class="comment"># c维的值，通道有几个mean就有几个</span></span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">3</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下用当前的均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        <span class="comment"># momentum 是一个超参数</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta  <span class="comment"># 拉伸和偏移</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean, moving_var</span><br></pre></td></tr></table></figure>
<h3 id="batch-norm-class"><a href="#batch-norm-class" class="headerlink" title="batch_norm class"></a>batch_norm class</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在Batch Norm函数的基础上定义此类，作用是维护学习参数和超参数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features, num_dims)</span>:</span> </span><br><span class="line">        super(BatchNorm, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features) <span class="comment">#全连接层输出神经元 </span></span><br><span class="line">            <span class="comment"># num_features代表输出神经元的个数</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)  <span class="comment">#通道数</span></span><br><span class="line">        <span class="comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment"># 不参与求梯度和迭代的变量，全在内存上初始化成0</span></span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上</span></span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        <span class="comment"># 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false</span></span><br><span class="line">        Y, self.moving_mean, self.moving_var = batch_norm(self.training, </span><br><span class="line">            X, self.gamma, self.beta, self.moving_mean,</span><br><span class="line">            self.moving_var, eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<h3 id="基于LeNet的应用"><a href="#基于LeNet的应用" class="headerlink" title="基于LeNet的应用"></a>基于LeNet的应用</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            <span class="comment"># 直接作为一个参数加到LeNet中就行</span></span><br><span class="line">            BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 体现了仿射函数之后激活函数之前的结构</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            d2l.FlattenLayer(),</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>
<h3 id="load-data"><a href="#load-data" class="headerlink" title="load data"></a>load data</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#batch_size = 256  </span></span><br><span class="line"><span class="comment">##cpu要调小batchsize</span></span><br><span class="line">batch_size=<span class="number">16</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None, root=<span class="string">'/home/kesci/input/FashionMNIST2065'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Download the fashion mnist dataset and then load into memory."""</span></span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.append(torchvision.transforms.Resize(size=resize))</span><br><span class="line">    trans.append(torchvision.transforms.ToTensor())</span><br><span class="line">    </span><br><span class="line">    transform = torchvision.transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<h3 id="train-and-test"><a href="#train-and-test" class="headerlink" title="train and test"></a>train and test</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<h2 id="简化模型"><a href="#简化模型" class="headerlink" title="简化模型"></a>简化模型</h2><blockquote>
<p>nn中有内置的BatchNorm2d（卷积层）和BatchNorm1d（全连接层）</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">在自己应用时不需要写class和function</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">6</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            d2l.FlattenLayer(),</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="残差网络（ResNet）"><a href="#残差网络（ResNet）" class="headerlink" title="残差网络（ResNet）"></a>残差网络（ResNet）</h1><center><b>深层网络能够拟合出的映射就一定能够包含浅层网络拟合出的映射</b></center><br/>

<center><b>但 CNN 模型在建立的时候并不是越深越好</b></center><br/>

<blockquote>
<p>深度学习的问题</p>
</blockquote>
<p>深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，准确率也变得更差。</p>
<h2 id="残差块（Residual-Block）"><a href="#残差块（Residual-Block）" class="headerlink" title="残差块（Residual Block）"></a>残差块（Residual Block）</h2><p><strong>恒等映射：</strong>  </p>
<ul>
<li>左边：$f(x)=x$                                               </li>
<li>右边：$f(x)-x=0$ （易于捕捉恒等映射的细微波动 ; 易于优化）</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw4bGhub3Q0LnBuZw?x-oss-process=image/format,png" alt="Image Name"><br>$$<br>神经网络普通层(left)残差网络(right)<br>$$</p>
<blockquote>
<p>在残差块中，输⼊可通过跨层的数据线路更快 地向前传播。</p>
</blockquote>
<h3 id="残差块实现"><a href="#残差块实现" class="headerlink" title="残差块实现"></a>残差块实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span><span class="params">(nn.Module)</span>:</span>  </span><br><span class="line">    <span class="comment">#可以设定输出通道数、是否使用额外的1x1卷积层来修改通道数以及卷积层的步幅。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, use_1x1conv=False, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(Residual, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=stride)</span><br><span class="line">        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=stride)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y + X)</span><br></pre></td></tr></table></figure>
<h3 id="是否需要-1-times1-卷积层"><a href="#是否需要-1-times1-卷积层" class="headerlink" title="是否需要 $1\times1$ 卷积层"></a>是否需要 $1\times1$ 卷积层</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不需要使用1*1卷积层 输入和输出相同</span></span><br><span class="line">blk = Residual(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">X = torch.rand((<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line">blk(X).shape <span class="comment"># torch.Size([4, 3, 6, 6])</span></span><br><span class="line"><span class="comment">#需要使用</span></span><br><span class="line">blk = Residual(<span class="number">3</span>, <span class="number">6</span>, use_1x1conv=<span class="literal">True</span>, stride=<span class="number">2</span>)</span><br><span class="line">blk(X).shape <span class="comment"># torch.Size([4, 6, 3, 3])</span></span><br></pre></td></tr></table></figure>
<h2 id="ResNet模型"><a href="#ResNet模型" class="headerlink" title="ResNet模型"></a>ResNet模型</h2><h3 id="简化实现"><a href="#简化实现" class="headerlink" title="简化实现"></a>简化实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_block</span><span class="params">(in_channels, out_channels, num_residuals, first_block=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> first_block:</span><br><span class="line">        <span class="keyword">assert</span> in_channels == out_channels <span class="comment"># 第一个模块的通道数同输入通道数一致</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            <span class="comment"># 把in_channels放缩到out_channels的个数</span></span><br><span class="line">            blk.append(Residual(in_channels, out_channels, use_1x1conv=<span class="literal">True</span>, stride=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 保证输入和输出都是out_channels</span></span><br><span class="line">            blk.append(Residual(out_channels, out_channels))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把四个残差block放到net里</span></span><br><span class="line">net.add_module(<span class="string">"resnet_block1"</span>, resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">net.add_module(<span class="string">"resnet_block2"</span>, resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">"resnet_block3"</span>, resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">"resnet_block4"</span>, resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<h3 id="全局平均池化"><a href="#全局平均池化" class="headerlink" title="全局平均池化"></a>全局平均池化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.add_module(<span class="string">"global_avg_pool"</span>, d2l.GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, 512, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="train-and-test-1"><a href="#train-and-test-1" class="headerlink" title="train and test"></a>train and test</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>ResNet 的引申设计</p>
</blockquote>
<h1 id="稠密连接网络（DenseNet）"><a href="#稠密连接网络（DenseNet）" class="headerlink" title="稠密连接网络（DenseNet）"></a>稠密连接网络（DenseNet）</h1><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw4bWk3OHl6LnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<blockquote>
<p>特征：concat 连接</p>
</blockquote>
<h2 id="主要构建模块："><a href="#主要构建模块：" class="headerlink" title="主要构建模块："></a>主要构建模块：</h2><ul>
<li>稠密块（dense block）： 定义了输入和输出是如何连结的。  </li>
<li>过渡层（transition layer）：用来控制通道数，使之不过大。<h2 id="稠密块"><a href="#稠密块" class="headerlink" title="稠密块"></a>稠密块</h2></li>
<li>输出通道数=输入通道数+卷积层个数*卷积输出通道数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(in_channels, out_channels)</span>:</span></span><br><span class="line">    blk = nn.Sequential(nn.BatchNorm2d(in_channels), </span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># num_convs:用了几个卷积层</span></span><br><span class="line">    <span class="comment"># in_channels代表全部输入，但是out_channels不代表全部输出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_convs, in_channels, out_channels)</span>:</span></span><br><span class="line">        super(DenseBlock, self).__init__()</span><br><span class="line">        net = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_convs):</span><br><span class="line">            <span class="comment"># 卷积层输入的通道数</span></span><br><span class="line">            in_c = in_channels + i * out_channels</span><br><span class="line">            net.append(conv_block(in_c, out_channels))</span><br><span class="line">        self.net = nn.ModuleList(net)</span><br><span class="line">        self.out_channels = in_channels + num_convs * out_channels <span class="comment"># 计算输出通道数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            <span class="comment"># concat连接</span></span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)  <span class="comment"># 在通道维上将输入和输出连结</span></span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blk = DenseBlock(<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">Y = blk(X)</span><br><span class="line">Y.shape <span class="comment"># torch.Size([4, 23, 8, 8]) 3+2*10</span></span><br></pre></td></tr></table></figure>
<h2 id="过渡层"><a href="#过渡层" class="headerlink" title="过渡层"></a>过渡层</h2><ul>
<li>$1\times1$卷积层：来减小通道数  </li>
<li>步幅为2的平均池化层：减半高和宽</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span><span class="params">(in_channels, out_channels)</span>:</span></span><br><span class="line">    blk = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(in_channels), </span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line">blk = transition_block(<span class="number">23</span>, <span class="number">10</span>)</span><br><span class="line">blk(Y).shape <span class="comment"># torch.Size([4, 10, 4, 4])</span></span><br></pre></td></tr></table></figure>
<h3 id="DenseNet模型"><a href="#DenseNet模型" class="headerlink" title="DenseNet模型"></a>DenseNet模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 宽高减半</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    <span class="comment"># 宽高减半</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span>  <span class="comment"># num_channels为当前的通道数</span></span><br><span class="line">num_convs_in_dense_blocks = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, num_convs <span class="keyword">in</span> enumerate(num_convs_in_dense_blocks):</span><br><span class="line">    DB = DenseBlock(num_convs, num_channels, growth_rate)</span><br><span class="line">    net.add_module(<span class="string">"DenseBlosk_%d"</span> % i, DB)</span><br><span class="line">    <span class="comment"># 上一个稠密块的输出通道数</span></span><br><span class="line">    num_channels = DB.out_channels</span><br><span class="line">    <span class="comment"># 在稠密块之间加入通道数减半的过渡层</span></span><br><span class="line">    <span class="keyword">if</span> i != len(num_convs_in_dense_blocks) - <span class="number">1</span>:</span><br><span class="line">        net.add_module(<span class="string">"transition_block_%d"</span> % i, transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">        num_channels = num_channels // <span class="number">2</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.add_module(<span class="string">"BN"</span>, nn.BatchNorm2d(num_channels))</span><br><span class="line">net.add_module(<span class="string">"relu"</span>, nn.ReLU())</span><br><span class="line">net.add_module(<span class="string">"global_avg_pool"</span>, d2l.GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, num_channels, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(), nn.Linear(num_channels, <span class="number">10</span>))) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">X = torch.rand((<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"><span class="keyword">for</span> name, layer <span class="keyword">in</span> net.named_children():</span><br><span class="line">    X = layer(X)</span><br><span class="line">    print(name, <span class="string">' output shape:\t'</span>, X.shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#batch_size = 256</span></span><br><span class="line">batch_size=<span class="number">16</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line">train_iter, test_iter =load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>LeNet &amp;&amp; ModernCNN</title>
    <url>/2020/02/28/LeNet-&amp;&amp;-ModernCNN/</url>
    <content><![CDATA[<h1 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h1><blockquote>
<p>学而习之：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104380762" target="_blank" rel="noopener">Fundamentals of Convolutional Neural Networks</a></p>
</blockquote>
<p>使用全连接层的局限性：</p>
<ul>
<li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li>
<li>对于大尺寸的输入图像，使用全连接层容易导致模型过大。</li>
</ul>
<p>使用卷积层的优势：</p>
<ul>
<li>卷积层保留输入形状。</li>
<li>卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</li>
</ul>
<p><strong>卷积神经网络就是含卷积层的网络。</strong></p>
<h2 id="LeNet-模型"><a href="#LeNet-模型" class="headerlink" title="LeNet 模型"></a>LeNet 模型</h2><blockquote>
<p>90%以上的参数都在全连接层块</p>
</blockquote>
<p>LeNet分为卷积层块和全连接层块两个部分<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5kd3Ntc2FvLnBuZw?x-oss-process=image/format,png" alt="Image Name"><br><strong>解释：</strong><br>&ensp;&ensp;&ensp;&ensp;卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5 \times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。</p>
<p>&ensp;&ensp;&ensp;&ensp;全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p>
<blockquote>
<p>卷积层块里的基本单位</p>
<blockquote>
<p>是卷积层后接平均池化层</p>
</blockquote>
<p>卷积层用来识别图像里的空间模式，如线条和物体局部<br/><br>之后的平均池化层则用来降低卷积层对位置的敏感性。</p>
</blockquote>
<p><strong>LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类</strong></p>
<h3 id="通过-Sequential-类实现-LeNet-模型"><a href="#通过-Sequential-类实现-LeNet-模型" class="headerlink" title="通过 Sequential 类实现 LeNet 模型"></a>通过 Sequential 类实现 LeNet 模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#import</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to FashionMNIST2065"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#net</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Flatten</span><span class="params">(torch.nn.Module)</span>:</span>  <span class="comment">#展平操作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Reshape</span><span class="params">(torch.nn.Module)</span>:</span> <span class="comment">#将图像大小重定型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x.view(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)      <span class="comment">#(B x C x H x W)</span></span><br><span class="line"></span><br><span class="line">net = torch.nn.Sequential(     <span class="comment">#Lelet                                                  </span></span><br><span class="line">    Reshape(),  </span><br><span class="line">    <span class="comment"># 公式：[(nh-kh+ph+sh)/sh]*[(nw-kw+pw+sw)/sw]</span></span><br><span class="line">    nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), <span class="comment">#b*1*28*28  =&gt;b*6*28*28</span></span><br><span class="line">    nn.Sigmoid(),     </span><br><span class="line">    <span class="comment"># 平均池化</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),                     <span class="comment">#b*6*28*28  =&gt;b*6*14*14</span></span><br><span class="line">    nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>),  <span class="comment">#b*6*14*14  =&gt;b*16*10*10</span></span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),                     <span class="comment">#b*16*10*10  =&gt; b*16*5*5</span></span><br><span class="line">    <span class="comment"># 展平</span></span><br><span class="line">    Flatten(),                                                 <span class="comment">#b*16*5*5   =&gt; b*400</span></span><br><span class="line">    <span class="comment"># 三个全连接层</span></span><br><span class="line">    nn.Linear(in_features=<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, out_features=<span class="number">120</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(</span><br><span class="line">    batch_size=batch_size, root=<span class="string">'path to FashionMNIST2065'</span>)</span><br><span class="line"><span class="comment"># 训练集批次数</span></span><br><span class="line">print(len(train_iter))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">额外的数据表示，以图像形式显示</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment">#数据展示</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># define drawing function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_fashion_mnist</span><span class="params">(images, labels)</span>:</span></span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    <span class="comment"># 这里的_表示我们忽略（不使用）的变量</span></span><br><span class="line">    _, figs = plt.subplots(<span class="number">1</span>, len(images), figsize=(<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> f, img, lbl <span class="keyword">in</span> zip(figs, images, labels):</span><br><span class="line">        f.imshow(img.view((<span class="number">28</span>, <span class="number">28</span>)).numpy())</span><br><span class="line">        f.set_title(lbl)</span><br><span class="line">        f.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        f.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> Xdata,ylabel <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">X, y = [], []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(Xdata[i].shape,ylabel[i].numpy())</span><br><span class="line">    X.append(Xdata[i]) <span class="comment"># 将第i个feature加到X中</span></span><br><span class="line">    y.append(ylabel[i].numpy()) <span class="comment"># 将第i个label加到y中</span></span><br><span class="line">show_fashion_mnist(X, y)</span><br></pre></td></tr></table></figure>
<hr>
<p>因为卷积神经网络计算比多层感知机要复杂，建议使用GPU来加速计算。我们查看看是否可以用GPU，如果成功则使用 cuda:0，否则仍然使用 cpu。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># This function has been saved in the d2l package for future use</span></span><br><span class="line"><span class="comment">#use GPU</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""If GPU is available, return torch.device as cuda:0; else return torch.device as cpu."""</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        device = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        device = torch.device(<span class="string">'cpu'</span>)</span><br><span class="line">    <span class="keyword">return</span> device</span><br><span class="line"></span><br><span class="line">device = try_gpu()</span><br><span class="line">device</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h4 id="计算准确率"><a href="#计算准确率" class="headerlink" title="计算准确率"></a>计算准确率</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">(1). net.train()</span></span><br><span class="line"><span class="string">  启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为True</span></span><br><span class="line"><span class="string">(2). net.eval()</span></span><br><span class="line"><span class="string">不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">:param:data_iter:测试集</span></span><br><span class="line"><span class="string">:param:acc_sum:模型预测正确的总数</span></span><br><span class="line"><span class="string">:param:n:预测总数</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net,device=torch.device<span class="params">(<span class="string">'cpu'</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Evaluate accuracy of a model on the given data set."""</span></span><br><span class="line">    acc_sum,n = torch.tensor([<span class="number">0</span>],dtype=torch.float32,device=device),<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="comment"># If device is the GPU, copy the data to the GPU.</span></span><br><span class="line">        <span class="comment"># 把tensorc传到device中</span></span><br><span class="line">        X,y = X.to(device),y.to(device)</span><br><span class="line">        <span class="comment"># 网络正在进行预测</span></span><br><span class="line">        net.eval()</span><br><span class="line">        该区域涉及到的数据不需要计算梯度，也不进行反向传播</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            y = y.long()</span><br><span class="line">            <span class="comment"># 将一个批次的训练数据X通过网络模型net输出</span></span><br><span class="line">            <span class="comment"># 经过argmax得到预测值</span></span><br><span class="line">            <span class="comment"># dim维度选择为1</span></span><br><span class="line">            acc_sum += torch.sum((torch.argmax(net(X), dim=<span class="number">1</span>) == y))  <span class="comment">#[[0.2 ,0.4 ,0.5 ,0.6 ,0.8] ,[ 0.1,0.2 ,0.4 ,0.3 ,0.1]] =&gt; [ 4 , 2 ]</span></span><br><span class="line">            <span class="comment"># 预测的总数相加</span></span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 预测准确率</span></span><br><span class="line">    <span class="keyword">return</span> acc_sum.item()/n</span><br></pre></td></tr></table></figure>
<h4 id="训练函数"><a href="#训练函数" class="headerlink" title="训练函数"></a>训练函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch5</span><span class="params">(net, train_iter, test_iter,criterion, num_epochs, batch_size, device,lr=None)</span>:</span></span><br><span class="line">    <span class="string">"""Train and evaluate a model with CPU or GPU."""</span></span><br><span class="line">    print(<span class="string">'training on'</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum = torch.tensor([<span class="number">0.0</span>],dtype=torch.float32,device=device)</span><br><span class="line">        train_acc_sum = torch.tensor([<span class="number">0.0</span>],dtype=torch.float32,device=device)</span><br><span class="line">        n, start = <span class="number">0</span>, time.time()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            net.train()</span><br><span class="line">            <span class="comment"># 梯度清零 不同批次的梯度不相关</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X,y = X.to(device),y.to(device) </span><br><span class="line">            <span class="comment"># 预测值</span></span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            loss = criterion(y_hat, y)</span><br><span class="line">            <span class="comment"># 梯度回传</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                y = y.long()</span><br><span class="line">                train_l_sum += loss.float()</span><br><span class="line">                <span class="comment"># 训练集中预测正确的总数</span></span><br><span class="line">                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=<span class="number">1</span>) == y))).float()</span><br><span class="line">                n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net,device)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '</span></span><br><span class="line">              <span class="string">'time %.1f sec'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum/n, train_acc_sum/n, test_acc,</span><br><span class="line">                 time.time() - start))</span><br></pre></td></tr></table></figure>
<h4 id="训练进程"><a href="#训练进程" class="headerlink" title="训练进程"></a>训练进程</h4><p>模型参数初始化到 device 中，并使用 Xavier 随机初始化。损失函数和训练算法则依然使用交叉熵损失函数和小批量随机梯度下降。</p>
<blockquote>
<p>Xavier 随机初始化 —参考<a href="https://blog.csdn.net/RokoBasilisk/article/details/104349123" target="_blank" rel="noopener">学而后思,方能发展;思而立行,终将卓越</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练 学习率0.9</span></span><br><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear <span class="keyword">or</span> type(m) == nn.Conv2d:</span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line">net = net.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()   <span class="comment">#交叉熵描述了两个概率分布之间的距离，交叉熵越小说明两者之间越接近</span></span><br><span class="line">train_ch5(net, train_iter, test_iter, criterion,num_epochs, batch_size,device, lr)</span><br></pre></td></tr></table></figure>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line"><span class="keyword">for</span> testdata,testlabe <span class="keyword">in</span> test_iter:</span><br><span class="line">    testdata,testlabe = testdata.to(device),testlabe.to(device)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">print(testdata.shape,testlabe.shape)</span><br><span class="line">net.eval()</span><br><span class="line">y_pre = net(testdata)</span><br><span class="line"><span class="comment"># 预测值</span></span><br><span class="line">print(torch.argmax(y_pre,dim=<span class="number">1</span>)[:<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 真实标签</span></span><br><span class="line">print(testlabe[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<h2 id="深度卷积神经网络（AlexNet）"><a href="#深度卷积神经网络（AlexNet）" class="headerlink" title="深度卷积神经网络（AlexNet）"></a>深度卷积神经网络（AlexNet）</h2><blockquote>
<p>2014年ImgNet竞赛中</p>
<blockquote>
<p>证明了学习到的特征可以超过手工设计的特征, 打破计算机视觉研究的前状</p>
</blockquote>
</blockquote>
<center><b><font size=5>知缺陷方能进步</font></b></center><br/>

<blockquote>
<p>LeNet:  在大的真实数据集上的表现并不尽如⼈意。     </p>
<blockquote>
<p>1.神经网络计算复杂。  <br/><br>2.还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。  </p>
</blockquote>
</blockquote>
<p>在此之后，针对特征的选择分为两派：</p>
<ul>
<li>机器学习的特征提取:手工定义的特征提取函数  </li>
<li>神经网络的特征提取：通过学习得到数据的多级表征，并逐级表⽰越来越抽象的概念或模式。  </li>
</ul>
<p>神经网络发展的限制:数据、硬件</p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><blockquote>
<p>设计理念核Lenet相似</p>
</blockquote>
<p><strong>特征：</strong></p>
<ol>
<li>8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。</li>
<li>将sigmoid激活函数改成了更加简单的ReLU激活函数。</li>
<li>用Dropout来控制全连接层的模型复杂度。</li>
<li>引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解泛化能力不好导致的过拟合。</li>
</ol>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWt2NGdweDg4LnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>数据集.MINIST(Left) IMAGENET(Right)<br>$$</p>
<blockquote>
<p>利用padding 的作用：使得输入和输出的形状相同<br/><br>可以参考 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104380762" target="_blank" rel="noopener">Fundamentals of Convolutional Neural Networks</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to FashionMNIST2065"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="实现-AlexNet-模型"><a href="#实现-AlexNet-模型" class="headerlink" title="实现 AlexNet 模型"></a>实现 AlexNet 模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            <span class="comment"># 默认不做padding</span></span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment">#由于使用CPU镜像，精简网络，若为GPU镜像可添加该层</span></span><br><span class="line">            <span class="comment">#nn.Linear(4096, 4096),</span></span><br><span class="line">            <span class="comment">#nn.ReLU(),</span></span><br><span class="line">            <span class="comment">#nn.Dropout(0.5),</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, img)</span>:</span></span><br><span class="line"></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], <span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h4 id="载入数据"><a href="#载入数据" class="headerlink" title="载入数据"></a>载入数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None, root=<span class="string">'/home/kesci/input/FashionMNIST2065'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Download the fashion mnist dataset and then load into memory."""</span></span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.append(torchvision.transforms.Resize(size=resize))</span><br><span class="line">    trans.append(torchvision.transforms.ToTensor())</span><br><span class="line">    </span><br><span class="line">    transform = torchvision.transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"><span class="comment">#batchsize=128</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size,<span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">    print(<span class="string">'X ='</span>, X.shape,</span><br><span class="line">        <span class="string">'\nY ='</span>, Y.type(torch.int32))</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">3</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<h2 id="使用重复元素的网络（VGG）"><a href="#使用重复元素的网络（VGG）" class="headerlink" title="使用重复元素的网络（VGG）"></a>使用重复元素的网络（VGG）</h2><center><b><font size=5>知缺陷方能进步</font></b></center><br/>

<blockquote>
<p>AlxNet</p>
<blockquote>
<p>并没有提供简单的规则来制造新的网络</p>
</blockquote>
<p>结构比较死板</p>
</blockquote>
<p><strong>所以</strong> VGG：通过重复使⽤简单的基础块来构建深度模型。  </p>
<p>Block:数个相同的填充为1、窗口形状为$3\times 3$的卷积层,接上一个步幅为2、窗口形状为$2\times 2$的最大池化层。  </p>
<p>卷积层保持输入的高和宽不变，而池化层则对其减半</p>
<p><img src="https://img-blog.csdnimg.cn/20200218210146126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="VGG11的实现"><a href="#VGG11的实现" class="headerlink" title="VGG11的实现"></a>VGG11的实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以修改的参数 e每个vgg_block结构相同但是参数可能不相同</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span><span class="params">(num_convs, in_channels, out_channels)</span>:</span> <span class="comment">#卷积层个数，输入通道数，输出通道数</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_convs):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">    blk.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)) <span class="comment"># 这里会使宽高减半</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">64</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">128</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">256</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line"><span class="comment"># 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7</span></span><br><span class="line">fc_features = <span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span> <span class="comment"># c * w * h</span></span><br><span class="line">fc_hidden_units = <span class="number">4096</span> <span class="comment"># 任意</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vgg模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg</span><span class="params">(conv_arch, fc_features, fc_hidden_units=<span class="number">4096</span>)</span>:</span></span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> i, (num_convs, in_channels, out_channels) <span class="keyword">in</span> enumerate(conv_arch):</span><br><span class="line">        <span class="comment"># 每经过一个vgg_block都会使宽高减半</span></span><br><span class="line">        net.add_module(<span class="string">"vgg_block_"</span> + str(i+<span class="number">1</span>), vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">    <span class="comment"># 全连接层部分</span></span><br><span class="line">    net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(),</span><br><span class="line">                                 nn.Linear(fc_features, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, <span class="number">10</span>)</span><br><span class="line">                                ))</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = vgg(conv_arch, fc_features, fc_hidden_units)</span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)</span></span><br><span class="line"><span class="keyword">for</span> name, blk <span class="keyword">in</span> net.named_children(): </span><br><span class="line">    X = blk(X)</span><br><span class="line">    print(name, <span class="string">'output shape: '</span>, X.shape)</span><br><span class="line">ratio = <span class="number">8</span></span><br><span class="line"><span class="comment"># 减小vgg结构，针对minist数据集较小，数据少参数多容易造成过拟合</span></span><br><span class="line">small_conv_arch = [(<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>//ratio), (<span class="number">1</span>, <span class="number">64</span>//ratio, <span class="number">128</span>//ratio), (<span class="number">2</span>, <span class="number">128</span>//ratio, <span class="number">256</span>//ratio), </span><br><span class="line">                   (<span class="number">2</span>, <span class="number">256</span>//ratio, <span class="number">512</span>//ratio), (<span class="number">2</span>, <span class="number">512</span>//ratio, <span class="number">512</span>//ratio)]</span><br><span class="line">net = vgg(small_conv_arch, fc_features // ratio, fc_hidden_units // ratio)</span><br><span class="line">print(net)</span><br><span class="line"></span><br><span class="line">batchsize=<span class="number">16</span></span><br><span class="line"><span class="comment">#batch_size = 64</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line"><span class="comment"># train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<h2 id="网络中的网络（NiN）"><a href="#网络中的网络（NiN）" class="headerlink" title="网络中的网络（NiN）"></a>网络中的网络（NiN）</h2><center><b><font size=5>知缺陷方能进步</font></b></center><br/>

<blockquote>
<p>LeNet、AlexNet和VGG</p>
<blockquote>
<p>先以由卷积层构成的模块充分抽取 空间特征<br/><br>再以由全连接层构成的模块来输出分类结果</p>
</blockquote>
</blockquote>
<blockquote>
<p>NiN</p>
<blockquote>
<p>串联多个由卷积层和 “全连接” 层构成的小⽹络来构建⼀个深层⽹络。  </p>
</blockquote>
<p>NiN去掉了全连接层，而是用平均池化层</p>
</blockquote>
<p>⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。  这样的设计显著的<strong>减少了参数尺寸防止过拟合</strong>，但是<strong>增加了训练时间</strong></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw2dTFwNXZ5LnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<hr>
<p>$1\times1$卷积核作用:</p>
<ol>
<li>放缩通道数：通过控制卷积核的数量达到通道数的放缩。  </li>
<li>增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。  </li>
<li>计算参数少   </li>
</ol>
<hr>
<h3 id="NiN-的实现"><a href="#NiN-的实现" class="headerlink" title="NiN 的实现"></a>NiN 的实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建组成模块nin_block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nin_block</span><span class="params">(in_channels, out_channels, kernel_size, stride, padding)</span>:</span></span><br><span class="line">    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU())</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalAvgPool2d</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(GlobalAvgPool2d, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.avg_pool2d(x, kernel_size=x.size()[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), </span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    GlobalAvgPool2d(), </span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小, 10)</span></span><br><span class="line">    d2l.FlattenLayer())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line"><span class="comment">#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)</span></span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.002</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>NiN</p>
<blockquote>
<ul>
<li>NiN重复使⽤由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层⽹络 ;  </li>
<li>NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数 的NiN块和全局平均池化层 ;</li>
<li>NiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计 ;</li>
</ul>
</blockquote>
</blockquote>
<h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><blockquote>
<p>牺牲了串联网络的思想</p>
</blockquote>
<ol>
<li>由 Inception 基础块组成。  </li>
<li>Inception 块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。   </li>
<li>可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 </li>
<li>使用 padding 来保证输入输出形状相同<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw2dW9ydHcucG5n?x-oss-process=image/format,png" alt="Image Name"><h3 id="Inception-基础块实现"><a href="#Inception-基础块实现" class="headerlink" title="Inception 基础块实现"></a>Inception 基础块实现</h3></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># c1 - c4为每条线路里的层的输出通道数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, c1, c2, c3, c4)</span>:</span></span><br><span class="line">        super(Inception, self).__init__()</span><br><span class="line">        <span class="comment"># 线路1，单1 x 1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1 x 1卷积层后接3 x 3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_c, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1 x 1卷积层后接5 x 5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_c, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3 x 3最大池化层后接1 x 1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)  <span class="comment"># 在通道维上连结输出</span></span><br></pre></td></tr></table></figure>

<h3 id="完整模型结构"><a href="#完整模型结构" class="headerlink" title="完整模型结构"></a>完整模型结构</h3><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw2eDBmeXluLnBuZw?x-oss-process=image/format,png" /></center> 

<p>$$<br>input -1\times96\times96<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取特征来减小大小</span></span><br><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   d2l.GlobalAvgPool2d())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, </span><br><span class="line">                    d2l.FlattenLayer(), nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, d2l.FlattenLayer(), nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net.children(): </span><br><span class="line">    X = blk(X)</span><br><span class="line">    print(<span class="string">'output shape: '</span>, X.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#batchsize=128</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line"><span class="comment">#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)</span></span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Fundamentals of Convolutional Neural Networks</title>
    <url>/2020/02/28/Fundamentals-of-Convolutional-Neural-Networks/</url>
    <content><![CDATA[<h2 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h2><blockquote>
<p>常用于处理图像数据。</p>
</blockquote>
<h3 id="二维互相关运算"><a href="#二维互相关运算" class="headerlink" title="二维互相关运算"></a>二维互相关运算</h3><p>二维互相关（cross-correlation）运算的输入是一个二维输入数组和一个二维核（kernel）数组，输出也是一个二维数组，其中核数组通常称为卷积核或过滤器（filter）。卷积核的尺寸通常小于输入数组，卷积核在输入数组上滑动，在每个位置上，卷积核与该位置处的输入子数组按元素相乘并求和，得到输出数组中相应位置的元素。图1展示了一个互相关运算的例子，阴影部分分别是输入的第一个计算区域、核数组以及对应的输出。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mZGJoY3c1LnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图1-二维互相关运算<br>$$</p>
<blockquote>
<p>用 corr2d 函数实现二维互相关运算</p>
<blockquote>
<p>它接受输入数组 X 与核数组 K，并输出数组 Y。</p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 做二维互相关运算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    H, W = X.shape</span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros(H - h + <span class="number">1</span>, W - w + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<p><strong>验证：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造上图中的输入数组 X 、核数组 K 来验证二维互相关运算的输出</span></span><br><span class="line">X = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">Y = corr2d(X, K)</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure>
<p><strong>结果：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">19.</span>, <span class="number">25.</span>],</span><br><span class="line">        [<span class="number">37.</span>, <span class="number">43.</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="二维卷积层-1"><a href="#二维卷积层-1" class="headerlink" title="二维卷积层"></a>二维卷积层</h3><p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的模型参数包括卷积核和标量偏置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kernel_size)</span>:</span></span><br><span class="line">        super(Conv2D, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure>
<p><strong>For Example:</strong></p>
<p>构造一张$6 \times 8$的图像，中间4列为黑（0），其余为白（1），希望检测到颜色边缘。我们的标签是一个$6 \times 7$的二维数组，第2列是1（从1到0的边缘），第6列是-1（从0到1的边缘）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.ones(<span class="number">6</span>, <span class="number">8</span>)</span><br><span class="line">Y = torch.zeros(<span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line">X[:, <span class="number">2</span>: <span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">Y[:, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">Y[:, <span class="number">5</span>] = <span class="number">-1</span></span><br><span class="line">print(X)</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure>
<p><strong>结果：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<p>我们希望学习一个$1 \times 2$卷积层，通过卷积层来检测颜色边缘。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1*2的二维卷积层</span></span><br><span class="line">conv2d = Conv2D(kernel_size=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">step = <span class="number">30</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(step):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = ((Y_hat - Y) ** <span class="number">2</span>).sum()</span><br><span class="line">    <span class="comment"># 后向计算得到梯度</span></span><br><span class="line">    l.backward()</span><br><span class="line">    <span class="comment"># 梯度下降</span></span><br><span class="line">    <span class="comment"># 参数值减去学习率并乘以梯度</span></span><br><span class="line">    conv2d.weight.data -= lr * conv2d.weight.grad</span><br><span class="line">    conv2d.bias.data -= lr * conv2d.bias.grad</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度清零</span></span><br><span class="line">    conv2d.weight.grad.zero_()</span><br><span class="line">    conv2d.bias.grad.zero_()</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step %d, loss %.3f'</span> % (i + <span class="number">1</span>, l.item()))</span><br><span class="line"><span class="comment"># 卷积核</span></span><br><span class="line">print(conv2d.weight.data)</span><br><span class="line"><span class="comment"># 偏置</span></span><br><span class="line">print(conv2d.bias.data)</span><br></pre></td></tr></table></figure>
<h3 id="互相关运算与卷积运算"><a href="#互相关运算与卷积运算" class="headerlink" title="互相关运算与卷积运算"></a>互相关运算与卷积运算</h3><p>卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。我们将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，给定核数组，对于核数组中每一个元素，构建一个从核数组中元素到输入数组元素之间的对应关系，再进行相加求和，本质是一样的。所以使用互相关运算与使用卷积运算并无本质区别。</p>
<h3 id="特征图与感受野"><a href="#特征图与感受野" class="headerlink" title="特征图与感受野"></a>特征图与感受野</h3><p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素 $x$ 的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做 $x$ 的感受野（receptive field）。</p>
<blockquote>
<p>解释:</p>
<blockquote>
<p>输出是一个特征图，对于19来说，其感受野就是0 1 3 4</p>
</blockquote>
<blockquote>
<p>如果引入一个$2\times 2$的新的卷积核，核输出做互相关运算，得到一个$1\times1$的输出，感受野就是前面的所有元素</p>
</blockquote>
</blockquote>
<p>以图1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为$2 \times 2$的输出记为$Y$，将$Y$与另一个形状为$2 \times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。</p>
<h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>我们介绍卷积层的两个超参数，即填充和步幅，它们可以对给定形状的输入和卷积核改变输出形状。</p>
<h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），图2里我们在原输入高和宽的两侧分别添加了值为0的元素。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mbDZlank0LnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图2-在输入的高和宽两侧分别填充了0元素的二维互相关计算<br>$$</p>
<p>如果原输入的高和宽是$n_h$和$n_w$，卷积核的高和宽是$k_h$和$k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，则输出形状为：</p>
<p>$$<br>(n_h+p_h-k_h+1)\times(n_w+p_w-k_w+1)<br>$$</p>
<blockquote>
<p>在卷积神经网络中使用奇数高宽的核<br/><br>比如$3 \times 3$，$5 \times 5$的卷积核，对于高度（或宽度）为大小为 $2 k + 1$ 的核，令步幅为1，在高（或宽）两侧选择大小为$k$的填充，便可保持输入与输出尺寸相同。</p>
</blockquote>
<h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><p>在互相关运算中，卷积核在输入数组上滑动，每次滑动的行数与列数即是步幅（stride）。此前我们使用的步幅都是1，图3展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mbG9obnFnLnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图3-高和宽上步幅分别为3和2的二维互相关运算<br>$$</p>
<p>一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：</p>
<p>$$<br>\lfloor(n_h+p_h-k_h+s_h)/s_h\rfloor \times \lfloor(n_w+p_w-k_w+s_w)/s_w\rfloor<br>$$</p>
<center><img src="https://img-blog.csdnimg.cn/20200218162515171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>$$<br>解释图<br>$$</p>
<p>找后续卷积核可以覆盖到的区域只需要关注最后一个元素，看最后一个元素在输入上找到几个位置，要做的就是往下移动 $s_{h}$ ,卷积核的最后一个元素下一个位置就找到了</p>
<p>如果$p_h=k_h-1$，$p_w=k_w-1$，那么输出形状将简化为$\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h / s_h) \times (n_w/s_w)$。</p>
<p>当$p_h = p_w = p$时，我们称填充为$p$；当$s_h = s_w = s$时，我们称步幅为$s$。</p>
<h2 id="多输入通道和多输出通道"><a href="#多输入通道和多输出通道" class="headerlink" title="多输入通道和多输出通道"></a>多输入通道和多输出通道</h2><p>之前的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3 \times h \times w$的多维数组，将大小为3的这一维称为通道（channel）维。</p>
<h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p>卷积层的输入可以包含多个通道，图4展示了一个含2个输入通道的二维互相关计算的例子。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mbWRud2JxLnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图4-含2个输入通道的互相关计算<br>$$</p>
<p>假设输入数据的通道数为$c_i$，卷积核形状为$k_h\times k_w$，我们为每个输入通道各分配一个形状为$k_h\times k_w$的核数组，将$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组作为输出。我们把$c_i$个核数组在通道维上连结，即得到一个形状为$c_i\times k_h\times k_w$的卷积核。</p>
<h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3><p>卷积层的输出也可以包含多个通道，设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\times k_h\times k_w$的核数组，将它们在输出通道维上连结，卷积核的形状即$c_o\times c_i\times k_h\times k_w$。</p>
<p>对于输出通道的卷积核，我们提供这样一种理解，一个$c_i \times k_h \times k_w$的核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个这样的$c_i \times k_h \times k_w$的核数组，不同的核数组提取的是不同的特征。</p>
<h3 id="1x1卷积层"><a href="#1x1卷积层" class="headerlink" title="1x1卷积层"></a>1x1卷积层</h3><p>最后讨论形状为$1 \times 1$的卷积核，我们通常称这样的卷积运算为$1 \times 1$卷积，称包含这种卷积核的卷积层为$1 \times 1$卷积层。图5展示了使用输入通道数为3、输出通道数为2的$1\times 1$卷积核的互相关计算。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mbXE5ODByLnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图5-1\times1卷积核的互相关计算。输入和输出具有相同的高和宽<br>$$</p>
<p>$1 \times 1$卷积核可在不改变高宽的情况下，调整通道数。$1 \times 1$卷积核不识别高和宽维度上相邻元素构成的模式，其主要计算发生在通道维上。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么$1\times 1$卷积层的作用与全连接层等价。</p>
<h2 id="卷积层与全连接层的对比"><a href="#卷积层与全连接层的对比" class="headerlink" title="卷积层与全连接层的对比"></a>卷积层与全连接层的对比</h2><ul>
<li><p>全连接层做图像分类</p>
</li>
<li><p>二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：</p>
</li>
<li><p>一是全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。</p>
</li>
<li><p>二是卷积层的参数量更少。不考虑偏置的情况下，一个形状为$(c_i, c_o, h, w)$的卷积核的参数量是$c_i \times c_o \times h \times w$，与输入图像的宽高无关。假如一个卷积层的输入和输出形状分别是$(c_1, h_1, w_1)$和$(c_2, h_2, w_2)$，如果要用全连接层进行连接，参数数量就是$c_1 \times c_2 \times h_1 \times w_1 \times h_2 \times w_2$。使用卷积层可以以较少的参数数量来处理更大的图像。</p>
</li>
</ul>
<h2 id="卷积层的简洁实现"><a href="#卷积层的简洁实现" class="headerlink" title="卷积层的简洁实现"></a>卷积层的简洁实现</h2><p>我们使用Pytorch中的 nn.Conv2d 类来实现二维卷积层</p>
<p>forward 函数的参数为一个四维张量，形状为$(N, C_{in}, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C_{out}, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。</p>
<p><strong>代码讲解：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">print(X.shape)</span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">2</span>, out_channels=<span class="number">3</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), stride=<span class="number">1</span>, padding=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">Y = conv2d(X)</span><br><span class="line">print(<span class="string">'Y.shape: '</span>, Y.shape)</span><br><span class="line">print(<span class="string">'weight.shape: '</span>, conv2d.weight.shape)</span><br><span class="line">print(<span class="string">'bias.shape: '</span>, conv2d.bias.shape)</span><br></pre></td></tr></table></figure>
<h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><h3 id="二维池化层"><a href="#二维池化层" class="headerlink" title="二维池化层"></a>二维池化层</h3><p>池化层有参与模型的正向计算，同样也会参与反向传播</p>
<p>池化层直接对窗口内的元素求最大值或平均值，并没有模型参数参与计算，所以没有模型参数</p>
<p>池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做最大池化或平均池化。图6展示了池化窗口形状为$2\times 2$的最大池化。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mb2Izb2RvLnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>池化窗口形状为 2 \times 2 的最大池化<br>$$</p>
<p>二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$p \times q$的池化层称为$p \times q$池化层，其中的池化运算叫作$p \times q$池化。</p>
<p>池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。</p>
<p>在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的输出通道数与输入通道数相等。</p>
<h3 id="池化层的简洁实现"><a href="#池化层的简洁实现" class="headerlink" title="池化层的简洁实现"></a>池化层的简洁实现</h3><p>我们使用Pytorch中的 nn.MaxPool2d 实现最大池化层</p>
<p>forward 函数的参数为一个四维张量，形状为$(N, C, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。</p>
<p>代码讲解</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">32</span>, dtype=torch.float32).view(<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">pool2d = nn.MaxPool2d(kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">Y = pool2d(X)</span><br><span class="line">print(X)</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>平均池化层使用的是 nn.AvgPool2d，使用方法与 nn.MaxPool2d 相同。</p>
</blockquote>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>注意力机制和Seq2seq模型</title>
    <url>/2020/02/28/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8CSeq2seq%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<center><b><font size=6>Attention Mechanism</font></b></center><br/>

<blockquote>
<p>注意力机制借鉴了人类的注意力思维方式，以获得需要重点关注的目标区域</p>
</blockquote>
<p>&ensp;&ensp;&ensp;&ensp;在 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">编码器—解码器（seq2seq)</a> 中，解码器在各个时间步依赖相同的背景变量（context vector）来获取输⼊序列信息。解码器输入的语境向量(context vector)不同，每个位置都会计算各自的 attention 输出。 当编码器为循环神经⽹络时，背景变量来⾃它最终时间步的隐藏状态。将源序列输入信息以循环单位状态编码，然后将其传递给解码器以生成目标序列。</p>
<p>&ensp;&ensp;&ensp;&ensp;然而这种结构存在着问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。与此同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。例如，当把 “Hello world” 翻译成 “Bonjour le monde” 时，“Hello” 映射成 “Bonjour”，“world” 映射成  “monde”。</p>
<p>&ensp;&ensp;&ensp;&ensp;在 seq2seq 模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWttNGR3Z2Y5LlBORw?x-oss-process=image/format,png" alt="Image Name"></p>
<h2 id="注意力机制框架"><a href="#注意力机制框架" class="headerlink" title="注意力机制框架"></a>注意力机制框架</h2><p>Attention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。 $k_i∈R^{d_k}, v_i∈R^{d_v}$. Query  $q∈R^{d_q}$ ,  attention layer 得到输出与value的维度一致 $o∈R^{d_v}$.  对于一个query来说，attention layer 会与每一个 key 计算注意力分数并进行权重的归一化，输出的向量 $o$ 则是 value 的加权求和，而每个 key 计算的权重与 value 一一对应。</p>
<p>为了计算输出，我们首先假设有一个函数$\alpha$ 用于计算query和key的相似性，然后可以计算所有的 attention scores $a_1, \ldots, a_n$  by</p>
<p>$$<br>a_i = \alpha(\mathbf q, \mathbf k_i).<br>$$</p>
<p>我们使用 softmax 函数 获得注意力权重：</p>
<p>$$<br>b_1, \ldots, b_n = \textrm{softmax}(a_1, \ldots, a_n).<br>$$</p>
<p>最终的输出就是 value 的加权求和：</p>
<p>$$<br>\mathbf o = \sum_{i=1}^n b_i \mathbf v_i.<br>$$</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWttNG9veXUyLlBORw?x-oss-process=image/format,png" /></center>

<blockquote>
<p>不同的 attetion layer 的区别在于 score 函数的选择</p>
</blockquote>
<p>接下来将利用[机器翻译及其相关技术介绍]一文中的(<a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104367653</a>)</p>
<h2 id="介绍两个常用的注意层"><a href="#介绍两个常用的注意层" class="headerlink" title="介绍两个常用的注意层"></a>介绍两个常用的注意层</h2><blockquote>
<ul>
<li>Dot-product Attention <br/></li>
<li>Multilayer Perceptron Attention</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># import dataset</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.listdir(<span class="string">'path to storaged file of dataset'</span>)</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>工具1:</strong> Masked Softmax</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 排除padding位置的影响</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">-1e6</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\n',X_len[:, None] )</span></span><br><span class="line">    <span class="comment"># shape as same as X</span></span><br><span class="line">    mask = torch.arange((maxlen),dtype=torch.float)[<span class="literal">None</span>, :] &gt;= X_len[:, <span class="literal">None</span>]   </span><br><span class="line">    <span class="comment">#print(mask)</span></span><br><span class="line">    X[mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span><span class="params">(X, valid_length)</span>:</span></span><br><span class="line">    <span class="comment"># X: 3-D tensor, valid_length: 1-D or 2-D tensor</span></span><br><span class="line">    softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> softmax(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_length.dim() == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = valid_length.reshape((<span class="number">-1</span>,))</span><br><span class="line">        <span class="comment"># fill masked elements with a large negative, whose exp is 0</span></span><br><span class="line">        X = SequenceMask(X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>])), valid_length)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> softmax(X).reshape(shape)</span><br></pre></td></tr></table></figure>

<p><strong>工具2：</strong> 超出2维矩阵的乘法</p>
<p>$X$ 和 $Y$ 是维度分别为$(b,n,m)$ 和$(b, m, k)$的张量，进行 $b$ 次二维矩阵乘法后得到 $Z$, 维度为 $(b, n, k)$。</p>
<p>$$<br> Z[i,:,:] = dot(X[i,:,:], Y[i,:,:])\qquad for\ i= 1,…,n\ .<br>$$</p>
<hr>
<h3 id="Dot-Product-Attention"><a href="#Dot-Product-Attention" class="headerlink" title="Dot Product Attention"></a>Dot Product Attention</h3><p>The dot product 假设query和keys有相同的维度, 即 $\forall i, q,k_i ∈ R_d$. 通过计算 query 和 key 转置的乘积来计算 attention score ,通常还会除去 $\sqrt{d}$ 减少计算出来的 score 对维度 𝑑 的依赖性，如下</p>
<p>$$<br>α (q,k)=⟨q,k⟩/ \sqrt{d}<br>$$</p>
<p>假设 $Q∈R^{m×d}$ 有 $m$ 个query，$K∈R^{n×d}$ 有 $n$ 个 keys. 我们可以通过矩阵运算的方式计算所有 $mn$ 个 score：</p>
<p>$$<br>α (Q,K)=QK^T/\sqrt{d}<br>$$</p>
<p>它支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># query: (batch_size, #queries, d)</span></span><br><span class="line">    <span class="comment"># key: (batch_size, #kv_pairs, d)</span></span><br><span class="line">    <span class="comment"># value: (batch_size, #kv_pairs, dim_v)</span></span><br><span class="line">    <span class="comment"># valid_length: either (batch_size, ) or (batch_size, xx)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length=None)</span>:</span></span><br><span class="line">        d = query.shape[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># set transpose_b=True to swap the last two dimensions of key</span></span><br><span class="line">        </span><br><span class="line">        scores = torch.bmm(query, key.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        print(<span class="string">"attention_weight\n"</span>,attention_weights)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure>

<p><strong>测试：</strong></p>
<p>创建了两个批，每个批有一个query和10个key-values对。</p>
<p>通过valid_length指定，对于第一批，只关注前2个键-值对，而对于第二批，检查前6个键-值对</p>
<p>因此，尽管这两个批处理具有相同的查询和键值对，但我们获得的输出是不同的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">atten = DotProductAttention(dropout=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">keys = torch.ones((<span class="number">2</span>,<span class="number">10</span>,<span class="number">2</span>),dtype=torch.float)</span><br><span class="line">values = torch.arange((<span class="number">40</span>), dtype=torch.float).view(<span class="number">1</span>,<span class="number">10</span>,<span class="number">4</span>).repeat(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">atten(torch.ones((<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>),dtype=torch.float), keys, values, torch.FloatTensor([<span class="number">2</span>, <span class="number">6</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Result</span></span><br><span class="line">attention_weight</span><br><span class="line"> tensor([[[<span class="number">0.5000</span>, <span class="number">0.5000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>,</span><br><span class="line">          <span class="number">0.0000</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>,</span><br><span class="line">          <span class="number">0.0000</span>, <span class="number">0.0000</span>]]])</span><br><span class="line">tensor([[[ <span class="number">2.0000</span>,  <span class="number">3.0000</span>,  <span class="number">4.0000</span>,  <span class="number">5.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10.0000</span>, <span class="number">11.0000</span>, <span class="number">12.0000</span>, <span class="number">13.0000</span>]]])</span><br></pre></td></tr></table></figure>

<h3 id="Multilayer-Porceptron-Attentiion"><a href="#Multilayer-Porceptron-Attentiion" class="headerlink" title="Multilayer Porceptron Attentiion"></a>Multilayer Porceptron Attentiion</h3><p>在多层感知器中，我们首先将 query and keys 投影到  $R^ℎ$ .为了更具体，我们将可以学习的参数做如下映射<br>$W_k∈R^{h×d_k}$ ,  $W_q∈R^{h×d_q}$ , and  $v∈R^h$ .  将 score 函数定义</p>
<p>$$<br>α(k,q)=v^Ttanh(W_kk+W_qq)<br>$$<br>.<br>然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为  ℎ  and 输出的size为 1 .隐层激活函数为tanh，无偏置.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLPAttention</span><span class="params">(nn.Module)</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units,ipt_dim,dropout, **kwargs)</span>:</span></span><br><span class="line">        super(MLPAttention, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># Use flatten=True to keep query's and key's 3-D shapes.</span></span><br><span class="line">        self.W_k = nn.Linear(ipt_dim, units, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(ipt_dim, units, bias=<span class="literal">False</span>)</span><br><span class="line">        self.v = nn.Linear(units, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length)</span>:</span></span><br><span class="line">        query, key = self.W_k(query), self.W_q(key)</span><br><span class="line">        <span class="comment">#print("size",query.size(),key.size())</span></span><br><span class="line">        <span class="comment"># expand query to (batch_size, #querys, 1, units), and key to</span></span><br><span class="line">        <span class="comment"># (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.</span></span><br><span class="line">        features = query.unsqueeze(<span class="number">2</span>) + key.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print("features:",features.size())  #--------------开启</span></span><br><span class="line">        scores = self.v(features).squeeze(<span class="number">-1</span>) </span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure>

<p><strong>测试：</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;尽管 MLPAttention 包含一个额外的 MLP 模型，但如果给定相同的输入和相同的键，我们将获得与DotProductAttention相同的输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">atten = MLPAttention(ipt_dim=<span class="number">2</span>,units = <span class="number">8</span>, dropout=<span class="number">0</span>)</span><br><span class="line">atten(torch.ones((<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>), dtype = torch.float), keys, values, torch.FloatTensor([<span class="number">2</span>, <span class="number">6</span>]))      </span><br><span class="line"><span class="comment">#Result</span></span><br><span class="line">tensor([[[ <span class="number">2.0000</span>,  <span class="number">3.0000</span>,  <span class="number">4.0000</span>,  <span class="number">5.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10.0000</span>, <span class="number">11.0000</span>, <span class="number">12.0000</span>, <span class="number">13.0000</span>]]], grad_fn=&lt;BmmBackward&gt;)</span><br></pre></td></tr></table></figure>

<h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><blockquote>
<p>在Dot-product Attention中，key与query维度需要一致，在MLP Attention中则不需要。</p>
</blockquote>
<h2 id="Seq2seq模型"><a href="#Seq2seq模型" class="headerlink" title="Seq2seq模型"></a>Seq2seq模型</h2><blockquote>
<p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">机器翻译及其相关技术介绍</a></p>
</blockquote>
<p>seq2seq 模型的预测需人为设定终止条件，设定最长序列长度或者输出 [EOS] 结束符号，若不加以限制则可能生成无穷长度序列。</p>
<p>引出：</p>
<h2 id="引入注意力机制的Seq2seq模型"><a href="#引入注意力机制的Seq2seq模型" class="headerlink" title="引入注意力机制的Seq2seq模型"></a>引入注意力机制的Seq2seq模型</h2><p>注意力机制本身有高效的并行性，但引入注意力并不能改变seq2seq内部RNN的迭代机制，因此无法加速。</p>
<blockquote>
<p>将注意机制添加到 sequence to sequence 模型中，以显式地使用权重聚合 states。</p>
</blockquote>
<p>下图展示 encoding 和 decoding 的模型结构，在时间步为 $t$ 的时候。此刻 attention layer 保存着 encodering 看到的所有信息——即 encoding 的每一步输出。在 decoding 阶段，解码器的 $t$ 时刻的隐藏状态被当作 query，encoder 的每个时间步的 hidden states 作为 key 和 value 进行 attention 聚合. </p>
<p>Attetion model 的输出当作成上下文信息 context vector，并与解码器输入 $D_t$ 拼接起来一起送到解码器：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWttN284ejkzLlBORw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>Fig1具有注意机制的seq-to-seq模型解码的第二步<br>$$</p>
<p>下图展示了seq2seq机制的所以层的关系，下面展示了encoder和decoder的layer结构</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWttOGRpaGxyLlBORw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>Fig2具有注意机制的seq-to-seq模型中层结构<br>$$</p>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>由于带有注意机制的 seq2seq 的编码器与之前章节中的 Seq2SeqEncoder 相同，所以在此处我们只关注解码器。</p>
<p>我们添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态:</p>
<ul>
<li>the encoder outputs of all timesteps：encoder 输出的各个状态，被用于attetion layer 的 memory 部分，有相同的 key 和 values ；</li>
</ul>
<ul>
<li>the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state ；</li>
</ul>
<ul>
<li>the encoder valid length: 编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings）；</li>
</ul>
<p>在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的 query。</p>
<p>然后，将注意力模型的输出与输入嵌入向量连接起来，输入到 RNN 层。虽然 RNN 层隐藏状态也包含来自解码器的历史信息，但是 attention model 的输出显式地选择了 enc_valid_len 以内的编码器输出，这样 attention机制就会尽可能排除其他不相关的信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqAttentionDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens,vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, enc_valid_len, *args)</span>:</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line"><span class="comment">#         print("first:",outputs.size(),hidden_state[0].size(),hidden_state[1].size())</span></span><br><span class="line">        <span class="comment"># Transpose outputs to (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>), hidden_state, enc_valid_len)</span><br><span class="line">        <span class="comment">#outputs.swapaxes(0, 1)</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_len = state</span><br><span class="line">        <span class="comment">#("X.size",X.size())</span></span><br><span class="line">        X = self.embedding(X).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">#         print("Xembeding.size2",X.size())</span></span><br><span class="line">        outputs = []</span><br><span class="line">        <span class="keyword">for</span> l, x <span class="keyword">in</span> enumerate(X):</span><br><span class="line"><span class="comment">#             print(f"\n&#123;l&#125;-th token")</span></span><br><span class="line"><span class="comment">#             print("x.first.size()",x.size())</span></span><br><span class="line">            <span class="comment"># query shape: (batch_size, 1, hidden_size)</span></span><br><span class="line">            <span class="comment"># select hidden state of the last rnn layer as query</span></span><br><span class="line">            query = hidden_state[<span class="number">0</span>][<span class="number">-1</span>].unsqueeze(<span class="number">1</span>) <span class="comment"># np.expand_dims(hidden_state[0][-1], axis=1)</span></span><br><span class="line">            <span class="comment"># context has same shape as query</span></span><br><span class="line"><span class="comment">#             print("query enc_outputs, enc_outputs:\n",query.size(), enc_outputs.size(), enc_outputs.size())</span></span><br><span class="line">            context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len)</span><br><span class="line">            <span class="comment"># Concatenate on the feature dimension</span></span><br><span class="line"><span class="comment">#             print("context.size:",context.size())</span></span><br><span class="line">            x = torch.cat((context, x.unsqueeze(<span class="number">1</span>)), dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># Reshape x to (1, batch_size, embed_size+hidden_size)</span></span><br><span class="line"><span class="comment">#             print("rnn",x.size(), len(hidden_state))</span></span><br><span class="line">            out, hidden_state = self.rnn(x.transpose(<span class="number">0</span>,<span class="number">1</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.transpose(<span class="number">0</span>, <span class="number">1</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                        enc_valid_len]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>,</span><br><span class="line">                            num_hiddens=<span class="number">16</span>, num_layers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># encoder.initialize()</span></span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>,</span><br><span class="line">                                  num_hiddens=<span class="number">16</span>, num_layers=<span class="number">2</span>)</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>),dtype=torch.long)</span><br><span class="line">print(<span class="string">"batch size=4\nseq_length=7\nhidden dim=16\nnum_layers=2\n"</span>)</span><br><span class="line">print(<span class="string">'encoder output size:'</span>, encoder(X)[<span class="number">0</span>].size())</span><br><span class="line">print(<span class="string">'encoder hidden size:'</span>, encoder(X)[<span class="number">1</span>][<span class="number">0</span>].size())</span><br><span class="line">print(<span class="string">'encoder memory size:'</span>, encoder(X)[<span class="number">1</span>][<span class="number">1</span>].size())</span><br><span class="line">state = decoder.init_state(encoder(X), <span class="literal">None</span>)</span><br><span class="line">out, state = decoder(X, state)</span><br><span class="line">out.shape, len(state), state[<span class="number">0</span>].shape, len(state[<span class="number">1</span>]), state[<span class="number">1</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span> <span class="comment"># This class is saved in d2l.</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">    <span class="comment"># sort by frequency and token</span></span><br><span class="line">    counter = collections.Counter(tokens)</span><br><span class="line">    token_freqs = sorted(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    token_freqs.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">      <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">      self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">      tokens = [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.unk = <span class="number">0</span></span><br><span class="line">      tokens = [<span class="string">''</span>]</span><br><span class="line">    tokens += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs <span class="keyword">if</span> freq &gt;= min_freq]</span><br><span class="line">    self.idx_to_token = []</span><br><span class="line">    self.token_to_idx = dict()</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">      self.idx_to_token.append(token)</span><br><span class="line">      self.token_to_idx[token] = len(self.idx_to_token) - <span class="number">1</span></span><br><span class="line">      </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">      <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">      <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span><span class="params">(batch_size, max_len, num_examples=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Download an NMT dataset, return its vocabulary and data iterator."""</span></span><br><span class="line">    <span class="comment"># Download and preprocess</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_raw</span><span class="params">(text)</span>:</span></span><br><span class="line">        text = text.replace(<span class="string">'\u202f'</span>, <span class="string">' '</span>).replace(<span class="string">'\xa0'</span>, <span class="string">' '</span>)</span><br><span class="line">        out = <span class="string">''</span></span><br><span class="line">        <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(text.lower()):</span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">in</span> (<span class="string">','</span>, <span class="string">'!'</span>, <span class="string">'.'</span>) <span class="keyword">and</span> text[i<span class="number">-1</span>] != <span class="string">' '</span>:</span><br><span class="line">                out += <span class="string">' '</span></span><br><span class="line">            out += char</span><br><span class="line">        <span class="keyword">return</span> out </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'/home/kesci/input/fraeng6506/fra.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">      raw_text = f.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    text = preprocess_raw(raw_text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(text.split(<span class="string">'\n'</span>)):</span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">if</span> len(parts) &gt;= <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">' '</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build vocab</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(tokens)</span>:</span></span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">        <span class="keyword">return</span> Vocab(tokens, min_freq=<span class="number">3</span>, use_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert to index arrays</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pad</span><span class="params">(line, max_len, padding_token)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(line) &gt; max_len:</span><br><span class="line">            <span class="keyword">return</span> line[:max_len]</span><br><span class="line">        <span class="keyword">return</span> line + [padding_token] * (max_len - len(line))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_array</span><span class="params">(lines, vocab, max_len, is_source)</span>:</span></span><br><span class="line">        lines = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_source:</span><br><span class="line">            lines = [[vocab.bos] + line + [vocab.eos] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        array = torch.tensor([pad(line, max_len, vocab.pad) <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line">        valid_len = (array != vocab.pad).sum(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line">    src_array, src_valid_len = build_array(source, src_vocab, max_len, <span class="literal">True</span>)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, <span class="literal">False</span>)</span><br><span class="line">    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    train_iter = data.DataLoader(train_data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> src_vocab, tgt_vocab, train_iter</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.0</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, ctx = <span class="number">0.005</span>, <span class="number">500</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = d2l.Seq2SeqEncoder(</span><br><span class="line">    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(</span><br><span class="line">    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">model = d2l.EncoderDecoder(encoder, decoder)</span><br></pre></td></tr></table></figure>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_s2s_ch9(model, train_iter, lr, num_epochs, ctx)</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> [<span class="string">'Go .'</span>, <span class="string">'Good Night !'</span>, <span class="string">"I'm OK ."</span>, <span class="string">'I won !'</span>]:</span><br><span class="line">    print(sentence + <span class="string">' =&gt; '</span> + d2l.predict_s2s_ch9(</span><br><span class="line">        model, sentence, src_vocab, tgt_vocab, num_steps, ctx))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>MLP</tag>
        <tag>Seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>机器翻译及其相关技术介绍</title>
    <url>/2020/02/28/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%8F%8A%E5%85%B6%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h1 id="机器翻译-MT-实践"><a href="#机器翻译-MT-实践" class="headerlink" title="机器翻译(MT)_实践"></a>机器翻译(MT)_实践</h1><blockquote>
<p>将一段文本从一种语言自动翻译为另一种语言<br/><br>用神经网络解决这个问题通常称为神经机器翻译（NMT）。</p>
</blockquote>
<p>主要特征：输出是单词序列而不是单个单词。 输出序列的长度可能与源序列的长度不同。</p>
<center><b>实现一个从英语到法语的机器翻译</b></center><br/>

<p>首先准备一个数据集，汇总一些常见单词和日用句子，数据集中有足够且保证正确的对应数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># For Example</span></span><br><span class="line">Go.	Va !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#2877272 (CM) &amp; #1158250 (Wittydev)</span></span><br><span class="line">Hi.	Salut !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#538123 (CM) &amp; #509819 (Aiji)</span></span><br><span class="line">Hi.	Salut.	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#538123 (CM) &amp; #4320462 (gillux)</span></span><br><span class="line">Run!	Cours !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#906328 (papabear) &amp; #906331 (sacredceltic)</span></span><br><span class="line">Run!	Courez !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#906328 (papabear) &amp; #906332 (sacredceltic)</span></span><br><span class="line">Who?	Qui ?	CC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) &amp; #4366796 (gillux)</span><br><span class="line">Wow!	Ça alors !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#52027 (Zifre) &amp; #374631 (zmoo)</span></span><br><span class="line">Fire!	Au feu !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#1829639 (Spamster) &amp; #4627939 (sacredceltic)</span></span><br><span class="line">Help!	À l  aide !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#435084 (lukaszpp) &amp; #128430 (sysko)</span></span><br><span class="line">Jump.	Saute.	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#631038 (Shishir) &amp; #2416938 (Phoenix)</span></span><br><span class="line">Stop!	Ça suffit !	CC-BY <span class="number">2.0</span> (France) Attribution: tato</span><br></pre></td></tr></table></figure>

<h3 id="导入包和模块以及数据文件"><a href="#导入包和模块以及数据文件" class="headerlink" title="导入包和模块以及数据文件"></a>导入包和模块以及数据文件</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import dataset</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.listdir(<span class="string">'path to storaged file of dataset'</span>)</span><br><span class="line"><span class="comment"># import package and module</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'path to file storge d2lzh1981'</span>)</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> d2l</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">from</span> d2l.data.base <span class="keyword">import</span> Vocab</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br></pre></td></tr></table></figure>

<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>将数据集清洗、转化为神经网络的输入minbatch</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data file'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">      raw_text = f.read()</span><br><span class="line">print(raw_text[<span class="number">0</span>:<span class="number">1000</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">针对上边的example data 进行处理</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 去掉乱码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_raw</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="comment"># 去掉法文中的空格</span></span><br><span class="line">    text = text.replace(<span class="string">'\u202f'</span>, <span class="string">' '</span>).replace(<span class="string">'\xa0'</span>, <span class="string">' '</span>)</span><br><span class="line">    out = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 大小写归一</span></span><br><span class="line">    <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(text.lower()):</span><br><span class="line">        <span class="comment"># 在单词和标点符号之间加上空格</span></span><br><span class="line">        <span class="keyword">if</span> char <span class="keyword">in</span> (<span class="string">','</span>, <span class="string">'!'</span>, <span class="string">'.'</span>) <span class="keyword">and</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> text[i<span class="number">-1</span>] != <span class="string">' '</span>:</span><br><span class="line">            out += <span class="string">' '</span></span><br><span class="line">        out += char</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">text = preprocess_raw(raw_text)</span><br><span class="line">print(text[<span class="number">0</span>:<span class="number">1000</span>])</span><br></pre></td></tr></table></figure>
<p>&ensp;&ensp;&ensp;&ensp;字符在计算机里是以编码的形式存在，我们通常所用的空格是 \x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。</p>
<p>&ensp;&ensp;&ensp;&ensp;而 \xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。再数据预处理的过程中，我们首先需要对数据进行清洗。</p>
<h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><blockquote>
<p>字符串：单词组成的列表</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_examples = <span class="number">50000</span></span><br><span class="line">source, target = [], []</span><br><span class="line"><span class="comment"># 分开每个样本</span></span><br><span class="line"><span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(text.split(<span class="string">'\n'</span>)):</span><br><span class="line">    <span class="keyword">if</span> i &gt; num_examples:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 取元素</span></span><br><span class="line">    parts = line.split(<span class="string">'\t'</span>)</span><br><span class="line">    <span class="keyword">if</span> len(parts) &gt;= <span class="number">2</span>:</span><br><span class="line">        source.append(parts[<span class="number">0</span>].split(<span class="string">' '</span>))</span><br><span class="line">        target.append(parts[<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"># test     </span></span><br><span class="line"><span class="string">source[0:3], target[0:3]</span></span><br><span class="line"><span class="string"># result</span></span><br><span class="line"><span class="string">([['go', '.'], ['hi', '.'], ['hi', '.']],</span></span><br><span class="line"><span class="string"> [['va', '!'], ['salut', '!'], ['salut', '.']])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">d2l.set_figsize()</span><br><span class="line">d2l.plt.hist([[len(l) <span class="keyword">for</span> l <span class="keyword">in</span> source], [len(l) <span class="keyword">for</span> l <span class="keyword">in</span> target]],label=[<span class="string">'source'</span>, <span class="string">'target'</span>])</span><br><span class="line">d2l.plt.legend(loc=<span class="string">'upper right'</span>);</span><br></pre></td></tr></table></figure>

<h3 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h3><blockquote>
<p>此处利用 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104297072" target="_blank" rel="noopener"><strong>文本预处理Text Preprocessing</strong></a>中的 Vocab 类</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(tokens)</span>:</span></span><br><span class="line">    <span class="comment"># 取出单词连成列表</span></span><br><span class="line">    tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">return</span> d2l.data.base.Vocab(tokens, min_freq=<span class="number">3</span>, use_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">src_vocab = build_vocab(source)</span><br><span class="line">len(src_vocab)</span><br></pre></td></tr></table></figure>

<h3 id="载入数据集得到数据生成器"><a href="#载入数据集得到数据生成器" class="headerlink" title="载入数据集得到数据生成器"></a>载入数据集得到数据生成器</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad</span><span class="params">(line, max_len, padding_token)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(line) &gt; max_len:</span><br><span class="line">        <span class="keyword">return</span> line[:max_len]</span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (max_len - len(line))</span><br><span class="line">pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab.pad)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># is_source a判断是否是法语</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_array</span><span class="params">(lines, vocab, max_len, is_source)</span>:</span></span><br><span class="line">    lines = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_source:</span><br><span class="line">        lines = [[vocab.bos] + line + [vocab.eos] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([pad(line, max_len, vocab.pad) <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line">    <span class="comment"># 有效长度：保留句子的有效长度</span></span><br><span class="line">    valid_len = (array != vocab.pad).sum(<span class="number">1</span>) <span class="comment">#第一个维度</span></span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据生成器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span><span class="params">(batch_size, max_len)</span>:</span> <span class="comment"># This function is saved in d2l.</span></span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line">    src_array, src_valid_len = build_array(source, src_vocab, max_len, <span class="literal">True</span>)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, <span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 验证四个参数是否都相同</span></span><br><span class="line">    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    train_iter = data.DataLoader(train_data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> src_vocab, tgt_vocab, train_iter</span><br></pre></td></tr></table></figure>

<blockquote>
<p>机器翻译</p>
<blockquote>
<p>困难：输入和输出不等价</p>
</blockquote>
</blockquote>
<h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>Encoder经常用循环神经网络，Decoder通过判断对后一个输出是不是eos来判断翻译是否结束</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjYXQzYzhtLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<blockquote>
<p>应用</p>
<blockquote>
<p>Encoder-Decoder常应用于输入序列和输出序列的长度是可变的，而分类问题的输出是固定的类别，不需要使用Encoder-Decoder</p>
</blockquote>
<p>机器翻译 、语音识别任务、对话机器人【属于】<br>文本分类任务【不属于】</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, *args)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, *args)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, **kwargs)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, enc_X, dec_X, *args)</span>:</span></span><br><span class="line">        <span class="comment"># 类似于H_&#123;-1&#125;</span></span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure>
<h2 id="Sequence-to-Sequence模型"><a href="#Sequence-to-Sequence模型" class="headerlink" title="Sequence to Sequence模型"></a>Sequence to Sequence模型</h2><h3 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h3><p>训练<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjN2E1M3B0LnBuZw?x-oss-process=image/format,png" alt="Image Name"><br>预测</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjZWN4Y2JhLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<h3 id="具体结构：-LSTM"><a href="#具体结构：-LSTM" class="headerlink" title="具体结构：(LSTM)"></a>具体结构：(LSTM)</h3><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjY2poa2lpLnBuZw?x-oss-process=image/format,png" /></center>

<h4 id="Encoder-–-state"><a href="#Encoder-–-state" class="headerlink" title="Encoder – state"></a>Encoder – state</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqEncoder</span><span class="params">(d2l.Encoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens=num_hiddens</span><br><span class="line">        self.num_layers=num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span><span class="params">(self, batch_size, device)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),</span><br><span class="line">                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, *args)</span>:</span></span><br><span class="line">        X = self.embedding(X) <span class="comment"># X shape: (batch_size, seq_len, embed_size)</span></span><br><span class="line">        <span class="comment"># 第0维和第1维之间调换</span></span><br><span class="line">        X = X.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># RNN needs first axes to be time</span></span><br><span class="line">        <span class="comment"># state = self.begin_state(X.shape[1], device=X.device)</span></span><br><span class="line">        out, state = self.rnn(X)</span><br><span class="line">        <span class="comment"># The shape of out is (seq_len, batch_size, num_hiddens).</span></span><br><span class="line">        <span class="comment"># state contains the hidden state and the memory cell</span></span><br><span class="line">        <span class="comment"># of the last time step, the shape is (num_layers, batch_size, num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> out, state</span><br></pre></td></tr></table></figure>
<h4 id="Decoder-–-out"><a href="#Decoder-–-out" class="headerlink" title="Decoder – out"></a>Decoder – out</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">        <span class="comment"># 输出的全连接层 映射翻译结果</span></span><br><span class="line">        self.dense = nn.Linear(num_hiddens,vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, *args)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        X = self.embedding(X).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        out, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># Make the batch to be the first dimension to simplify loss computation.</span></span><br><span class="line">        out = self.dense(out).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out, state</span><br></pre></td></tr></table></figure>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">0</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    mask = torch.arange(maxlen)[<span class="literal">None</span>, :].to(X_len.device) &lt; X_len[:, <span class="literal">None</span>]   </span><br><span class="line">    X[~mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 继承交叉损失熵函数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskedSoftmaxCELoss</span><span class="params">(nn.CrossEntropyLoss)</span>:</span></span><br><span class="line">    <span class="comment"># pred shape: (batch_size, seq_len, vocab_size)</span></span><br><span class="line">    <span class="comment"># label shape: (batch_size, seq_len)</span></span><br><span class="line">    <span class="comment"># valid_length shape: (batch_size, )</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, pred, label, valid_length)</span>:</span></span><br><span class="line">        <span class="comment"># the sample weights shape should be (batch_size, seq_len)</span></span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        weights = SequenceMask(weights, valid_length).float()</span><br><span class="line">        self.reduction=<span class="string">'none'</span></span><br><span class="line">        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(<span class="number">1</span>,<span class="number">2</span>), label)</span><br><span class="line">        <span class="keyword">return</span> (output*weights).mean(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch7</span><span class="params">(model, data_iter, lr, num_epochs, device)</span>:</span>  <span class="comment"># Saved in d2l</span></span><br><span class="line">    model.to(device)</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    tic = time.time()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, num_epochs+<span class="number">1</span>):</span><br><span class="line">        l_sum, num_tokens_sum = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_vlen, Y, Y_vlen = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">            <span class="comment"># bos word eos</span></span><br><span class="line">            Y_input, Y_label, Y_vlen = Y[:,:<span class="number">-1</span>], Y[:,<span class="number">1</span>:], Y_vlen<span class="number">-1</span></span><br><span class="line">            </span><br><span class="line">            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)</span><br><span class="line">            <span class="comment"># 评估训练好坏</span></span><br><span class="line">            l = loss(Y_hat, Y_label, Y_vlen).sum()</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                d2l.grad_clipping_nn(model, <span class="number">5</span>, device)</span><br><span class="line">            num_tokens = Y_vlen.sum().item()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.sum().item()</span><br><span class="line">            num_tokens_sum += num_tokens</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"epoch &#123;0:4d&#125;,loss &#123;1:.3f&#125;, time &#123;2:.1f&#125; sec"</span>.format( </span><br><span class="line">                  epoch, (l_sum/num_tokens_sum), time.time()-tic))</span><br><span class="line">            tic = time.time()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.0</span></span><br><span class="line">batch_size, num_examples, max_len = <span class="number">64</span>, <span class="number">1e3</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, ctx = <span class="number">0.005</span>, <span class="number">300</span>, d2l.try_gpu()</span><br><span class="line">src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(</span><br><span class="line">    batch_size, max_len,num_examples)</span><br><span class="line">encoder = Seq2SeqEncoder(</span><br><span class="line">    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqDecoder(</span><br><span class="line">    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">model = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">train_ch7(model, train_iter, lr, num_epochs, ctx)</span><br></pre></td></tr></table></figure>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate_ch7</span><span class="params">(model, src_sentence, src_vocab, tgt_vocab, max_len, device)</span>:</span></span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">' '</span>)]</span><br><span class="line">    src_len = len(src_tokens)</span><br><span class="line">    <span class="keyword">if</span> src_len &lt; max_len:</span><br><span class="line">        src_tokens += [src_vocab.pad] * (max_len - src_len)</span><br><span class="line">    enc_X = torch.tensor(src_tokens, device=device)</span><br><span class="line">    enc_valid_length = torch.tensor([src_len], device=device)</span><br><span class="line">    <span class="comment"># use expand_dim to add the batch_size dimension.</span></span><br><span class="line">    enc_outputs = model.encoder(enc_X.unsqueeze(dim=<span class="number">0</span>), enc_valid_length)</span><br><span class="line">    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)</span><br><span class="line">    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line">    predict_tokens = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(max_len):</span><br><span class="line">        Y, dec_state = model.decoder(dec_X, dec_state)</span><br><span class="line">        <span class="comment"># The token with highest score is used as the next time step input.</span></span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        py = dec_X.squeeze(dim=<span class="number">0</span>).int().item()</span><br><span class="line">        <span class="keyword">if</span> py == tgt_vocab.eos:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        predict_tokens.append(py)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join(tgt_vocab.to_tokens(predict_tokens))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> [<span class="string">'Go .'</span>, <span class="string">'Wow !'</span>, <span class="string">"I'm OK ."</span>, <span class="string">'I love you !'</span>]:</span><br><span class="line">    print(sentence + <span class="string">' =&gt; '</span> + translate_ch7(</span><br><span class="line">        model, sentence, src_vocab, tgt_vocab, max_len, ctx))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Result</span></span><br><span class="line">Go . =&gt; va !</span><br><span class="line">Wow ! =&gt; &lt;unk&gt; !</span><br><span class="line">I<span class="string">'m OK . =&gt; je vais bien .</span></span><br><span class="line"><span class="string">I love you ! =&gt; reste &lt;unk&gt; !</span></span><br></pre></td></tr></table></figure>
<h2 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h2><ul>
<li>简单 贪心搜索（greedy search）：</li>
</ul>
<blockquote>
<p>针对每一个 out 取最大概率<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjaHFvcHBuLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
</blockquote>
<ul>
<li>维特比算法：选择整体分数最高的句子（搜索空间太大）</li>
<li>集束搜索：<blockquote>
<p>集束搜索是维特比算法的贪心形式，所以集束搜索得到的并非是全局最优解<br/><br>集束搜索使用 beam size 参数来限制在每一步保留下来的可能性词的数量</p>
</blockquote>
</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjaWE4NnoxLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>集束搜索</tag>
        <tag>Encoder-Decoder</tag>
        <tag>Seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>学而后思,方能发展;思而立行,终将卓越</title>
    <url>/2020/02/28/%E5%AD%A6%E8%80%8C%E5%90%8E%E6%80%9D,%E6%96%B9%E8%83%BD%E5%8F%91%E5%B1%95;%E6%80%9D%E8%80%8C%E7%AB%8B%E8%A1%8C,%E7%BB%88%E5%B0%86%E5%8D%93%E8%B6%8A/</url>
    <content><![CDATA[<center><b><font size=5>学而后思</font></b></center>

<blockquote>
<p>梯度爆炸和梯度衰减问题</p>
</blockquote>
<p>深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。</p>
<p><strong>当神经网络的层数较多时，模型的数值稳定性容易变差。</strong></p>
<p>假设一个层数为 $L$ 的多层感知机的第 $l$ 层 $\boldsymbol{H}^{(l)}$ 的权重参数为$\boldsymbol{W}^{(l)}$，输出层 $\boldsymbol{H}^{(L)}$ 的权重参数为 $\boldsymbol{W}^{(L)}$。</p>
<p>为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为<code>恒等映射</code>（identity mapping）$\phi(x) = x$。</p>
<p>给定输入$\boldsymbol{X}$，多层感知机的第$l$层的输出$\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。</p>
<p>For Example：假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\boldsymbol{X}$分别与$0.2^{30} \approx 1 \times 10^{-21}$（消失）和$5^{30} \approx 9 \times 10^{20}$（爆炸）的乘积。当层数较多时，梯度的计算也容易出现消失或爆炸。</p>
<hr>
<p>解决方案：</p>
<ul>
<li>梯度爆炸：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">裁剪梯度</a></li>
<li>梯度衰减：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104340035" target="_blank" rel="noopener">GRU</a></li>
</ul>
<hr>
<blockquote>
<p>过拟合和欠拟合问题</p>
</blockquote>
<ul>
<li><a href="https://blog.csdn.net/RokoBasilisk/article/details/104344237" target="_blank" rel="noopener">从模型训练中认知拟合现象</a></li>
</ul>
<blockquote>
<p>随机初始化模型参数</p>
<blockquote>
<p>如何占据神经网络中不可或缺的位置？</p>
</blockquote>
</blockquote>
<p>话起 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104287595" target="_blank" rel="noopener">多层感知机 [ Multilayer Perceptron ]</a> </p>
<p>假设输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。</p>
<p>如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。</p>
<p>在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。</p>
<p>在这种情况下，无论隐藏单元有多少，<code>隐藏层本质上只有1个隐藏单元在发挥作用</code>。因此通常将神经网络的模型参数，特别是权重参数，进行随机初始化。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpnNzZrbG95LnBuZw?x-oss-process=image/format,png" /></center>

<blockquote>
<blockquote>
<p>列举两种随机初始化方式</p>
</blockquote>
</blockquote>
<h4 id="PyTorch的默认随机初始化"><a href="#PyTorch的默认随机初始化" class="headerlink" title="PyTorch的默认随机初始化"></a>PyTorch的默认随机初始化</h4><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104269986" target="_blank" rel="noopener">Design and Realization of Linear Regression</a> 中，使用 torch.nn.init.normal_() 使模型 net 的权重参数采用正态分布的随机初始化方式。</p>
<p>PyTorch中 nn.Module 的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考<a href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules" target="_blank" rel="noopener">源代码</a>），因此一般不用我们考虑。</p>
<h4 id="Xavier随机初始化"><a href="#Xavier随机初始化" class="headerlink" title="Xavier随机初始化"></a>Xavier随机初始化</h4><p>假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p>
<p>$$<br>U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).<br>$$</p>
<p>模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。</p>
<blockquote>
<p>环境因素带来的问题</p>
<blockquote>
<p>1.协变量偏移<br/><br>2.标签偏移<br/><br>3.概念偏移</p>
</blockquote>
</blockquote>
<blockquote>
<p>协变量偏移</p>
</blockquote>
<p><strong>For Example : 一个在冬季部署的物品推荐系统在夏季的物品推荐列表中出现了圣诞礼物</strong></p>
<p>我们假设，虽然输入的分布P(x)可能随时间而改变，但是标记函数，即条件分布P（y∣x）不会改变。【注意：容易忽视】</p>
<p>对于区分猫和狗，假设我们的训练数据使用的是猫和狗的真实的照片，但是在测试时，我们被要求对猫和狗的卡通图片进行分类。显然，这不太可能奏效。</p>
<p>训练集由照片组成，而测试集只包含卡通。在一个看起来与测试集有着本质不同的数据集上进行训练，而不考虑如何适应新的情况。</p>
<p>问题的根源在于特征分布的变化 ( 即协变量的变化 ) , 统计学家称这种协变量变化。</p>
<blockquote>
<p>标签偏移</p>
</blockquote>
<p>如果数据量足够的情况下，确保训练数据集和测试集中的数据取自同一个数据集，可以防止协变量偏移和标签偏移是正确的。如果数据量很少，少到测试集中存在训练集中未包含的标签，就会发生标签偏移。</p>
<p>当我们认为导致偏移的是标签P（y）上的边缘分布的变化，但类条件分布是不变的P（x∣y）时，就会出现相反的问题。当我们认为y导致x时，标签偏移是一个合理的假设。</p>
<p>For Example：</p>
<ul>
<li>通常我们希望根据其表现来预测诊断结果。在这种情况下，我们认为诊断引起的表现，即疾病引起的症状。<br/><br>病因（要预测的诊断结果）导致 症状（观察到的结果）。  训练数据集，数据很少只包含流感p(y)的样本。  而测试数据集有流感p(y)和流感q(y)，其中不变的是流感症状p(x|y)。</li>
<li>有时标签偏移和协变量移位假设可以同时成立。例如，当真正的标签函数是确定的和不变的。</li>
</ul>
<p><font color=#FF0000 >在概念转换中，有一种标签本身的定义发生变化的情况：<br></font></p>
<blockquote>
<p>概念偏移</p>
</blockquote>
<p>如果我们要建立一个机器翻译系统，分布P（y∣x）可能因我们的位置而异。这个问题很难发现。另一个可取之处是P（y∣x）通常只是逐渐变化。</p>
<p>换句话说就是：<strong>概念偏移可以根据其缓慢变化的特点缓解。</strong></p>
<center><b><font size=5>思而立行</font></b></center><br/>

<p>Advanced Regression Techniques：House Prices</p>
<h3 id="导入数据包和模块"><a href="#导入数据包和模块" class="headerlink" title="导入数据包和模块"></a>导入数据包和模块</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">torch.set_default_tensor_type(torch.FloatTensor)</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span>  norm</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model, svm, gaussian_process</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>

<h3 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_data = pd.read_csv(<span class="string">"path to test.csv"</span>)</span><br><span class="line">train_data = pd.read_csv(<span class="string">"path to train.csv"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="分析数据变化趋势"><a href="#分析数据变化趋势" class="headerlink" title="分析数据变化趋势"></a>分析数据变化趋势</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># analyze SalePrice of the train_data by describe</span></span><br><span class="line">train_data[<span class="string">'SalePrice'</span>].describe()</span><br><span class="line"><span class="comment"># show trend of SalePrice in train_data</span></span><br><span class="line">sns.distplot(train_data[<span class="string">'SalePrice'</span>])</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200216221220752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p><strong>根据SalePrice变化趋势分析为正态分布，设定两个图像特征峰度(Kurtosis)和偏度(Skewness)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#skewness and kurtosis</span></span><br><span class="line">print(<span class="string">"Skewness: %f"</span> % train_data[<span class="string">'SalePrice'</span>].skew())</span><br><span class="line">print(<span class="string">"Kurtosis: %f"</span> % train_data[<span class="string">'SalePrice'</span>].kurt())</span><br></pre></td></tr></table></figure>
<h3 id="提取有效特征"><a href="#提取有效特征" class="headerlink" title="提取有效特征"></a>提取有效特征</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">corrmat = train_data.corr()</span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">20</span>, <span class="number">9</span>))</span><br><span class="line">sns.heatmap(corrmat, vmax=<span class="number">0.8</span>, square=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200216220930264.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70"  width=500/></center>

<h3 id="特征取舍和离散值参与分析"><a href="#特征取舍和离散值参与分析" class="headerlink" title="特征取舍和离散值参与分析"></a>特征取舍和离散值参与分析</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">f_names = [<span class="string">'CentralAir'</span>, <span class="string">'Neighborhood'</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> f_names:</span><br><span class="line">    label = preprocessing.LabelEncoder()</span><br><span class="line">    train_data[x] = label.fit_transform(train_data[x])</span><br><span class="line">corrmat = train_data.corr()</span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">20</span>, <span class="number">9</span>))</span><br><span class="line">sns.heatmap(corrmat, vmax=<span class="number">0.8</span>, square=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><strong>由相关性图可得：’CentralAir’, ‘Neighborhood’这两个特征对房价的影响并不大,舍去特征</strong></p>
<h3 id="列出关系矩阵"><a href="#列出关系矩阵" class="headerlink" title="列出关系矩阵"></a>列出关系矩阵</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k  = <span class="number">10</span> <span class="comment"># 关系矩阵中将显示10个特征</span></span><br><span class="line">cols = corrmat.nlargest(k, <span class="string">'SalePrice'</span>)[<span class="string">'SalePrice'</span>].index</span><br><span class="line">cm = np.corrcoef(train_data[cols].values.T)</span><br><span class="line">sns.set(font_scale=<span class="number">1.25</span>)</span><br><span class="line">hm = sns.heatmap(cm, cbar=<span class="literal">True</span>, annot=<span class="literal">True</span>, \</span><br><span class="line">                 square=<span class="literal">True</span>, fmt=<span class="string">'.2f'</span>, annot_kws=&#123;<span class="string">'size'</span>: <span class="number">10</span>&#125;, yticklabels=cols.values, xticklabels=cols.values)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200216221404494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="列出散点关系图"><a href="#列出散点关系图" class="headerlink" title="列出散点关系图"></a>列出散点关系图</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.set()</span><br><span class="line">cols = [<span class="string">'SalePrice'</span>,<span class="string">'OverallQual'</span>,<span class="string">'GrLivArea'</span>, <span class="string">'GarageCars'</span>,<span class="string">'TotalBsmtSF'</span>, <span class="string">'FullBath'</span>, <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">sns.pairplot(train_data[cols], size = <span class="number">2.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="数据模拟"><a href="#数据模拟" class="headerlink" title="数据模拟"></a>数据模拟</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">cols = [<span class="string">'OverallQual'</span>,<span class="string">'GrLivArea'</span>, <span class="string">'GarageCars'</span>,<span class="string">'TotalBsmtSF'</span>, <span class="string">'FullBath'</span>, <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">x = train_data[cols].values</span><br><span class="line">y = train_data[<span class="string">'SalePrice'</span>].values</span><br><span class="line">x_scaled = preprocessing.StandardScaler().fit_transform(x)</span><br><span class="line">y_scaled = preprocessing.StandardScaler().fit_transform(y.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">X_train,X_test, y_train, y_test = train_test_split(x_scaled, y_scaled, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<h3 id="随机森林回归"><a href="#随机森林回归" class="headerlink" title="随机森林回归"></a>随机森林回归</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机森林回归</span></span><br><span class="line">clfs = &#123;</span><br><span class="line">        <span class="string">'svm'</span>:svm.SVR(), </span><br><span class="line">        <span class="string">'RandomForestRegressor'</span>:RandomForestRegressor(n_estimators=<span class="number">400</span>),</span><br><span class="line">        <span class="string">'BayesianRidge'</span>:linear_model.BayesianRidge()</span><br><span class="line">       &#125;</span><br><span class="line"><span class="keyword">for</span> clf <span class="keyword">in</span> clfs:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        clfs[clf].fit(X_train, y_train)</span><br><span class="line">        y_pred = clfs[clf].predict(X_test)</span><br><span class="line">        print(clf + <span class="string">" cost:"</span> + str(np.sum(y_pred-y_test)/len(y_pred)) )</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(clf + <span class="string">" Error:"</span>)</span><br><span class="line">        print(str(e))</span><br></pre></td></tr></table></figure>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 归一化数据的预测结果</span></span><br><span class="line">cols = [<span class="string">'OverallQual'</span>,<span class="string">'GrLivArea'</span>, <span class="string">'GarageCars'</span>,<span class="string">'TotalBsmtSF'</span>, <span class="string">'FullBath'</span>, <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">x = train_data[cols].values</span><br><span class="line">y = train_data[<span class="string">'SalePrice'</span>].values</span><br><span class="line">X_train,X_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">clf = RandomForestRegressor(n_estimators=<span class="number">400</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line">print(y_pred)</span><br><span class="line">print(y_test)</span><br><span class="line">print(sum(abs(y_pred - y_test))/len(y_pred))</span><br></pre></td></tr></table></figure>

<h3 id="检验"><a href="#检验" class="headerlink" title="检验"></a>检验</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># clf:训练模型</span></span><br><span class="line">rfr = clf</span><br><span class="line">test_data[cols].isnull().sum()</span><br><span class="line">test_data[<span class="string">'GarageCars'</span>].describe()</span><br><span class="line">test_data[<span class="string">'TotalBsmtSF'</span>].describe()</span><br><span class="line"></span><br><span class="line">cols2 = [<span class="string">'OverallQual'</span>,<span class="string">'GrLivArea'</span>, <span class="string">'FullBath'</span>, <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">cars = test_data[<span class="string">'GarageCars'</span>].fillna(<span class="number">1.766118</span>)</span><br><span class="line">bsmt = test_data[<span class="string">'TotalBsmtSF'</span>].fillna(<span class="number">1046.117970</span>)</span><br><span class="line">data_test_x = pd.concat( [test_data[cols2], cars, bsmt] ,axis=<span class="number">1</span>)</span><br><span class="line">data_test_x.isnull().sum()</span><br><span class="line"></span><br><span class="line">x = data_test_x.values</span><br><span class="line">y_te_pred = rfr.predict(x)</span><br><span class="line">print(y_te_pred)</span><br><span class="line"></span><br><span class="line">print(y_te_pred.shape)</span><br><span class="line">print(x.shape)</span><br><span class="line">print(data_test_x)</span><br></pre></td></tr></table></figure>
<p><strong>输出数据表格式：1459 rows × 7 columns</strong></p>
<h3 id="输出结果到文件"><a href="#输出结果到文件" class="headerlink" title="输出结果到文件"></a>输出结果到文件</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prediction = pd.DataFrame(y_te_pred, columns=[<span class="string">'SalePrice'</span>])</span><br><span class="line">result = pd.concat([ test_data[<span class="string">'Id'</span>], prediction], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># result = result.drop(resultlt.columns[0], 1)</span></span><br><span class="line">result.columns</span><br><span class="line"><span class="comment"># 保存预测结果</span></span><br><span class="line">result.to_csv(<span class="string">'./Predictions.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>梯度问题</tag>
        <tag>拟合与误差</tag>
      </tags>
  </entry>
  <entry>
    <title>从模型训练中认知拟合现象</title>
    <url>/2020/02/28/%E4%BB%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%AD%E8%AE%A4%E7%9F%A5%E6%8B%9F%E5%90%88%E7%8E%B0%E8%B1%A1/</url>
    <content><![CDATA[<p>机器学习中模型训练是必需的，在模型训练中存在两类典型的问题：</p>
<blockquote>
<p>欠拟合 (underfitting) </p>
<blockquote>
<p>模型无法得到较低的训练误差</p>
</blockquote>
<p>过拟合 (overfitting)</p>
<blockquote>
<p>模型的训练误差远小于它在测试数据集上的误差</p>
</blockquote>
</blockquote>
<p>实际训练过程中可能会出现两类问题的并发症，而且会有多种因素直接或间接地导致这种情况出现</p>
<h2 id="影响因素"><a href="#影响因素" class="headerlink" title="影响因素"></a>影响因素</h2><center>介绍其中两个因素：模型复杂度和训练数据集大小。</center>

<h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><p>以多项式函数拟合为例。给定一个由标量数据特征$x$和对应的标量标签$y$组成的训练数据集，多项式函数拟合的目标是找一个$K$阶多项式函数</p>
<p>$$<br> \hat{y} = b + \sum_{k=1}^K x^k w_k<br>$$</p>
<p>来近似 $y$。</p>
<p>在上式中，$w_k$是模型的权重参数，$b$是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。</p>
<p>给定训练数据集，模型复杂度和误差之间的关系：</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjMjd3eG9qLnBuZw?x-oss-process=image/format,png"></center>

<h3 id="训练数据集大小"><a href="#训练数据集大小" class="headerlink" title="训练数据集大小"></a>训练数据集大小</h3><p>一般来说，如果训练数据集中<strong>样本数过少</strong>，特别是比模型参数数量（按元素计）更少时，<strong>过拟合</strong>更容易发生。</p>
<p>此外，<strong>泛化误差</strong>不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p>
<hr>
<center>训练误差和泛化误差</center><br/>

<p>&ensp;&ensp;&ensp;&ensp;通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。</p>
<p>&ensp;&ensp;&ensp;&ensp;计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。</p>
<p>&ensp;&ensp;&ensp;&ensp;机器学习模型应关注降低泛化误差。</p>
<hr>
<h2 id="多项式函数拟合实验"><a href="#多项式函数拟合实验" class="headerlink" title="多项式函数拟合实验"></a>多项式函数拟合实验</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_train, n_test, true_w, true_b = <span class="number">100</span>, <span class="number">100</span>, [<span class="number">1.2</span>, <span class="number">-3.4</span>, <span class="number">5.6</span>], <span class="number">5</span></span><br><span class="line">features = torch.randn((n_train + n_test, <span class="number">1</span>))</span><br><span class="line">poly_features = torch.cat((features, torch.pow(features, <span class="number">2</span>), torch.pow(features, <span class="number">3</span>)), <span class="number">1</span>) </span><br><span class="line">labels = (true_w[<span class="number">0</span>] * poly_features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * poly_features[:, <span class="number">1</span>]</span><br><span class="line">          + true_w[<span class="number">2</span>] * poly_features[:, <span class="number">2</span>] + true_b)</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float)</span><br></pre></td></tr></table></figure>

<h3 id="定义和训练参数模型"><a href="#定义和训练参数模型" class="headerlink" title="定义和训练参数模型"></a>定义和训练参数模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">semilogy</span><span class="params">(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,</span></span></span><br><span class="line"><span class="function"><span class="params">             legend=None, figsize=<span class="params">(<span class="number">3.5</span>, <span class="number">2.5</span>)</span>)</span>:</span></span><br><span class="line">    <span class="comment"># d2l.set_figsize(figsize)</span></span><br><span class="line">    d2l.plt.xlabel(x_label)</span><br><span class="line">    d2l.plt.ylabel(y_label)</span><br><span class="line">    d2l.plt.semilogy(x_vals, y_vals)</span><br><span class="line">    <span class="keyword">if</span> x2_vals <span class="keyword">and</span> y2_vals:</span><br><span class="line">        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=<span class="string">':'</span>)</span><br><span class="line">        d2l.plt.legend(legend)</span><br><span class="line"></span><br><span class="line">num_epochs, loss = <span class="number">100</span>, torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot</span><span class="params">(train_features, test_features, train_labels, test_labels)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化网络模型</span></span><br><span class="line">    net = torch.nn.Linear(train_features.shape[<span class="number">-1</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 通过Linear文档可知，pytorch已经将参数初始化了，所以我们这里就不手动初始化了</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置批量大小</span></span><br><span class="line">    batch_size = min(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])    </span><br><span class="line">     <span class="comment"># 设置数据集</span></span><br><span class="line">    dataset = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">    <span class="comment"># 设置获取数据方式     </span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>) </span><br><span class="line">    <span class="comment"># 设置优化函数，使用的是随机梯度下降优化</span></span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)                      </span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    	<span class="comment"># 取一个批量的数据</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:               </span><br><span class="line">        	<span class="comment"># 输入到网络中计算输出，并和标签比较求得损失函数                                  </span></span><br><span class="line">            l = loss(net(X), y.view(<span class="number">-1</span>, <span class="number">1</span>)) </span><br><span class="line">            <span class="comment"># 梯度清零，防止梯度累加干扰优化                                   </span></span><br><span class="line">            optimizer.zero_grad()                                               </span><br><span class="line">            l.backward() <span class="comment"># 求梯度</span></span><br><span class="line">            optimizer.step() <span class="comment"># 迭代优化函数，进行参数优化</span></span><br><span class="line">        train_labels = train_labels.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        test_labels = test_labels.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将训练损失保存到train_ls中</span></span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).item())</span><br><span class="line">        <span class="comment"># 将测试损失保存到test_ls中         </span></span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).item())            </span><br><span class="line">    print(<span class="string">'final epoch: train loss'</span>, train_ls[<span class="number">-1</span>], <span class="string">'test loss'</span>, test_ls[<span class="number">-1</span>])    </span><br><span class="line">    semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">             range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'weight:'</span>, net.weight.data,</span><br><span class="line">          <span class="string">'\nbias:'</span>, net.bias.data)</span><br></pre></td></tr></table></figure>

<h3 id="对比三种拟合现象"><a href="#对比三种拟合现象" class="headerlink" title="对比三种拟合现象"></a>对比三种拟合现象</h3><blockquote>
<p>正常</p>
<blockquote>
<p>fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:])<br><img src="https://img-blog.csdnimg.cn/20200216165618535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></p>
</blockquote>
</blockquote>
<blockquote>
<p>欠拟合</p>
<blockquote>
<p>fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train], labels[n_train:])<br><img src="https://img-blog.csdnimg.cn/20200216172452546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" width=400 /></p>
</blockquote>
</blockquote>
<blockquote>
<p>过拟合</p>
<blockquote>
<p>fit_and_plot(poly_features[0:2, :], poly_features[n_train:, :], labels[0:2], labels[n_train:])<br><img src="https://img-blog.csdnimg.cn/20200216172416864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" width=400/></p>
</blockquote>
</blockquote>
<h2 id="针对两类拟合问题的解决办法"><a href="#针对两类拟合问题的解决办法" class="headerlink" title="针对两类拟合问题的解决办法"></a>针对两类拟合问题的解决办法</h2><h3 id="权重衰减–过拟合"><a href="#权重衰减–过拟合" class="headerlink" title="权重衰减–过拟合"></a>权重衰减–过拟合</h3><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>&ensp;&ensp;&ensp;&ensp;权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。</p>
<h4 id="L2-范数正则化（regularization）"><a href="#L2-范数正则化（regularization）" class="headerlink" title="L2 范数正则化（regularization）"></a>L2 范数正则化（regularization）</h4><p>&ensp;&ensp;&ensp;&ensp;$L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以线性回归中的线性回归损失函数为例</p>
<p>$$<br> \ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2<br>$$</p>
<p>其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为</p>
<p>$$<br>\ell(w_1, w_2, b) + \frac{\lambda}{2n} |\boldsymbol{w}|^2,<br>$$</p>
<p>其中超参数$\lambda &gt; 0$。当权重参数均为0时，惩罚项最小。当$\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。</p>
<p>&ensp;&ensp;&ensp;&ensp;有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104269986" target="_blank" rel="noopener">Design and Realization of Linear Regression</a> 中权重$w_1$和$w_2$的迭代方式更改为</p>
<p>$$<br> \begin{aligned} w_1 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\ w_2 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}<br>$$</p>
<p>可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。</p>
<p>因此，<code>$L_2$范数正则化又叫权重衰减</code>。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，<code>这可能对过拟合有效</code>。</p>
<h4 id="应用【剑指高维线性回归带来的过拟合】"><a href="#应用【剑指高维线性回归带来的过拟合】" class="headerlink" title="应用【剑指高维线性回归带来的过拟合】"></a>应用【剑指高维线性回归带来的过拟合】</h4><p>&ensp;&ensp;&ensp;&ensp;设数据样本特征的维度为$p$。对于训练数据集和测试数据集中特征为$x_1, x_2, \ldots, x_p$的任一样本，使用如下的线性函数来生成该样本的标签：</p>
<p>$$<br> y = 0.05 + \sum_{i = 1}^p 0.01x_i + \epsilon<br>$$</p>
<p>其中噪声项$\epsilon$服从均值为0、标准差为0.01的正态分布。为了较容易地观察过拟合，我们考虑高维线性回归问题，如设维度$p=200$；同时，我们特意把训练数据集的样本数设低，如20。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h5 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_train, n_test, num_inputs = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span></span><br><span class="line">true_w, true_b = torch.ones(num_inputs, <span class="number">1</span>) * <span class="number">0.01</span>, <span class="number">0.05</span></span><br><span class="line"></span><br><span class="line">features = torch.randn((n_train + n_test, num_inputs))</span><br><span class="line">labels = torch.matmul(features, true_w) + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float)</span><br><span class="line">train_features, test_features = features[:n_train, :], features[n_train:, :]</span><br><span class="line">train_labels, test_labels = labels[:n_train], labels[n_train:]</span><br><span class="line"><span class="comment"># 定义参数初始化函数，初始化模型参数并且附上梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_params</span><span class="params">()</span>:</span></span><br><span class="line">    w = torch.randn((num_inputs, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure>
<h5 id="定义L2范数惩罚项"><a href="#定义L2范数惩罚项" class="headerlink" title="定义L2范数惩罚项"></a>定义L2范数惩罚项</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l2_penalty</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (w**<span class="number">2</span>).sum() / <span class="number">2</span></span><br></pre></td></tr></table></figure>

<h5 id="训练和测试"><a href="#训练和测试" class="headerlink" title="训练和测试"></a>训练和测试</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size, num_epochs, lr = <span class="number">1</span>, <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">net, loss = d2l.linreg, d2l.squared_loss</span><br><span class="line"></span><br><span class="line">dataset = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot</span><span class="params">(lambd)</span>:</span></span><br><span class="line">    w, b = init_params()</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="comment"># 添加了L2范数惩罚项</span></span><br><span class="line">            l = loss(net(X, w, b), y) + lambd * l2_penalty(w)</span><br><span class="line">            l = l.sum()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> w.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                w.grad.data.zero_()</span><br><span class="line">                b.grad.data.zero_()</span><br><span class="line">            l.backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())</span><br><span class="line">        test_ls.append(loss(net(test_features, w, b), test_labels).mean().item())</span><br><span class="line">    d2l.semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">                 range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'L2 norm of w:'</span>, w.norm().item())</span><br></pre></td></tr></table></figure>
<h5 id="对比过拟合与权重衰减处理"><a href="#对比过拟合与权重衰减处理" class="headerlink" title="对比过拟合与权重衰减处理"></a>对比过拟合与权重衰减处理</h5><blockquote>
<p>过拟合</p>
<blockquote>
<p>fit_and_plot(lambd=0)<br><img src="https://img-blog.csdnimg.cn/20200216172039167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></p>
</blockquote>
</blockquote>
<blockquote>
<p>权重衰减处理</p>
<blockquote>
<p>fit_and_plot(lambd=3)<br><img src="https://img-blog.csdnimg.cn/20200216172215405.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></p>
</blockquote>
</blockquote>
<h4 id="权重衰减的简化"><a href="#权重衰减的简化" class="headerlink" title="权重衰减的简化"></a>权重衰减的简化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot_pytorch</span><span class="params">(wd)</span>:</span></span><br><span class="line">    <span class="comment"># 对权重参数衰减。权重名称一般是以weight结尾</span></span><br><span class="line">    net = nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.weight, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.bias, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) <span class="comment"># 对权重参数衰减</span></span><br><span class="line">    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  <span class="comment"># 不对偏差参数衰减</span></span><br><span class="line">    </span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X), y).mean()</span><br><span class="line">            optimizer_w.zero_grad()</span><br><span class="line">            optimizer_b.zero_grad()</span><br><span class="line">            </span><br><span class="line">            l.backward()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差</span></span><br><span class="line">            optimizer_w.step()</span><br><span class="line">            optimizer_b.step()</span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).mean().item())</span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).mean().item())</span><br><span class="line">    d2l.semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">                 range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'L2 norm of w:'</span>, net.weight.data.norm().item())</span><br></pre></td></tr></table></figure>
<blockquote>
<p>训练结果与手动实现基本一致</p>
</blockquote>
<h3 id="丢弃法–过拟合"><a href="#丢弃法–过拟合" class="headerlink" title="丢弃法–过拟合"></a>丢弃法–过拟合</h3><blockquote>
<p>只能用于模型训练</p>
</blockquote>
<p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104287595" target="_blank" rel="noopener">Multilayer Perceptron &amp; Classify image</a> 中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \ldots, 5$）的计算表达式为</p>
<p>$$<br> h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right)<br>$$</p>
<p>这里$\phi$是激活函数，$x_1, \ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \ldots, w_{4i}$，偏差参数为$b_i$。</p>
<p>当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。</p>
<p>丢弃概率是丢弃法的超参数。具体来说，设随机变量$\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i’$</p>
<p>$$<br> h_i’ = \frac{\xi_i}{1-p} h_i<br>$$</p>
<p>由于$E(\xi_i) = 1-p$，因此</p>
<p>$$<br> E(h_i’) = \frac{E(\xi_i)}{1-p}h_i = h_i<br>$$</p>
<p>即丢弃法不改变其输入的期望值。</p>
<p>对之前多层感知机的神经网络中的隐藏层使用丢弃法，一种可能的结果如图所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \ldots, h_5$中的任一个。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpkNjlpbjNtLnBuZw?x-oss-process=image/format,png" /></center>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h4 id="丢弃函数"><a href="#丢弃函数" class="headerlink" title="丢弃函数"></a>丢弃函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">‘’‘</span><br><span class="line">:param:drop_prob:丢失率</span><br><span class="line">:param:keep_prob:保存率</span><br><span class="line">’‘’</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(X, drop_prob)</span>:</span></span><br><span class="line">    X = X.float()</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= drop_prob &lt;= <span class="number">1</span></span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    <span class="comment"># 这种情况下把全部元素都丢弃 keep_prob 是保存率</span></span><br><span class="line">    <span class="keyword">if</span> keep_prob == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    mask = (torch.rand(X.shape) &lt; keep_prob).float()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mask * X / keep_prob</span><br></pre></td></tr></table></figure>

<h4 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 参数的初始化</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b1 = torch.zeros(num_hiddens1, requires_grad=<span class="literal">True</span>)</span><br><span class="line">W2 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b2 = torch.zeros(num_hiddens2, requires_grad=<span class="literal">True</span>)</span><br><span class="line">W3 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b3 = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2, W3, b3]</span><br></pre></td></tr></table></figure>

<h4 id="添加丢弃层"><a href="#添加丢弃层" class="headerlink" title="添加丢弃层"></a>添加丢弃层</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">drop_prob1, drop_prob2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X, is_training=True)</span>:</span></span><br><span class="line">    X = X.view(<span class="number">-1</span>, num_inputs)</span><br><span class="line">    H1 = (torch.matmul(X, W1) + b1).relu()</span><br><span class="line">    <span class="keyword">if</span> is_training:  <span class="comment"># 只在训练模型时使用丢弃法</span></span><br><span class="line">        H1 = dropout(H1, drop_prob1)  <span class="comment"># 在第一层全连接后添加丢弃层</span></span><br><span class="line">    H2 = (torch.matmul(H1, W2) + b2).relu()</span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">        H2 = dropout(H2, drop_prob2)  <span class="comment"># 在第二层全连接后添加丢弃层</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(H2, W3) + b3</span><br></pre></td></tr></table></figure>

<h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net)</span>:</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">if</span> isinstance(net, torch.nn.Module):</span><br><span class="line">            net.eval()  <span class="comment"># 评估模式, 这会关闭dropout</span></span><br><span class="line">            acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">            net.train()  <span class="comment"># 改回训练模式</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 自定义的模型</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="string">'is_training'</span> <span class="keyword">in</span> net.__code__.co_varnames):  <span class="comment"># 如果有is_training这个参数</span></span><br><span class="line">                <span class="comment"># 将is_training设置成False</span></span><br><span class="line">                acc_sum += (net(X, is_training=<span class="literal">False</span>).argmax(dim=<span class="number">1</span>)</span><br><span class="line">                            == y).float().sum().item()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br></pre></td></tr></table></figure>
<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs, lr, batch_size = <span class="number">5</span>, <span class="number">100.0</span>, <span class="number">256</span>  <span class="comment"># 这里的学习率设置的很大，原因与之前相同。</span></span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(</span><br><span class="line">    batch_size, root=<span class="string">'path to FashionMNIST2065'</span>)</span><br><span class="line">d2l.train_ch3(</span><br><span class="line">    net,</span><br><span class="line">    train_iter,</span><br><span class="line">    test_iter,</span><br><span class="line">    loss,</span><br><span class="line">    num_epochs,</span><br><span class="line">    batch_size,</span><br><span class="line">    params,</span><br><span class="line">    lr)</span><br></pre></td></tr></table></figure>
<h4 id="丢弃法的简化"><a href="#丢弃法的简化" class="headerlink" title="丢弃法的简化"></a>丢弃法的简化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens1),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob1),</span><br><span class="line">        nn.Linear(num_hiddens1, num_hiddens2), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob2),</span><br><span class="line">        nn.Linear(num_hiddens2, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    nn.init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>

<center><b>综上<b/><center/><br/>

<p>对于训练误差较低但是泛化误差依然较高，二者相差较大的过拟合 </p>
<ul>
<li>权重衰减</li>
<li>丢弃法</li>
<li>更加全面的方案【<a href="https://www.cnblogs.com/XDU-Lakers/p/10536101.html" target="_blank" rel="noopener">https://www.cnblogs.com/XDU-Lakers/p/10536101.html</a>】</li>
</ul>
<p>对于模型无法达到一个较低的误差的欠拟合</p>
<ul>
<li><p>增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间</p>
</li>
<li><p>添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强</p>
</li>
<li><p>减少正则化参数，正则化的目的是用来防止过拟合的，但是模型出现了欠拟合，则需要减少正则化参数</p>
</li>
<li><p>使用非线性模型，比如核SVM 、决策树、深度学习等模型</p>
</li>
<li><p>调整模型的容量(capacity)，通俗地，模型的容量是指其拟合各种函数的能力</p>
</li>
<li><p>容量低的模型可能很难拟合训练集；使用集成学习方法，如Bagging ,将多个弱学习器Bagging</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>L2范数正则化</tag>
        <tag>拟合与误差</tag>
      </tags>
  </entry>
  <entry>
    <title>ModernRNN</title>
    <url>/2020/02/28/ModernRNN/</url>
    <content><![CDATA[<p>在循环神经网络的基础上进行了 RNN 的改进，我将介绍四种进化版的循环神经网络</p>
<ol>
<li>GRU</li>
<li>LSTM</li>
<li>深度循环神经网络</li>
<li>双向循环神经网络</li>
</ol>
<blockquote>
<p>循环神经网络初识：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104307813</a><br/><br>RNN 出现的梯度爆炸和梯度衰减问题</p>
<blockquote>
<p>解决梯度爆炸的裁剪梯度方法：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104307813</a></p>
</blockquote>
</blockquote>
<blockquote>
<p>解决梯度衰减问题</p>
</blockquote>
<h2 id="GRU-RNN"><a href="#GRU-RNN" class="headerlink" title="GRU RNN"></a>GRU RNN</h2><p>称为 [ 门控循环神经网络 ] ：通过捕捉时间序列中时间步较大的依赖关系</p>
<p>对比  <code>普通神经网络</code> 与 <code>GRU</code></p>
<center>
    <img src="https://img-blog.csdnimg.cn/20200216134327427.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70"/>
</center>
<br/>
<center>
    <img src="https://img-blog.csdnimg.cn/20200216134348404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70"/>
</center>

<p>• 重置⻔ : 有助于捕捉时间序列⾥短期的依赖关系;<br/><br>• 更新⻔ : 有助于捕捉时间序列⾥⻓期的依赖关系。  </p>
<hr>
<p><strong>参数详释</strong><br/><br>根据参数理解需要初始化的参数，首先表达式中的权重和偏置，6个W和3个b，是更新门、重置门和候选隐藏状态的初始化，紧接着作为下一个 GRU 的 $H_{t-1}$ ,此阶段输出时需要初始化输出层参数。</p>
<p>那么假如是第一层这样没有再上一层的$H_{t-1}$输入,就需要初始化最初的状态</p>
<hr>
<h3 id="实践中理解设计"><a href="#实践中理解设计" class="headerlink" title="实践中理解设计"></a>实践中理解设计</h3><blockquote>
<p>尽管一个 nn.GRU 包揽全盘，但是为了理解 GRU 的设计…</p>
</blockquote>
<h4 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line"><span class="comment"># print('will use', device)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32) <span class="comment">#正态分布</span></span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line">     </span><br><span class="line">    W_xz, W_hz, b_z = _three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = _three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = _three()  <span class="comment"># 候选隐藏状态参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_gru_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span>   <span class="comment">#隐藏状态初始化</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>
<h4 id="GRU-模型"><a href="#GRU-模型" class="headerlink" title="GRU 模型"></a>GRU 模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gru</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh(torch.matmul(X, W_xh) + R * torch.matmul(H, W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H,)</span><br></pre></td></tr></table></figure>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><blockquote>
<p>较 GRUB，LSTM 多了记忆功能，也就是结构上多了个记忆细胞</p>
</blockquote>
<p>包含三个门，引入了记忆细胞，和隐藏状态相似，用来记忆额外的信息</p>
<center>
<img src="https://img-blog.csdnimg.cn/20200216140544366.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" />
</center>

<center>长短期记忆 ( long short-term memory )</center><br />

<ul>
<li>遗忘门:控制上一时间步的记忆细胞 </li>
<li>输入门:控制当前时间步的输入  </li>
<li>输出门:控制从记忆细胞到隐藏状态  </li>
<li>记忆细胞：⼀种特殊的隐藏状态的信息的流动  </li>
</ul>
<h3 id="实践中理解设计-1"><a href="#实践中理解设计-1" class="headerlink" title="实践中理解设计"></a>实践中理解设计</h3><h4 id="初始化参数-1"><a href="#初始化参数-1" class="headerlink" title="初始化参数"></a>初始化参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">print(<span class="string">'will use'</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    W_xi, W_hi, b_i = _three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = _three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = _three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = _three()  <span class="comment"># 候选记忆细胞参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), </span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure>

<h4 id="LSTM-模型"><a href="#LSTM-模型" class="headerlink" title="LSTM 模型"></a>LSTM 模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">print(<span class="string">'will use'</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    W_xi, W_hi, b_i = _three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = _three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = _three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = _three()  <span class="comment"># 候选记忆细胞参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), </span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure>

<h2 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h2><blockquote>
<p>深度代表高度，对于神经网络隐藏层来说，并非如此，层数的加深会导致收敛困难</p>
</blockquote>
<p>对比循环神经网络：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104307813</a></p>
<div align=center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWprbTB2NDRpLnBuZw?x-oss-process=image/format,png" /></div>
深度循环神经网络就是多了隐藏层
<center><img src="https://img-blog.csdnimg.cn/20200216141700404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h4 id="实现方式相同，改变的是-num-layer"><a href="#实现方式相同，改变的是-num-layer" class="headerlink" title="实现方式相同，改变的是 num_layer"></a>实现方式相同，改变的是 num_layer</h4><h2 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h2><blockquote>
<p>常用于 NLP</p>
<blockquote>
<p>特点：预测不再仅依赖于前面的元素，而是同时结合了前后元素，一个词：content <br/><br>两层隐藏层之间的连接采用了concat的方式</p>
</blockquote>
</blockquote>
<center><img src="https://img-blog.csdnimg.cn/20200216143253637.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h4 id="改变的是-bidirectional-参数"><a href="#改变的是-bidirectional-参数" class="headerlink" title="改变的是 bidirectional 参数"></a>改变的是 bidirectional 参数</h4>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>梯度问题</tag>
      </tags>
  </entry>
  <entry>
    <title>Language Model &amp; Data Sampling</title>
    <url>/2020/02/28/Language-Model-&amp;-Data-Sampling/</url>
    <content><![CDATA[<h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><p>一段自然语言文本可以看作是一个离散时间序列，给定一个长度为$T$的词的序列$w_1, w_2, \ldots, w_T$。</p>
<p>语言模型的目标就是评估该序列是否合理，即计算该序列的概率：</p>
<p>$$<br>P(w_1, w_2, \ldots, w_T).<br>$$<br>假设序列$w_1, w_2, \ldots, w_T$中的每个词是依次生成的，则有</p>
<p>$$<br>\begin{aligned}<br>P(w_1, w_2, \ldots, w_T)<br>&amp;= \prod_{t=1}^T P(w_t \mid w_1, \ldots, w_{t-1})\\<br>&amp;= P(w_1)P(w_2 \mid w_1) \cdots P(w_T \mid w_1w_2\cdots w_{T-1})<br>\end {aligned}<br>$$<br>例如，一段含有4个词的文本序列的概率</p>
<p>$$<br>P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3).<br>$$</p>
<p>语言模型的参数就是词的概率以及给定前几个词情况下的条件概率。设训练数据集为一个大型文本语料库，如维基百科的所有条目.</p>
<p>词的概率可以通过该词在训练数据集中的相对词频来计算，例如，$w_1$的概率可以计算为：</p>
<p>$$<br>\hat P(w_1) = \frac{n(w_1)}{n}<br>$$</p>
<blockquote>
<p>其中$n(w_1)$为语料库中以$w_1$作为第一个词的文本的数量，$n$为语料库中文本的总数量。</p>
</blockquote>
<p>类似的，给定$w_1$情况下，$w_2$的条件概率可以计算为：</p>
<p>$$<br>\hat P(w_2 \mid w_1) = \frac{n(w_1, w_2)}{n(w_1)}<br>$$</p>
<blockquote>
<p>其中$n(w_1, w_2)$为语料库中以$w_1$作为第一个词，$w_2$作为第二个词的文本的数量。</p>
</blockquote>
<hr>
<center>加点料 < 统计学知识 ></center>

<h2 id="n-元语法"><a href="#n-元语法" class="headerlink" title="n 元语法"></a>n 元语法</h2><p>序列长度增加，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设简化模型，马尔科夫假设是指一个词的出现只与前面$n$个词相关，即$n$阶马尔可夫链（Markov chain of order $n$），如果$n=1$，那么有$P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)$。基于$n-1$阶马尔可夫链，我们可以将语言模型改写为</p>
<p>$$<br>P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1}) .<br>$$</p>
<p>以上也叫$n$元语法（$n$-grams），它是基于$n - 1$阶马尔可夫链的概率语言模型。例如，当$n=2$时，含有4个词的文本序列的概率就可以改写为:</p>
<p>$$<br>\begin{aligned}<br>P(w_1, w_2, w_3, w_4)<br>&amp;= P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3)\\<br>&amp;= P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3)<br>\end{aligned}<br>$$</p>
<p>当$n$分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列$w_1, w_2, w_3, w_4$在一元语法、二元语法和三元语法中的概率分别为</p>
<p>$$<br>\begin{aligned}<br>P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\<br>P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3) ,\\<br>P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_2, w_3) .<br>\end{aligned}<br>$$</p>
<p>当$n$较小时，$n$元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当$n$较大时，$n$元语法需要计算并存储大量的词频和多词相邻频率。</p>
<hr>
<blockquote>
<p>深入理解元语法的缺陷</p>
</blockquote>
<ul>
<li><p>参数空间过大 </p>
<p>  n 元语法当 n 足够大的时候词频和使用频率的计算会越来越大</p>
</li>
<li><p>数据稀疏 </p>
<p>  齐夫定律：按频率递减顺序排列的频率词表中，单词的频率与它的序号之间存在“幂律”(power law)关系，即如果把单词按使用频率排序，那么使用频率与序号之间几乎恰好成反比。</p>
</li>
</ul>
<blockquote>
<p>在缺陷的基础上寻找问题的解决办法</p>
<blockquote>
<p>数据采样</p>
</blockquote>
<p>随机采样 &amp;&amp; 相邻采样</p>
</blockquote>
<h3 id="引入数据集"><a href="#引入数据集" class="headerlink" title="引入数据集"></a>引入数据集</h3><blockquote>
<p>利用周杰伦的歌词作为数据集 jaychou_lyrics.txt</p>
<blockquote>
<p><a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/data/jaychou_lyrics.txt.zip?raw=true" target="_blank" rel="noopener">下载地址</a></p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># read data</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'path to jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    corpus_chars = f.read()</span><br><span class="line">print(len(corpus_chars))</span><br><span class="line">print(corpus_chars[: <span class="number">40</span>])</span><br><span class="line">corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">corpus_chars = corpus_chars[: <span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># build character index</span></span><br><span class="line">idx_to_char = list(set(corpus_chars)) <span class="comment"># 去重，得到索引到字符的映射</span></span><br><span class="line">char_to_idx = &#123;char: i <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)&#125; <span class="comment"># 字符到索引的映射 enumerate枚举</span></span><br><span class="line">vocab_size = len(char_to_idx)</span><br><span class="line">print(vocab_size)</span><br><span class="line">corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]  <span class="comment"># 将每个字符转化为索引，得到一个索引的序列</span></span><br><span class="line">sample = corpus_indices[: <span class="number">20</span>]</span><br><span class="line">print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample])) <span class="comment">#join 进行字符的拼接</span></span><br><span class="line">print(<span class="string">'indices:'</span>, sample)</span><br></pre></td></tr></table></figure>

<h3 id="数据采样"><a href="#数据采样" class="headerlink" title="数据采样"></a>数据采样</h3><p>在训练中我们需要每次随机读取小批量样本和标签。时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”，即$X$=“想要有直升”，$Y$=“要有直升机”。</p>
<p>现在我们考虑序列“想要有直升机，想要和你飞到宇宙去”，如果时间步数为5，有以下可能的样本和标签：</p>
<ul>
<li>$X$：“想要有直升”，$Y$：“要有直升机”</li>
<li>$X$：“要有直升机”，$Y$：“有直升机，”</li>
<li>$X$：“有直升机，”，$Y$：“直升机，想”</li>
<li>…</li>
<li>$X$：“要和你飞到”，$Y$：“和你飞到宇”</li>
<li>$X$：“和你飞到宇”，$Y$：“你飞到宇宙”</li>
<li>$X$：“你飞到宇宙”，$Y$：“飞到宇宙去”</li>
</ul>
<p>可以看到，如果序列的长度为$T$，时间步数为$n$，那么一共有$T-n$个合法的样本，但是这些样本有大量的重合，我们通常采用更加高效的采样方式。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。</p>
<h3 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h3><p>下面的代码每次从数据里随机采样一个小批量。其中批量大小<code>batch_size</code>是每个小批量的样本数，<code>num_steps</code>是每个样本所包含的时间步数。<br>在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。</p>
<div align=center>
<img src= "https://img-blog.csdnimg.cn/20200214095637385.png" />
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, device=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为对于长度为n的序列，X最多只有包含其中的前n - 1个字符</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps  <span class="comment"># 下取整，得到不重叠情况下的样本个数</span></span><br><span class="line">    example_indices = [i * num_steps <span class="keyword">for</span> i <span class="keyword">in</span> range(num_examples)]  <span class="comment"># 每个样本的第一个字符在corpus_indices中的下标</span></span><br><span class="line">    random.shuffle(example_indices) <span class="comment">#因为做随机采样，shuffle进行捣乱</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(i)</span>:</span></span><br><span class="line">        <span class="comment"># 返回从i开始的长为num_steps的序列</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[i: i + num_steps]</span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        <span class="comment"># 每次选出batch_size个随机样本</span></span><br><span class="line">        batch_indices = example_indices[i: i + batch_size]  <span class="comment"># 当前batch的各个样本的首字符的下标</span></span><br><span class="line">        X = [_data(j) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X, device=device), torch.tensor(Y, device=device)</span><br></pre></td></tr></table></figure>

<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_seq = list(range(<span class="number">30</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">6</span>):</span><br><span class="line">    print(<span class="string">'X: '</span>, X, <span class="string">'\nY:'</span>, Y, <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">X:  tensor([[ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">        [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">        [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]]) </span><br><span class="line"></span><br><span class="line">X:  tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [<span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="相邻采样"><a href="#相邻采样" class="headerlink" title="相邻采样"></a>相邻采样</h3><p>在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。</p>
<div align=center> <img src="https://img-blog.csdnimg.cn/20200214101159177.png" /></div>

<blockquote>
<p>三部分堆叠构成二维的 tensor</p>
</blockquote>
<div align=center><img src="https://img-blog.csdnimg.cn/20200214101954759.png" /></div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, device=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">    corpus_len = len(corpus_indices) // batch_size * batch_size  <span class="comment"># 保留下来的序列的长度</span></span><br><span class="line">    corpus_indices = corpus_indices[: corpus_len]  <span class="comment"># 仅保留前corpus_len个字符</span></span><br><span class="line">    indices = torch.tensor(corpus_indices, device=device)</span><br><span class="line">    indices = indices.view(batch_size, <span class="number">-1</span>)  <span class="comment"># resize成(batch_size, )</span></span><br><span class="line">    batch_num = (indices.shape[<span class="number">1</span>] - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_num):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i: i + num_steps] <span class="comment">#构建索引 Ｘ是样本</span></span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>: i + num_steps + <span class="number">1</span>]　<span class="comment"># Y 是标签</span></span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure>

<h4 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter_consecutive(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">6</span>):</span><br><span class="line">    print(<span class="string">'X: '</span>, X, <span class="string">'\nY:'</span>, Y, <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">X:  tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>]]) </span><br><span class="line"></span><br><span class="line">X:  tensor([[ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">        [<span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">        [<span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>]])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>元语法</tag>
      </tags>
  </entry>
  <entry>
    <title>Text Preprocessing</title>
    <url>/2020/02/28/Text-Preprocessing/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>打开 Google， 输入搜索关键词，显示上百条搜索结果</p>
</blockquote>
<blockquote>
<p>打开 Google Translate， 输入待翻译文本，翻译结果框中显示出翻译结果</p>
</blockquote>
<p>以上二者的共同点便是文本预处理 Pre-Processing</p>
</blockquote>
<p>在 NLP 项目中，文本预处理占据了超过半数的时间，其重要性不言而喻。</p>
<hr>
<div align=center>
当然<br>
也可以利用完备且效率可观的工具可以快速完成项目
</div>
For Example： 我一直在使用的由 哈工大社会计算与信息检索研究中心开发的 （LTP，Language Technology Platform ）语言技术平台

<hr>
<h1 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h1><p>文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：</p>
<p>【较为简单的步骤，如果需要专门做NLP相关的时候，要进行特殊的预处理】</p>
<ol>
<li>读入文本</li>
<li>分词</li>
<li>建立字典，将每个词映射到一个唯一的索引（index）</li>
<li>将文本从词的序列转换为索引的序列，方便输入模型</li>
</ol>
<h2 id="中英文文本预处理的特点"><a href="#中英文文本预处理的特点" class="headerlink" title="中英文文本预处理的特点"></a>中英文文本预处理的特点</h2><ul>
<li><p>中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般我们需要用分词算法来完成分词。</p>
</li>
<li><p>英文文本的预处理特殊在拼写问题，很多时候，对英文预处理要包括拼写检查，比如“Helo World”这样的错误，我们不能在分析的时候再去纠错。还有就是词干提取(stemming)和词形还原(lemmatization)，主要是因为英文中一个词会存在不同的形式，这个步骤有点像孙悟空的火眼金睛，直接得到单词的原始形态。For Example：” faster “、” fastest “ -&gt; “ fast “；“ leafs ”、“ leaves ” -&gt;  “ leaf “ 。</p>
</li>
</ul>
<blockquote>
<p>本文进行简要的介绍和实现<br>详细可参考：<a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/</a></p>
</blockquote>
<h3 id="读入文本"><a href="#读入文本" class="headerlink" title="读入文本"></a>读入文本</h3><p>引入数据源：<a href="http://www.gutenberg.org/ebooks/35" target="_blank" rel="noopener">http://www.gutenberg.org/ebooks/35</a>【小说 Time Machine】</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'path to timemachine.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f: <span class="comment">#每次处理一行</span></span><br><span class="line">        lines = [re.sub(<span class="string">'[^a-z]+'</span>, <span class="string">' '</span>, line.strip().lower()) <span class="keyword">for</span> line <span class="keyword">in</span> f]  <span class="comment">#正则表达式</span></span><br><span class="line">    <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(<span class="string">'# sentences %d'</span> % len(lines))</span><br></pre></td></tr></table></figure>

<h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>对每个句子进行分词，也就是将一个句子划分成若干个词（token），转换为一个词的序列。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentences, token=<span class="string">'word'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Split sentences into word or char tokens"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'word'</span>:</span><br><span class="line">        <span class="keyword">return</span> [sentence.split(<span class="string">' '</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>:</span><br><span class="line">        <span class="keyword">return</span> [list(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'ERROR: unkown token type '</span>+token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">tokens[<span class="number">0</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#分词结果:</span></span><br><span class="line">[[<span class="string">'the'</span>, <span class="string">'time'</span>, <span class="string">'machine'</span>, <span class="string">'by'</span>, <span class="string">'h'</span>, <span class="string">'g'</span>, <span class="string">'wells'</span>, <span class="string">''</span>], [<span class="string">''</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="建立字典"><a href="#建立字典" class="headerlink" title="建立字典"></a>建立字典</h3><p>为了方便模型处理，将字符串转换为数字。因此先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 构建Vocab类时注意：句子长度统计与构建字典无关 | 所以不必要进行句子长度统计</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">        counter = count_corpus(tokens)  <span class="comment">#&lt;key,value&gt;:&lt;词，词频&gt; </span></span><br><span class="line">        self.token_freqs = list(counter.items()) <span class="comment">#去重并统计词频</span></span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">            <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">            self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            self.idx_to_token += [<span class="string">'&lt;pad&gt;'</span>, <span class="string">'&lt;bos&gt;'</span>, <span class="string">'&lt;eos&gt;'</span>, <span class="string">'&lt;unk&gt;'</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.unk = <span class="number">0</span></span><br><span class="line">            self.idx_to_token += [<span class="string">'&lt;unk&gt;'</span>]</span><br><span class="line">        self.idx_to_token += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> self.token_freqs</span><br><span class="line">                        <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.idx_to_token]</span><br><span class="line">        self.token_to_idx = dict()</span><br><span class="line">        <span class="keyword">for</span> idx, token <span class="keyword">in</span> enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] = idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span> <span class="comment"># 返回字典的大小</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span>  <span class="comment"># 词到索引的映射</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span>  <span class="comment">#索引到词的映射</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    tokens = [tk <span class="keyword">for</span> st <span class="keyword">in</span> sentences <span class="keyword">for</span> tk <span class="keyword">in</span> st]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)  <span class="comment"># 返回一个字典，记录每个词的出现次数</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>&lt; unk &gt;    较为特殊，表示为登录词，无论 use_special_token 参数是否为真，都会用到</p>
</blockquote>
<h3 id="将词转为索引"><a href="#将词转为索引" class="headerlink" title="将词转为索引"></a>将词转为索引</h3><p>使用字典，我们可以将原文本中的句子从单词序列转换为索引序列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'words:'</span>, tokens[i])</span><br><span class="line">    print(<span class="string">'indices:'</span>, vocab[tokens[i]])</span><br></pre></td></tr></table></figure>
<hr>
<p>到此，按照常见步骤已经介绍完了</p>
<p>我们对于分词这一重要关卡需要考虑的更多，上边实现的简要分词许多情形还未完善，只能实现部分特定的分词</p>
<ol>
<li>标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了</li>
<li>类似“shouldn’t”, “doesn’t”这样的词会被错误地处理</li>
<li>类似”Mr.”, “Dr.”这样的词会被错误地处理</li>
</ol>
<p>一者，我们可以通过引入更复杂的规则来解决这些问题，但是事实上，有一些现有的工具可以很好地进行分词，在这里简单介绍其中的两个：<a href="https://spacy.io/" target="_blank" rel="noopener">spaCy</a>和<a href="https://www.nltk.org/" target="_blank" rel="noopener">NLTK</a>。</p>
<p>For Example :</p>
<blockquote>
<p>text = “Mr. Chen doesn’t agree with my suggestion.”</p>
</blockquote>
<h2 id="spaCy"><a href="#spaCy" class="headerlink" title="spaCy"></a>spaCy</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">doc = nlp(text)</span><br><span class="line">print([token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">[<span class="string">'Mr.'</span>, <span class="string">'Chen'</span>, <span class="string">'does'</span>, <span class="string">"n't"</span>, <span class="string">'agree'</span>, <span class="string">'with'</span>, <span class="string">'my'</span>, <span class="string">'suggestion'</span>, <span class="string">'.'</span>]</span><br></pre></td></tr></table></figure>

<h2 id="NLTK"><a href="#NLTK" class="headerlink" title="NLTK"></a>NLTK</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> data</span><br><span class="line">data.path.append(<span class="string">'/home/kesci/input/nltk_data3784/nltk_data'</span>)</span><br><span class="line">print(word_tokenize(text))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">[<span class="string">'Mr.'</span>, <span class="string">'Chen'</span>, <span class="string">'does'</span>, <span class="string">"n't"</span>, <span class="string">'agree'</span>, <span class="string">'with'</span>, <span class="string">'my'</span>, <span class="string">'suggestion'</span>, <span class="string">'.'</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Multilayer Perceptron &amp; Classify image</title>
    <url>/2020/02/28/Multilayer-Perceptron-&amp;-Classify-image/</url>
    <content><![CDATA[<h2 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h2><p>以多层感知机为例，概述多层神经网络</p>
<h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>此图为多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。</p>
<div align=center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWhvNjg0am1oLnBuZw?x-oss-process=image/format,png"/> </div>

<h3 id="表达公式"><a href="#表达公式" class="headerlink" title="表达公式"></a>表达公式</h3><p>具体来说，给定一个小批量样本$\boldsymbol{X} \in \mathbb{R}^{n \times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层【其中隐藏单元个数为$h$】记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\boldsymbol{H}$，有$\boldsymbol{H} \in \mathbb{R}^{n \times h}$。</p>
<p>因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\boldsymbol{W}_h \in \mathbb{R}^{d \times h}$和 $\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，输出层的权重和偏差参数分别为$\boldsymbol{W}_o \in \mathbb{R}^{h \times q}$和$\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}$。</p>
<p>含单隐藏层的多层感知机的设计。其输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算为</p>
<p>$$<br> \begin{aligned} \boldsymbol{H} &amp;= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\\ \boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}<br>$$</p>
<p>也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到</p>
<p>$$<br> \boldsymbol{O} = (\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X} \boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o.<br>$$</p>
<blockquote>
<blockquote>
<p>存在的问题</p>
</blockquote>
<ul>
<li>虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络</li>
<li>其中输出层权重参数为$\boldsymbol{W}_h\boldsymbol{W}_o$，偏差参数为$\boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o$、</li>
</ul>
</blockquote>
<blockquote>
<p>结论：隐藏层未起到作用</p>
</blockquote>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><blockquote>
<blockquote>
<p>问题解释</p>
</blockquote>
<p>全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。</p>
</blockquote>
<blockquote>
<blockquote>
<p>解决方法</p>
</blockquote>
<p>引入非线性变换<br>Example:对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。<br>这个非线性函数被称为激活函数（activation function）。</p>
</blockquote>
<ul>
<li><h4 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h4>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为</li>
</ul>
<p>$$<br>\text{ReLU}(x) = \max(x, 0).<br>$$</p>
<p>可以看出，ReLU函数只保留正数元素，并将负数元素清零。</p>
<div align=center><img src=" https://img-blog.csdnimg.cn/20200228104905655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70 " /></div>


<ul>
<li><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4>sigmoid函数可以将元素的值变换到0和1之间：</li>
</ul>
<p>$$<br>\text{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.<br>$$</p>
<div align=center><img src=" https://img-blog.csdnimg.cn/20200228104942978.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70 " /> </div>

<ul>
<li><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h4>tanh（双曲正切）函数可以将元素的值变换到-1和1之间：</li>
</ul>
<p>$$<br>\text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.<br>$$</p>
<p>我们接着绘制tanh函数。当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。</p>
<div align=center><img src="https://img-blog.csdnimg.cn/20200228105017679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></div>

<h4 id="激活函数的选择"><a href="#激活函数的选择" class="headerlink" title="激活函数的选择"></a>激活函数的选择</h4><blockquote>
<blockquote>
<p>ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</p>
</blockquote>
<blockquote>
<p>用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。  </p>
</blockquote>
<blockquote>
<p>在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</p>
</blockquote>
<blockquote>
<p>在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</p>
</blockquote>
</blockquote>
<p>那么之前表达式中输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算变为：</p>
<p>$$<br> \begin{aligned} \boldsymbol{H} &amp;= \phi(\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h),\\ \boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}<br>$$</p>
<p>其中$\phi$表示激活函数。</p>
<h2 id="多层感知机实现"><a href="#多层感知机实现" class="headerlink" title="多层感知机实现"></a>多层感知机实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure>
<h3 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,</span><br><span class="line">			root=<span class="string">'path to FashionMNIST.zip'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="定义模型参数"><a href="#定义模型参数" class="headerlink" title="定义模型参数"></a>定义模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, num_hiddens)), dtype=torch.float)</span><br><span class="line">b1 = torch.zeros(num_hiddens, dtype=torch.float)</span><br><span class="line">W2 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_hiddens, num_outputs)), dtype=torch.float)</span><br><span class="line">b2 = torch.zeros(num_outputs, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="定义激活函数"><a href="#定义激活函数" class="headerlink" title="定义激活函数"></a>定义激活函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.max(input=X, other=torch.tensor(<span class="number">0.0</span>))</span><br><span class="line"><span class="comment">#进行0和Ｘ的大小比较</span></span><br></pre></td></tr></table></figure>

<h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    X = X.view((<span class="number">-1</span>, num_inputs))</span><br><span class="line">    H = relu(torch.matmul(X, W1) + b1)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(H, W2) + b2</span><br></pre></td></tr></table></figure>

<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">100.0</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="多层感知机Pytorch简化"><a href="#多层感知机Pytorch简化" class="headerlink" title="多层感知机Pytorch简化"></a>多层感知机Pytorch简化</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init model and param</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">    </span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(num_hiddens, num_outputs), </span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> params <span class="keyword">in</span> net.parameters():</span><br><span class="line">    init.normal_(params, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>

<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=<span class="string">'path to FashionMNIST.zip'</span>)</span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>MLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Softmax &amp; 分类模型</title>
    <url>/2020/02/28/Softmax%20&amp;%20%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h1><blockquote>
<p>与候选采样相对</p>
</blockquote>
<p>Softmax function, a wonderful activation function that turns numbers aka logits into probabilities that sum to one. Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes.</p>
<p>一种函数，可提供多类别分类模型中每个可能类别的概率。这些概率的总和正好为 1.0。</p>
<p>Example: softmax 可能会得出某个图像是狗、猫和马的概率分别是 0.9、0.08 和 0.02。（也称为完整 softmax。）<img src="https://img-blog.csdnimg.cn/20200212195050635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="softmax的基本概念"><a href="#softmax的基本概念" class="headerlink" title="softmax的基本概念"></a>softmax的基本概念</h2><ul>
<li><p>分类问题<br>一个简单的图像分类问题，输入图像的高和宽均为2像素，色彩为灰度。<br>图像中的4像素分别记为$x_1, x_2, x_3, x_4$。<br>假设真实标签为狗、猫或者鸡，这些标签对应的离散值为$y_1, y_2, y_3$。<br>我们通常使用离散的数值来表示类别，例如$y_1=1, y_2=2, y_3=3$。</p>
</li>
<li><p>权重矢量  </p>
</li>
</ul>
<p>$$<br> \begin{aligned} o_1 &amp;= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1 \end{aligned}<br>$$</p>
<p>$$<br> \begin{aligned} o_2 &amp;= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2 \end{aligned}<br>$$</p>
<p>$$<br> \begin{aligned} o_3 &amp;= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3 \end{aligned}<br>$$</p>
<ul>
<li>神经网络图<br>下图用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$o_1, o_2, o_3$的计算都要依赖于所有的输入$x_1, x_2, x_3, x_4$，softmax回归的输出层也是一个全连接层。</li>
</ul>
<div align=center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWhteW1lem9nLnBuZw?x-oss-process=image/format,png" /></div> 

<p>$$<br>\begin{aligned}softmax回归是一个单层神经网络\end{aligned}<br>$$</p>
<p>既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$当作预测类别是$i$的置信度，并将值最大的输出所对应的类作为预测输出，即输出 $\underset{i}{\arg\max} o_i$。例如，如果$o_1,o_2,o_3$分别为$0.1,10,0.1$，由于$o_2$最大，那么预测类别为2，其代表猫。</p>
<ul>
<li>输出问题<br>直接使用输出层的输出有两个问题：<ol>
<li>一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果$o_1=o_3=10^3$，那么输出值10却又表示图像类别为猫的概率很低。</li>
<li>另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。</li>
</ol>
</li>
</ul>
<p>softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：</p>
<p>$$<br> \hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3)<br>$$</p>
<p>其中</p>
<p>$$<br> \hat{y}1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.<br>$$</p>
<p>容易看出$\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$且$0 \leq \hat{y}_1, \hat{y}_2, \hat{y}_3 \leq 1$，因此$\hat{y}_1, \hat{y}_2, \hat{y}_3$是一个合法的概率分布。这时候，如果$\hat{y}_2=0.8$，不管$\hat{y}_1$和$\hat{y}_3$的值是多少，我们都知道图像类别为猫的概率是80%。此外，我们注意到</p>
<p>$$<br> \underset{i}{\arg\max} o_i = \underset{i}{\arg\max} \hat{y}_i<br>$$</p>
<p>因此softmax运算不改变预测类别输出。</p>
<ul>
<li>计算效率<ul>
<li>单样本矢量计算表达式<br>为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为</li>
</ul>
</li>
</ul>
<p>$$<br> \boldsymbol{W} = \begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} \\ w_{21} &amp; w_{22} &amp; w_{23} \\ w_{31} &amp; w_{32} &amp; w_{33} \\ w_{41} &amp; w_{42} &amp; w_{43} \end{bmatrix},\quad \boldsymbol{b} = \begin{bmatrix} b_1 &amp; b_2 &amp; b_3 \end{bmatrix},<br>$$</p>
<p>设高和宽分别为2个像素的图像样本$i$的特征为</p>
<p>$$<br>\boldsymbol{x}^{(i)} = \begin{bmatrix}x_1^{(i)} &amp; x_2^{(i)} &amp; x_3^{(i)} &amp; x_4^{(i)}\end{bmatrix},<br>$$</p>
<p>输出层的输出为</p>
<p>$$<br>\boldsymbol{o}^{(i)} = \begin{bmatrix}o_1^{(i)} &amp; o_2^{(i)} &amp; o_3^{(i)}\end{bmatrix},<br>$$</p>
<p>预测为狗、猫或鸡的概率分布为</p>
<p>$$<br>\boldsymbol{\hat{y}}^{(i)} = \begin{bmatrix}\hat{y}_1^{(i)} &amp; \hat{y}_2^{(i)} &amp; \hat{y}_3^{(i)}\end{bmatrix}.<br>$$</p>
<p>softmax回归对样本$i$分类的矢量计算表达式为</p>
<p>$$<br> \begin{aligned} \boldsymbol{o}^{(i)} &amp;= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{y}}^{(i)} &amp;= \text{softmax}(\boldsymbol{o}^{(i)}). \end{aligned}<br>$$</p>
<ul>
<li>小批量矢量计算表达式<br>  为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为$n$，输入个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$\boldsymbol{X} \in \mathbb{R}^{n \times d}$。假设softmax回归的权重和偏差参数分别为$\boldsymbol{W} \in \mathbb{R}^{d \times q}$和$\boldsymbol{b} \in \mathbb{R}^{1 \times q}$。softmax回归的矢量计算表达式为</li>
</ul>
<p>$$<br> \begin{aligned} \boldsymbol{O} &amp;= \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{Y}} &amp;= \text{softmax}(\boldsymbol{O}), \end{aligned}<br>$$</p>
<p>其中的加法运算使用了广播机制，$\boldsymbol{O}, \boldsymbol{\hat{Y}} \in \mathbb{R}^{n \times q}$且这两个矩阵的第$i$行分别为样本$i$的输出$\boldsymbol{o}^{(i)}$和概率分布$\boldsymbol{\hat{y}}^{(i)}$。</p>
<hr>
<p>两种操作对比</p>
<blockquote>
<p>numpy 操作：np.exp(x) / np.sum(np.exp(x), axis=0)<br>pytorch 操作：torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1,1)</p>
</blockquote>
<h2 id="引入Fashion-MNIST"><a href="#引入Fashion-MNIST" class="headerlink" title="引入Fashion-MNIST"></a>引入Fashion-MNIST</h2><blockquote>
<p>为方便介绍 Softmax， 为了更加直观的观察到算法之间的差异<br>引入较为复杂的多分类图像分类数据集</p>
<blockquote>
<p>导入：torchvision 包【构建计算机视觉模型】</p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import needed package</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(torch.__version__)</span></span><br><span class="line"><span class="comment"># print(torchvision.__version__)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get dataset</span></span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">'path to file storge FashionMNIST.zip'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">'path to file storge FashionMNIST.zip'</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fashion_mnist_labels</span><span class="params">(labels)</span>:</span></span><br><span class="line">    text_labels = [<span class="string">'t-shirt'</span>, <span class="string">'trouser'</span>, <span class="string">'pullover'</span>, <span class="string">'dress'</span>, <span class="string">'coat'</span>,</span><br><span class="line">                   <span class="string">'sandal'</span>, <span class="string">'shirt'</span>, <span class="string">'sneaker'</span>, <span class="string">'bag'</span>, <span class="string">'ankle boot'</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[int(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_fashion_mnist</span><span class="params">(images, labels)</span>:</span></span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    <span class="comment"># 这里的_表示我们忽略（不使用）的变量</span></span><br><span class="line">    _, figs = plt.subplots(<span class="number">1</span>, len(images), figsize=(<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> f, img, lbl <span class="keyword">in</span> zip(figs, images, labels):</span><br><span class="line">        f.imshow(img.view((<span class="number">28</span>, <span class="number">28</span>)).numpy())</span><br><span class="line">        f.set_title(lbl)</span><br><span class="line">        f.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        f.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">X, y = [], []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    X.append(mnist_train[i][<span class="number">0</span>]) <span class="comment"># 将第i个feature加到X中</span></span><br><span class="line">    y.append(mnist_train[i][<span class="number">1</span>]) <span class="comment"># 将第i个label加到y中</span></span><br><span class="line">show_fashion_mnist(X, get_fashion_mnist_labels(y))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># read data</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">num_workers = <span class="number">4</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line">print(<span class="string">'%.2f sec'</span> % (time.time() - start))</span><br></pre></td></tr></table></figure>

<h2 id="Softmax-手动实现"><a href="#Softmax-手动实现" class="headerlink" title="Softmax 手动实现"></a>Softmax 手动实现</h2><h3 id="import-package-and-module"><a href="#import-package-and-module" class="headerlink" title="import package and module"></a>import package and module</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br><span class="line">print(torchvision.__version__)</span><br></pre></td></tr></table></figure>
<h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>

<h3 id="模型参数初始化"><a href="#模型参数初始化" class="headerlink" title="模型参数初始化"></a>模型参数初始化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init module param </span></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">print(<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, num_outputs)), dtype=torch.float)</span><br><span class="line">b = torch.zeros(num_outputs, dtype=torch.float)</span><br><span class="line">W.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line">b.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Sofmax-定义"><a href="#Sofmax-定义" class="headerlink" title="Sofmax 定义"></a>Sofmax 定义</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define softmax function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></span><br><span class="line">    X_exp = X.exp()</span><br><span class="line">    partition = X_exp.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># print("X size is ", X_exp.size())</span></span><br><span class="line">    <span class="comment"># print("partition size is ", partition, partition.size())</span></span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure>

<h3 id="softmax-回归模型"><a href="#softmax-回归模型" class="headerlink" title="softmax 回归模型"></a>softmax 回归模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define regression model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> softmax(torch.mm(X.view((<span class="number">-1</span>, num_inputs)), W) + b)</span><br></pre></td></tr></table></figure>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat.gather(<span class="number">1</span>, y.view(<span class="number">-1</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="准确率"><a href="#准确率" class="headerlink" title="准确率"></a>准确率</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat.argmax(dim=<span class="number">1</span>) == y).float().mean().item()</span><br></pre></td></tr></table></figure>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch3</span><span class="params">(net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">              params=None, lr=None, optimizer=None)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y).sum()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">            <span class="keyword">elif</span> params <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line">            </span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                d2l.sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                optimizer.step() </span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            train_l_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).sum().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)</span><br></pre></td></tr></table></figure>
<h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = iter(test_iter).next()</span><br><span class="line"></span><br><span class="line">true_labels = d2l.get_fashion_mnist_labels(y.numpy())</span><br><span class="line">pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=<span class="number">1</span>).numpy())</span><br><span class="line">titles = [true + <span class="string">'\n'</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> zip(true_labels, pred_labels)]</span><br><span class="line"></span><br><span class="line">d2l.show_fashion_mnist(X[<span class="number">0</span>:<span class="number">9</span>], titles[<span class="number">0</span>:<span class="number">9</span>])</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Pytorch-改进"><a href="#Pytorch-改进" class="headerlink" title="Pytorch 改进"></a>Pytorch 改进</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<h3 id="初始化参数和获取数据"><a href="#初始化参数和获取数据" class="headerlink" title="初始化参数和获取数据"></a>初始化参数和获取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init param and get data</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<h3 id="定义网络模型"><a href="#定义网络模型" class="headerlink" title="定义网络模型"></a>定义网络模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_inputs, num_outputs)</span>:</span></span><br><span class="line">        super(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(num_inputs, num_outputs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># x 的形状: (batch, 1, 28, 28)</span></span><br><span class="line">        y = self.linear(x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line"><span class="comment"># net = LinearNet(num_inputs, num_outputs)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FlattenLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(FlattenLayer, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># x 的形状: (batch, *, *, ...)</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        <span class="comment"># FlattenLayer(),</span></span><br><span class="line">        <span class="comment"># LinearNet(num_inputs, num_outputs) </span></span><br><span class="line">        OrderedDict([</span><br><span class="line">           (<span class="string">'flatten'</span>, FlattenLayer()),</span><br><span class="line">           (<span class="string">'linear'</span>, nn.Linear(num_inputs, num_outputs))]) <span class="comment"># 或者写成我们自己定义的 LinearNet(num_inputs, num_outputs) 也可以</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init module param</span></span><br><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net.linear.bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss() <span class="comment"># 下面是他的函数原型</span></span><br><span class="line"><span class="comment"># class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')</span></span><br></pre></td></tr></table></figure>

<h3 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>) <span class="comment"># 下面是函数原型</span></span><br><span class="line"><span class="comment"># class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)</span></span><br></pre></td></tr></table></figure>

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>

<h3 id="训练结果分析"><a href="#训练结果分析" class="headerlink" title="训练结果分析"></a>训练结果分析</h3><p><img src="https://img-blog.csdnimg.cn/20200212213623439.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>最开始：训练数据集上的准确率低于测试数据集上的准确率</p>
</blockquote>
<blockquote>
<blockquote>
<p>原因</p>
</blockquote>
<p>训练集上的准确率是在一个epoch的过程中计算得到的<br>测试集上的准确率是在一个epoch结束后计算得到的<br>Result: 后者的模型参数更优</p>
</blockquote>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Softmax</tag>
      </tags>
  </entry>
  <entry>
    <title>Design and Realization of Linear Regression</title>
    <url>/2020/02/28/Design-and-Realization-of%20Linear-Regression/</url>
    <content><![CDATA[<h2 id="线性回归的基本要素"><a href="#线性回归的基本要素" class="headerlink" title="线性回归的基本要素"></a>线性回归的基本要素</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>$$<br>\mathrm{y} = w \cdot \mathrm{x} + b<br>$$</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们在收集到的数据中寻找合适的模型参数来使模型的预测价格与真实价格的误差最小。被训练的数据的集合称为训练数据集（training data set）或训练集（training set），每一条数据的主体作为一个样本（sample），被预测值称作标签（label），用来预测标签的因素叫作特征（feature）。特征用来表征样本的特点。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为 $i$ 的样本误差的表达式为</p>
<p>$$<br>l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,<br>$$</p>
<p>$$<br>L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.<br>$$</p>
<h3 id="优化函数-随机梯度下降"><a href="#优化函数-随机梯度下降" class="headerlink" title="优化函数 - 随机梯度下降"></a>优化函数 - 随机梯度下降</h3><p>鉴于大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值，在这种求数值解的优化算法中，通常使用小批量随机梯度下降（mini-batch stochastic gradient descent）。</p>
<blockquote>
<p>算法</p>
</blockquote>
<p>先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。   </p>
<p>$$<br>(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)<br>$$</p>
<p>学习率: $\eta$代表在每次优化中，能够学习的步长的大小<br>批量大小: $\mathcal{B}$是小批量计算中的批量大小batch size   </p>
<p>总结一下，优化函数的有以下两个步骤：</p>
<ul>
<li>初始化模型参数，一般使用随机初始化；</li>
<li>在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。</li>
</ul>
<h2 id="线性回归模型的实现"><a href="#线性回归模型的实现" class="headerlink" title="线性回归模型的实现"></a>线性回归模型的实现</h2><blockquote>
<p>以房屋为样本，价格为标签，面积和房龄为特征<br>$$<br>\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b<br>$$</p>
</blockquote>
<h3 id="导入包和模块"><a href="#导入包和模块" class="headerlink" title="导入包和模块"></a>导入包和模块</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import packages and modules</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>使用线性模型来生成数据集，生成一个 1000 个样本的数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set input feature number </span></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line"><span class="comment"># set example number</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set true weight and bias in order to generate corresponded label</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line"></span><br><span class="line">features = torch.randn(num_examples, num_inputs,</span><br><span class="line">                      dtype=torch.float32) <span class="comment">#1000*2 vector</span></span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()),</span><br><span class="line">                       dtype=torch.float32) <span class="comment">#偏差通过正态分布随机生成</span></span><br></pre></td></tr></table></figure>
<h3 id="通过图像观察生成的数据合适程度"><a href="#通过图像观察生成的数据合适程度" class="headerlink" title="通过图像观察生成的数据合适程度"></a>通过图像观察生成的数据合适程度</h3><p><img src="https://img-blog.csdnimg.cn/20200211213555899.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(batch_size, features, labels)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(indices)  <span class="comment"># random read 10 samples</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) <span class="comment"># the last time may be not enough for a whole batch</span></span><br><span class="line">        <span class="keyword">yield</span>  features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span> <span class="comment"># read 10 samples</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels): <span class="comment"># input features and labels</span></span><br><span class="line">    print(X, <span class="string">'\n'</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init parameter</span></span><br><span class="line">w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, <span class="number">1</span>)), dtype=torch.float32)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">w.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line">b.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linreg</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.mm(X, w) + b</span><br></pre></td></tr></table></figure>
<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><blockquote>
<p>均方误差损失函数<br>$$<br>l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,<br>$$</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span><span class="params">(y_hat, y)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.view(y_hat.size())) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><blockquote>
<p>小批量随机梯度下降优化</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define optimization function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span> </span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size <span class="comment"># ues .data to operate param without gradient track</span></span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># super parameters init</span></span><br><span class="line">lr = <span class="number">0.03</span> <span class="comment">#学习率</span></span><br><span class="line">num_epochs = <span class="number">5</span> <span class="comment">#训练周期</span></span><br><span class="line"></span><br><span class="line">net = linreg <span class="comment">#单层网络</span></span><br><span class="line">loss = squared_loss <span class="comment">#均方误差损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):  <span class="comment"># training repeats num_epochs times</span></span><br><span class="line">    <span class="comment"># in each epoch, all the samples in dataset will be used once</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># X is the feature and y is the label of a batch sample</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y).sum()  </span><br><span class="line">        <span class="comment"># calculate the gradient of batch sample loss </span></span><br><span class="line">        l.backward()  </span><br><span class="line">        <span class="comment"># using small batch random gradient descent to iter model parameters</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  </span><br><span class="line">        <span class="comment"># reset parameter gradient</span></span><br><span class="line">        w.grad.data.zero_()<span class="comment">#参数梯度清零，为防止叠加</span></span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, train_l.mean().item()))</span><br></pre></td></tr></table></figure>
<h3 id="检验训练结果"><a href="#检验训练结果" class="headerlink" title="检验训练结果"></a>检验训练结果</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># output result of trainning </span></span><br><span class="line">w, true_w, b, true_b</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="使用-torch-简化代码"><a href="#使用-torch-简化代码" class="headerlink" title="使用 torch 简化代码"></a>使用 torch 简化代码</h2><blockquote>
<p>未单独重写的步骤默认与上一致</p>
</blockquote>
<h3 id="读取数据集-1"><a href="#读取数据集-1" class="headerlink" title="读取数据集"></a>读取数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># combine featues and labels of dataset</span></span><br><span class="line">dataset = Data.TensorDataset(features, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># put dataset into DataLoader</span></span><br><span class="line">data_iter = Data.DataLoader(</span><br><span class="line">    dataset=dataset,            <span class="comment"># torch TensorDataset format</span></span><br><span class="line">    batch_size=batch_size,      <span class="comment"># mini batch size</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,               <span class="comment"># whether shuffle the data or not</span></span><br><span class="line">    num_workers=<span class="number">2</span>,              <span class="comment"># read data in multithreading</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">    print(X, <span class="string">'\n'</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature)</span>:</span></span><br><span class="line">        super(LinearNet, self).__init__()      <span class="comment"># call father function to init </span></span><br><span class="line">        self.linear = nn.Linear(n_feature, <span class="number">1</span>)  <span class="comment"># function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line">net = LinearNet(num_inputs) </span><br><span class="line">print(net) <span class="comment">#单层线性网络</span></span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line">init.normal_(net[<span class="number">0</span>].weight, mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net[<span class="number">0</span>].bias, val=<span class="number">0.0</span>)  <span class="comment"># or you can use `net[0].bias.data.fill_(0)` to modify it directly</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    print(param)</span><br></pre></td></tr></table></figure>

<h3 id="定义损失函数-1"><a href="#定义损失函数-1" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.MSELoss()    <span class="comment"># nn built-in squared loss function</span></span><br><span class="line">                       <span class="comment"># function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`</span></span><br></pre></td></tr></table></figure>

<h3 id="定义优化函数-1"><a href="#定义优化函数-1" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)   <span class="comment"># built-in random gradient descent function</span></span><br><span class="line">print(optimizer)  <span class="comment"># function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`</span></span><br></pre></td></tr></table></figure>
<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)   <span class="comment"># built-in random gradient descent function</span></span><br><span class="line">print(optimizer)  <span class="comment"># function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`</span></span><br></pre></td></tr></table></figure>

<h3 id="检验训练结果-1"><a href="#检验训练结果-1" class="headerlink" title="检验训练结果"></a>检验训练结果</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># result comparision</span></span><br><span class="line">dense = net[<span class="number">0</span>]</span><br><span class="line">print(true_w, dense.weight.data)</span><br><span class="line">print(true_b, dense.bias.data)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Linear Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer(Google 机器翻译模型)</title>
    <url>/2020/02/26/Transformer(Goole%20%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B)/</url>
    <content><![CDATA[<center><b><font size=5>双壁合一</font></b></center>

<p><strong>卷积神经网络(CNNS)</strong></p>
<ul>
<li><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104380762" target="_blank" rel="noopener">Fundamentals of Convolutional Neural Networks</a></p>
</li>
<li><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104381637" target="_blank" rel="noopener">LeNet &amp;&amp; ModernCNN</a></p>
</li>
</ul>
<blockquote>
<p>CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。</p>
</blockquote>
<p><strong>循环神经网络(RNNS)</strong></p>
<ul>
<li><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">Fundamentals of Recurrent Neural Network</a></p>
</li>
<li><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104340035" target="_blank" rel="noopener">ModernRNN</a></p>
</li>
</ul>
<blockquote>
<p>RNNs 适合捕捉长距离变长序列的依赖，但是自身的recurrent特性却难以实现并行化处理序列。</p>
</blockquote>
<p><strong>整合CNN和RNN的优势，Vaswani et al., 2017 创新性地使用注意力机制设计了 Transformer 模型。</strong></p>
<hr>
<p>该模型利用 attention 机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的 tokens ，上述优势使得 Transformer 模型在性能优异的同时大大减少了训练时间。</p>
<hr>
<p>如图展示了 Transformer 模型的架构，与<a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">机器翻译及其相关技术介绍</a>中介绍的 seq2seq <strong>相似</strong>，Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：</p>
<ol>
<li>Transformer blocks：$循环网络_{seq2seq模型}$–&gt; Transformer Blocks<br/><br>Transform Blocks模块包含一个多头注意力层（Multi-head Attention Layers）以及两个 position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。</li>
<li>Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理<br>该层包含残差结构以及<font color=gree>层归一化</font>。</li>
<li>Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。</li>
</ol>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwYmoyY2o1LnBuZw?x-oss-process=image/format,png" alt="Fig. 10.3.1 The Transformer architecture."></p>
<p>$$<br>Transformer 架构.<br>$$</p>
<p>鉴于新子块第一次出现，在此前 CNNS 和 RNNS 的基础上，实现 Transform 子模块，并且就<a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">机器翻译及其相关技术介绍</a>中的英法翻译数据集实现一个新的机器翻译模型。</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'path to file storge d2lzh1981'</span>)</span><br><span class="line"><span class="keyword">import</span> d2l</span><br></pre></td></tr></table></figure>

<h2 id="masked-softmax"><a href="#masked-softmax" class="headerlink" title="masked softmax"></a>masked softmax</h2><blockquote>
<p>参考<a href="https://blog.csdn.net/RokoBasilisk/article/details/104369799" target="_blank" rel="noopener">$注意力机制和Seq2seq模型_{工具1}$</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">-1e6</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    X_len = X_len.to(X.device)</span><br><span class="line">    <span class="comment">#print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\n',X_len[:, None] )</span></span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float, device=X.device)</span><br><span class="line">    mask = mask[<span class="literal">None</span>, :] &lt; X_len[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment">#print(mask)</span></span><br><span class="line">    X[~mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span><span class="params">(X, valid_length)</span>:</span></span><br><span class="line">    <span class="comment"># X: 3-D tensor, valid_length: 1-D or 2-D tensor</span></span><br><span class="line">    softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> softmax(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_length.dim() == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = valid_length.reshape((<span class="number">-1</span>,))</span><br><span class="line">        <span class="comment"># fill masked elements with a large negative, whose exp is 0</span></span><br><span class="line">        X = SequenceMask(X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>])), valid_length)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> softmax(X).reshape(shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># query: (batch_size, #queries, d)</span></span><br><span class="line">    <span class="comment"># key: (batch_size, #kv_pairs, d)</span></span><br><span class="line">    <span class="comment"># value: (batch_size, #kv_pairs, dim_v)</span></span><br><span class="line">    <span class="comment"># valid_length: either (batch_size, ) or (batch_size, xx)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length=None)</span>:</span></span><br><span class="line">        d = query.shape[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># set transpose_b=True to swap the last two dimensions of key</span></span><br><span class="line">        scores = torch.bmm(query, key.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure>
<h2 id="多头注意力层"><a href="#多头注意力层" class="headerlink" title="多头注意力层"></a>多头注意力层</h2><hr>
<p>引入:<strong>自注意力（self-attention）</strong></p>
<p>自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。</p>
<p>与循环神经网络相比，自注意力对每个元素输出的<strong>计算是并行</strong>的，所以我们可以<strong>高效</strong>的实现这个模块。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwY2t2MzhxLnBuZw?x-oss-process=image/format,png" alt="Fig. 10.3.2 自注意力结构"></p>
<p>$$<br>自注意力结构<br>$$</p>
<p>$$<br>输出了一个与输入长度相同的表征序列<br>$$</p>
<hr>
<p>多头注意力层包含$h$个<strong>并行的自注意力层</strong>，每一个这种层被成为一个head。</p>
<p>对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，这$h$个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwY3NvemlkLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>多头注意力<br>$$</p>
<p>假设query，key和value的维度分别是$d_q$、$d_k$和$d_v$。那么对于每一个头$i=1,\ldots,h$，我们可以训练相应的模型权重$W_q^{(i)} \in \mathbb{R}^{p_q\times d_q}$、$W_k^{(i)} \in \mathbb{R}^{p_k\times d_k}$和$W_v^{(i)} \in \mathbb{R}^{p_v\times d_v}$，以得到每个头的输出：</p>
<p>$$<br>o^{(i)} = attention(W_q^{(i)}q, W_k^{(i)}k, W_v^{(i)}v)<br>$$</p>
<p>这里的attention可以是任意的attention function，之后我们将所有head对应的输出拼接起来，送入最后一个线性层进行整合，这个层的权重可以表示为$W_o\in \mathbb{R}^{d_0 \times hp_v}$</p>
<p>$$<br>o = W_o[o^{(1)}, \ldots, o^{(h)}]<br>$$</p>
<p>接下来实现多头注意力，假设有h个头，隐藏层权重 $hidden_size = p_q = p_k = p_v$ 与query，key，value的维度一致。除此之外，因为多头注意力层保持输入与输出张量的维度不变，所以输出feature 的维度也设置为 $d_0 = hidden_size$。</p>
<h3 id="MultiHeadAttention-class"><a href="#MultiHeadAttention-class" class="headerlink" title="MultiHeadAttention class"></a><div id ="MultiHeadAttention">MultiHeadAttention class</div></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, num_heads, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_k = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_v = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_o = nn.Linear(hidden_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length)</span>:</span></span><br><span class="line">        <span class="comment"># query, key, and value shape: (batch_size, seq_len, dim),</span></span><br><span class="line">        <span class="comment"># where seq_len is the length of input sequence</span></span><br><span class="line">        <span class="comment"># valid_length shape is either (batch_size, )</span></span><br><span class="line">        <span class="comment"># or (batch_size, seq_len).</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Project and transpose query, key, and value from</span></span><br><span class="line">        <span class="comment"># (batch_size, seq_len, hidden_size * num_heads) to</span></span><br><span class="line">        <span class="comment"># (batch_size * num_heads, seq_len, hidden_size).</span></span><br><span class="line">        </span><br><span class="line">        query = transpose_qkv(self.W_q(query), self.num_heads)</span><br><span class="line">        key = transpose_qkv(self.W_k(key), self.num_heads)</span><br><span class="line">        value = transpose_qkv(self.W_v(value), self.num_heads)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Copy valid_length by num_heads times</span></span><br><span class="line">            device = valid_length.device</span><br><span class="line">            valid_length = valid_length.cpu().numpy() <span class="keyword">if</span> valid_length.is_cuda <span class="keyword">else</span> valid_length.numpy()</span><br><span class="line">            <span class="keyword">if</span> valid_length.ndim == <span class="number">1</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">            valid_length = valid_length.to(device)</span><br><span class="line">            </span><br><span class="line">        output = self.attention(query, key, value, valid_length)</span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure>
<h3 id="转置函数"><a href="#转置函数" class="headerlink" title="转置函数"></a>转置函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_qkv</span><span class="params">(X, num_heads)</span>:</span></span><br><span class="line">    <span class="comment"># Original X shape: (batch_size, seq_len, hidden_size * num_heads),</span></span><br><span class="line">    <span class="comment"># -1 means inferring its value, after first reshape, X shape:</span></span><br><span class="line">    <span class="comment"># (batch_size, seq_len, num_heads, hidden_size)</span></span><br><span class="line">    X = X.view(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)</span></span><br><span class="line">    X = X.transpose(<span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Merge the first two dimensions. Use reverse=True to infer shape from</span></span><br><span class="line">    <span class="comment"># right to left.</span></span><br><span class="line">    <span class="comment"># output shape: (batch_size * num_heads, seq_len, hidden_size)</span></span><br><span class="line">    output = X.view(<span class="number">-1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Saved in the d2l package for later use</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_output</span><span class="params">(X, num_heads)</span>:</span></span><br><span class="line">    <span class="comment"># A reversed version of transpose_qkv</span></span><br><span class="line">    X = X.view(<span class="number">-1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.transpose(<span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">    <span class="keyword">return</span> X.view(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], <span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="测试输出"><a href="#测试输出" class="headerlink" title="测试输出"></a>测试输出</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cell = MultiHeadAttention(<span class="number">5</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">0.5</span>)</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">valid_length = torch.FloatTensor([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">cell(X, X, X, valid_length).shape</span><br></pre></td></tr></table></figure>

<h2 id="Position-Wise-Feed-Forward-Networks"><a href="#Position-Wise-Feed-Forward-Networks" class="headerlink" title="Position-Wise Feed-Forward Networks"></a>Position-Wise Feed-Forward Networks</h2><p>Transformer 模块另一个非常重要的部分就是<strong>基于位置的前馈网络（FFN）</strong>，它接受一个形状为（batch_size，seq_length, feature_size）的三维张量。</p>
<p>Position-wise FFN 由两个全连接层组成，它们<strong>作用在最后一维</strong>上。因为序列的每个位置的状态都会被单独地更新，所以我们称为 position-wise，其等效于一个 1x1 的卷积。</p>
<h3 id="Position-wise-FFN-class"><a href="#Position-wise-FFN-class" class="headerlink" title="Position-wise FFN  class"></a>Position-wise FFN  class</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionWiseFFN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, ffn_hidden_size, hidden_size_out, **kwargs)</span>:</span></span><br><span class="line">        super(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.ffn_1 = nn.Linear(input_size, ffn_hidden_size)</span><br><span class="line">        self.ffn_2 = nn.Linear(ffn_hidden_size, hidden_size_out)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.ffn_2(F.relu(self.ffn_1(X)))</span><br></pre></td></tr></table></figure>
<p>与<a href="#MultiHeadAttention">多头注意力层</a>相似，FFN层同样只会对最后一维的大小进行改变；除此之外，对于两个完全相同的输入，FFN层的输出也将相等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">out = ffn(torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line">print(out, out.shape)</span><br></pre></td></tr></table></figure>
<h2 id="Add-and-Norm"><a href="#Add-and-Norm" class="headerlink" title="Add and Norm"></a>Add and Norm</h2><blockquote>
<p>Transformer还有一个重要的<strong>相加归一化层</strong></p>
</blockquote>
<p>它可以平滑地整合输入和其他层的输出，因此我们在每个多头注意力层和 FFN 层后面都添加一个含残差连接的Layer Norm层。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener">Layer Norm 相似于 Batch Norm</a> </p>
</blockquote>
<p><strong>唯一的区别</strong></p>
<blockquote>
<ul>
<li>Batch Norm是对于batch size这个维度进行计算均值和方差的，</li>
<li>Layer Norm则是对最后一维进行计算。<br/></li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">layernorm = nn.LayerNorm(normalized_shape=<span class="number">2</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">batchnorm = nn.BatchNorm1d(num_features=<span class="number">2</span>, affine=<span class="literal">True</span>)</span><br><span class="line">X = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">print(<span class="string">'layer norm:'</span>, layernorm(X))</span><br><span class="line">print(<span class="string">'batch norm:'</span>, batchnorm(X))</span><br></pre></td></tr></table></figure>

<p><font color=gree>层归一化</font>可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能。</p>
<h3 id="AddNorm-class"><a href="#AddNorm-class" class="headerlink" title="AddNorm class"></a>AddNorm class</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.norm = nn.LayerNorm(hidden_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>【注意】：由于残差连接，X和Y需要有相同的维度。</p>
</blockquote>
<h3 id="模块测试"><a href="#模块测试" class="headerlink" title="模块测试"></a>模块测试</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_norm = AddNorm(<span class="number">4</span>, <span class="number">0.5</span>)</span><br><span class="line">add_norm(torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)), torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))).shape</span><br></pre></td></tr></table></figure>

<hr>
<p>以上是Transformer 模型的三个模块，还记得 Transformer 模型高效并行的特性，得益于：</p>
<blockquote>
<p>多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新</p>
</blockquote>
<p>但是在这种特性下，却丢失了重要的序列顺序的信息，为了更好的捕捉序列信息，需要一种可以保持序列元素位置的模块，因而引入位置编码。</p>
<h1 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h1><p>假设输入序列的嵌入表示 $X\in \mathbb{R}^{l\times d}$, 序列长度为$l$嵌入向量维度为$d$，则其位置编码为$P \in \mathbb{R}^{l\times d}$ ，输出的向量就是二者相加 $X + P$。</p>
<p>位置编码是一个二维的矩阵，i对应着序列中的顺序，j对应其embedding vector内部的维度索引。我们可以通过以下等式计算位置编码：</p>
<p>$$<br>P_{i,2j} = sin(i/10000^{2j/d})<br>$$</p>
<p>$$<br>P_{i,2j+1} = cos(i/10000^{2j/d})<br>$$</p>
<p>$$<br>for\ i=0,\ldots, l-1\ and\ j=0,\ldots,\lfloor (d-1)/2 \rfloor<br>$$</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwZTBsdTM4LnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>位置编码<br>$$</p>
<h2 id="PositionalEncoding-class"><a href="#PositionalEncoding-class" class="headerlink" title="PositionalEncoding class"></a>PositionalEncoding class</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, dropout, max_len=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.P = np.zeros((<span class="number">1</span>, max_len, embedding_size))</span><br><span class="line">        X = np.arange(<span class="number">0</span>, max_len).reshape(<span class="number">-1</span>, <span class="number">1</span>) / np.power(</span><br><span class="line">            <span class="number">10000</span>, np.arange(<span class="number">0</span>, embedding_size, <span class="number">2</span>)/embedding_size)</span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = np.sin(X)</span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = np.cos(X)</span><br><span class="line">        self.P = torch.FloatTensor(self.P)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> X.is_cuda <span class="keyword">and</span> <span class="keyword">not</span> self.P.is_cuda:</span><br><span class="line">            self.P = self.P.cuda()</span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br></pre></td></tr></table></figure>
<h2 id="模块测试-1"><a href="#模块测试-1" class="headerlink" title="模块测试"></a>模块测试</h2><p>可视化其中四个维度，可以看到，第 4 维和第 5 维有相同的频率但偏置不同。第 6 维和第 7 维具有更低的频率；因此 positional encoding 对于不同维度具有可区分性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">Y = pe(torch.zeros((<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))).numpy()</span><br><span class="line">d2l.plot(np.arange(<span class="number">100</span>), Y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].T, figsize=(<span class="number">6</span>, <span class="number">2.5</span>),</span><br><span class="line">         legend=[<span class="string">"dim %d"</span> % p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200219115650513.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>$$<br>Result-to-Test<br>$$</p>
<h1 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h1><p>有了组成Transformer的各个模块，可以搭建一个编码器。</p>
<p>编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层。</p>
<p>对于attention模型以及FFN模型，由于残差连接导致输出维度都是与 embedding 维度一致的，因此要将前一层的输出与原始输入相加并归一化。</p>
<h2 id="Encoder-Block基础块"><a href="#Encoder-Block基础块" class="headerlink" title="Encoder Block基础块"></a>Encoder Block基础块</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, ffn_hidden_size, num_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, **kwargs)</span>:</span></span><br><span class="line">        super(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_1 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)</span><br><span class="line">        self.addnorm_2 = AddNorm(embedding_size, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, valid_length)</span>:</span></span><br><span class="line">        Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm_2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># batch_size = 2, seq_len = 100, embedding_size = 24</span></span><br><span class="line"><span class="comment"># ffn_hidden_size = 48, num_head = 8, dropout = 0.5</span></span><br><span class="line"></span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">encoder_blk = EncoderBlock(<span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder_blk(X, valid_length).shape</span><br></pre></td></tr></table></figure>
<p>整个编码器由 n 个 Encoder Block 堆叠而成，利用 Encoder Block 基础块实现 Transformer 编码器。</p>
<p><strong>两个注意点：</strong></p>
<ul>
<li>残差连接的缘故，中间状态的维度始终与嵌入向量的维度 d 一致；</li>
<li>同时注意到我们把嵌入向量乘以 $\sqrt{d}$ 以防止其值过小。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span><span class="params">(d2l.Encoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_size, ffn_hidden_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads, num_layers, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size)</span><br><span class="line">        self.pos_encoding = PositionalEncoding(embedding_size, dropout)</span><br><span class="line">        self.blks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.blks.append(</span><br><span class="line">                EncoderBlock(embedding_size, ffn_hidden_size,</span><br><span class="line">                             num_heads, dropout))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, valid_length, *args)</span>:</span></span><br><span class="line">        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X = blk(X, valid_length)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test encoder</span></span><br><span class="line">encoder = TransformerEncoder(<span class="number">200</span>, <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>)).long(), valid_length).shape</span><br></pre></td></tr></table></figure>
<h1 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h1><p>Transformer 模型的解码器与编码器结构类似，然而，除了之前介绍的几个模块之外，解码器部分有另一个子模块。该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。</p>
<p>与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和<font color=gree>层归一化</font>将各个子层的输出相连。</p>
<p>在第t个时间步，当前输入$x_t$是query，那么self attention接受了第t步以及前t-1步的所有输入$x_1,\ldots, x_{t-1}$。在训练时，由于第t位置的输入可以观测到全部的序列，这与预测阶段的情形项矛盾，所以我们要通过将第t个时间步所对应的可观测长度设置为t，以消除不需要看到的未来的信息。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwZWZoY3lnLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<h2 id="Decoder-Block-基础块"><a href="#Decoder-Block-基础块" class="headerlink" title="Decoder Block 基础块"></a>Decoder Block 基础块</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, ffn_hidden_size, num_heads,dropout,i,**kwargs)</span>:</span></span><br><span class="line">        super(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention_1 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_1 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.attention_2 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_2 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)</span><br><span class="line">        self.addnorm_3 = AddNorm(embedding_size, dropout)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        enc_outputs, enc_valid_length = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Example Demo:</span></span><br><span class="line">        <span class="comment"># love dogs ! [EOS]</span></span><br><span class="line">        <span class="comment">#  |    |   |   |</span></span><br><span class="line">        <span class="comment">#   Transformer </span></span><br><span class="line">        <span class="comment">#    Decoder</span></span><br><span class="line">        <span class="comment">#  |   |   |   |</span></span><br><span class="line">        <span class="comment">#  I love dogs !</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># shape of key_values = (batch_size, t, hidden_size)</span></span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), dim=<span class="number">1</span>) </span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, seq_len, _ = X.shape</span><br><span class="line">            <span class="comment"># Shape: (batch_size, seq_len), the values in the j-th column are j+1</span></span><br><span class="line">            valid_length = torch.FloatTensor(np.tile(np.arange(<span class="number">1</span>, seq_len+<span class="number">1</span>), (batch_size, <span class="number">1</span>))) </span><br><span class="line">            valid_length = valid_length.to(X.device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        X2 = self.attention_1(X, key_values, key_values, valid_length)</span><br><span class="line">        Y = self.addnorm_1(X, X2)</span><br><span class="line">        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length)</span><br><span class="line">        Z = self.addnorm_2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm_3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(<span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_length), valid_length, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>
<p>对于 Transformer 解码器来说，构造方式与编码器一样，除了最后一层添加一个 dense layer 以获得输出的置信度分数。</p>
<p><strong>Transformer Decoder 参数设置：</strong></p>
<ul>
<li>编码器的输出 enc_outputs </li>
<li>句子有效长度 enc_valid_length</li>
<li>常规的超参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_size, ffn_hidden_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads, num_layers, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size)</span><br><span class="line">        self.pos_encoding = PositionalEncoding(embedding_size, dropout)</span><br><span class="line">        self.blks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.blks.append(</span><br><span class="line">                DecoderBlock(embedding_size, ffn_hidden_size, num_heads,</span><br><span class="line">                             dropout, i))</span><br><span class="line">        self.dense = nn.Linear(embedding_size, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, enc_valid_length, *args)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_length, [<span class="literal">None</span>]*self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br></pre></td></tr></table></figure>

<h1 id="机器翻译模型-Transformer-训练"><a href="#机器翻译模型-Transformer-训练" class="headerlink" title="机器翻译模型 Transformer 训练"></a>机器翻译模型 Transformer 训练</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">    <span class="comment"># sort by frequency and token</span></span><br><span class="line">    counter = collections.Counter(tokens)</span><br><span class="line">    token_freqs = sorted(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    token_freqs.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">      <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">      self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">      tokens = [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.unk = <span class="number">0</span></span><br><span class="line">      tokens = [<span class="string">''</span>]</span><br><span class="line">    tokens += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs <span class="keyword">if</span> freq &gt;= min_freq]</span><br><span class="line">    self.idx_to_token = []</span><br><span class="line">    self.token_to_idx = dict()</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">      self.idx_to_token.append(token)</span><br><span class="line">      self.token_to_idx[token] = len(self.idx_to_token) - <span class="number">1</span></span><br><span class="line">      </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">      <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">      <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span><span class="params">(batch_size, max_len, num_examples=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Download an NMT dataset, return its vocabulary and data iterator."""</span></span><br><span class="line">    <span class="comment"># Download and preprocess</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_raw</span><span class="params">(text)</span>:</span></span><br><span class="line">        text = text.replace(<span class="string">'\u202f'</span>, <span class="string">' '</span>).replace(<span class="string">'\xa0'</span>, <span class="string">' '</span>)</span><br><span class="line">        out = <span class="string">''</span></span><br><span class="line">        <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(text.lower()):</span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">in</span> (<span class="string">','</span>, <span class="string">'!'</span>, <span class="string">'.'</span>) <span class="keyword">and</span> text[i<span class="number">-1</span>] != <span class="string">' '</span>:</span><br><span class="line">                out += <span class="string">' '</span></span><br><span class="line">            out += char</span><br><span class="line">        <span class="keyword">return</span> out </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'path to data dile'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">      raw_text = f.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    text = preprocess_raw(raw_text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(text.split(<span class="string">'\n'</span>)):</span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">if</span> len(parts) &gt;= <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">' '</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build vocab</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(tokens)</span>:</span></span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">        <span class="keyword">return</span> Vocab(tokens, min_freq=<span class="number">3</span>, use_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert to index arrays</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pad</span><span class="params">(line, max_len, padding_token)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(line) &gt; max_len:</span><br><span class="line">            <span class="keyword">return</span> line[:max_len]</span><br><span class="line">        <span class="keyword">return</span> line + [padding_token] * (max_len - len(line))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_array</span><span class="params">(lines, vocab, max_len, is_source)</span>:</span></span><br><span class="line">        lines = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_source:</span><br><span class="line">            lines = [[vocab.bos] + line + [vocab.eos] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        array = torch.tensor([pad(line, max_len, vocab.pad) <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line">        valid_len = (array != vocab.pad).sum(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line">    src_array, src_valid_len = build_array(source, src_vocab, max_len, <span class="literal">True</span>)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, <span class="literal">False</span>)</span><br><span class="line">    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    train_iter = data.DataLoader(train_data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> src_vocab, tgt_vocab, train_iter</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> d2l</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"1"</span></span><br><span class="line"></span><br><span class="line">embed_size, embedding_size, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.05</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, ctx = <span class="number">0.005</span>, <span class="number">250</span>, d2l.try_gpu()</span><br><span class="line">print(ctx)</span><br><span class="line">num_hiddens, num_heads = <span class="number">64</span>, <span class="number">4</span></span><br><span class="line"></span><br><span class="line">src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    len(src_vocab), embedding_size, num_hiddens, num_heads, num_layers,</span><br><span class="line">    dropout)</span><br><span class="line">decoder = TransformerDecoder(</span><br><span class="line">    len(src_vocab), embedding_size, num_hiddens, num_heads, num_layers,</span><br><span class="line">    dropout)</span><br><span class="line">model = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_s2s_ch9(model, train_iter, lr, num_epochs, ctx)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.eval()</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> [<span class="string">'Go .'</span>, <span class="string">'Wow !'</span>, <span class="string">"I'm OK ."</span>, <span class="string">'I won !'</span>]:</span><br><span class="line">    print(sentence + <span class="string">' =&gt; '</span> + d2l.predict_s2s_ch9(</span><br><span class="line">        model, sentence, src_vocab, tgt_vocab, num_steps, ctx))</span><br></pre></td></tr></table></figure>
<hr>
<p>以上是利用 Transformer 机器翻译模型实现翻译 Demo 的全部分块，针对其中的层归一化进行总结：</p>
<p>层归一化</p>
<ol>
<li>层归一化有利于加快收敛，减少训练时间成本</li>
<li>层归一化对一个中间层的所有神经元进行归一化 </li>
<li>层归一化的效果不会受到batch大小的影响</li>
</ol>
<p>补充：</p>
<p>&ensp;&ensp;&ensp;批归一化</p>
<p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;每个神经元的输入数据以mini-batch为单位进行汇总</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Softmax</tag>
        <tag>位置编码</tag>
        <tag>Encoder-Decoder</tag>
      </tags>
  </entry>
</search>
