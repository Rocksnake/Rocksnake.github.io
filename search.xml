<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>自增主键的前世今生</title>
    <url>/2020/02/29/%E8%87%AA%E5%A2%9E%E4%B8%BB%E9%94%AE%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/</url>
    <content><![CDATA[<p>以MySQL(Innodb存储)为例介绍自增主键，介入场景分析主键的目的，从面试题下手，深入理解主键机制</p>
<a id="more"></a>

<p><font size=5><strong>引入：</strong></font></p>
<hr>
<p>使用MySQL建表时，我们通常会创建一个自增字段(AUTO_INCREMENT)，并以此字段作为主键</p>
<hr>
<p>本文将分三点阐述：</p>
<ol>
<li><a href="#title1">你可能不知道的自增主键</a></li>
<li><a href="#title2">应对变化的自增主键</a></li>
<li><a href="#title3"><font color=red>[坑]</font>如果自增主键用完怎么办</a></li>
</ol>
<h2 id="1-你可能不知道的自增主键"><a href="#1-你可能不知道的自增主键" class="headerlink" title="1.你可能不知道的自增主键"></a><div id="title1">1.你可能不知道的自增主键</div></h2><blockquote>
<p>使用自增主键可以提高数据存储效率</p>
</blockquote>
<p>在MySQL中(Innodb 存储引擎)，数据记录本身被存于主索引（B+Tree）的叶子节点上<br><font color=blue>*补充：【要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放】</font></p>
<hr>
<p>针对索引，</p>
<ul>
<li>如果我们<strong>定义了主键</strong>(PRIMARY KEY)，那么InnoDB会选择主键作为聚集索引</li>
<li>如果<strong>没有显式定义</strong>主键，则InnoDB会选择第一个<strong>不包含有NULL值的唯一索引</strong>作为主键索引</li>
<li>如果也<strong>没有这样的唯一索引</strong>，则InnoDB会选择内置6字节长的<strong>ROWID</strong>作为隐含的聚集索</li>
</ul>
<p><font color=blue>*补充：【ROWID随着行记录的写入而主键递增，这个ROWID不像ORACLE的ROWID那样可引用，是隐含的】</font></p>
<hr>
<p><strong>Q：</strong></p>
<p>每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）。</p>
<p><strong>A：</strong></p>
<ul>
<li><p>如果表<strong>使用自增主键</strong>。每次插入新的记录，会顺序添加到当前索引节点的后续位置，一页写满，自动开辟一个新的页</p>
</li>
<li><p>如果<strong>使用非自增主键</strong>（For Example:身份证号或学号等）【每次插入主键的值近似于随机】，每次新纪录都要被插到现有索引页中间某个位置，MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来<br/><br>这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。</p>
</li>
</ul>
<blockquote>
<p>自增id是增长的，不一定连续</p>
</blockquote>
<p><strong>原因有以下几点：</strong></p>
<ul>
<li>唯一键冲突</li>
<li>事务回滚</li>
<li>insert … select语句批量申请</li>
</ul>
<p><strong>针对自增值的保存策略：</strong></p>
<p>InnoDB 引擎中，自增值保存在了内存中，继 MySQL 8.0 之后，出现了<strong>自增值持久化</strong>，自增值的变更记录存储在 redo log 中，重启时可以依靠其恢复之前的值</p>
<p><font color=blue>*补充：【自增值持久化：如果发生重启，表的自增值可以恢复为 MySQL 重启前的值】</font></p>
<h2 id="2-应对变化的自增主键"><a href="#2-应对变化的自增主键" class="headerlink" title="2.应对变化的自增主键"></a><div id="title2">2.应对变化的自增主键</div></h2><p><strong>导入：</strong></p>
<blockquote>
<p>在设计数据库时不需要费尽心思去考虑设置哪个字段为主键</p>
</blockquote>
<p> 但是应用到实际场景，自增主键的主要目的还是应对变化。</p>
<p>设计一个场景：</p>
<p>&ensp;&ensp;&ensp;&ensp;<a href="https://mp.weixin.qq.com/s/rh0tg4L9Ffj1fy32rKCkHw" target="_blank" rel="noopener">维护商业账号的资质相关信息</a></p>
<p><strong>最初设计：</strong> 账号是由全局唯一且自增的分布式ID生成器生成的，很显然这个时候我们把账号作为主键这就天然合理</p>
<p><strong>业务迭代一定时间:</strong>  提出了新的需求，一个账号，在不同业务线，需要享有不同资质</p>
<p><strong>比较：</strong> accountId 较之前不唯一，因为，同一个账号，不同业务线，资质是不一样的【无法像最初那样作为主键】</p>
<p><strong>解决方式：</strong> <a href="https://mp.weixin.qq.com/s/rh0tg4L9Ffj1fy32rKCkHw" target="_blank" rel="noopener">见场景中</a></p>
<h2 id="3-如果自增主键用完怎么办"><a href="#3-如果自增主键用完怎么办" class="headerlink" title="3.如果自增主键用完怎么办"></a><div id="title3">3.如果自增主键用完怎么办</div></h2><p><strong>老掉牙但经典：</strong>【面试题】</p>
<blockquote>
<p>面试官:”用过mysql吧，你们是用自增主键还是UUID？”<br>你:”用的是自增主键”<br>面试官:”为什么是自增主键？”<br>你:”因为采用自增主键，数据在物理结构上是顺序存储，性能最好，blabla…”<br>面试官:”那自增主键达到最大值了，用完了怎么办？”<br>你:”what，没复习啊！！”<br>( 然后，你就可以回去等通知了！)</p>
</blockquote>
<p><strong>说明：</strong> 自增 id 是整型字段，常用 int 类型来定义增长 id ，而 int 类型有上限 即增长 id 也是有上限的。</p>
<p>既然 int 不够了，首先想到的是改为 BigInt 类型【做个对比】</p>
<center><img src="https://img-blog.csdnimg.cn/20200229113709135.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<center><img src="https://img-blog.csdnimg.cn/20200229113551427.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<p>如果回答：把自增主键的类型改为BigInt类型就好了<br/><br>面试官:”你在线上怎么修改列的数据类型的？”</p>
</blockquote>
<p>修改方法：</p>
<ul>
<li>使用mysql5.6+提供的在线修改功能</li>
<li>借助第三方工具</li>
<li>改从库表结构，然后主从切换</li>
</ul>
<p><strong>差不多就算结束了这个问题了。但是回过头想一想，是不是一条路走到黑了，或许从头开始就错了呢！！！</strong></p>
<hr>
<p><strong>插入一条生存力测试，形象生动：</strong></p>
<p>&ensp;&ensp;假如女朋友问：我刚才吃药时看窗外，你猜我看到了什么？</p>
<p>&ensp;&ensp;&ensp;&ensp;<strong>歧途：</strong> 白云？你为什么不看我呢。</p>
<p>&ensp;&ensp;&ensp;&ensp;<strong>正解：</strong> 你怎么要吃药呢</p>
<hr>
<p>那么<strong>正解</strong>应该是什么呢？</p>
<blockquote>
<p>这问题没遇到过，因为自增主键一般用int类型，一般达不到最大值，我们就分库分表了，所以不曾遇见过！</p>
</blockquote>
<p><a href="https://mp.weixin.qq.com/s/kVqj4VdZewuvR_OsXgY_RQ" target="_blank" rel="noopener">具体介绍…</a></p>
]]></content>
      <categories>
        <category>Database Systems</category>
      </categories>
      <tags>
        <tag>自增主键</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Fundamentals of Recurrent Neural Network</title>
    <url>/2020/02/29/Fundamentals%20of%20Recurrent%20Neural%20Network/</url>
    <content><![CDATA[<h2 id="基于循环神经网络实现语言模型。"><a href="#基于循环神经网络实现语言模型。" class="headerlink" title="基于循环神经网络实现语言模型。"></a>基于循环神经网络实现语言模型。</h2><blockquote>
<p>对于语言模型的介绍</p>
<blockquote>
<p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104303197" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104303197</a></p>
</blockquote>
</blockquote>
<p>我们的目的是基于当前的输入与过去的输入序列，预测序列的下一个字符。循环神经网络引入一个隐藏变量$H$，用$H_{t}$表示$H$在时间步$t$的值。$H_{t}$的计算基于$X_{t}$和$H_{t-1}$，可以认为$H_{t}$记录了到当前字符为止的序列信息，利用$H_{t}$对序列的下一个字符进行预测。</p>
<div align=center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWprbTB2NDRpLnBuZw?x-oss-process=image/format,png" /></div>

<h2 id="构造-Structure"><a href="#构造-Structure" class="headerlink" title="构造(Structure)"></a>构造(Structure)</h2><p>我们先看循环神经网络的具体构造。假设 $\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$ 是时间步 $t$ 的小批量输入，$\boldsymbol{H}_t  \in \mathbb{R}^{n \times h}$ 是该时间步的隐藏变量，则：</p>
<center>【广播机制】</center>

<p>$$<br>\boldsymbol{H}_t = \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}  + \boldsymbol{b}_h).<br>$$</p>
<p>其中，$\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}$，$\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}$, $\boldsymbol{b}_{h} \in \mathbb{R}^{1 \times h}$, $\phi$ 函数是非线性激活函数。</p>
<p>由于引入了 $\boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}$，$H_{t}$ 能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。</p>
<p>由于$H_{t}$的计算基于$H_{t-1}$，上式的计算是循环的，使用循环计算的网络即循环神经网络（recurrent neural network）。</p>
<p>在时间步$t$，输出层的输出为：</p>
<p>$$<br>\boldsymbol{O}_t = \boldsymbol{H}_t \boldsymbol{W}_{hq} + \boldsymbol{b}_q.<br>$$<br>其中$\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}$，$\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}$。</p>
<h2 id="手动实现"><a href="#手动实现" class="headerlink" title="手动实现"></a>手动实现</h2><blockquote>
<p>实现一个基于字符级循环神经网络的语言模型，仍然使用周杰伦的歌词作为语料</p>
<blockquote>
<p>下载地址：<a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/data/jaychou_lyrics.txt.zip?raw=true" target="_blank" rel="noopener">见语言模型一章</a>【点击可直接下载】</p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2l_jay9460 <span class="keyword">as</span> d2l</span><br><span class="line">(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="one-hot向量"><a href="#one-hot向量" class="headerlink" title="one-hot向量"></a>one-hot向量</h3><blockquote>
<p>在此采用one-hot向量将字符表示成向量</p>
</blockquote>
<p>假设词典大小是$N$，每次字符对应一个从$0$到$N-1$的唯一的索引，则该字符的向量是一个长度为$N$的向量，若字符的索引是$i$，则该向量的第$i$个位置为$1$，其他位置为$0$。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span><span class="params">(x, n_class, dtype=torch.float32)</span>:</span></span><br><span class="line">    result = torch.zeros(x.shape[<span class="number">0</span>], n_class, dtype=dtype, device=x.device)  <span class="comment"># shape: (n, n_class)</span></span><br><span class="line">    result.scatter_(<span class="number">1</span>, x.long().view(<span class="number">-1</span>, <span class="number">1</span>), <span class="number">1</span>)  <span class="comment"># result[i, x[i, 0]] = 1</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">    </span><br><span class="line">x = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">x_one_hot = one_hot(x, vocab_size)</span><br><span class="line">print(x_one_hot)</span><br><span class="line">print(x_one_hot.shape)</span><br><span class="line">print(x_one_hot.sum(axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>每次采样的小批量的形状是（批量大小, 时间步数）。我们将其变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。也就是说，时间步$t$的输入为</p>
<p>$$<br>\boldsymbol{X}_t \in \mathbb{R}^{n \times d}<br>$$</p>
<p>其中$n$为批量大小，$d$为词向量大小，即one-hot向量长度（词典大小)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_onehot</span><span class="params">(X, n_class)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [one_hot(X[:, i], n_class) <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>])]</span><br><span class="line"></span><br><span class="line">X = torch.arange(<span class="number">10</span>).view(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">inputs = to_onehot(X, vocab_size)</span><br><span class="line">print(len(inputs), inputs[<span class="number">0</span>].shape)</span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init module param</span></span><br><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line"><span class="comment"># num_inputs: d</span></span><br><span class="line"><span class="comment"># num_hiddens: h, 隐藏单元的个数是超参数</span></span><br><span class="line"><span class="comment"># num_outputs: q</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span> <span class="comment"># 随机初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        param = torch.zeros(shape, device=device, dtype=torch.float32)</span><br><span class="line">        nn.init.normal_(param, <span class="number">0</span>, <span class="number">0.01</span>) <span class="comment"># 随机体现</span></span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(param)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = _one((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = _one((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device))<span class="comment"># 偏置参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device))<span class="comment"># 偏置参数</span></span><br><span class="line">    <span class="keyword">return</span> (W_xh, W_hh, b_h, W_hq, b_q)</span><br></pre></td></tr></table></figure>

<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>函数<code>rnn</code>用循环的方式依次完成循环神经网络每个时间步的计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn</span><span class="params">(inputs, state, params)</span>:</span> <span class="comment"># 前向计算</span></span><br><span class="line">    <span class="comment"># inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state <span class="comment"># 提供了需要维护的状态的初始值 state定义成了元组</span></span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H,) <span class="comment"># 返回新的状态Ｈ，以便于相邻采样</span></span><br></pre></td></tr></table></figure>
<p>函数init_rnn_state初始化隐藏变量，这里的返回值是一个元组。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rnn_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>
<h3 id="裁剪梯度-clip-gradient"><a href="#裁剪梯度-clip-gradient" class="headerlink" title="裁剪梯度(clip gradient)"></a>裁剪梯度(clip gradient)</h3><blockquote>
<p>针对梯度爆炸问题</p>
</blockquote>
<p>循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。假设我们把所有模型参数的梯度拼接成一个向量 $\boldsymbol{g}$，并设裁剪的阈值是$\theta$。裁剪后的梯度</p>
<p>$$<br> \min\left(\frac{\theta}{|\boldsymbol{g}|}, 1\right)\boldsymbol{g}<br>$$</p>
<p>的$L_2$范数不超过$\theta$。</p>
<p>反向传播方式：时间反向传播【DPTT】</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, device)</span>:</span> <span class="comment"># theta 预设的阈值</span></span><br><span class="line">    norm = torch.tensor([<span class="number">0.0</span>], device=device)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad.data ** <span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().item()</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad.data *= (theta / norm)</span><br></pre></td></tr></table></figure>
<h3 id="定义预测函数"><a href="#定义预测函数" class="headerlink" title="定义预测函数"></a>定义预测函数</h3><blockquote>
<p>基于前缀 <code>prefix</code>（含有数个字符的字符串）来预测接下来的 <code>num_chars</code> 个字符。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn</span><span class="params">(prefix, num_chars, rnn, params, init_rnn_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                num_hiddens, vocab_size, device, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    <span class="comment"># 模型处理前缀prefix，隐藏状态H就记录了相关信息，模型在处理prefix 最后一个字符时，就已经预测出了下一个字符，所以可以作为之后的输入</span></span><br><span class="line">    state = init_rnn_state(<span class="number">1</span>, num_hiddens, device) <span class="comment">#　构造并且初始化状态</span></span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]   <span class="comment"># output记录prefix加上预测的num_chars个字符</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一时间步的输出作为当前时间步的输入</span></span><br><span class="line">        X = to_onehot(torch.tensor([[output[<span class="number">-1</span>]]], device=device), vocab_size)</span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        (Y, state) = rnn(X, state, params)</span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(Y[<span class="number">0</span>].argmax(dim=<span class="number">1</span>).item())　<span class="comment"># 最大的一列</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br></pre></td></tr></table></figure>
<h3 id="困惑度"><a href="#困惑度" class="headerlink" title="困惑度"></a>困惑度</h3><p>我们通常使用困惑度（perplexity）来评价语言模型的好坏。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，</p>
<blockquote>
<p>交叉熵损失函数</p>
<blockquote>
<p>损失函数详解：<a href="https://zhuanlan.zhihu.com/p/35709485" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35709485</a></p>
</blockquote>
</blockquote>
<ul>
<li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；</li>
<li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li>
<li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li>
</ul>
<p>显然，任何一个有效模型的困惑度必须小于类别个数。此处困惑度必须小于词典大小<code>vocab_size</code>。</p>
<h3 id="定义模型训练函数"><a href="#定义模型训练函数" class="headerlink" title="定义模型训练函数"></a>定义模型训练函数</h3><p>跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：</p>
<ol>
<li>使用困惑度评价模型。</li>
<li>在迭代模型参数前裁剪梯度。</li>
<li>对时序数据采用不同采样方法将导致隐藏状态初始化的不同。</li>
</ol>
<p>相邻采样，开始的时候初始化隐藏状态，容易引起开销过大，通常将隐藏状态分离</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn</span><span class="params">(rnn, get_params, init_rnn_state, num_hiddens,</span></span></span><br><span class="line"><span class="function"><span class="params">                          vocab_size, device, corpus_indices, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                          char_to_idx, is_random_iter, num_epochs, num_steps,</span></span></span><br><span class="line"><span class="function"><span class="params">                          lr, clipping_theta, batch_size, pred_period,</span></span></span><br><span class="line"><span class="function"><span class="params">                          pred_len, prefixes)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_random_iter:</span><br><span class="line">        data_iter_fn = d2l.data_iter_random <span class="comment"># 随机采样</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data_iter_fn = d2l.data_iter_consecutive <span class="comment">#相邻采样</span></span><br><span class="line">    params = get_params()</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_random_iter:  <span class="comment"># 如使用相邻采样，在epoch开始时初始化隐藏状态</span></span><br><span class="line">            state = init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> is_random_iter:  <span class="comment"># 如使用随机采样，在每个小批量更新前初始化隐藏状态</span></span><br><span class="line">                state = init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 否则需要使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">            <span class="comment"># inputs是num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">            inputs = to_onehot(X, vocab_size)</span><br><span class="line">            <span class="comment"># outputs有num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">            (outputs, state) = rnn(inputs, state, params) <span class="comment">#循环神经网路的前向计算</span></span><br><span class="line">            <span class="comment"># 拼接之后形状为(num_steps * batch_size, vocab_size)</span></span><br><span class="line">            outputs = torch.cat(outputs, dim=<span class="number">0</span>) <span class="comment"># 拼接</span></span><br><span class="line">            <span class="comment"># Y的形状是(batch_size, num_steps)，转置后再变成形状为</span></span><br><span class="line">            <span class="comment"># (num_steps * batch_size,)的向量，这样跟输出的行一一对应</span></span><br><span class="line">            y = torch.flatten(Y.T)</span><br><span class="line">            <span class="comment"># 使用交叉熵损失计算平均分类误差</span></span><br><span class="line">            l = loss(outputs, y.long())</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 梯度清0</span></span><br><span class="line">            <span class="keyword">if</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(params, clipping_theta, device)  <span class="comment"># 裁剪梯度</span></span><br><span class="line">            d2l.sgd(params, lr, <span class="number">1</span>)  <span class="comment"># 因为误差已经取过均值，梯度不用再做平均</span></span><br><span class="line">            l_sum += l.item() * y.shape[<span class="number">0</span>]</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_period == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,</span><br><span class="line">                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))</span><br></pre></td></tr></table></figure>
<h3 id="训练模型并创作歌词"><a href="#训练模型并创作歌词" class="headerlink" title="训练模型并创作歌词"></a>训练模型并创作歌词</h3><blockquote>
<ul>
<li>设置超参数</li>
<li>前缀：“分开”和“不分开”</li>
<li>歌词长度：50个字符（不考虑前缀长度）</li>
<li>周期：50</li>
<li>采样方式：随机采样 &amp;&amp; 相邻采样</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set super param</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line"><span class="comment"># set prefix and recurrent </span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"><span class="comment"># training by random sampling</span></span><br><span class="line">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, <span class="literal">True</span>, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                      prefixes)</span><br><span class="line"><span class="comment"># training by adjacent sampling</span></span><br><span class="line">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, <span class="literal">False</span>, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                      prefixes)</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="简化实现"><a href="#简化实现" class="headerlink" title="简化实现"></a>简化实现</h2><h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><blockquote>
<p>使用 Pytorch 中的 nn.RNN 构造神经网络</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个基于循环神经网络的语言模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_layer, vocab_size)</span>:</span> <span class="comment">#rnn_layer 是pytorch中的一个类</span></span><br><span class="line">        super(RNNModel, self).__init__()</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.hidden_size = rnn_layer.hidden_size * (<span class="number">2</span> <span class="keyword">if</span> rnn_layer.bidirectional <span class="keyword">else</span> <span class="number">1</span>) </span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.dense = nn.Linear(self.hidden_size, vocab_size) <span class="comment">#定义一个线性层作为输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">        <span class="comment"># inputs.shape: (batch_size, num_steps)</span></span><br><span class="line">        X = to_onehot(inputs, vocab_size)</span><br><span class="line">        X = torch.stack(X)  <span class="comment"># X.shape: (num_steps, batch_size, vocab_size)</span></span><br><span class="line">        hiddens, state = self.rnn(X, state)</span><br><span class="line">        hiddens = hiddens.view(<span class="number">-1</span>, hiddens.shape[<span class="number">-1</span>])  <span class="comment"># hiddens.shape: (num_steps * batch_size, hidden_size)</span></span><br><span class="line">        output = self.dense(hiddens)</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure>

<h3 id="预测函数"><a href="#预测函数" class="headerlink" title="预测函数"></a>预测函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn_pytorch</span><span class="params">(prefix, num_chars, model, vocab_size, device, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                      char_to_idx)</span>:</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]  <span class="comment"># output记录prefix加上预测的num_chars个字符</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        X = torch.tensor([output[<span class="number">-1</span>]], device=device).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        (Y, state) = model(X, state)  <span class="comment"># 前向计算不需要传入模型参数</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(Y.argmax(dim=<span class="number">1</span>).item())</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><blockquote>
<p>采用相邻采样</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># training function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn_pytorch</span><span class="params">(model, num_hiddens, vocab_size, device,</span></span></span><br><span class="line"><span class="function"><span class="params">                                corpus_indices, idx_to_char, char_to_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                num_epochs, num_steps, lr, clipping_theta,</span></span></span><br><span class="line"><span class="function"><span class="params">                                batch_size, pred_period, pred_len, prefixes)</span>:</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=lr) <span class="comment">#优化模型参数</span></span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) <span class="comment"># 相邻采样</span></span><br><span class="line">        state = <span class="literal">None</span> <span class="comment">#构造 并初始化</span></span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">if</span> isinstance (state, tuple): <span class="comment"># LSTM, state:(h, c)  </span></span><br><span class="line">                    state[<span class="number">0</span>].detach_()</span><br><span class="line">                    state[<span class="number">1</span>].detach_()</span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    state.detach_()</span><br><span class="line">            (output, state) = model(X, state) <span class="comment"># output.shape: (num_steps * batch_size, vocab_size)</span></span><br><span class="line">            y = torch.flatten(Y.T)</span><br><span class="line">            l = loss(output, y.long())</span><br><span class="line">            </span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(model.parameters(), clipping_theta, device)</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.item() * y.shape[<span class="number">0</span>]</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_period == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn_pytorch(</span><br><span class="line">                    prefix, pred_len, model, vocab_size, device, idx_to_char,</span><br><span class="line">                    char_to_idx))</span><br><span class="line">       </span><br><span class="line">num_epochs, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">32</span>, <span class="number">1e-3</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                            corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                            num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                            batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>语言模型</tag>
        <tag>RNN</tag>
        <tag>梯度现象</tag>
        <tag>广播</tag>
        <tag>one-hot</tag>
      </tags>
  </entry>
  <entry>
    <title>词嵌入之 Word2Vec</title>
    <url>/2020/02/28/%E8%AF%8D%E5%B5%8C%E5%85%A5%E4%B9%8B%20Word2Vec/</url>
    <content><![CDATA[<h1 id="词嵌入基础"><a href="#词嵌入基础" class="headerlink" title="词嵌入基础"></a>词嵌入基础</h1><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">循环神经网络的从零开始实现</a>中使用 one-hot 向量表示单词，虽然它们构造起来很容易，但通常并不是一个好选择。</p>
<blockquote>
<p><strong>原因：</strong></p>
<blockquote>
<p>one-hot 词向量无法准确表达不同词之间的相似度，如我们常常使用的余弦相似度<br/><br>任意单词间的余弦相似度都为零。</p>
</blockquote>
</blockquote>
<p>Word2Vec 词嵌入工具的提出正是为了解决上面这个问题，它将每个词表示成一个定长的向量，并通过在语料库上的预训练使得这些向量能较好地表达不同词之间的相似和类比关系，以引入一定的语义信息。</p>
<p>基于两种概率模型的假设，介绍以下两种 Word2Vec 模型：</p>
<ol>
<li>Skip-Gram 跳字模型：假设背景词由中心词生成，即建模 $P(w_o\mid w_c)$，其中 $w_c$ 为中心词，$w_o$ 为任一背景词；</li>
</ol>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW1qc3E4NG85LnBuZw?x-oss-process=image/format,png" /></center>

<ol start="2">
<li>CBOW (continuous bag-of-words) 连续词袋模型：假设中心词由背景词生成，即建模 $P(w_c\mid \mathcal{W}_o)$，其中 $\mathcal{W}_o$ 为背景词的集合。</li>
</ol>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW1qdDRyMDJuLnBuZw?x-oss-process=image/format,png" /></center>

<p>在这里我们主要介绍 Skip-Gram 模型的实现，CBOW 实现与其类似，读者可之后自己尝试实现。后续的内容将大致从以下四个部分展开：</p>
<ol>
<li><a href="#ptb">PTB 数据集</a></li>
<li><a href="#skip-gram">Skip-Gram 跳字模型</a></li>
<li><a href="#nsa">负采样近似</a></li>
<li><a href="#train">训练模型</a></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br></pre></td></tr></table></figure>
<h2 id="PTB-数据集"><a href="#PTB-数据集" class="headerlink" title="PTB 数据集"></a><div id="ptb">PTB 数据集</div></h2><p>简单来说，Word2Vec 能从语料中学到如何将离散的词映射为连续空间中的向量，并保留其语义上的相似关系。</p>
<p>为了训练 Word2Vec 模型，我们就需要准备一个自然语言语料库，模型将从中学习各个单词间的关系，这里我们使用经典的 PTB 语料库进行训练。</p>
<h3 id="载入数据集"><a href="#载入数据集" class="headerlink" title="载入数据集"></a>载入数据集</h3><p>数据集训练文件 <code>ptb.train.txt</code> 示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">aer banknote berlitz calloway centrust cluett fromstein gitano guterman ...</span><br><span class="line">pierre  N years old will join the board as a nonexecutive director nov. N </span><br><span class="line">mr.  is chairman of  n.v. the dutch publishing group </span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'path to ptb.train.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    lines = f.readlines() <span class="comment"># 该数据集中句子以换行符为分割</span></span><br><span class="line">    raw_dataset = [st.split() <span class="keyword">for</span> st <span class="keyword">in</span> lines] <span class="comment"># st是sentence的缩写，单词以空格为分割</span></span><br><span class="line">print(<span class="string">'# sentences: %d'</span> % len(raw_dataset))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于数据集的前3个句子，打印每个句子的词数和前5个词</span></span><br><span class="line"><span class="comment"># 句尾符为 '' ，生僻词全用 '' 表示，数字则被替换成了 'N'</span></span><br><span class="line"><span class="keyword">for</span> st <span class="keyword">in</span> raw_dataset[:<span class="number">3</span>]:</span><br><span class="line">    print(<span class="string">'# tokens:'</span>, len(st), st[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line"><span class="comment"># sentences: 42068</span></span><br><span class="line"><span class="comment"># tokens: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']</span></span><br><span class="line"><span class="comment"># tokens: 15 ['pierre', '&lt;unk&gt;', 'N', 'years', 'old']</span></span><br><span class="line"><span class="comment"># tokens: 11 ['mr.', '&lt;unk&gt;', 'is', 'chairman', 'of']</span></span><br></pre></td></tr></table></figure>
<h3 id="建立词语索引"><a href="#建立词语索引" class="headerlink" title="建立词语索引"></a>建立词语索引</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">counter = collections.Counter([tk <span class="keyword">for</span> st <span class="keyword">in</span> raw_dataset <span class="keyword">for</span> tk <span class="keyword">in</span> st]) <span class="comment"># tk是token的缩写</span></span><br><span class="line">counter = dict(filter(<span class="keyword">lambda</span> x: x[<span class="number">1</span>] &gt;= <span class="number">5</span>, counter.items())) <span class="comment"># 只保留在数据集中至少出现5次的词</span></span><br><span class="line"></span><br><span class="line">idx_to_token = [tk <span class="keyword">for</span> tk, _ <span class="keyword">in</span> counter.items()]</span><br><span class="line">token_to_idx = &#123;tk: idx <span class="keyword">for</span> idx, tk <span class="keyword">in</span> enumerate(idx_to_token)&#125;</span><br><span class="line">dataset = [[token_to_idx[tk] <span class="keyword">for</span> tk <span class="keyword">in</span> st <span class="keyword">if</span> tk <span class="keyword">in</span> token_to_idx]</span><br><span class="line">           <span class="keyword">for</span> st <span class="keyword">in</span> raw_dataset] <span class="comment"># raw_dataset中的单词在这一步被转换为对应的idx</span></span><br><span class="line">num_tokens = sum([len(st) <span class="keyword">for</span> st <span class="keyword">in</span> dataset])</span><br><span class="line"><span class="string">'# tokens: %d'</span> % num_tokens</span><br></pre></td></tr></table></figure>
<h3 id="二次采样"><a href="#二次采样" class="headerlink" title="二次采样"></a>二次采样</h3><p>文本数据中一般会出现一些高频词，如英文中的“the”“a”和“in”。通常来说</p>
<p>在一个背景窗口中，一个词（如“chip”）和较低频词（如“microprocessor”）同时出现比和较高频词（如“the”）同时出现对训练词嵌入模型更有益。</p>
<p>因此，训练词嵌入模型时可以对词进行二次采样。 具体来说，数据集中每个被索引词 $w_i$ 将有一定概率被丢弃，该丢弃概率为</p>
<p>$$<br>P(w_i)=\max(1-\sqrt{\frac{t}{f(w_i)}},0)<br>$$</p>
<p>其中  $f(w_i)$  是数据集中词 $w_i$ 的个数与总词数之比，常数 $t$ 是一个超参数（实验中设为 $10^{−4}$）。</p>
<p>可见，只有当 $f(w_i)&gt;t$ 时，我们才有可能在二次采样中丢弃词 $w_i$，并且越高频的词被丢弃的概率越大。具体的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discard</span><span class="params">(idx)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        idx: 单词的下标</span></span><br><span class="line"><span class="string">    @return: True/False 表示是否丢弃该单词</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> random.uniform(<span class="number">0</span>, <span class="number">1</span>) &lt; <span class="number">1</span> - math.sqrt(</span><br><span class="line">        <span class="number">1e-4</span> / counter[idx_to_token[idx]] * num_tokens)</span><br><span class="line"></span><br><span class="line">subsampled_dataset = [[tk <span class="keyword">for</span> tk <span class="keyword">in</span> st <span class="keyword">if</span> <span class="keyword">not</span> discard(tk)] <span class="keyword">for</span> st <span class="keyword">in</span> dataset]</span><br><span class="line">print(<span class="string">'# tokens: %d'</span> % sum([len(st) <span class="keyword">for</span> st <span class="keyword">in</span> subsampled_dataset]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compare_counts</span><span class="params">(token)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'# %s: before=%d, after=%d'</span> % (token, sum(</span><br><span class="line">        [st.count(token_to_idx[token]) <span class="keyword">for</span> st <span class="keyword">in</span> dataset]), sum(</span><br><span class="line">        [st.count(token_to_idx[token]) <span class="keyword">for</span> st <span class="keyword">in</span> subsampled_dataset]))</span><br><span class="line"></span><br><span class="line">print(compare_counts(<span class="string">'the'</span>))</span><br><span class="line">print(compare_counts(<span class="string">'join'</span>))</span><br></pre></td></tr></table></figure>
<h3 id="提取中心词和背景词"><a href="#提取中心词和背景词" class="headerlink" title="提取中心词和背景词"></a>提取中心词和背景词</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_centers_and_contexts</span><span class="params">(dataset, max_window_size)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        dataset: 数据集为句子的集合，每个句子则为单词的集合，此时单词已经被转换为相应数字下标</span></span><br><span class="line"><span class="string">        max_window_size: 背景词的词窗大小的最大值</span></span><br><span class="line"><span class="string">    @return:</span></span><br><span class="line"><span class="string">        centers: 中心词的集合</span></span><br><span class="line"><span class="string">        contexts: 背景词窗的集合，与中心词对应，每个背景词窗则为背景词的集合</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    centers, contexts = [], []</span><br><span class="line">    <span class="keyword">for</span> st <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="keyword">if</span> len(st) &lt; <span class="number">2</span>:  <span class="comment"># 每个句子至少要有2个词才可能组成一对“中心词-背景词”</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        centers += st</span><br><span class="line">        <span class="keyword">for</span> center_i <span class="keyword">in</span> range(len(st)):</span><br><span class="line">            window_size = random.randint(<span class="number">1</span>, max_window_size) <span class="comment"># 随机选取背景词窗大小</span></span><br><span class="line">            indices = list(range(max(<span class="number">0</span>, center_i - window_size),</span><br><span class="line">                                 min(len(st), center_i + <span class="number">1</span> + window_size)))</span><br><span class="line">            indices.remove(center_i)  <span class="comment"># 将中心词排除在背景词之外</span></span><br><span class="line">            contexts.append([st[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> indices])</span><br><span class="line">    <span class="keyword">return</span> centers, contexts</span><br><span class="line"></span><br><span class="line">all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">tiny_dataset = [list(range(<span class="number">7</span>)), list(range(<span class="number">7</span>, <span class="number">10</span>))]</span><br><span class="line">print(<span class="string">'dataset'</span>, tiny_dataset)</span><br><span class="line"><span class="keyword">for</span> center, context <span class="keyword">in</span> zip(*get_centers_and_contexts(tiny_dataset, <span class="number">2</span>)):</span><br><span class="line">    print(<span class="string">'center'</span>, center, <span class="string">'has contexts'</span>, context)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><em>注：数据批量读取的实现需要依赖负采样近似的实现，故放于负采样近似部分进行讲解。</em></p>
</blockquote>
<h2 id="Skip-Gram-跳字模型"><a href="#Skip-Gram-跳字模型" class="headerlink" title="Skip-Gram 跳字模型"></a><div id="skip-gram">Skip-Gram 跳字模型</div></h2><p>在跳字模型中，每个词被表示成两个 $d$ 维向量，用来计算条件概率。</p>
<p>假设这个词在词典中索引为 $i$ ，当它为中心词时向量表示为 $\boldsymbol{v}_i\in\mathbb{R}^d$，而为背景词时向量表示为 $\boldsymbol{u}_i\in\mathbb{R}^d$ 。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_o$ 在词典中索引为 $o$</p>
<p>我们假设给定中心词生成背景词的条件概率满足下式：</p>
<p>$$<br>P(w_o\mid w_c)=\frac{\exp(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{\sum_{i\in\mathcal{V}}\exp(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}<br>$$</p>
<h3 id="PyTorch-预置的-Embedding-层"><a href="#PyTorch-预置的-Embedding-层" class="headerlink" title="PyTorch 预置的 Embedding 层"></a>PyTorch 预置的 Embedding 层</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed = nn.Embedding(num_embeddings=<span class="number">10</span>, embedding_dim=<span class="number">4</span>)</span><br><span class="line">print(embed.weight)</span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.long)</span><br><span class="line">print(embed(x))</span><br></pre></td></tr></table></figure>
<h3 id="PyTorch-预置的批量乘法"><a href="#PyTorch-预置的批量乘法" class="headerlink" title="PyTorch 预置的批量乘法"></a>PyTorch 预置的批量乘法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">print(torch.bmm(X, Y).shape)</span><br></pre></td></tr></table></figure>
<h3 id="Skip-Gram-模型的前向计算"><a href="#Skip-Gram-模型的前向计算" class="headerlink" title="Skip-Gram 模型的前向计算"></a>Skip-Gram 模型的前向计算</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">skip_gram</span><span class="params">(center, contexts_and_negatives, embed_v, embed_u)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        center: 中心词下标，形状为 (n, 1) 的整数张量</span></span><br><span class="line"><span class="string">        contexts_and_negatives: 背景词和噪音词下标，形状为 (n, m) 的整数张量</span></span><br><span class="line"><span class="string">        embed_v: 中心词的 embedding 层</span></span><br><span class="line"><span class="string">        embed_u: 背景词的 embedding 层</span></span><br><span class="line"><span class="string">    @return:</span></span><br><span class="line"><span class="string">        pred: 中心词与背景词（或噪音词）的内积，之后可用于计算概率 p(w_o|w_c)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    v = embed_v(center) <span class="comment"># shape of (n, 1, d)</span></span><br><span class="line">    u = embed_u(contexts_and_negatives) <span class="comment"># shape of (n, m, d)</span></span><br><span class="line">    pred = torch.bmm(v, u.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)) <span class="comment"># bmm((n, 1, d), (n, d, m)) =&gt; shape of (n, 1, m)</span></span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure>
<h2 id="负采样近似"><a href="#负采样近似" class="headerlink" title="负采样近似"></a><div id="nsa">负采样近似</div></h2><p><strong>问题：</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;由于 softmax 运算考虑了背景词可能是词典 $\mathcal{V}$ 中的任一词，对于含几十万或上百万词的较大词典，就可能导致计算的开销过大。</p>
<p>我们将<strong>以 skip-gram 模型为例</strong>，介绍负采样 (negative sampling) 的实现来尝试解决这个问题。</p>
<p>负采样方法用以下公式来近似条件概率 $P(w_o\mid w_c)=\frac{\exp(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{\sum_{i\in\mathcal{V}}\exp(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}$：</p>
<p>$$<br>P(w_o\mid w_c)=P(D=1\mid w_c,w_o)\prod_{k=1,w_k\sim P(w)}^K P(D=0\mid w_c,w_k)<br>$$</p>
<p>其中 $P(D=1\mid w_c,w_o)=\sigma(\boldsymbol{u}_o^\top\boldsymbol{v}_c)$，$\sigma(\cdot)$ 为 sigmoid 函数。对于一对中心词和背景词，我们从词典中随机采样 $K$ 个噪声词（实验中设 $K=5$）。</p>
<hr>
<p>根据 Word2Vec 论文的建议，噪声词采样概率 $P(w)$ 设为 $w$ 词频与总词频之比的 $0.75$ 次方。</p>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_negatives</span><span class="params">(all_contexts, sampling_weights, K)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        all_contexts: [[w_o1, w_o2, ...], [...], ... ]</span></span><br><span class="line"><span class="string">        sampling_weights: 每个单词的噪声词采样概率</span></span><br><span class="line"><span class="string">        K: 随机采样个数</span></span><br><span class="line"><span class="string">    @return:</span></span><br><span class="line"><span class="string">        all_negatives: [[w_n1, w_n2, ...], [...], ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    all_negatives, neg_candidates, i = [], [], <span class="number">0</span></span><br><span class="line">    population = list(range(len(sampling_weights)))</span><br><span class="line">    <span class="keyword">for</span> contexts <span class="keyword">in</span> all_contexts:</span><br><span class="line">        negatives = []</span><br><span class="line">        <span class="keyword">while</span> len(negatives) &lt; len(contexts) * K:</span><br><span class="line">            <span class="keyword">if</span> i == len(neg_candidates):</span><br><span class="line">                <span class="comment"># 根据每个词的权重（sampling_weights）随机生成k个词的索引作为噪声词。</span></span><br><span class="line">                <span class="comment"># 为了高效计算，可以将k设得稍大一点</span></span><br><span class="line">                i, neg_candidates = <span class="number">0</span>, random.choices(</span><br><span class="line">                    population, sampling_weights, k=int(<span class="number">1e5</span>))</span><br><span class="line">            neg, i = neg_candidates[i], i + <span class="number">1</span></span><br><span class="line">            <span class="comment"># 噪声词不能是背景词</span></span><br><span class="line">            <span class="keyword">if</span> neg <span class="keyword">not</span> <span class="keyword">in</span> set(contexts):</span><br><span class="line">                negatives.append(neg)</span><br><span class="line">        all_negatives.append(negatives)</span><br><span class="line">    <span class="keyword">return</span> all_negatives</span><br><span class="line"></span><br><span class="line">sampling_weights = [counter[w]**<span class="number">0.75</span> <span class="keyword">for</span> w <span class="keyword">in</span> idx_to_token]</span><br><span class="line">all_negatives = get_negatives(all_contexts, sampling_weights, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>*注：除负采样方法外，还有层序 softmax (hiererarchical softmax) 方法也可以用来解决计算量过大的问题</p>
</blockquote>
<h3 id="批量读取数据"><a href="#批量读取数据" class="headerlink" title="批量读取数据"></a>批量读取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, centers, contexts, negatives)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> len(centers) == len(contexts) == len(negatives)</span><br><span class="line">        self.centers = centers</span><br><span class="line">        self.contexts = contexts</span><br><span class="line">        self.negatives = negatives</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (self.centers[index], self.contexts[index], self.negatives[index])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.centers)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    用作DataLoader的参数collate_fn</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        data: 长为batch_size的列表，列表中的每个元素都是__getitem__得到的结果</span></span><br><span class="line"><span class="string">    @outputs:</span></span><br><span class="line"><span class="string">        batch: 批量化后得到 (centers, contexts_negatives, masks, labels) 元组</span></span><br><span class="line"><span class="string">            centers: 中心词下标，形状为 (n, 1) 的整数张量</span></span><br><span class="line"><span class="string">            contexts_negatives: 背景词和噪声词的下标，形状为 (n, m) 的整数张量</span></span><br><span class="line"><span class="string">            masks: 与补齐相对应的掩码，形状为 (n, m) 的0/1整数张量</span></span><br><span class="line"><span class="string">            labels: 指示中心词的标签，形状为 (n, m) 的0/1整数张量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    max_len = max(len(c) + len(n) <span class="keyword">for</span> _, c, n <span class="keyword">in</span> data)</span><br><span class="line">    centers, contexts_negatives, masks, labels = [], [], [], []</span><br><span class="line">    <span class="keyword">for</span> center, context, negative <span class="keyword">in</span> data:</span><br><span class="line">        cur_len = len(context) + len(negative)</span><br><span class="line">        centers += [center]</span><br><span class="line">        contexts_negatives += [context + negative + [<span class="number">0</span>] * (max_len - cur_len)]</span><br><span class="line">        masks += [[<span class="number">1</span>] * cur_len + [<span class="number">0</span>] * (max_len - cur_len)] <span class="comment"># 使用掩码变量mask来避免填充项对损失函数计算的影响</span></span><br><span class="line">        labels += [[<span class="number">1</span>] * len(context) + [<span class="number">0</span>] * (max_len - len(context))]</span><br><span class="line">        batch = (torch.tensor(centers).view(<span class="number">-1</span>, <span class="number">1</span>), torch.tensor(contexts_negatives),</span><br><span class="line">            torch.tensor(masks), torch.tensor(labels))</span><br><span class="line">    <span class="keyword">return</span> batch</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">512</span></span><br><span class="line">num_workers = <span class="number">0</span> <span class="keyword">if</span> sys.platform.startswith(<span class="string">'win32'</span>) <span class="keyword">else</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">dataset = MyDataset(all_centers, all_contexts, all_negatives)</span><br><span class="line">data_iter = Data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            collate_fn=batchify, </span><br><span class="line">                            num_workers=num_workers)</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">    <span class="keyword">for</span> name, data <span class="keyword">in</span> zip([<span class="string">'centers'</span>, <span class="string">'contexts_negatives'</span>, <span class="string">'masks'</span>,</span><br><span class="line">                           <span class="string">'labels'</span>], batch):</span><br><span class="line">        print(name, <span class="string">'shape:'</span>, data.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a><div id="train">训练模型</div></h2><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>应用负采样方法后，我们可利用最大似然估计的对数等价形式将损失函数定义为如下</p>
<p>$$<br>\sum_{t=1}^T\sum_{-m\le j\le m,j\ne 0} [-\log P(D=1\mid w^{(t)},w^{(t+j)})-\sum_{k=1,w_k\sim P(w)^K}\log P(D=0\mid w^{(t)},w_k)]<br>$$</p>
<p>根据这个损失函数的定义，我们可以直接使用<strong>二元交叉熵损失函数</strong>进行计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SigmoidBinaryCrossEntropyLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(SigmoidBinaryCrossEntropyLoss, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets, mask=None)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        @params:</span></span><br><span class="line"><span class="string">            inputs: 经过sigmoid层后为预测D=1的概率</span></span><br><span class="line"><span class="string">            targets: 0/1向量，1代表背景词，0代表噪音词</span></span><br><span class="line"><span class="string">        @return:</span></span><br><span class="line"><span class="string">            res: 平均到每个label的loss</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        inputs, targets, mask = inputs.float(), targets.float(), mask.float()</span><br><span class="line">        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=<span class="string">"none"</span>, weight=mask)</span><br><span class="line">        res = res.sum(dim=<span class="number">1</span>) / mask.float().sum(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">loss = SigmoidBinaryCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">pred = torch.tensor([[<span class="number">1.5</span>, <span class="number">0.3</span>, <span class="number">-1</span>, <span class="number">2</span>], [<span class="number">1.1</span>, <span class="number">-0.6</span>, <span class="number">2.2</span>, <span class="number">0.4</span>]])</span><br><span class="line">label = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]]) <span class="comment"># 标签变量label中的1和0分别代表背景词和噪声词</span></span><br><span class="line">mask = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]])  <span class="comment"># 掩码变量</span></span><br><span class="line">print(loss(pred, label, mask))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmd</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> - math.log(<span class="number">1</span> / (<span class="number">1</span> + math.exp(-x)))</span><br><span class="line">print(<span class="string">'%.4f'</span> % ((sigmd(<span class="number">1.5</span>) + sigmd(<span class="number">-0.3</span>) + sigmd(<span class="number">1</span>) + sigmd(<span class="number">-2</span>)) / <span class="number">4</span>)) <span class="comment"># 注意1-sigmoid(x) = sigmoid(-x)</span></span><br><span class="line">print(<span class="string">'%.4f'</span> % ((sigmd(<span class="number">1.1</span>) + sigmd(<span class="number">-0.6</span>) + sigmd(<span class="number">-2.2</span>)) / <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<h3 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed_size = <span class="number">100</span></span><br><span class="line">net = nn.Sequential(nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),</span><br><span class="line">                    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size))</span><br></pre></td></tr></table></figure>
<h3 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(net, lr, num_epochs)</span>:</span></span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">    print(<span class="string">"train on"</span>, device)</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        start, l_sum, n = time.time(), <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            center, context_negative, mask, label = [d.to(device) <span class="keyword">for</span> d <span class="keyword">in</span> batch]</span><br><span class="line">            </span><br><span class="line">            pred = skip_gram(center, context_negative, net[<span class="number">0</span>], net[<span class="number">1</span>])</span><br><span class="line">            </span><br><span class="line">            l = loss(pred.view(label.shape), label, mask).mean() <span class="comment"># 一个batch的平均loss</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.cpu().item()</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">        print(<span class="string">'epoch %d, loss %.2f, time %.2fs'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, l_sum / n, time.time() - start))</span><br><span class="line"></span><br><span class="line">train(net, <span class="number">0.01</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>*注：最好在ＧＰＵ上运行</p>
</blockquote>
<h3 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_similar_tokens</span><span class="params">(query_token, k, embed)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        query_token: 给定的词语</span></span><br><span class="line"><span class="string">        k: 近义词的个数</span></span><br><span class="line"><span class="string">        embed: 预训练词向量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    W = embed.weight.data</span><br><span class="line">    x = W[token_to_idx[query_token]]</span><br><span class="line">    <span class="comment"># 添加的1e-9是为了数值稳定性</span></span><br><span class="line">    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=<span class="number">1</span>) * torch.sum(x * x) + <span class="number">1e-9</span>).sqrt()</span><br><span class="line">    _, topk = torch.topk(cos, k=k+<span class="number">1</span>)</span><br><span class="line">    topk = topk.cpu().numpy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> topk[<span class="number">1</span>:]:  <span class="comment"># 除去输入词</span></span><br><span class="line">        print(<span class="string">'cosine sim=%.3f: %s'</span> % (cos[i], (idx_to_token[i])))</span><br><span class="line">        </span><br><span class="line">get_similar_tokens(<span class="string">'chip'</span>, <span class="number">3</span>, net[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>PTB</tag>
        <tag>Skip-Gram</tag>
        <tag>负近似采样</tag>
        <tag>CBOW</tag>
        <tag>Word2Vec</tag>
      </tags>
  </entry>
  <entry>
    <title>Advanced Optimization</title>
    <url>/2020/02/28/Advanced%20Optimization/</url>
    <content><![CDATA[<p>基于<a href="https://blog.csdn.net/RokoBasilisk/article/details/104413638" target="_blank" rel="noopener">凸优化和梯度下降优化算法</a>，进一步展开阐述 :</p>
<ol>
<li><a href="#momentum">Momentum;</a></li>
<li><a href="#adagrad">AdaGrad;</a></li>
<li><a href="#rmsprop">RMSProp;</a></li>
<li><a href="#adadelta">AdaDelta;</a></li>
<li><a href="#adam">Adam</a></li>
</ol>
<h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a><div id="momentum">Momentum</div></h1><p>在<a href="https://blog.csdn.net/RokoBasilisk/article/details/104413638" target="_blank" rel="noopener">随机梯度下降</a>中，我们提到，目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。对于noisy gradient,我们需要谨慎的选取学习率和batch size, 来控制梯度方差和收敛的结果。</p>
<p>$$<br>\mathbf{g}<em>t = \partial</em>{\mathbf{w}} \frac{1}{|\mathcal{B}<em>t|} \sum</em>{i \in \mathcal{B}<em>t} f(\mathbf{x}</em>{i}, \mathbf{w}<em>{t-1}) = \frac{1}{|\mathcal{B}_t|} \sum</em>{i \in \mathcal{B}<em>t} \mathbf{g}</em>{i, t-1}.<br>$$</p>
<h2 id="An-ill-conditioned-Problem"><a href="#An-ill-conditioned-Problem" class="headerlink" title="An ill-conditioned Problem"></a>An ill-conditioned Problem</h2><p>Condition Number of Hessian Matrix:</p>
<p>$$<br> cond_{H} = \frac{\lambda_{max}}{\lambda_{min}}<br>$$</p>
<p>where $\lambda_{max}, \lambda_{min}$ is the maximum amd minimum eignvalue of Hessian matrix.</p>
<p>让我们考虑一个输入和输出分别为二维向量$\boldsymbol{x} = [x_1, x_2]^\top$和标量的目标函数:</p>
<p>$$<br> f(\boldsymbol{x})=0.1x_1^2+2x_2^2<br>$$</p>
<p>$$<br> cond_{H} = \frac{4}{0.2} = 20 \quad \rightarrow \quad \text{ill-conditioned}<br>$$</p>
<h2 id="Supp-Preconditioning"><a href="#Supp-Preconditioning" class="headerlink" title="Supp: Preconditioning"></a>Supp: Preconditioning</h2><center><img src="https://img-blog.csdnimg.cn/2020022114561129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>在二阶优化中，我们使用Hessian matrix的逆矩阵(或者pseudo inverse)来左乘梯度向量 $i.e. \Delta_{x} = H^{-1}\mathbf{g}$，这样的做法称为precondition，相当于将 $H$ 映射为一个单位矩阵，拥有分布均匀的Spectrum，也即我们去优化的等价标函数的Hessian matrix为良好的identity matrix。</p>
<p>将$x_1^2$系数从$1$减小到了$0.1$。下面实现基于这个目标函数的梯度下降，并展示使用学习率为$0.4$时自变量的迭代轨迹。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.4</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * <span class="number">0.2</span> * x1, x2 - eta * <span class="number">4</span> * x2, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223165505355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>可以看到，同一位置上，目标函数在竖直方向（$x_2$轴方向）比在水平方向（$x_1$轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。</p>
<p>接下来将学习率 $eta$ 加大+<font size=6>+</font><font size=10>+</font></p>
<p>此时自变量在竖直方向不断越过最优解并逐渐发散。</p>
<h3 id="Solution-to-ill-condition"><a href="#Solution-to-ill-condition" class="headerlink" title="Solution to ill-condition"></a>Solution to ill-condition</h3><ul>
<li><strong>Preconditioning gradient vector</strong>: applied in Adam, RMSProp, AdaGrad, Adelta, KFC, Natural gradient and other secord-order optimization algorithms.</li>
<li><strong>Averaging history gradient</strong>: like momentum, which allows larger learning rates to accelerate convergence; applied in Adam, RMSProp, SGD momentum.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.6</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223165906709.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Momentum-Algorithm"><a href="#Momentum-Algorithm" class="headerlink" title="Momentum Algorithm"></a>Momentum Algorithm</h2><p>动量法的提出是为了解决梯度下降的上述问题。设时间步 $t$ 的自变量为 $\boldsymbol{x}_t$，学习率为 $\eta_t$。<br>在时间步 $t=0$，动量法创建速度变量 $\boldsymbol{m}_0$，并将其元素初始化成 0。在时间步 $t&gt;0$，动量法对每次迭代的步骤做如下修改：</p>
<p>$$<br>\begin{aligned}<br>\boldsymbol{m}_t &amp;\leftarrow \beta \boldsymbol{m}_{t-1} + \eta_t \boldsymbol{g}_t, \<br>\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{m}_t,<br>\end{aligned}<br>$$</p>
<p>Another version:</p>
<p>$$<br>\begin{aligned}<br>\boldsymbol{m}_t &amp;\leftarrow \beta \boldsymbol{m}_{t-1} + (1-\beta) \boldsymbol{g}_t, \<br>\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}_{t-1} - \alpha_t \boldsymbol{m}_t,<br>\end{aligned}<br>$$</p>
<p>$$<br>\alpha_t = \frac{\eta_t}{1-\beta}<br>$$</p>
<p>其中，动量超参数 $\beta$满足 $0 \leq \beta &lt; 1$。当 $\beta=0$ 时，动量法等价于小批量随机梯度下降。</p>
<p>利用梯度下降在使用动量法后的迭代轨迹更加方便理解数学推导</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">momentum_2d</span><span class="params">(x1, x2, v1, v2)</span>:</span></span><br><span class="line">    v1 = beta * v1 + eta * <span class="number">0.2</span> * x1</span><br><span class="line">    v2 = beta * v2 + eta * <span class="number">4</span> * x2</span><br><span class="line">    <span class="keyword">return</span> x1 - v1, x2 - v2, v1, v2</span><br><span class="line"></span><br><span class="line">eta, beta = <span class="number">0.4</span>, <span class="number">0.5</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223170104740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>可以看到使用较小的学习率 $\eta=0.4$ 和动量超参数 $\beta=0.5$ 时，动量法在竖直方向上的移动更加平滑，且在水平方向上更快逼近最优解。下面使用较大的学习率 $\eta=0.6$，此时自变量也不再发散。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.6</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223170204913.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="Exponential-Moving-Average"><a href="#Exponential-Moving-Average" class="headerlink" title="Exponential Moving Average"></a>Exponential Moving Average</h3><blockquote>
<p>从数学上理解动量法</p>
</blockquote>
<hr>
<p>指数加权移动平均（exponential moving average）</p>
<p>给定超参数 $0 \leq \beta &lt; 1$，当前时间步 $t$ 的变量 $y_t$ 是上一时间步 $t-1$ 的变量 $y_{t-1}$ 和当前时间步另一变量 $x_t$ 的线性组合：</p>
<p>$$<br>y_t = \beta y_{t-1} + (1-\beta) x_t.<br>$$</p>
<p>我们可以对 $y_t$ 展开：</p>
<p>$$<br>\begin{aligned}<br>y_t  &amp;= (1-\beta) x_t + \beta y_{t-1}\<br>         &amp;= (1-\beta)x_t + (1-\beta) \cdot \beta x_{t-1} + \beta^2y_{t-2}\<br>         &amp;= (1-\beta)x_t + (1-\beta) \cdot \beta x_{t-1} + (1-\beta) \cdot \beta^2x_{t-2} + \beta^3y_{t-3}\<br>         &amp;= (1-\beta) \sum_{i=0}^{t} \beta^{i}x_{t-i}<br>\end{aligned}<br>$$</p>
<p>$$<br>(1-\beta)\sum_{i=0}^{t} \beta^{i} = \frac{1-\beta^{t}}{1-\beta} (1-\beta) = (1-\beta^{t})<br>$$</p>
<h3 id="Supp-Approximate"><a href="#Supp-Approximate" class="headerlink" title="Supp Approximate"></a>Supp Approximate</h3><p>Average of $\frac{1}{1-\beta}$ Steps</p>
<p>令 $n = 1/(1-\beta)$，那么 $\left(1-1/n\right)^n = \beta^{1/(1-\beta)}$。因为</p>
<p>$$<br> \lim_{n \rightarrow \infty}  \left(1-\frac{1}{n}\right)^n = \exp(-1) \approx 0.3679,<br>$$</p>
<p>所以当 $\beta \rightarrow 1$时，$\beta^{1/(1-\beta)}=\exp(-1)$，如 $0.95^{20} \approx \exp(-1)$。如果把 $\exp(-1)$ 当作一个比较小的数，我们可以在近似中忽略所有含 $\beta^{1/(1-\beta)}$ 和比 $\beta^{1/(1-\beta)}$ 更高阶的系数的项。例如，当 $\beta=0.95$ 时，</p>
<p>$$<br>y_t \approx 0.05 \sum_{i=0}^{19} 0.95^i x_{t-i}.<br>$$</p>
<p>因此，在实际中，我们常常将 $y_t$ 看作是对最近 $1/(1-\beta)$ 个时间步的 $x_t$ 值的加权平均。例如，当 $\gamma = 0.95$ 时，$y_t$ 可以被看作对最近20个时间步的 $x_t$ 值的加权平均；当 $\beta = 0.9$ 时，$y_t$ 可以看作是对最近10个时间步的 $x_t$ 值的加权平均。而且，离当前时间步 $t$ 越近的 $x_t$ 值获得的权重越大（越接近1）。</p>
<hr>
<h3 id="由指数加权移动平均理解动量法"><a href="#由指数加权移动平均理解动量法" class="headerlink" title="由指数加权移动平均理解动量法"></a>由指数加权移动平均理解动量法</h3><p>现在，我们对动量法的速度变量做变形：</p>
<p>$$<br>\boldsymbol{m}_t \leftarrow \beta \boldsymbol{m}_{t-1} + (1 - \beta) \left(\frac{\eta_t}{1 - \beta} \boldsymbol{g}_t\right).<br>$$</p>
<p>Another version:</p>
<p>$$<br>\boldsymbol{m}_t \leftarrow \beta \boldsymbol{m}_{t-1} + (1 - \beta) \boldsymbol{g}_t.<br>$$</p>
<p>$$<br>\begin{aligned}<br>\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}_{t-1} - \alpha_t \boldsymbol{m}_t,<br>\end{aligned}<br>$$</p>
<p>$$<br>\alpha_t = \frac{\eta_t}{1-\beta}<br>$$</p>
<p>由指数加权移动平均的形式可得，速度变量 $\boldsymbol{v}_t$ 实际上对序列 $\{\eta_{t-i}\boldsymbol{g}_{t-i} /(1-\beta):i=0,\ldots,1/(1-\beta)-1\}$ 做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，动量法在每个时间步的自变量更新量近似于将前者对应的最近 $1/(1-\beta)$ 个时间步的更新量做了指数加权移动平均后再除以 $1-\beta$。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。在本节之前示例的优化问题中，所有梯度在水平方向上为正（向右），而在竖直方向上时正（向上）时负（向下）。这样，我们就可以使用较大的学习率，从而使自变量向最优解更快移动。</p>
<h2 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h2><p>相对于小批量随机梯度下降，动量法需要对每一个自变量维护一个同它一样形状的速度变量，且超参数里多了动量超参数。实现中，我们将速度变量用更广义的状态变量<code>states</code>表示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span>  </span><br><span class="line">    data = np.genfromtxt(<span class="string">'/home/kesci/input/airfoil4755/airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), \</span><br><span class="line">        torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_momentum_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32)</span><br><span class="line">    v_b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> p, v <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v.data = hyperparams[<span class="string">'momentum'</span>] * v.data + hyperparams[<span class="string">'lr'</span>] * p.grad.data</span><br><span class="line">        p.data -= v.data</span><br></pre></td></tr></table></figure>
<blockquote>
<p>令 momentum = 0.5</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(sgd_momentum, init_momentum_states(),</span><br><span class="line">              &#123;<span class="string">'lr'</span>: <span class="number">0.02</span>, <span class="string">'momentum'</span>: <span class="number">0.5</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223170543454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<p>令 momentum = 0.9</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(sgd_momentum, init_momentum_states(),</span><br><span class="line">              &#123;<span class="string">'lr'</span>: <span class="number">0.02</span>, <span class="string">'momentum'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223170642172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>可见目标函数值在后期迭代过程中的变化不够平滑。直觉上，10倍小批量梯度比2倍小批量梯度大了5倍，我们可以试着将学习率减小到原来的1/5。此时目标函数值在下降了一段时间后变化更加平滑。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(sgd_momentum, init_momentum_states(),</span><br><span class="line">              &#123;<span class="string">'lr'</span>: <span class="number">0.004</span>, <span class="string">'momentum'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223170904126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class"><a href="#Pytorch-Class" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><blockquote>
<p>在Pytorch中，torch.optim.SGD 已实现了Momentum</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.SGD, &#123;<span class="string">'lr'</span>: <span class="number">0.004</span>, <span class="string">'momentum'</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">                    features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/202002231714533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h1 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a><div id="adagrad">AdaGrad</div></h1><p>在之前介绍过的优化算法中，目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。举个例子，假设目标函数为$f$，自变量为一个二维向量$[x_1, x_2]^\top$，该向量中每一个元素在迭代时都使用相同的学习率。例如，在学习率为$\eta$的梯度下降中，元素$x_1$和$x_2$都使用相同的学习率$\eta$来自我迭代：</p>
<p>$$<br>x_1 \leftarrow x_1 - \eta \frac{\partial{f}}{\partial{x_1}}, \quad<br>x_2 \leftarrow x_2 - \eta \frac{\partial{f}}{\partial{x_2}}.<br>$$</p>
<p><a href="#momentum">动量法</a>中当 $x_1$ 和 $x_2$ 的梯度值有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。</p>
<p>动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。</p>
<p>AdaGrad 算法，它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题 。</p>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>AdaGrad算法会使用一个小批量随机梯度 $\boldsymbol{g}_t$ 按元素平方的累加变量$\boldsymbol{s}_t$。在时间步0，AdaGrad将 $\boldsymbol{s}_0$ 中每个元素初始化为0。在时间步$t$，首先将小批量随机梯度$\boldsymbol{g}_t$按元素平方后累加到变量$\boldsymbol{s}_t$：</p>
<p>$$<br>\boldsymbol{s}_t \leftarrow \boldsymbol{s}_{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t,<br>$$</p>
<p>其中$\odot$是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：</p>
<p>$$<br>\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t,<br>$$</p>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。</p>
<h2 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h2><p>需要强调的是，小批量随机梯度按元素平方的累加变量$\boldsymbol{s}_t$出现在学习率的分母项中。</p>
<ul>
<li><p>因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；</p>
</li>
<li><p>反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。</p>
</li>
<li><p>然而，由于$\boldsymbol{s}_t$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。</p>
</li>
<li><p>所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。</p>
</li>
</ul>
<p>下面我们仍然以目标函数$f(\boldsymbol{x})=0.1x_1^2+2x_2^2$为例观察AdaGrad算法对自变量的迭代轨迹。我们实现AdaGrad算法并使用和上一节实验中相同的学习率0.4。可以看到，自变量的迭代轨迹较平滑。但由于$\boldsymbol{s}_t$的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span>  <span class="comment"># 前两项为自变量梯度</span></span><br><span class="line">    s1 += g1 ** <span class="number">2</span></span><br><span class="line">    s2 += g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.4</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223171954469.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<p>下面将学习率增大到2。可以看到自变量更为迅速地逼近了最优解</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">2</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223172353999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Implement-1"><a href="#Implement-1" class="headerlink" title="Implement"></a>Implement</h2><p>同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。我们根据AdaGrad算法中的公式实现该算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span>  </span><br><span class="line">    data = np.genfromtxt(<span class="string">'path to airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), </span><br><span class="line">        torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32)</span><br><span class="line">        </span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adagrad_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32)</span><br><span class="line">    s_b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    eps = <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s.data += (p.grad.data**<span class="number">2</span>)</span><br><span class="line">        p.data -= hyperparams[<span class="string">'lr'</span>] * p.grad.data / torch.sqrt(s + eps)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>增大学习率</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(adagrad, init_adagrad_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223173629947.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class-1"><a href="#Pytorch-Class-1" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><p>通过名称为 “adagrad” 的 Trainer 实例，我们便可使用 Pytorch 提供的 AdaGrad 算法来训练模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adagrad, &#123;<span class="string">'lr'</span>: <span class="number">0.1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223173755189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h1 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a><div id="rmsprop">RMSProp</div></h1><p><a href="#adagrad">“AdaGrad算法”</a>中因为调整学习率时分母上的变量 $\boldsymbol{s}_t$ 一直在累加按元素平方的小批量随机梯度，所以目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。因此，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp算法对AdaGrad算法做了修改。该算法源自Coursera上的一门课程，即“机器学习的神经网络”。</p>
<h2 id="Algorithm-1"><a href="#Algorithm-1" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>我们在<a href="#momentum">“动量法”</a>一节里介绍过指数加权移动平均。不同于AdaGrad算法里状态变量$\boldsymbol{s}_t$是截至时间步$t$所有小批量随机梯度$\boldsymbol{g}_t$按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数$0 \leq \gamma 0$计算</p>
<p>$$<br>\boldsymbol{v}_t \leftarrow \beta \boldsymbol{v}_{t-1} + (1 - \beta) \boldsymbol{g}_t \odot \boldsymbol{g}_t.<br>$$</p>
<p>和AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量</p>
<p>$$<br>\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\alpha}{\sqrt{\boldsymbol{v}_t + \epsilon}} \odot \boldsymbol{g}_t,<br>$$</p>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。因为RMSProp算法的状态变量$\boldsymbol{s}_t$是对平方项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$的指数加权移动平均，所以可以看作是最近$1/(1-\beta)$个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。</p>
<p>照例，让我们先观察RMSProp算法对目标函数$f(\boldsymbol{x})=0.1x_1^2+2x_2^2$中自变量的迭代轨迹。回忆在<a href="#adagrad">“AdaGrad算法”</a>中使用的学习率为0.4的AdaGrad算法，自变量在迭代后期的移动幅度较小。但在同样的学习率下，RMSProp算法可以更快逼近最优解。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span></span><br><span class="line">    s1 = beta * s1 + (<span class="number">1</span> - beta) * g1 ** <span class="number">2</span></span><br><span class="line">    s2 = beta * s2 + (<span class="number">1</span> - beta) * g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= alpha / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= alpha / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">alpha, beta = <span class="number">0.4</span>, <span class="number">0.9</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/2020022317423677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Implement-2"><a href="#Implement-2" class="headerlink" title="Implement"></a>Implement</h2><blockquote>
<p>接下来按照RMSProp算法中的公式实现该算法。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span>  </span><br><span class="line">    data = np.genfromtxt(<span class="string">'path to airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), \</span><br><span class="line">        torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32)</span><br><span class="line">        </span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rmsprop_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32)</span><br><span class="line">    s_b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    gamma, eps = hyperparams[<span class="string">'beta'</span>], <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s.data = gamma * s.data + (<span class="number">1</span> - gamma) * (p.grad.data)**<span class="number">2</span></span><br><span class="line">        p.data -= hyperparams[<span class="string">'lr'</span>] * p.grad.data / torch.sqrt(s + eps)</span><br></pre></td></tr></table></figure>
<p>我们将初始学习率设为0.01，并将超参数$\gamma$设为0.9。此时，变量$\boldsymbol{s}_t$可看作是最近$1/(1-0.9) = 10$个时间步的平方项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$的加权平均。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(rmsprop, init_rmsprop_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'beta'</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">              features, labels)</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/2020022317441345.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class-2"><a href="#Pytorch-Class-2" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><p>通过名称为“rmsprop”的Trainer实例，我们便可使用Gluon提供的RMSProp算法来训练模型。注意，超参数$\gamma$通过gamma1指定。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.RMSprop, &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'alpha'</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">                    features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223174542606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h1 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a><div id="adadelta">AdaDelta</div></h1><p>除了<a href="#rmsprop">RMSProp算法</a>以外，另一个常用优化算法AdaDelta算法也针对<a href="#adagrad">AdaGrad算法</a>在迭代后期可能较难找到有用解的问题做了改进 .</p>
<blockquote>
<p>AdaDelta算法没有学习率这一超参数。</p>
</blockquote>
<h2 id="Algorithm-2"><a href="#Algorithm-2" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>AdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度$\boldsymbol{g}_t$按元素平方的指数加权移动平均变量$\boldsymbol{s}_t$。在时间步0，它的所有元素被初始化为0。给定超参数$0 \leq \rho 0$，同RMSProp算法一样计算</p>
<p>$$<br>\boldsymbol{s}_t \leftarrow \rho \boldsymbol{s}_{t-1} + (1 - \rho) \boldsymbol{g}_t \odot \boldsymbol{g}_t.<br>$$</p>
<p>与RMSProp算法不同的是，AdaDelta算法还维护一个额外的状态变量$\Delta\boldsymbol{x}<em>t$，其元素同样在时间步0时被初始化为0。我们使用$\Delta\boldsymbol{x}</em>{t-1}$来计算自变量的变化量：</p>
<p>$$<br> \boldsymbol{g}_t’ \leftarrow \sqrt{\frac{\Delta\boldsymbol{x}_{t-1} + \epsilon}{\boldsymbol{s}_t + \epsilon}}   \odot \boldsymbol{g}_t,<br>$$</p>
<p>其中$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-5}$。接着更新自变量：</p>
<p>$$<br>\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}’_t.<br>$$</p>
<p>最后，我们使用$\Delta\boldsymbol{x}_t$来记录自变量变化量$\boldsymbol{g}’_t$按元素平方的指数加权移动平均：</p>
<p>$$<br>\Delta\boldsymbol{x}_t \leftarrow \rho \Delta\boldsymbol{x}_{t-1} + (1 - \rho) \boldsymbol{g}’_t \odot \boldsymbol{g}’_t.<br>$$</p>
<p>可以看到，如不考虑$\epsilon$的影响，AdaDelta算法与RMSProp算法的不同之处在于使用$\sqrt{\Delta\boldsymbol{x}_{t-1}}$来替代超参数$\eta$。</p>
<h2 id="Implement-3"><a href="#Implement-3" class="headerlink" title="Implement"></a>Implement</h2><p>AdaDelta算法需要对每个自变量维护两个状态变量，即$\boldsymbol{s}_t$和$\Delta\boldsymbol{x}_t$。我们按AdaDelta算法中的公式实现该算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adadelta_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w, s_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    delta_w, delta_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> ((s_w, delta_w), (s_b, delta_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adadelta</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    rho, eps = hyperparams[<span class="string">'rho'</span>], <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> p, (s, delta) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s[:] = rho * s + (<span class="number">1</span> - rho) * (p.grad.data**<span class="number">2</span>)</span><br><span class="line">        g =  p.grad.data * torch.sqrt((delta + eps) / (s + eps))</span><br><span class="line">        p.data -= g</span><br><span class="line">        delta[:] = rho * delta + (<span class="number">1</span> - rho) * g * g</span><br><span class="line">d2l.train_ch7(adadelta, init_adadelta_states(), &#123;<span class="string">'rho'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223174834831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class-3"><a href="#Pytorch-Class-3" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><p>通过名称为 “adadelta” 的 Traine r实例，我们便可使用 pytorch 提供的 AdaDelta 算法。它的超参数可以通过 rho 来指定。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adadelta, &#123;<span class="string">'rho'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/2020022317495777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a><div id="adam">Adam</div></h1><blockquote>
<p>相当于是<a href="#rmsprop">RMSProp算法</a>和<a href="#momentum">动量算法</a>的结合</p>
</blockquote>
<center>RMSProp算法</center>
<center>+</center>
<center>对小批量随机梯度也做了指数加权移动平均</center>
<center>||</center>
<center>Adam</center>

<h2 id="Algorithm-3"><a href="#Algorithm-3" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Adam算法使用了动量变量$\boldsymbol{m}_t$和RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量$\boldsymbol{v}_t$，并在时间步0将它们中每个元素初始化为0。给定超参数$0 \leq \beta_1 &lt; 1$（算法作者建议设为0.9），时间步$t$的动量变量$\boldsymbol{m}_t$即小批量随机梯度$\boldsymbol{g}_t$的指数加权移动平均：</p>
<p>$$<br>\boldsymbol{m}_t \leftarrow \beta_1 \boldsymbol{m}_{t-1} + (1 - \beta_1) \boldsymbol{g}_t.<br>$$</p>
<p>和RMSProp算法中一样，给定超参数$0 \leq \beta_2 &lt; 1$（算法作者建议设为0.999），<br>将小批量随机梯度按元素平方后的项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$做指数加权移动平均得到$\boldsymbol{v}_t$：</p>
<p>$$<br>\boldsymbol{v}_t \leftarrow \beta_2 \boldsymbol{v}_{t-1} + (1 - \beta_2) \boldsymbol{g}_t \odot \boldsymbol{g}_t.<br>$$</p>
<p>由于我们将$\boldsymbol{m}_0$和$\boldsymbol{s}_0$中的元素都初始化为0，<br>在时间步$t$我们得到$\boldsymbol{m}_t =  (1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} \boldsymbol{g}_i$。将过去各时间步小批量随机梯度的权值相加，得到 $(1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} = 1 - \beta_1^t$。需要注意的是，当$t$较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当$\beta_1 = 0.9$时，$\boldsymbol{m}_1 = 0.1\boldsymbol{g}_1$。为了消除这样的影响，对于任意时间步$t$，我们可以将$\boldsymbol{m}_t$再除以$1 - \beta_1^t$，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，我们对变量$\boldsymbol{m}_t$和$\boldsymbol{v}_t$均作偏差修正：</p>
<p>$$<br>\hat{\boldsymbol{m}}_t \leftarrow \frac{\boldsymbol{m}_t}{1 - \beta_1^t},<br>$$</p>
<p>$$<br>\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_2^t}.<br>$$</p>
<p>接下来，Adam算法使用以上偏差修正后的变量$\hat{\boldsymbol{m}}_t$和$\hat{\boldsymbol{m}}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整：</p>
<p>$$<br>\boldsymbol{g}_t’ \leftarrow \frac{\eta \hat{\boldsymbol{m}}_t}{\sqrt{\hat{\boldsymbol{v}}_t} + \epsilon},<br>$$</p>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-8}$。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用$\boldsymbol{g}_t’$迭代自变量：</p>
<p>$$<br>\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}_t’.<br>$$</p>
<h2 id="Implement-4"><a href="#Implement-4" class="headerlink" title="Implement"></a>Implement</h2><p>我们按照Adam算法中的公式实现该算法。其中时间步$t$通过 hyperparams 参数传入 adam 函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span>  </span><br><span class="line">    data = np.genfromtxt(<span class="string">'path to airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), \</span><br><span class="line">        torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32)</span><br><span class="line">        </span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adam_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w, v_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    s_w, s_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad.data</span><br><span class="line">        s[:] = beta2 * s + (<span class="number">1</span> - beta2) * p.grad.data**<span class="number">2</span></span><br><span class="line">        v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">'t'</span>])</span><br><span class="line">        s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">'t'</span>])</span><br><span class="line">        p.data -= hyperparams[<span class="string">'lr'</span>] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)</span><br><span class="line">    hyperparams[<span class="string">'t'</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(adam, init_adam_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'t'</span>: <span class="number">1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223180150289.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class-4"><a href="#Pytorch-Class-4" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adam, &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>


<center><img src="https://img-blog.csdnimg.cn/20200223180238893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Momentum</tag>
        <tag>AdaGrad</tag>
        <tag>RMSProp</tag>
        <tag>AdaDelta</tag>
        <tag>Adam</tag>
      </tags>
  </entry>
  <entry>
    <title>Optimization including Convex Optimization and Gradient Descent</title>
    <url>/2020/02/28/Optimization%20including%20Convex%20Optimization%20and%20Gradient%20Descent/</url>
    <content><![CDATA[<p><font color=red>温馨提示：</font></p>
<p>&ensp;&ensp;&ensp;&ensp;<font color=red>本文将介绍统计学中的优化知识，凸优化和梯度下降，多为公式推导和图形化展示，较为硬核</font></p>
<h1 id="优化与深度学习"><a href="#优化与深度学习" class="headerlink" title="优化与深度学习"></a>优化与深度学习</h1><h2 id="优化与估计"><a href="#优化与估计" class="headerlink" title="优化与估计"></a>优化与估计</h2><p>尽管优化方法可以最小化深度学习中的损失函数值，但本质上优化方法达到的目标与深度学习的目标并不相同。</p>
<ul>
<li><p><strong>优化方法目标</strong>：训练集损失函数值</p>
</li>
<li><p><strong>深度学习目标</strong>：测试集损失函数值（泛化性）</p>
</li>
<li><p><strong>借助图形直观比较</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'path to file storge d2lzh1981'</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d <span class="comment"># 三维画图</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span> <span class="keyword">return</span> x * np.cos(np.pi * x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(x)</span>:</span> <span class="keyword">return</span> f(x) + <span class="number">0.2</span> * np.cos(<span class="number">5</span> * np.pi * x)</span><br><span class="line"></span><br><span class="line">d2l.set_figsize((<span class="number">5</span>, <span class="number">3</span>))</span><br><span class="line">x = np.arange(<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">0.01</span>)</span><br><span class="line">fig_f, = d2l.plt.plot(x, f(x),label=<span class="string">"train error"</span>)</span><br><span class="line">fig_g, = d2l.plt.plot(x, g(x),<span class="string">'--'</span>, c=<span class="string">'purple'</span>, label=<span class="string">"test error"</span>)</span><br><span class="line">fig_f.axes.annotate(<span class="string">'empirical risk'</span>, (<span class="number">1.0</span>, <span class="number">-1.2</span>), (<span class="number">0.5</span>, <span class="number">-1.1</span>),arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">fig_g.axes.annotate(<span class="string">'expected risk'</span>, (<span class="number">1.1</span>, <span class="number">-1.05</span>), (<span class="number">0.95</span>, <span class="number">-0.5</span>),arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'risk'</span>)</span><br><span class="line">d2l.plt.legend(loc=<span class="string">"upper right"</span>)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220165647572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="优化在深度学习中的挑战"><a href="#优化在深度学习中的挑战" class="headerlink" title="优化在深度学习中的挑战"></a>优化在深度学习中的挑战</h2><ol>
<li><a href="#Local_minimum"><strong>局部最小值</strong></a></li>
<li><a href="#saddle_point"><strong>鞍点</strong></a></li>
<li><a href="#vanishing_gradient"><strong>梯度消失</strong></a></li>
</ol>
<div id="Local_minimum"><b>局部最小值</b></div>

<p>$$<br>f(x) = x\cos \pi x<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * np.cos(np.pi * x)</span><br><span class="line"></span><br><span class="line">d2l.set_figsize((<span class="number">4.5</span>, <span class="number">2.5</span>))</span><br><span class="line">x = np.arange(<span class="number">-1.0</span>, <span class="number">2.0</span>, <span class="number">0.1</span>)</span><br><span class="line">fig,  = d2l.plt.plot(x, f(x))</span><br><span class="line">fig.axes.annotate(<span class="string">'local minimum'</span>, xy=(<span class="number">-0.3</span>, <span class="number">-0.25</span>), xytext=(<span class="number">-0.77</span>, <span class="number">-1.0</span>),</span><br><span class="line">                  arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">fig.axes.annotate(<span class="string">'global minimum'</span>, xy=(<span class="number">1.1</span>, <span class="number">-0.95</span>), xytext=(<span class="number">0.6</span>, <span class="number">0.8</span>),</span><br><span class="line">                  arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'f(x)'</span>);</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220170158309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<div id="saddle_point"><b>鞍点</b></div><br/>

<blockquote>
<p>函数在一阶导数为零处（驻点）的黑塞矩阵为不定矩阵。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">-2.0</span>, <span class="number">2.0</span>, <span class="number">0.1</span>)</span><br><span class="line">fig, = d2l.plt.plot(x, x**<span class="number">3</span>)</span><br><span class="line">fig.axes.annotate(<span class="string">'saddle point'</span>, xy=(<span class="number">0</span>, <span class="number">-0.2</span>), xytext=(<span class="number">-0.52</span>, <span class="number">-5.0</span>),</span><br><span class="line">                  arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'f(x)'</span>);</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220170416636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<hr>
<p><strong>海森矩阵</strong></p>
<p>$$<br>A=\left[\begin{array}{cccc}{\frac{\partial^{2} f}{\partial x_{1}^{2}}} &amp; {\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}}} \\ {\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}} &amp; {\frac{\partial^{2} f}{\partial x_{2}^{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{2} \partial x_{n}}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} &amp; {\frac{\partial^{2} f}{\partial x_{n} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{n}^{2}}}\end{array}\right]<br>$$</p>
<blockquote>
<p>海森矩阵特征值和鞍点还有局部极小值的点的关系</p>
<blockquote>
<p>偏导数为零的点</p>
<ul>
<li>特征值都大于零是局部极小值点</li>
<li>都为负数是局部极大指点</li>
<li>有正有负就是鞍点</li>
</ul>
</blockquote>
</blockquote>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x, y = np.mgrid[<span class="number">-1</span>: <span class="number">1</span>: <span class="number">31j</span>, <span class="number">-1</span>: <span class="number">1</span>: <span class="number">31j</span>]</span><br><span class="line">z = x**<span class="number">2</span> - y**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">d2l.set_figsize((<span class="number">6</span>, <span class="number">4</span>))</span><br><span class="line">ax = d2l.plt.figure().add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.plot_wireframe(x, y, z, **&#123;<span class="string">'rstride'</span>: <span class="number">2</span>, <span class="string">'cstride'</span>: <span class="number">2</span>&#125;)</span><br><span class="line">ax.plot([<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">0</span>], <span class="string">'ro'</span>, markersize=<span class="number">10</span>)</span><br><span class="line">ticks = [<span class="number">-1</span>,  <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">d2l.plt.xticks(ticks)</span><br><span class="line">d2l.plt.yticks(ticks)</span><br><span class="line">ax.set_zticks(ticks)</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'y'</span>);</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220171025698.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<div id="vanishing_gradient"><b>梯度消失</b></div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">-2.0</span>, <span class="number">5.0</span>, <span class="number">0.01</span>)</span><br><span class="line">fig, = d2l.plt.plot(x, np.tanh(x))</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'f(x)'</span>)</span><br><span class="line">fig.axes.annotate(<span class="string">'vanishing gradient'</span>, (<span class="number">4</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">0.0</span>) ,arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220171233420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'path to file storge d2lzh1981'</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h3 id="一维梯度下降"><a href="#一维梯度下降" class="headerlink" title="一维梯度下降"></a>一维梯度下降</h3><hr>
<p><strong>证明：沿梯度反方向移动自变量可以减小函数值</strong></p>
<p>泰勒展开：</p>
<p>$$<br>f(x+\epsilon)=f(x)+\epsilon f^{\prime}(x)+\mathcal{O}\left(\epsilon^{2}\right)<br>$$</p>
<p>代入沿梯度方向的移动量 $\eta f^{\prime}(x)$：</p>
<p>$$<br>f\left(x-\eta f^{\prime}(x)\right)=f(x)-\eta f^{\prime 2}(x)+\mathcal{O}\left(\eta^{2} f^{\prime 2}(x)\right)<br>$$</p>
<p>$$<br>f\left(x-\eta f^{\prime}(x)\right) \lesssim f(x)<br>$$</p>
<p>$$<br>x \leftarrow x-\eta f^{\prime}(x)<br>$$</p>
<hr>
<p>e.g.</p>
<p>$$<br>f(x) = x^2<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span>  <span class="comment"># Objective function</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x  <span class="comment"># Its derivative</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd</span><span class="params">(eta)</span>:</span></span><br><span class="line">    x = <span class="number">10</span></span><br><span class="line">    results = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        <span class="comment"># eta 学习率</span></span><br><span class="line">        x -= eta * gradf(x)</span><br><span class="line">        results.append(x)</span><br><span class="line">    print(<span class="string">'epoch 20, x:'</span>, x)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">res = gd(<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<p><strong>梯度下降轨迹</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_trace</span><span class="params">(res)</span>:</span></span><br><span class="line">    n = max(abs(min(res)), abs(max(res)))</span><br><span class="line">    f_line = np.arange(-n, n, <span class="number">0.01</span>)</span><br><span class="line">    d2l.set_figsize((<span class="number">3.5</span>, <span class="number">2.5</span>))</span><br><span class="line">    d2l.plt.plot(f_line, [f(x) <span class="keyword">for</span> x <span class="keyword">in</span> f_line],<span class="string">'-'</span>)</span><br><span class="line">    d2l.plt.plot(res, [f(x) <span class="keyword">for</span> x <span class="keyword">in</span> res],<span class="string">'-o'</span>)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'f(x)'</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">show_trace(res)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220171815966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<center><font size=5>学习率</font></center>

<blockquote>
<p>学习率过小 Code：show_trace(gd(0.05))<br><img src="https://img-blog.csdnimg.cn/2020022017211762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
<blockquote>
<p>学习率过大  Code：show_trace(gd(1.1))<br><img src="https://img-blog.csdnimg.cn/20200220172154883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
</blockquote>
<blockquote>
<center><font size=4>局部极小值</font></center>
</blockquote>
<p>$$<br>f(x) = x\cos cx<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = <span class="number">0.15</span> * np.pi</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * np.cos(c * x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.cos(c * x) - c * x * np.sin(c * x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率不合适容易导致</span></span><br><span class="line">show_trace(gd(<span class="number">2</span>))</span><br><span class="line">show_trace(gd(<span class="number">0.5</span>))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220172910884.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="多维梯度下降"><a href="#多维梯度下降" class="headerlink" title="多维梯度下降"></a>多维梯度下降</h3><p>$$<br>\nabla f(\mathbf{x})=\left[\frac{\partial f(\mathbf{x})}{\partial x_{1}}, \frac{\partial f(\mathbf{x})}{\partial x_{2}}, \dots, \frac{\partial f(\mathbf{x})}{\partial x_{d}}\right]^{\top}<br>$$</p>
<p>$$<br>f(\mathbf{x}+\epsilon)=f(\mathbf{x})+\epsilon^{\top} \nabla f(\mathbf{x})+\mathcal{O}\left(|\epsilon|^{2}\right)<br>$$</p>
<p>$$<br>\mathbf{x} \leftarrow \mathbf{x}-\eta \nabla f(\mathbf{x})<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练 trainer展示x如何更新</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_2d</span><span class="params">(trainer, steps=<span class="number">20</span>)</span>:</span></span><br><span class="line">    x1, x2 = <span class="number">-5</span>, <span class="number">-2</span></span><br><span class="line">    results = [(x1, x2)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(steps):</span><br><span class="line">        x1, x2 = trainer(x1, x2)</span><br><span class="line">        results.append((x1, x2))</span><br><span class="line">    print(<span class="string">'epoch %d, x1 %f, x2 %f'</span> % (i + <span class="number">1</span>, x1, x2))</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"><span class="comment"># 垂直于等高线梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_trace_2d</span><span class="params">(f, results)</span>:</span> </span><br><span class="line">    d2l.plt.plot(*zip(*results), <span class="string">'-o'</span>, color=<span class="string">'#ff7f0e'</span>)</span><br><span class="line">    x1, x2 = np.meshgrid(np.arange(<span class="number">-5.5</span>, <span class="number">1.0</span>, <span class="number">0.1</span>), np.arange(<span class="number">-3.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>))</span><br><span class="line">    d2l.plt.contour(x1, x2, f(x1, x2), colors=<span class="string">'#1f77b4'</span>)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'x2'</span>)</span><br></pre></td></tr></table></figure>
<p>e.g.</p>
<p>$$<br>f(x) = x_1^2 + 2x_2^2<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span>  <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="keyword">return</span> x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * <span class="number">2</span> * x1, x2 - eta * <span class="number">4</span> * x2)</span><br><span class="line"></span><br><span class="line">show_trace_2d(f_2d, train_2d(gd_2d))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220214102821.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="自适应方法"><a href="#自适应方法" class="headerlink" title="自适应方法"></a>自适应方法</h2><h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><blockquote>
<p><strong>优势 :</strong><br/><br/></p>
<blockquote>
<p>梯度下降“步幅”的确定比较困难<br/><br>而牛顿法相当于可以通过Hessian矩阵来调整“步幅”。</p>
</blockquote>
<p>在牛顿法中，局部极小值也可以通过调整学习率来解决。</p>
</blockquote>
<p>在 $x + \epsilon$ 处泰勒展开：</p>
<p>$$<br>f(\mathbf{x}+\epsilon)=f(\mathbf{x})+\epsilon^{\top} \nabla f(\mathbf{x})+\frac{1}{2} \epsilon^{\top} \nabla \nabla^{\top} f(\mathbf{x}) \epsilon+\mathcal{O}\left(|\epsilon|^{3}\right)<br>$$</p>
<p>最小值点处满足: $\nabla f(\mathbf{x})=0$, 即我们希望 $\nabla f(\mathbf{x} + \epsilon)=0$, 对上式关于 $\epsilon$ 求导，忽略高阶无穷小，有：</p>
<p>$$<br>\nabla f(\mathbf{x})+\boldsymbol{H}<em>{f} \boldsymbol{\epsilon}=0 \text { and hence } \epsilon=-\boldsymbol{H}</em>{f}^{-1} \nabla f(\mathbf{x})<br>$$</p>
<blockquote>
<p>牛顿法需要计算Hessian矩阵的逆，计算量比较大。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.cosh(c * x)  <span class="comment"># Objective</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> c * np.sinh(c * x)  <span class="comment"># Derivative</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hessf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> c**<span class="number">2</span> * np.cosh(c * x)  <span class="comment"># Hessian</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hide learning rate for now</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newton</span><span class="params">(eta=<span class="number">1</span>)</span>:</span></span><br><span class="line">    x = <span class="number">10</span></span><br><span class="line">    results = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        x -= eta * gradf(x) / hessf(x)</span><br><span class="line">        results.append(x)</span><br><span class="line">    print(<span class="string">'epoch 10, x:'</span>, x)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">show_trace(newton())</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220214418174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 牛顿法对于有局部极小值的情况</span></span><br><span class="line"><span class="comment"># 和梯度下降的方法有一样的效果</span></span><br><span class="line"><span class="comment"># 正确的方法还是降低学习率</span></span><br><span class="line">c = <span class="number">0.15</span> * np.pi</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * np.cos(c * x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.cos(c * x) - c * x * np.sin(c * x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hessf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> - <span class="number">2</span> * c * np.sin(c * x) - x * c**<span class="number">2</span> * np.cos(c * x)</span><br><span class="line"></span><br><span class="line">show_trace(newton())</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220214459356.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<p>show_trace(newton(0.5))<br><img src="https://img-blog.csdnimg.cn/20200220214557927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
<h3 id="收敛性分析"><a href="#收敛性分析" class="headerlink" title="收敛性分析"></a>收敛性分析</h3><p>只考虑在函数为凸函数, 且最小值点上 $f’’(x^*) &gt;0$ 时的收敛速度：</p>
<p>令 $x_k$ 为第 $k$ 次迭代后 $x$ 的值， $e_{k}:=x_{k}-x^{*}$ 表示 $x_k$ 到最小值点 $x^{*}$ 的距离，由 $f’(x^{*}) = 0$:</p>
<p>$$<br>0=f^{\prime}\left(x_{k}-e_{k}\right)=f^{\prime}\left(x_{k}\right)-e_{k} f^{\prime \prime}\left(x_{k}\right)+\frac{1}{2} e_{k}^{2} f^{\prime \prime \prime}\left(\xi_{k}\right) \text{for some } \xi_{k} \in\left[x_{k}-e_{k}, x_{k}\right]<br>$$</p>
<p>两边除以 $f’’(x_k)$, 有：</p>
<p>$$<br>e_{k}-f^{\prime}\left(x_{k}\right) / f^{\prime \prime}\left(x_{k}\right)=\frac{1}{2} e_{k}^{2} f^{\prime \prime \prime}\left(\xi_{k}\right) / f^{\prime \prime}\left(x_{k}\right)<br>$$</p>
<p>代入更新方程 $x_{k+1} = x_{k} - f^{\prime}\left(x_{k}\right) / f^{\prime \prime}\left(x_{k}\right)$, 得到：</p>
<p>$$<br>x_k - x^{*} - f^{\prime}\left(x_{k}\right) / f^{\prime \prime}\left(x_{k}\right) =\frac{1}{2} e_{k}^{2} f^{\prime \prime \prime}\left(\xi_{k}\right) / f^{\prime \prime}\left(x_{k}\right)<br>$$</p>
<p>$$<br>x_{k+1} - x^{*} = e_{k+1} = \frac{1}{2} e_{k}^{2} f^{\prime \prime \prime}\left(\xi_{k}\right) / f^{\prime \prime}\left(x_{k}\right)<br>$$</p>
<p>当 $\frac{1}{2} f^{\prime \prime \prime}\left(\xi_{k}\right) / f^{\prime \prime}\left(x_{k}\right) \leq c$ 时，有:</p>
<p>$$<br>e_{k+1} \leq c e_{k}^{2}<br>$$</p>
<h3 id="预处理-（Heissan阵辅助梯度下降）"><a href="#预处理-（Heissan阵辅助梯度下降）" class="headerlink" title="预处理 （Heissan阵辅助梯度下降）"></a>预处理 （Heissan阵辅助梯度下降）</h3><p>$$<br>\mathbf{x} \leftarrow \mathbf{x}-\eta \operatorname{diag}\left(H_{f}\right)^{-1} \nabla \mathbf{x}<br>$$</p>
<h3 id="梯度下降与线性搜索（共轭梯度法）"><a href="#梯度下降与线性搜索（共轭梯度法）" class="headerlink" title="梯度下降与线性搜索（共轭梯度法）"></a>梯度下降与线性搜索（共轭梯度法）</h3><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><h3 id="随机梯度下降参数更新"><a href="#随机梯度下降参数更新" class="headerlink" title="随机梯度下降参数更新"></a>随机梯度下降参数更新</h3><p>对于有 $n$ 个样本对训练数据集，设 $f_i(x)$ 是第 $i$ 个样本的损失函数, 则目标函数为:</p>
<p>$$<br>f(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} f_{i}(\mathbf{x})<br>$$</p>
<p>其梯度为:</p>
<p>$$<br>\nabla f(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} \nabla f_{i}(\mathbf{x})<br>$$</p>
<p>每一个样本的梯度是对整体的梯度的无偏估计</p>
<p>使用该梯度的一次更新的时间复杂度为 $\mathcal{O}(n)$</p>
<p>随机梯度下降更新公式 $\mathcal{O}(1)$:</p>
<p>$$<br>\mathbf{x} \leftarrow \mathbf{x}-\eta \nabla f_{i}(\mathbf{x})<br>$$</p>
<p>且有：</p>
<p>$$<br>\mathbb{E}<em>{i} \nabla f</em>{i}(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} \nabla f_{i}(\mathbf{x})=\nabla f(\mathbf{x})<br>$$<br>e.g. </p>
<p>$$<br>f(x_1, x_2) = x_1^2 + 2 x_2^2<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span>  <span class="comment"># Objective</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">2</span> * x1, <span class="number">4</span> * x2)  <span class="comment"># Gradient</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(x1, x2)</span>:</span>  <span class="comment"># Simulate noisy gradient</span></span><br><span class="line">    <span class="keyword">global</span> lr  <span class="comment"># Learning rate scheduler</span></span><br><span class="line">    (g1, g2) = gradf(x1, x2)  <span class="comment"># Compute gradient</span></span><br><span class="line">    (g1, g2) = (g1 + np.random.normal(<span class="number">0.1</span>), g2 + np.random.normal(<span class="number">0.1</span>))</span><br><span class="line">    eta_t = eta * lr()  <span class="comment"># Learning rate at time t</span></span><br><span class="line">    <span class="keyword">return</span> (x1 - eta_t * g1, x2 - eta_t * g2)  <span class="comment"># Update variables</span></span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.1</span></span><br><span class="line">lr = (<span class="keyword">lambda</span>: <span class="number">1</span>)  <span class="comment"># Constant learning rate</span></span><br><span class="line">show_trace_2d(f, train_2d(sgd, steps=<span class="number">50</span>))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/2020022021481780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="动态学习率"><a href="#动态学习率" class="headerlink" title="动态学习率"></a>动态学习率</h3><ul>
<li>在最开始学习率设计比较大，加速收敛</li>
<li>学习率可以设计为指数衰减或多项式衰减</li>
<li>在优化进行一段时间后可以适当减小学习率来避免振荡</li>
</ul>
<p>$$<br>\begin{array}{ll}{\eta(t)=\eta_{i} \text { if } t_{i} \leq t \leq t_{i+1}} &amp; {\text { piecewise constant }} \\ {\eta(t)=\eta_{0} \cdot e^{-\lambda t}} &amp; {\text { exponential }} \\ {\eta(t)=\eta_{0} \cdot(\beta t+1)^{-\alpha}} &amp; {\text { polynomial }}\end{array}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exponential</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ctr</span><br><span class="line">    ctr += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> math.exp(<span class="number">-0.1</span> * ctr)</span><br><span class="line"></span><br><span class="line">ctr = <span class="number">1</span></span><br><span class="line">lr = exponential  <span class="comment"># Set up learning rate</span></span><br><span class="line">show_trace_2d(f, train_2d(sgd, steps=<span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220215551658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polynomial</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 迭代次数</span></span><br><span class="line">    <span class="keyword">global</span> ctr</span><br><span class="line">    ctr += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> + <span class="number">0.1</span> * ctr)**(<span class="number">-0.5</span>)</span><br><span class="line"></span><br><span class="line">ctr = <span class="number">1</span></span><br><span class="line">lr = polynomial  <span class="comment"># Set up learning rate</span></span><br><span class="line">show_trace_2d(f, train_2d(sgd, steps=<span class="number">50</span>))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200220215558471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h2><h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><p><a href="https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise" target="_blank" rel="noopener">读取数据</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span></span><br><span class="line">    data = np.genfromtxt(<span class="string">'/home/kesci/input/airfoil4755/airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>) <span class="comment"># 标准化</span></span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), \</span><br><span class="line">           torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32) <span class="comment"># 前1500个样本(每个样本5个特征)</span></span><br><span class="line"></span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line">features.shape</span><br></pre></td></tr></table></figure>
<p><strong>数据可视化</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">'path to airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Stochastic Gradient Descent (SGD)函数</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        p.data -= hyperparams[<span class="string">'lr'</span>] * p.grad.data</span><br></pre></td></tr></table></figure>
<p><strong>训练</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch7</span><span class="params">(optimizer_fn, states, hyperparams, features, labels,</span></span></span><br><span class="line"><span class="function"><span class="params">              batch_size=<span class="number">10</span>, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    net, loss = d2l.linreg, d2l.squared_loss</span><br><span class="line">    </span><br><span class="line">    w = torch.nn.Parameter(torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(features.shape[<span class="number">1</span>], <span class="number">1</span>)), dtype=torch.float32),</span><br><span class="line">                           requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.nn.Parameter(torch.zeros(<span class="number">1</span>, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_loss</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> loss(net(features, w, b), labels).mean().item()</span><br><span class="line"></span><br><span class="line">    ls = [eval_loss()]</span><br><span class="line">    data_iter = torch.utils.data.DataLoader(</span><br><span class="line">        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">for</span> batch_i, (X, y) <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">            l = loss(net(X, w, b), y).mean()  <span class="comment"># 使用平均损失</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> w.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                w.grad.data.zero_()</span><br><span class="line">                b.grad.data.zero_()</span><br><span class="line">                </span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer_fn([w, b], states, hyperparams)  <span class="comment"># 迭代模型参数</span></span><br><span class="line">            <span class="keyword">if</span> (batch_i + <span class="number">1</span>) * batch_size % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                ls.append(eval_loss())  <span class="comment"># 每100个样本记录下当前训练误差</span></span><br><span class="line">    <span class="comment"># 打印结果和作图</span></span><br><span class="line">    print(<span class="string">'loss: %f, %f sec per epoch'</span> % (ls[<span class="number">-1</span>], time.time() - start))</span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    d2l.plt.plot(np.linspace(<span class="number">0</span>, num_epochs, len(ls)), ls)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'loss'</span>)</span><br></pre></td></tr></table></figure>
<p><strong>测试</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_sgd</span><span class="params">(lr, batch_size, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    train_ch7(sgd, <span class="literal">None</span>, &#123;<span class="string">'lr'</span>: lr&#125;, features, labels, batch_size, num_epochs)</span><br></pre></td></tr></table></figure>
<p><strong>Result</strong></p>
<blockquote>
<ul>
<li>train_sgd(1, 1500, 6)<br><img src="https://img-blog.csdnimg.cn/20200220220207217.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li>train_sgd(0.005, 1)<br><img src="https://img-blog.csdnimg.cn/20200220220222233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li>train_sgd(0.05, 10)<br><img src="https://img-blog.csdnimg.cn/20200220220233911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
</blockquote>
<p><strong>简化模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_pytorch_ch7</span><span class="params">(optimizer_fn, optimizer_hyperparams, features, labels,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_size=<span class="number">10</span>, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Linear(features.shape[<span class="number">-1</span>], <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    loss = nn.MSELoss()</span><br><span class="line">    optimizer = optimizer_fn(net.parameters(), **optimizer_hyperparams)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_loss</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> loss(net(features).view(<span class="number">-1</span>), labels).item() / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    ls = [eval_loss()]</span><br><span class="line">    data_iter = torch.utils.data.DataLoader(</span><br><span class="line">        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">for</span> batch_i, (X, y) <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">            <span class="comment"># 除以2是为了和train_ch7保持一致, 因为squared_loss中除了2</span></span><br><span class="line">            l = loss(net(X).view(<span class="number">-1</span>), y) / <span class="number">2</span> </span><br><span class="line">            </span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">if</span> (batch_i + <span class="number">1</span>) * batch_size % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                ls.append(eval_loss())</span><br><span class="line">    <span class="comment"># 打印结果和作图</span></span><br><span class="line">    print(<span class="string">'loss: %f, %f sec per epoch'</span> % (ls[<span class="number">-1</span>], time.time() - start))</span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    d2l.plt.plot(np.linspace(<span class="number">0</span>, num_epochs, len(ls)), ls)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'loss'</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>train_pytorch_ch7(optim.SGD, {“lr”: 0.05}, features, labels, 10)<br><img src="https://img-blog.csdnimg.cn/20200220220400842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>优化</tag>
        <tag>凸优化</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title>批量归一化 &amp;&amp; 残差网络</title>
    <url>/2020/02/28/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%20&amp;&amp;%20%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>基于此前对于CNN的介绍</p>
<ul>
<li><a href="https://blog.csdn.net/RokoBasilisk/article/details/104380762" target="_blank" rel="noopener">Fundamentals of Convolutional Neural Networks</a></li>
<li><a href="https://blog.csdn.net/RokoBasilisk/article/details/104381637" target="_blank" rel="noopener">LeNet &amp;&amp; ModernCNN</a></li>
</ul>
<p>就深层次 CNN 的结构进一步探讨归一化和残差网络。</p>
<h1 id="批量归一化（BatchNormalization）"><a href="#批量归一化（BatchNormalization）" class="headerlink" title="批量归一化（BatchNormalization）"></a>批量归一化（BatchNormalization）</h1><blockquote>
<p>让网络训练归一化变得更加容易，本质是一种对数据的标准化处理</p>
</blockquote>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><ul>
<li><strong>对输入的标准化（浅层模型）</strong></li>
</ul>
<p>处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。  标准化处理输入数据使各个特征的分布相近</p>
<ul>
<li><strong>批量归一化（深度模型）随着模型参数的迭代更新，靠近输出层的数据剧烈变化</strong></li>
</ul>
<p>利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li><strong>对全连接层做批量归一化</strong><blockquote>
<p>位置：全连接层中的仿射变换和激活函数之间。  </p>
</blockquote>
</li>
</ul>
<p><strong>全连接：</strong><br>$$<br>\boldsymbol{x} = \boldsymbol{W\boldsymbol{u} + \boldsymbol{b}} \\<br> output =\phi(\boldsymbol{x})<br> $$   </p>
<p>输入是u，经过仿射变化得到x，经过激活函数得到output，size=(batch_size，输出神经元的个数)</p>
<p><strong>批量归一化：</strong><br>$$<br>output=\phi(\text{BN}(\boldsymbol{x}))$$</p>
<p>$$<br>\boldsymbol{y}^{(i)} = \text{BN}(\boldsymbol{x}^{(i)})<br>$$</p>
<p>$$<br>\boldsymbol{\mu}<em>\mathcal{B} \leftarrow \frac{1}{m}\sum</em>{i = 1}^{m} \boldsymbol{x}^{(i)},<br>$$ </p>
<p>$$<br>\boldsymbol{\sigma}_\mathcal{B}^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m}(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B})^2,<br>$$</p>
<p>$$<br>\hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B}}{\sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}},<br>$$</p>
<p>$$<br>标准化处理<br>$$</p>
<p>这⾥ϵ &gt; 0是个很小的常数，保证分母大于0</p>
<p>$$<br>{\boldsymbol{y}}^{(i)} \leftarrow \boldsymbol{\gamma} \odot<br>\hat{\boldsymbol{x}}^{(i)} + \boldsymbol{\beta}.<br>$$</p>
<p>引入可学习参数：拉伸参数γ和偏移参数β。若$\boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}$和$\boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}$，批量归一化无效。</p>
<ul>
<li><strong>对卷积层做批量归⼀化</strong><blockquote>
<p>位置：卷积计算之后、应⽤激活函数之前</p>
</blockquote>
</li>
</ul>
<p>如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数。</p>
<p>计算：对单通道，$batchsize = m,卷积计算输出 = p \times q$</p>
<p>对该通道中 $m\times p\times q$ 个元素同时做批量归一化,使用相同的均值和方差。</p>
<ul>
<li><strong>预测时的批量归⼀化</strong></li>
</ul>
<p>训练：以 batch 为单位, 对每个 batch 计算均值和方差。  </p>
<p>预测：用移动平均估算整个训练数据集的样本均值和方差。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="batch-norm-function"><a href="#batch-norm-function" class="headerlink" title="batch_norm function"></a>batch_norm function</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 放到了BatchNorm类中，使用时直接调用forward函数，此函数将成为cell函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_norm</span><span class="params">(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum)</span>:</span></span><br><span class="line">    <span class="comment"># 判断当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">        <span class="comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> len(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> len(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>) <span class="comment">#是d维的值</span></span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持</span></span><br><span class="line">            <span class="comment"># X的形状以便后面可以做广播运算</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">3</span>, keepdim=<span class="literal">True</span>) <span class="comment"># c维的值，通道有几个mean就有几个</span></span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">3</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下用当前的均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        <span class="comment"># momentum 是一个超参数</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta  <span class="comment"># 拉伸和偏移</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean, moving_var</span><br></pre></td></tr></table></figure>
<h3 id="batch-norm-class"><a href="#batch-norm-class" class="headerlink" title="batch_norm class"></a>batch_norm class</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在Batch Norm函数的基础上定义此类，作用是维护学习参数和超参数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features, num_dims)</span>:</span> </span><br><span class="line">        super(BatchNorm, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features) <span class="comment">#全连接层输出神经元 </span></span><br><span class="line">            <span class="comment"># num_features代表输出神经元的个数</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)  <span class="comment">#通道数</span></span><br><span class="line">        <span class="comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment"># 不参与求梯度和迭代的变量，全在内存上初始化成0</span></span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上</span></span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        <span class="comment"># 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false</span></span><br><span class="line">        Y, self.moving_mean, self.moving_var = batch_norm(self.training, </span><br><span class="line">            X, self.gamma, self.beta, self.moving_mean,</span><br><span class="line">            self.moving_var, eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<h3 id="基于LeNet的应用"><a href="#基于LeNet的应用" class="headerlink" title="基于LeNet的应用"></a>基于LeNet的应用</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            <span class="comment"># 直接作为一个参数加到LeNet中就行</span></span><br><span class="line">            BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 体现了仿射函数之后激活函数之前的结构</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            d2l.FlattenLayer(),</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>
<h3 id="load-data"><a href="#load-data" class="headerlink" title="load data"></a>load data</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#batch_size = 256  </span></span><br><span class="line"><span class="comment">##cpu要调小batchsize</span></span><br><span class="line">batch_size=<span class="number">16</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None, root=<span class="string">'/home/kesci/input/FashionMNIST2065'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Download the fashion mnist dataset and then load into memory."""</span></span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.append(torchvision.transforms.Resize(size=resize))</span><br><span class="line">    trans.append(torchvision.transforms.ToTensor())</span><br><span class="line">    </span><br><span class="line">    transform = torchvision.transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<h3 id="train-and-test"><a href="#train-and-test" class="headerlink" title="train and test"></a>train and test</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<h2 id="简化模型"><a href="#简化模型" class="headerlink" title="简化模型"></a>简化模型</h2><blockquote>
<p>nn中有内置的BatchNorm2d（卷积层）和BatchNorm1d（全连接层）</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">在自己应用时不需要写class和function</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">6</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            d2l.FlattenLayer(),</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="残差网络（ResNet）"><a href="#残差网络（ResNet）" class="headerlink" title="残差网络（ResNet）"></a>残差网络（ResNet）</h1><center><b>深层网络能够拟合出的映射就一定能够包含浅层网络拟合出的映射</b></center><br/>

<center><b>但 CNN 模型在建立的时候并不是越深越好</b></center><br/>

<blockquote>
<p>深度学习的问题</p>
</blockquote>
<p>深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，准确率也变得更差。</p>
<h2 id="残差块（Residual-Block）"><a href="#残差块（Residual-Block）" class="headerlink" title="残差块（Residual Block）"></a>残差块（Residual Block）</h2><p><strong>恒等映射：</strong>  </p>
<ul>
<li>左边：$f(x)=x$                                               </li>
<li>右边：$f(x)-x=0$ （易于捕捉恒等映射的细微波动 ; 易于优化）</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw4bGhub3Q0LnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>神经网络普通层(left)残差网络(right)<br>$$</p>
<blockquote>
<p>在残差块中，输⼊可通过跨层的数据线路更快 地向前传播。</p>
</blockquote>
<h3 id="残差块实现"><a href="#残差块实现" class="headerlink" title="残差块实现"></a>残差块实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span><span class="params">(nn.Module)</span>:</span>  </span><br><span class="line">    <span class="comment">#可以设定输出通道数、是否使用额外的1x1卷积层来修改通道数以及卷积层的步幅。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, use_1x1conv=False, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(Residual, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=stride)</span><br><span class="line">        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=stride)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y + X)</span><br></pre></td></tr></table></figure>
<h3 id="是否需要-1-times1-卷积层"><a href="#是否需要-1-times1-卷积层" class="headerlink" title="是否需要 $1\times1$ 卷积层"></a>是否需要 $1\times1$ 卷积层</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不需要使用1*1卷积层 输入和输出相同</span></span><br><span class="line">blk = Residual(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">X = torch.rand((<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line">blk(X).shape <span class="comment"># torch.Size([4, 3, 6, 6])</span></span><br><span class="line"><span class="comment">#需要使用</span></span><br><span class="line">blk = Residual(<span class="number">3</span>, <span class="number">6</span>, use_1x1conv=<span class="literal">True</span>, stride=<span class="number">2</span>)</span><br><span class="line">blk(X).shape <span class="comment"># torch.Size([4, 6, 3, 3])</span></span><br></pre></td></tr></table></figure>
<h2 id="ResNet模型"><a href="#ResNet模型" class="headerlink" title="ResNet模型"></a>ResNet模型</h2><h3 id="简化实现"><a href="#简化实现" class="headerlink" title="简化实现"></a>简化实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_block</span><span class="params">(in_channels, out_channels, num_residuals, first_block=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> first_block:</span><br><span class="line">        <span class="keyword">assert</span> in_channels == out_channels <span class="comment"># 第一个模块的通道数同输入通道数一致</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            <span class="comment"># 把in_channels放缩到out_channels的个数</span></span><br><span class="line">            blk.append(Residual(in_channels, out_channels, use_1x1conv=<span class="literal">True</span>, stride=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 保证输入和输出都是out_channels</span></span><br><span class="line">            blk.append(Residual(out_channels, out_channels))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把四个残差block放到net里</span></span><br><span class="line">net.add_module(<span class="string">"resnet_block1"</span>, resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">net.add_module(<span class="string">"resnet_block2"</span>, resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">"resnet_block3"</span>, resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">"resnet_block4"</span>, resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<h3 id="全局平均池化"><a href="#全局平均池化" class="headerlink" title="全局平均池化"></a>全局平均池化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.add_module(<span class="string">"global_avg_pool"</span>, d2l.GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, 512, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="train-and-test-1"><a href="#train-and-test-1" class="headerlink" title="train and test"></a>train and test</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>ResNet 的引申设计</p>
</blockquote>
<h1 id="稠密连接网络（DenseNet）"><a href="#稠密连接网络（DenseNet）" class="headerlink" title="稠密连接网络（DenseNet）"></a>稠密连接网络（DenseNet）</h1><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw4bWk3OHl6LnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<blockquote>
<p>特征：concat 连接</p>
</blockquote>
<h2 id="主要构建模块："><a href="#主要构建模块：" class="headerlink" title="主要构建模块："></a>主要构建模块：</h2><ul>
<li>稠密块（dense block）： 定义了输入和输出是如何连结的。  </li>
<li>过渡层（transition layer）：用来控制通道数，使之不过大。<h2 id="稠密块"><a href="#稠密块" class="headerlink" title="稠密块"></a>稠密块</h2></li>
<li>输出通道数=输入通道数+卷积层个数*卷积输出通道数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(in_channels, out_channels)</span>:</span></span><br><span class="line">    blk = nn.Sequential(nn.BatchNorm2d(in_channels), </span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># num_convs:用了几个卷积层</span></span><br><span class="line">    <span class="comment"># in_channels代表全部输入，但是out_channels不代表全部输出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_convs, in_channels, out_channels)</span>:</span></span><br><span class="line">        super(DenseBlock, self).__init__()</span><br><span class="line">        net = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_convs):</span><br><span class="line">            <span class="comment"># 卷积层输入的通道数</span></span><br><span class="line">            in_c = in_channels + i * out_channels</span><br><span class="line">            net.append(conv_block(in_c, out_channels))</span><br><span class="line">        self.net = nn.ModuleList(net)</span><br><span class="line">        self.out_channels = in_channels + num_convs * out_channels <span class="comment"># 计算输出通道数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            <span class="comment"># concat连接</span></span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)  <span class="comment"># 在通道维上将输入和输出连结</span></span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blk = DenseBlock(<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">Y = blk(X)</span><br><span class="line">Y.shape <span class="comment"># torch.Size([4, 23, 8, 8]) 3+2*10</span></span><br></pre></td></tr></table></figure>
<h2 id="过渡层"><a href="#过渡层" class="headerlink" title="过渡层"></a>过渡层</h2><ul>
<li>$1\times1$卷积层：来减小通道数  </li>
<li>步幅为2的平均池化层：减半高和宽</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span><span class="params">(in_channels, out_channels)</span>:</span></span><br><span class="line">    blk = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(in_channels), </span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line">blk = transition_block(<span class="number">23</span>, <span class="number">10</span>)</span><br><span class="line">blk(Y).shape <span class="comment"># torch.Size([4, 10, 4, 4])</span></span><br></pre></td></tr></table></figure>
<h3 id="DenseNet模型"><a href="#DenseNet模型" class="headerlink" title="DenseNet模型"></a>DenseNet模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 宽高减半</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    <span class="comment"># 宽高减半</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span>  <span class="comment"># num_channels为当前的通道数</span></span><br><span class="line">num_convs_in_dense_blocks = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, num_convs <span class="keyword">in</span> enumerate(num_convs_in_dense_blocks):</span><br><span class="line">    DB = DenseBlock(num_convs, num_channels, growth_rate)</span><br><span class="line">    net.add_module(<span class="string">"DenseBlosk_%d"</span> % i, DB)</span><br><span class="line">    <span class="comment"># 上一个稠密块的输出通道数</span></span><br><span class="line">    num_channels = DB.out_channels</span><br><span class="line">    <span class="comment"># 在稠密块之间加入通道数减半的过渡层</span></span><br><span class="line">    <span class="keyword">if</span> i != len(num_convs_in_dense_blocks) - <span class="number">1</span>:</span><br><span class="line">        net.add_module(<span class="string">"transition_block_%d"</span> % i, transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">        num_channels = num_channels // <span class="number">2</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.add_module(<span class="string">"BN"</span>, nn.BatchNorm2d(num_channels))</span><br><span class="line">net.add_module(<span class="string">"relu"</span>, nn.ReLU())</span><br><span class="line">net.add_module(<span class="string">"global_avg_pool"</span>, d2l.GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, num_channels, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(), nn.Linear(num_channels, <span class="number">10</span>))) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">X = torch.rand((<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"><span class="keyword">for</span> name, layer <span class="keyword">in</span> net.named_children():</span><br><span class="line">    X = layer(X)</span><br><span class="line">    print(name, <span class="string">' output shape:\t'</span>, X.shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#batch_size = 256</span></span><br><span class="line">batch_size=<span class="number">16</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line">train_iter, test_iter =load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>批量归一化</tag>
        <tag>残差网络</tag>
      </tags>
  </entry>
  <entry>
    <title>LeNet &amp;&amp; ModernCNN</title>
    <url>/2020/02/28/LeNet%20&amp;&amp;%20ModernCNN/</url>
    <content><![CDATA[<h1 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h1><blockquote>
<p>学而习之：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104380762" target="_blank" rel="noopener">Fundamentals of Convolutional Neural Networks</a></p>
</blockquote>
<p>使用全连接层的局限性：</p>
<ul>
<li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li>
<li>对于大尺寸的输入图像，使用全连接层容易导致模型过大。</li>
</ul>
<p>使用卷积层的优势：</p>
<ul>
<li>卷积层保留输入形状。</li>
<li>卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</li>
</ul>
<p><strong>卷积神经网络就是含卷积层的网络。</strong></p>
<h2 id="LeNet-模型"><a href="#LeNet-模型" class="headerlink" title="LeNet 模型"></a>LeNet 模型</h2><blockquote>
<p>90%以上的参数都在全连接层块</p>
</blockquote>
<p>LeNet分为卷积层块和全连接层块两个部分<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5kd3Ntc2FvLnBuZw?x-oss-process=image/format,png" alt="Image Name"><br><strong>解释：</strong><br>&ensp;&ensp;&ensp;&ensp;卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5 \times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。</p>
<p>&ensp;&ensp;&ensp;&ensp;全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p>
<blockquote>
<p>卷积层块里的基本单位</p>
<blockquote>
<p>是卷积层后接平均池化层</p>
</blockquote>
<p>卷积层用来识别图像里的空间模式，如线条和物体局部<br/><br>之后的平均池化层则用来降低卷积层对位置的敏感性。</p>
</blockquote>
<p><strong>LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类</strong></p>
<h3 id="通过-Sequential-类实现-LeNet-模型"><a href="#通过-Sequential-类实现-LeNet-模型" class="headerlink" title="通过 Sequential 类实现 LeNet 模型"></a>通过 Sequential 类实现 LeNet 模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#import</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to FashionMNIST2065"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#net</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Flatten</span><span class="params">(torch.nn.Module)</span>:</span>  <span class="comment">#展平操作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Reshape</span><span class="params">(torch.nn.Module)</span>:</span> <span class="comment">#将图像大小重定型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x.view(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)      <span class="comment">#(B x C x H x W)</span></span><br><span class="line"></span><br><span class="line">net = torch.nn.Sequential(     <span class="comment">#Lelet                                                  </span></span><br><span class="line">    Reshape(),  </span><br><span class="line">    <span class="comment"># 公式：[(nh-kh+ph+sh)/sh]*[(nw-kw+pw+sw)/sw]</span></span><br><span class="line">    nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), <span class="comment">#b*1*28*28  =&gt;b*6*28*28</span></span><br><span class="line">    nn.Sigmoid(),     </span><br><span class="line">    <span class="comment"># 平均池化</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),                     <span class="comment">#b*6*28*28  =&gt;b*6*14*14</span></span><br><span class="line">    nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>),  <span class="comment">#b*6*14*14  =&gt;b*16*10*10</span></span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),                     <span class="comment">#b*16*10*10  =&gt; b*16*5*5</span></span><br><span class="line">    <span class="comment"># 展平</span></span><br><span class="line">    Flatten(),                                                 <span class="comment">#b*16*5*5   =&gt; b*400</span></span><br><span class="line">    <span class="comment"># 三个全连接层</span></span><br><span class="line">    nn.Linear(in_features=<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, out_features=<span class="number">120</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(</span><br><span class="line">    batch_size=batch_size, root=<span class="string">'path to FashionMNIST2065'</span>)</span><br><span class="line"><span class="comment"># 训练集批次数</span></span><br><span class="line">print(len(train_iter))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">额外的数据表示，以图像形式显示</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment">#数据展示</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># define drawing function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_fashion_mnist</span><span class="params">(images, labels)</span>:</span></span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    <span class="comment"># 这里的_表示我们忽略（不使用）的变量</span></span><br><span class="line">    _, figs = plt.subplots(<span class="number">1</span>, len(images), figsize=(<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> f, img, lbl <span class="keyword">in</span> zip(figs, images, labels):</span><br><span class="line">        f.imshow(img.view((<span class="number">28</span>, <span class="number">28</span>)).numpy())</span><br><span class="line">        f.set_title(lbl)</span><br><span class="line">        f.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        f.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> Xdata,ylabel <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">X, y = [], []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(Xdata[i].shape,ylabel[i].numpy())</span><br><span class="line">    X.append(Xdata[i]) <span class="comment"># 将第i个feature加到X中</span></span><br><span class="line">    y.append(ylabel[i].numpy()) <span class="comment"># 将第i个label加到y中</span></span><br><span class="line">show_fashion_mnist(X, y)</span><br></pre></td></tr></table></figure>
<hr>
<p>因为卷积神经网络计算比多层感知机要复杂，建议使用GPU来加速计算。我们查看看是否可以用GPU，如果成功则使用 cuda:0，否则仍然使用 cpu。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># This function has been saved in the d2l package for future use</span></span><br><span class="line"><span class="comment">#use GPU</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""If GPU is available, return torch.device as cuda:0; else return torch.device as cpu."""</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        device = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        device = torch.device(<span class="string">'cpu'</span>)</span><br><span class="line">    <span class="keyword">return</span> device</span><br><span class="line"></span><br><span class="line">device = try_gpu()</span><br><span class="line">device</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h4 id="计算准确率"><a href="#计算准确率" class="headerlink" title="计算准确率"></a>计算准确率</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">(1). net.train()</span></span><br><span class="line"><span class="string">  启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为True</span></span><br><span class="line"><span class="string">(2). net.eval()</span></span><br><span class="line"><span class="string">不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">:param:data_iter:测试集</span></span><br><span class="line"><span class="string">:param:acc_sum:模型预测正确的总数</span></span><br><span class="line"><span class="string">:param:n:预测总数</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net,device=torch.device<span class="params">(<span class="string">'cpu'</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Evaluate accuracy of a model on the given data set."""</span></span><br><span class="line">    acc_sum,n = torch.tensor([<span class="number">0</span>],dtype=torch.float32,device=device),<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="comment"># If device is the GPU, copy the data to the GPU.</span></span><br><span class="line">        <span class="comment"># 把tensorc传到device中</span></span><br><span class="line">        X,y = X.to(device),y.to(device)</span><br><span class="line">        <span class="comment"># 网络正在进行预测</span></span><br><span class="line">        net.eval()</span><br><span class="line">        该区域涉及到的数据不需要计算梯度，也不进行反向传播</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            y = y.long()</span><br><span class="line">            <span class="comment"># 将一个批次的训练数据X通过网络模型net输出</span></span><br><span class="line">            <span class="comment"># 经过argmax得到预测值</span></span><br><span class="line">            <span class="comment"># dim维度选择为1</span></span><br><span class="line">            acc_sum += torch.sum((torch.argmax(net(X), dim=<span class="number">1</span>) == y))  <span class="comment">#[[0.2 ,0.4 ,0.5 ,0.6 ,0.8] ,[ 0.1,0.2 ,0.4 ,0.3 ,0.1]] =&gt; [ 4 , 2 ]</span></span><br><span class="line">            <span class="comment"># 预测的总数相加</span></span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 预测准确率</span></span><br><span class="line">    <span class="keyword">return</span> acc_sum.item()/n</span><br></pre></td></tr></table></figure>
<h4 id="训练函数"><a href="#训练函数" class="headerlink" title="训练函数"></a>训练函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch5</span><span class="params">(net, train_iter, test_iter,criterion, num_epochs, batch_size, device,lr=None)</span>:</span></span><br><span class="line">    <span class="string">"""Train and evaluate a model with CPU or GPU."""</span></span><br><span class="line">    print(<span class="string">'training on'</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum = torch.tensor([<span class="number">0.0</span>],dtype=torch.float32,device=device)</span><br><span class="line">        train_acc_sum = torch.tensor([<span class="number">0.0</span>],dtype=torch.float32,device=device)</span><br><span class="line">        n, start = <span class="number">0</span>, time.time()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            net.train()</span><br><span class="line">            <span class="comment"># 梯度清零 不同批次的梯度不相关</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X,y = X.to(device),y.to(device) </span><br><span class="line">            <span class="comment"># 预测值</span></span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            loss = criterion(y_hat, y)</span><br><span class="line">            <span class="comment"># 梯度回传</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                y = y.long()</span><br><span class="line">                train_l_sum += loss.float()</span><br><span class="line">                <span class="comment"># 训练集中预测正确的总数</span></span><br><span class="line">                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=<span class="number">1</span>) == y))).float()</span><br><span class="line">                n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net,device)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '</span></span><br><span class="line">              <span class="string">'time %.1f sec'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum/n, train_acc_sum/n, test_acc,</span><br><span class="line">                 time.time() - start))</span><br></pre></td></tr></table></figure>
<h4 id="训练进程"><a href="#训练进程" class="headerlink" title="训练进程"></a>训练进程</h4><p>模型参数初始化到 device 中，并使用 Xavier 随机初始化。损失函数和训练算法则依然使用交叉熵损失函数和小批量随机梯度下降。</p>
<blockquote>
<p>Xavier 随机初始化 —参考<a href="https://blog.csdn.net/RokoBasilisk/article/details/104349123" target="_blank" rel="noopener">学而后思,方能发展;思而立行,终将卓越</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练 学习率0.9</span></span><br><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear <span class="keyword">or</span> type(m) == nn.Conv2d:</span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line">net = net.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()   <span class="comment">#交叉熵描述了两个概率分布之间的距离，交叉熵越小说明两者之间越接近</span></span><br><span class="line">train_ch5(net, train_iter, test_iter, criterion,num_epochs, batch_size,device, lr)</span><br></pre></td></tr></table></figure>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line"><span class="keyword">for</span> testdata,testlabe <span class="keyword">in</span> test_iter:</span><br><span class="line">    testdata,testlabe = testdata.to(device),testlabe.to(device)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">print(testdata.shape,testlabe.shape)</span><br><span class="line">net.eval()</span><br><span class="line">y_pre = net(testdata)</span><br><span class="line"><span class="comment"># 预测值</span></span><br><span class="line">print(torch.argmax(y_pre,dim=<span class="number">1</span>)[:<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 真实标签</span></span><br><span class="line">print(testlabe[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<h2 id="深度卷积神经网络（AlexNet）"><a href="#深度卷积神经网络（AlexNet）" class="headerlink" title="深度卷积神经网络（AlexNet）"></a>深度卷积神经网络（AlexNet）</h2><blockquote>
<p>2014年ImgNet竞赛中</p>
<blockquote>
<p>证明了学习到的特征可以超过手工设计的特征, 打破计算机视觉研究的前状</p>
</blockquote>
</blockquote>
<center><b><font size=5>知缺陷方能进步</font></b></center><br/>

<blockquote>
<p>LeNet:  在大的真实数据集上的表现并不尽如⼈意。     </p>
<blockquote>
<p>1.神经网络计算复杂。  <br/><br>2.还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。  </p>
</blockquote>
</blockquote>
<p>在此之后，针对特征的选择分为两派：</p>
<ul>
<li>机器学习的特征提取:手工定义的特征提取函数  </li>
<li>神经网络的特征提取：通过学习得到数据的多级表征，并逐级表⽰越来越抽象的概念或模式。  </li>
</ul>
<p>神经网络发展的限制:数据、硬件</p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><blockquote>
<p>设计理念核Lenet相似</p>
</blockquote>
<p><strong>特征：</strong></p>
<ol>
<li>8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。</li>
<li>将sigmoid激活函数改成了更加简单的ReLU激活函数。</li>
<li>用Dropout来控制全连接层的模型复杂度。</li>
<li>引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解泛化能力不好导致的过拟合。</li>
</ol>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWt2NGdweDg4LnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>数据集.MINIST(Left) IMAGENET(Right)<br>$$</p>
<blockquote>
<p>利用padding 的作用：使得输入和输出的形状相同<br/><br>可以参考 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104380762" target="_blank" rel="noopener">Fundamentals of Convolutional Neural Networks</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to FashionMNIST2065"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="实现-AlexNet-模型"><a href="#实现-AlexNet-模型" class="headerlink" title="实现 AlexNet 模型"></a>实现 AlexNet 模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            <span class="comment"># 默认不做padding</span></span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment">#由于使用CPU镜像，精简网络，若为GPU镜像可添加该层</span></span><br><span class="line">            <span class="comment">#nn.Linear(4096, 4096),</span></span><br><span class="line">            <span class="comment">#nn.ReLU(),</span></span><br><span class="line">            <span class="comment">#nn.Dropout(0.5),</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, img)</span>:</span></span><br><span class="line"></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], <span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h4 id="载入数据"><a href="#载入数据" class="headerlink" title="载入数据"></a>载入数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None, root=<span class="string">'/home/kesci/input/FashionMNIST2065'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Download the fashion mnist dataset and then load into memory."""</span></span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.append(torchvision.transforms.Resize(size=resize))</span><br><span class="line">    trans.append(torchvision.transforms.ToTensor())</span><br><span class="line">    </span><br><span class="line">    transform = torchvision.transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"><span class="comment">#batchsize=128</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size,<span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">    print(<span class="string">'X ='</span>, X.shape,</span><br><span class="line">        <span class="string">'\nY ='</span>, Y.type(torch.int32))</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">3</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<h2 id="使用重复元素的网络（VGG）"><a href="#使用重复元素的网络（VGG）" class="headerlink" title="使用重复元素的网络（VGG）"></a>使用重复元素的网络（VGG）</h2><center><b><font size=5>知缺陷方能进步</font></b></center><br/>

<blockquote>
<p>AlxNet</p>
<blockquote>
<p>并没有提供简单的规则来制造新的网络</p>
</blockquote>
<p>结构比较死板</p>
</blockquote>
<p><strong>所以</strong> VGG：通过重复使⽤简单的基础块来构建深度模型。  </p>
<p>Block:数个相同的填充为1、窗口形状为$3\times 3$的卷积层,接上一个步幅为2、窗口形状为$2\times 2$的最大池化层。  </p>
<p>卷积层保持输入的高和宽不变，而池化层则对其减半</p>
<p><img src="https://img-blog.csdnimg.cn/20200218210146126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="VGG11的实现"><a href="#VGG11的实现" class="headerlink" title="VGG11的实现"></a>VGG11的实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以修改的参数 e每个vgg_block结构相同但是参数可能不相同</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span><span class="params">(num_convs, in_channels, out_channels)</span>:</span> <span class="comment">#卷积层个数，输入通道数，输出通道数</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_convs):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">    blk.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)) <span class="comment"># 这里会使宽高减半</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">64</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">128</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">256</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line"><span class="comment"># 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7</span></span><br><span class="line">fc_features = <span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span> <span class="comment"># c * w * h</span></span><br><span class="line">fc_hidden_units = <span class="number">4096</span> <span class="comment"># 任意</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vgg模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg</span><span class="params">(conv_arch, fc_features, fc_hidden_units=<span class="number">4096</span>)</span>:</span></span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> i, (num_convs, in_channels, out_channels) <span class="keyword">in</span> enumerate(conv_arch):</span><br><span class="line">        <span class="comment"># 每经过一个vgg_block都会使宽高减半</span></span><br><span class="line">        net.add_module(<span class="string">"vgg_block_"</span> + str(i+<span class="number">1</span>), vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">    <span class="comment"># 全连接层部分</span></span><br><span class="line">    net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(),</span><br><span class="line">                                 nn.Linear(fc_features, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, <span class="number">10</span>)</span><br><span class="line">                                ))</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = vgg(conv_arch, fc_features, fc_hidden_units)</span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)</span></span><br><span class="line"><span class="keyword">for</span> name, blk <span class="keyword">in</span> net.named_children(): </span><br><span class="line">    X = blk(X)</span><br><span class="line">    print(name, <span class="string">'output shape: '</span>, X.shape)</span><br><span class="line">ratio = <span class="number">8</span></span><br><span class="line"><span class="comment"># 减小vgg结构，针对minist数据集较小，数据少参数多容易造成过拟合</span></span><br><span class="line">small_conv_arch = [(<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>//ratio), (<span class="number">1</span>, <span class="number">64</span>//ratio, <span class="number">128</span>//ratio), (<span class="number">2</span>, <span class="number">128</span>//ratio, <span class="number">256</span>//ratio), </span><br><span class="line">                   (<span class="number">2</span>, <span class="number">256</span>//ratio, <span class="number">512</span>//ratio), (<span class="number">2</span>, <span class="number">512</span>//ratio, <span class="number">512</span>//ratio)]</span><br><span class="line">net = vgg(small_conv_arch, fc_features // ratio, fc_hidden_units // ratio)</span><br><span class="line">print(net)</span><br><span class="line"></span><br><span class="line">batchsize=<span class="number">16</span></span><br><span class="line"><span class="comment">#batch_size = 64</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line"><span class="comment"># train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<h2 id="网络中的网络（NiN）"><a href="#网络中的网络（NiN）" class="headerlink" title="网络中的网络（NiN）"></a>网络中的网络（NiN）</h2><center><b><font size=5>知缺陷方能进步</font></b></center><br/>

<blockquote>
<p>LeNet、AlexNet和VGG</p>
<blockquote>
<p>先以由卷积层构成的模块充分抽取 空间特征<br/><br>再以由全连接层构成的模块来输出分类结果</p>
</blockquote>
</blockquote>
<blockquote>
<p>NiN</p>
<blockquote>
<p>串联多个由卷积层和 “全连接” 层构成的小⽹络来构建⼀个深层⽹络。  </p>
</blockquote>
<p>NiN去掉了全连接层，而是用平均池化层</p>
</blockquote>
<p>⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。  这样的设计显著的<strong>减少了参数尺寸防止过拟合</strong>，但是<strong>增加了训练时间</strong></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw2dTFwNXZ5LnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<hr>
<p>$1\times1$卷积核作用:</p>
<ol>
<li>放缩通道数：通过控制卷积核的数量达到通道数的放缩。  </li>
<li>增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。  </li>
<li>计算参数少   </li>
</ol>
<hr>
<h3 id="NiN-的实现"><a href="#NiN-的实现" class="headerlink" title="NiN 的实现"></a>NiN 的实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建组成模块nin_block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nin_block</span><span class="params">(in_channels, out_channels, kernel_size, stride, padding)</span>:</span></span><br><span class="line">    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU())</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalAvgPool2d</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(GlobalAvgPool2d, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.avg_pool2d(x, kernel_size=x.size()[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), </span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    GlobalAvgPool2d(), </span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小, 10)</span></span><br><span class="line">    d2l.FlattenLayer())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line"><span class="comment">#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)</span></span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.002</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>NiN</p>
<blockquote>
<ul>
<li>NiN重复使⽤由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层⽹络 ;  </li>
<li>NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数 的NiN块和全局平均池化层 ;</li>
<li>NiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计 ;</li>
</ul>
</blockquote>
</blockquote>
<h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><blockquote>
<p>牺牲了串联网络的思想</p>
</blockquote>
<ol>
<li>由 Inception 基础块组成。  </li>
<li>Inception 块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。   </li>
<li>可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 </li>
<li>使用 padding 来保证输入输出形状相同<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw2dW9ydHcucG5n?x-oss-process=image/format,png" alt="Image Name"><h3 id="Inception-基础块实现"><a href="#Inception-基础块实现" class="headerlink" title="Inception 基础块实现"></a>Inception 基础块实现</h3></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># c1 - c4为每条线路里的层的输出通道数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, c1, c2, c3, c4)</span>:</span></span><br><span class="line">        super(Inception, self).__init__()</span><br><span class="line">        <span class="comment"># 线路1，单1 x 1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1 x 1卷积层后接3 x 3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_c, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1 x 1卷积层后接5 x 5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_c, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3 x 3最大池化层后接1 x 1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)  <span class="comment"># 在通道维上连结输出</span></span><br></pre></td></tr></table></figure>

<h3 id="完整模型结构"><a href="#完整模型结构" class="headerlink" title="完整模型结构"></a>完整模型结构</h3><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw2eDBmeXluLnBuZw?x-oss-process=image/format,png" /></center> 

<p>$$<br>input -1\times96\times96<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取特征来减小大小</span></span><br><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   d2l.GlobalAvgPool2d())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, </span><br><span class="line">                    d2l.FlattenLayer(), nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, d2l.FlattenLayer(), nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net.children(): </span><br><span class="line">    X = blk(X)</span><br><span class="line">    print(<span class="string">'output shape: '</span>, X.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#batchsize=128</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line"><span class="comment">#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)</span></span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>LeNet</tag>
        <tag>AlexNet</tag>
        <tag>VGG</tag>
        <tag>NiN</tag>
        <tag>GoogLeNet</tag>
      </tags>
  </entry>
  <entry>
    <title>Fundamentals of Convolutional Neural Networks</title>
    <url>/2020/02/28/Fundamentals%20of%20Convolutional%20Neural%20Networks/</url>
    <content><![CDATA[<h2 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h2><blockquote>
<p>常用于处理图像数据。</p>
</blockquote>
<h3 id="二维互相关运算"><a href="#二维互相关运算" class="headerlink" title="二维互相关运算"></a>二维互相关运算</h3><p>二维互相关（cross-correlation）运算的输入是一个二维输入数组和一个二维核（kernel）数组，输出也是一个二维数组，其中核数组通常称为卷积核或过滤器（filter）。卷积核的尺寸通常小于输入数组，卷积核在输入数组上滑动，在每个位置上，卷积核与该位置处的输入子数组按元素相乘并求和，得到输出数组中相应位置的元素。图1展示了一个互相关运算的例子，阴影部分分别是输入的第一个计算区域、核数组以及对应的输出。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mZGJoY3c1LnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图1-二维互相关运算<br>$$</p>
<blockquote>
<p>用 corr2d 函数实现二维互相关运算</p>
<blockquote>
<p>它接受输入数组 X 与核数组 K，并输出数组 Y。</p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 做二维互相关运算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    H, W = X.shape</span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros(H - h + <span class="number">1</span>, W - w + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<p><strong>验证：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造上图中的输入数组 X 、核数组 K 来验证二维互相关运算的输出</span></span><br><span class="line">X = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">Y = corr2d(X, K)</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure>
<p><strong>结果：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">19.</span>, <span class="number">25.</span>],</span><br><span class="line">        [<span class="number">37.</span>, <span class="number">43.</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="二维卷积层-1"><a href="#二维卷积层-1" class="headerlink" title="二维卷积层"></a>二维卷积层</h3><p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的模型参数包括卷积核和标量偏置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kernel_size)</span>:</span></span><br><span class="line">        super(Conv2D, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure>
<p><strong>For Example:</strong></p>
<p>构造一张$6 \times 8$的图像，中间4列为黑（0），其余为白（1），希望检测到颜色边缘。我们的标签是一个$6 \times 7$的二维数组，第2列是1（从1到0的边缘），第6列是-1（从0到1的边缘）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.ones(<span class="number">6</span>, <span class="number">8</span>)</span><br><span class="line">Y = torch.zeros(<span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line">X[:, <span class="number">2</span>: <span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">Y[:, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">Y[:, <span class="number">5</span>] = <span class="number">-1</span></span><br><span class="line">print(X)</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure>
<p><strong>结果：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<p>我们希望学习一个$1 \times 2$卷积层，通过卷积层来检测颜色边缘。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1*2的二维卷积层</span></span><br><span class="line">conv2d = Conv2D(kernel_size=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">step = <span class="number">30</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(step):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = ((Y_hat - Y) ** <span class="number">2</span>).sum()</span><br><span class="line">    <span class="comment"># 后向计算得到梯度</span></span><br><span class="line">    l.backward()</span><br><span class="line">    <span class="comment"># 梯度下降</span></span><br><span class="line">    <span class="comment"># 参数值减去学习率并乘以梯度</span></span><br><span class="line">    conv2d.weight.data -= lr * conv2d.weight.grad</span><br><span class="line">    conv2d.bias.data -= lr * conv2d.bias.grad</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度清零</span></span><br><span class="line">    conv2d.weight.grad.zero_()</span><br><span class="line">    conv2d.bias.grad.zero_()</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step %d, loss %.3f'</span> % (i + <span class="number">1</span>, l.item()))</span><br><span class="line"><span class="comment"># 卷积核</span></span><br><span class="line">print(conv2d.weight.data)</span><br><span class="line"><span class="comment"># 偏置</span></span><br><span class="line">print(conv2d.bias.data)</span><br></pre></td></tr></table></figure>
<h3 id="互相关运算与卷积运算"><a href="#互相关运算与卷积运算" class="headerlink" title="互相关运算与卷积运算"></a>互相关运算与卷积运算</h3><p>卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。我们将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，给定核数组，对于核数组中每一个元素，构建一个从核数组中元素到输入数组元素之间的对应关系，再进行相加求和，本质是一样的。所以使用互相关运算与使用卷积运算并无本质区别。</p>
<h3 id="特征图与感受野"><a href="#特征图与感受野" class="headerlink" title="特征图与感受野"></a>特征图与感受野</h3><p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素 $x$ 的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做 $x$ 的感受野（receptive field）。</p>
<blockquote>
<p>解释:</p>
<blockquote>
<p>输出是一个特征图，对于19来说，其感受野就是0 1 3 4</p>
</blockquote>
<blockquote>
<p>如果引入一个$2\times 2$的新的卷积核，核输出做互相关运算，得到一个$1\times1$的输出，感受野就是前面的所有元素</p>
</blockquote>
</blockquote>
<p>以图1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为$2 \times 2$的输出记为$Y$，将$Y$与另一个形状为$2 \times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。</p>
<h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>我们介绍卷积层的两个超参数，即填充和步幅，它们可以对给定形状的输入和卷积核改变输出形状。</p>
<h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），图2里我们在原输入高和宽的两侧分别添加了值为0的元素。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mbDZlank0LnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图2-在输入的高和宽两侧分别填充了0元素的二维互相关计算<br>$$</p>
<p>如果原输入的高和宽是$n_h$和$n_w$，卷积核的高和宽是$k_h$和$k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，则输出形状为：</p>
<p>$$<br>(n_h+p_h-k_h+1)\times(n_w+p_w-k_w+1)<br>$$</p>
<blockquote>
<p>在卷积神经网络中使用奇数高宽的核<br/><br>比如$3 \times 3$，$5 \times 5$的卷积核，对于高度（或宽度）为大小为 $2 k + 1$ 的核，令步幅为1，在高（或宽）两侧选择大小为$k$的填充，便可保持输入与输出尺寸相同。</p>
</blockquote>
<h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><p>在互相关运算中，卷积核在输入数组上滑动，每次滑动的行数与列数即是步幅（stride）。此前我们使用的步幅都是1，图3展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mbG9obnFnLnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图3-高和宽上步幅分别为3和2的二维互相关运算<br>$$</p>
<p>一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：</p>
<p>$$<br>\lfloor(n_h+p_h-k_h+s_h)/s_h\rfloor \times \lfloor(n_w+p_w-k_w+s_w)/s_w\rfloor<br>$$</p>
<center><img src="https://img-blog.csdnimg.cn/20200218162515171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>$$<br>解释图<br>$$</p>
<p>找后续卷积核可以覆盖到的区域只需要关注最后一个元素，看最后一个元素在输入上找到几个位置，要做的就是往下移动 $s_{h}$ ,卷积核的最后一个元素下一个位置就找到了</p>
<p>如果$p_h=k_h-1$，$p_w=k_w-1$，那么输出形状将简化为$\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h / s_h) \times (n_w/s_w)$。</p>
<p>当$p_h = p_w = p$时，我们称填充为$p$；当$s_h = s_w = s$时，我们称步幅为$s$。</p>
<h2 id="多输入通道和多输出通道"><a href="#多输入通道和多输出通道" class="headerlink" title="多输入通道和多输出通道"></a>多输入通道和多输出通道</h2><p>之前的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3 \times h \times w$的多维数组，将大小为3的这一维称为通道（channel）维。</p>
<h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p>卷积层的输入可以包含多个通道，图4展示了一个含2个输入通道的二维互相关计算的例子。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mbWRud2JxLnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图4-含2个输入通道的互相关计算<br>$$</p>
<p>假设输入数据的通道数为$c_i$，卷积核形状为$k_h\times k_w$，我们为每个输入通道各分配一个形状为$k_h\times k_w$的核数组，将$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组作为输出。我们把$c_i$个核数组在通道维上连结，即得到一个形状为$c_i\times k_h\times k_w$的卷积核。</p>
<h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3><p>卷积层的输出也可以包含多个通道，设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\times k_h\times k_w$的核数组，将它们在输出通道维上连结，卷积核的形状即$c_o\times c_i\times k_h\times k_w$。</p>
<p>对于输出通道的卷积核，我们提供这样一种理解，一个$c_i \times k_h \times k_w$的核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个这样的$c_i \times k_h \times k_w$的核数组，不同的核数组提取的是不同的特征。</p>
<h3 id="1x1卷积层"><a href="#1x1卷积层" class="headerlink" title="1x1卷积层"></a>1x1卷积层</h3><p>最后讨论形状为$1 \times 1$的卷积核，我们通常称这样的卷积运算为$1 \times 1$卷积，称包含这种卷积核的卷积层为$1 \times 1$卷积层。图5展示了使用输入通道数为3、输出通道数为2的$1\times 1$卷积核的互相关计算。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mbXE5ODByLnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图5-1\times1卷积核的互相关计算。输入和输出具有相同的高和宽<br>$$</p>
<p>$1 \times 1$卷积核可在不改变高宽的情况下，调整通道数。$1 \times 1$卷积核不识别高和宽维度上相邻元素构成的模式，其主要计算发生在通道维上。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么$1\times 1$卷积层的作用与全连接层等价。</p>
<h2 id="卷积层与全连接层的对比"><a href="#卷积层与全连接层的对比" class="headerlink" title="卷积层与全连接层的对比"></a>卷积层与全连接层的对比</h2><ul>
<li><p>全连接层做图像分类</p>
</li>
<li><p>二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：</p>
</li>
<li><p>一是全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。</p>
</li>
<li><p>二是卷积层的参数量更少。不考虑偏置的情况下，一个形状为$(c_i, c_o, h, w)$的卷积核的参数量是$c_i \times c_o \times h \times w$，与输入图像的宽高无关。假如一个卷积层的输入和输出形状分别是$(c_1, h_1, w_1)$和$(c_2, h_2, w_2)$，如果要用全连接层进行连接，参数数量就是$c_1 \times c_2 \times h_1 \times w_1 \times h_2 \times w_2$。使用卷积层可以以较少的参数数量来处理更大的图像。</p>
</li>
</ul>
<h2 id="卷积层的简洁实现"><a href="#卷积层的简洁实现" class="headerlink" title="卷积层的简洁实现"></a>卷积层的简洁实现</h2><p>我们使用Pytorch中的 nn.Conv2d 类来实现二维卷积层</p>
<p>forward 函数的参数为一个四维张量，形状为$(N, C_{in}, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C_{out}, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。</p>
<p><strong>代码讲解：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">print(X.shape)</span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">2</span>, out_channels=<span class="number">3</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), stride=<span class="number">1</span>, padding=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">Y = conv2d(X)</span><br><span class="line">print(<span class="string">'Y.shape: '</span>, Y.shape)</span><br><span class="line">print(<span class="string">'weight.shape: '</span>, conv2d.weight.shape)</span><br><span class="line">print(<span class="string">'bias.shape: '</span>, conv2d.bias.shape)</span><br></pre></td></tr></table></figure>
<h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><h3 id="二维池化层"><a href="#二维池化层" class="headerlink" title="二维池化层"></a>二维池化层</h3><p>池化层有参与模型的正向计算，同样也会参与反向传播</p>
<p>池化层直接对窗口内的元素求最大值或平均值，并没有模型参数参与计算，所以没有模型参数</p>
<p>池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做最大池化或平均池化。图6展示了池化窗口形状为$2\times 2$的最大池化。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mb2Izb2RvLnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>池化窗口形状为 2 \times 2 的最大池化<br>$$</p>
<p>二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$p \times q$的池化层称为$p \times q$池化层，其中的池化运算叫作$p \times q$池化。</p>
<p>池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。</p>
<p>在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的输出通道数与输入通道数相等。</p>
<h3 id="池化层的简洁实现"><a href="#池化层的简洁实现" class="headerlink" title="池化层的简洁实现"></a>池化层的简洁实现</h3><p>我们使用Pytorch中的 nn.MaxPool2d 实现最大池化层</p>
<p>forward 函数的参数为一个四维张量，形状为$(N, C, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。</p>
<p>代码讲解</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">32</span>, dtype=torch.float32).view(<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">pool2d = nn.MaxPool2d(kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">Y = pool2d(X)</span><br><span class="line">print(X)</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>平均池化层使用的是 nn.AvgPool2d，使用方法与 nn.MaxPool2d 相同。</p>
</blockquote>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>卷积-池化</tag>
        <tag>特征与感受野</tag>
      </tags>
  </entry>
  <entry>
    <title>注意力机制和Seq2seq模型</title>
    <url>/2020/02/28/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8CSeq2seq%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<center><b><font size=6>Attention Mechanism</font></b></center><br/>

<blockquote>
<p>注意力机制借鉴了人类的注意力思维方式，以获得需要重点关注的目标区域</p>
</blockquote>
<p>&ensp;&ensp;&ensp;&ensp;在 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">编码器—解码器（seq2seq)</a> 中，解码器在各个时间步依赖相同的背景变量（context vector）来获取输⼊序列信息。解码器输入的语境向量(context vector)不同，每个位置都会计算各自的 attention 输出。 当编码器为循环神经⽹络时，背景变量来⾃它最终时间步的隐藏状态。将源序列输入信息以循环单位状态编码，然后将其传递给解码器以生成目标序列。</p>
<p>&ensp;&ensp;&ensp;&ensp;然而这种结构存在着问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。与此同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。例如，当把 “Hello world” 翻译成 “Bonjour le monde” 时，“Hello” 映射成 “Bonjour”，“world” 映射成  “monde”。</p>
<p>&ensp;&ensp;&ensp;&ensp;在 seq2seq 模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWttNGR3Z2Y5LlBORw?x-oss-process=image/format,png" alt="Image Name"></p>
<h2 id="注意力机制框架"><a href="#注意力机制框架" class="headerlink" title="注意力机制框架"></a>注意力机制框架</h2><p>Attention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。 $k_i∈R^{d_k}, v_i∈R^{d_v}$. Query  $q∈R^{d_q}$ ,  attention layer 得到输出与value的维度一致 $o∈R^{d_v}$.  对于一个query来说，attention layer 会与每一个 key 计算注意力分数并进行权重的归一化，输出的向量 $o$ 则是 value 的加权求和，而每个 key 计算的权重与 value 一一对应。</p>
<p>为了计算输出，我们首先假设有一个函数$\alpha$ 用于计算query和key的相似性，然后可以计算所有的 attention scores $a_1, \ldots, a_n$  by</p>
<p>$$<br>a_i = \alpha(\mathbf q, \mathbf k_i).<br>$$</p>
<p>我们使用 softmax 函数 获得注意力权重：</p>
<p>$$<br>b_1, \ldots, b_n = \textrm{softmax}(a_1, \ldots, a_n).<br>$$</p>
<p>最终的输出就是 value 的加权求和：</p>
<p>$$<br>\mathbf o = \sum_{i=1}^n b_i \mathbf v_i.<br>$$</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWttNG9veXUyLlBORw?x-oss-process=image/format,png" /></center>

<blockquote>
<p>不同的 attetion layer 的区别在于 score 函数的选择</p>
</blockquote>
<p>接下来将利用[机器翻译及其相关技术介绍]一文中的(<a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104367653</a>)</p>
<h2 id="介绍两个常用的注意层"><a href="#介绍两个常用的注意层" class="headerlink" title="介绍两个常用的注意层"></a>介绍两个常用的注意层</h2><blockquote>
<ul>
<li>Dot-product Attention <br/></li>
<li>Multilayer Perceptron Attention</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># import dataset</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.listdir(<span class="string">'path to storaged file of dataset'</span>)</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>工具1:</strong> Masked Softmax</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 排除padding位置的影响</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">-1e6</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\n',X_len[:, None] )</span></span><br><span class="line">    <span class="comment"># shape as same as X</span></span><br><span class="line">    mask = torch.arange((maxlen),dtype=torch.float)[<span class="literal">None</span>, :] &gt;= X_len[:, <span class="literal">None</span>]   </span><br><span class="line">    <span class="comment">#print(mask)</span></span><br><span class="line">    X[mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span><span class="params">(X, valid_length)</span>:</span></span><br><span class="line">    <span class="comment"># X: 3-D tensor, valid_length: 1-D or 2-D tensor</span></span><br><span class="line">    softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> softmax(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_length.dim() == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = valid_length.reshape((<span class="number">-1</span>,))</span><br><span class="line">        <span class="comment"># fill masked elements with a large negative, whose exp is 0</span></span><br><span class="line">        X = SequenceMask(X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>])), valid_length)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> softmax(X).reshape(shape)</span><br></pre></td></tr></table></figure>

<p><strong>工具2：</strong> 超出2维矩阵的乘法</p>
<p>$X$ 和 $Y$ 是维度分别为$(b,n,m)$ 和$(b, m, k)$的张量，进行 $b$ 次二维矩阵乘法后得到 $Z$, 维度为 $(b, n, k)$。</p>
<p>$$<br> Z[i,:,:] = dot(X[i,:,:], Y[i,:,:])\qquad for\ i= 1,…,n\ .<br>$$</p>
<hr>
<h3 id="Dot-Product-Attention"><a href="#Dot-Product-Attention" class="headerlink" title="Dot Product Attention"></a>Dot Product Attention</h3><p>The dot product 假设query和keys有相同的维度, 即 $\forall i, q,k_i ∈ R_d$. 通过计算 query 和 key 转置的乘积来计算 attention score ,通常还会除去 $\sqrt{d}$ 减少计算出来的 score 对维度 𝑑 的依赖性，如下</p>
<p>$$<br>α (q,k)=⟨q,k⟩/ \sqrt{d}<br>$$</p>
<p>假设 $Q∈R^{m×d}$ 有 $m$ 个query，$K∈R^{n×d}$ 有 $n$ 个 keys. 我们可以通过矩阵运算的方式计算所有 $mn$ 个 score：</p>
<p>$$<br>α (Q,K)=QK^T/\sqrt{d}<br>$$</p>
<p>它支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># query: (batch_size, #queries, d)</span></span><br><span class="line">    <span class="comment"># key: (batch_size, #kv_pairs, d)</span></span><br><span class="line">    <span class="comment"># value: (batch_size, #kv_pairs, dim_v)</span></span><br><span class="line">    <span class="comment"># valid_length: either (batch_size, ) or (batch_size, xx)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length=None)</span>:</span></span><br><span class="line">        d = query.shape[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># set transpose_b=True to swap the last two dimensions of key</span></span><br><span class="line">        </span><br><span class="line">        scores = torch.bmm(query, key.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        print(<span class="string">"attention_weight\n"</span>,attention_weights)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure>

<p><strong>测试：</strong></p>
<p>创建了两个批，每个批有一个query和10个key-values对。</p>
<p>通过valid_length指定，对于第一批，只关注前2个键-值对，而对于第二批，检查前6个键-值对</p>
<p>因此，尽管这两个批处理具有相同的查询和键值对，但我们获得的输出是不同的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">atten = DotProductAttention(dropout=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">keys = torch.ones((<span class="number">2</span>,<span class="number">10</span>,<span class="number">2</span>),dtype=torch.float)</span><br><span class="line">values = torch.arange((<span class="number">40</span>), dtype=torch.float).view(<span class="number">1</span>,<span class="number">10</span>,<span class="number">4</span>).repeat(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">atten(torch.ones((<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>),dtype=torch.float), keys, values, torch.FloatTensor([<span class="number">2</span>, <span class="number">6</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Result</span></span><br><span class="line">attention_weight</span><br><span class="line"> tensor([[[<span class="number">0.5000</span>, <span class="number">0.5000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>,</span><br><span class="line">          <span class="number">0.0000</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>,</span><br><span class="line">          <span class="number">0.0000</span>, <span class="number">0.0000</span>]]])</span><br><span class="line">tensor([[[ <span class="number">2.0000</span>,  <span class="number">3.0000</span>,  <span class="number">4.0000</span>,  <span class="number">5.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10.0000</span>, <span class="number">11.0000</span>, <span class="number">12.0000</span>, <span class="number">13.0000</span>]]])</span><br></pre></td></tr></table></figure>

<h3 id="Multilayer-Porceptron-Attentiion"><a href="#Multilayer-Porceptron-Attentiion" class="headerlink" title="Multilayer Porceptron Attentiion"></a>Multilayer Porceptron Attentiion</h3><p>在多层感知器中，我们首先将 query and keys 投影到  $R^ℎ$ .为了更具体，我们将可以学习的参数做如下映射<br>$W_k∈R^{h×d_k}$ ,  $W_q∈R^{h×d_q}$ , and  $v∈R^h$ .  将 score 函数定义</p>
<p>$$<br>α(k,q)=v^Ttanh(W_kk+W_qq)<br>$$<br>.<br>然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为  ℎ  and 输出的size为 1 .隐层激活函数为tanh，无偏置.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLPAttention</span><span class="params">(nn.Module)</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units,ipt_dim,dropout, **kwargs)</span>:</span></span><br><span class="line">        super(MLPAttention, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># Use flatten=True to keep query's and key's 3-D shapes.</span></span><br><span class="line">        self.W_k = nn.Linear(ipt_dim, units, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(ipt_dim, units, bias=<span class="literal">False</span>)</span><br><span class="line">        self.v = nn.Linear(units, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length)</span>:</span></span><br><span class="line">        query, key = self.W_k(query), self.W_q(key)</span><br><span class="line">        <span class="comment">#print("size",query.size(),key.size())</span></span><br><span class="line">        <span class="comment"># expand query to (batch_size, #querys, 1, units), and key to</span></span><br><span class="line">        <span class="comment"># (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.</span></span><br><span class="line">        features = query.unsqueeze(<span class="number">2</span>) + key.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print("features:",features.size())  #--------------开启</span></span><br><span class="line">        scores = self.v(features).squeeze(<span class="number">-1</span>) </span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure>

<p><strong>测试：</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;尽管 MLPAttention 包含一个额外的 MLP 模型，但如果给定相同的输入和相同的键，我们将获得与DotProductAttention相同的输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">atten = MLPAttention(ipt_dim=<span class="number">2</span>,units = <span class="number">8</span>, dropout=<span class="number">0</span>)</span><br><span class="line">atten(torch.ones((<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>), dtype = torch.float), keys, values, torch.FloatTensor([<span class="number">2</span>, <span class="number">6</span>]))      </span><br><span class="line"><span class="comment">#Result</span></span><br><span class="line">tensor([[[ <span class="number">2.0000</span>,  <span class="number">3.0000</span>,  <span class="number">4.0000</span>,  <span class="number">5.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10.0000</span>, <span class="number">11.0000</span>, <span class="number">12.0000</span>, <span class="number">13.0000</span>]]], grad_fn=&lt;BmmBackward&gt;)</span><br></pre></td></tr></table></figure>

<h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><blockquote>
<p>在Dot-product Attention中，key与query维度需要一致，在MLP Attention中则不需要。</p>
</blockquote>
<h2 id="Seq2seq模型"><a href="#Seq2seq模型" class="headerlink" title="Seq2seq模型"></a>Seq2seq模型</h2><blockquote>
<p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">机器翻译及其相关技术介绍</a></p>
</blockquote>
<p>seq2seq 模型的预测需人为设定终止条件，设定最长序列长度或者输出 [EOS] 结束符号，若不加以限制则可能生成无穷长度序列。</p>
<p>引出：</p>
<h2 id="引入注意力机制的Seq2seq模型"><a href="#引入注意力机制的Seq2seq模型" class="headerlink" title="引入注意力机制的Seq2seq模型"></a>引入注意力机制的Seq2seq模型</h2><p>注意力机制本身有高效的并行性，但引入注意力并不能改变seq2seq内部RNN的迭代机制，因此无法加速。</p>
<blockquote>
<p>将注意机制添加到 sequence to sequence 模型中，以显式地使用权重聚合 states。</p>
</blockquote>
<p>下图展示 encoding 和 decoding 的模型结构，在时间步为 $t$ 的时候。此刻 attention layer 保存着 encodering 看到的所有信息——即 encoding 的每一步输出。在 decoding 阶段，解码器的 $t$ 时刻的隐藏状态被当作 query，encoder 的每个时间步的 hidden states 作为 key 和 value 进行 attention 聚合. </p>
<p>Attetion model 的输出当作成上下文信息 context vector，并与解码器输入 $D_t$ 拼接起来一起送到解码器：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWttN284ejkzLlBORw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>Fig1具有注意机制的seq-to-seq模型解码的第二步<br>$$</p>
<p>下图展示了seq2seq机制的所以层的关系，下面展示了encoder和decoder的layer结构</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWttOGRpaGxyLlBORw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>Fig2具有注意机制的seq-to-seq模型中层结构<br>$$</p>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>由于带有注意机制的 seq2seq 的编码器与之前章节中的 Seq2SeqEncoder 相同，所以在此处我们只关注解码器。</p>
<p>我们添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态:</p>
<ul>
<li>the encoder outputs of all timesteps：encoder 输出的各个状态，被用于attetion layer 的 memory 部分，有相同的 key 和 values ；</li>
</ul>
<ul>
<li>the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state ；</li>
</ul>
<ul>
<li>the encoder valid length: 编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings）；</li>
</ul>
<p>在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的 query。</p>
<p>然后，将注意力模型的输出与输入嵌入向量连接起来，输入到 RNN 层。虽然 RNN 层隐藏状态也包含来自解码器的历史信息，但是 attention model 的输出显式地选择了 enc_valid_len 以内的编码器输出，这样 attention机制就会尽可能排除其他不相关的信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqAttentionDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens,vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, enc_valid_len, *args)</span>:</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line"><span class="comment">#         print("first:",outputs.size(),hidden_state[0].size(),hidden_state[1].size())</span></span><br><span class="line">        <span class="comment"># Transpose outputs to (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>), hidden_state, enc_valid_len)</span><br><span class="line">        <span class="comment">#outputs.swapaxes(0, 1)</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_len = state</span><br><span class="line">        <span class="comment">#("X.size",X.size())</span></span><br><span class="line">        X = self.embedding(X).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">#         print("Xembeding.size2",X.size())</span></span><br><span class="line">        outputs = []</span><br><span class="line">        <span class="keyword">for</span> l, x <span class="keyword">in</span> enumerate(X):</span><br><span class="line"><span class="comment">#             print(f"\n&#123;l&#125;-th token")</span></span><br><span class="line"><span class="comment">#             print("x.first.size()",x.size())</span></span><br><span class="line">            <span class="comment"># query shape: (batch_size, 1, hidden_size)</span></span><br><span class="line">            <span class="comment"># select hidden state of the last rnn layer as query</span></span><br><span class="line">            query = hidden_state[<span class="number">0</span>][<span class="number">-1</span>].unsqueeze(<span class="number">1</span>) <span class="comment"># np.expand_dims(hidden_state[0][-1], axis=1)</span></span><br><span class="line">            <span class="comment"># context has same shape as query</span></span><br><span class="line"><span class="comment">#             print("query enc_outputs, enc_outputs:\n",query.size(), enc_outputs.size(), enc_outputs.size())</span></span><br><span class="line">            context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len)</span><br><span class="line">            <span class="comment"># Concatenate on the feature dimension</span></span><br><span class="line"><span class="comment">#             print("context.size:",context.size())</span></span><br><span class="line">            x = torch.cat((context, x.unsqueeze(<span class="number">1</span>)), dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># Reshape x to (1, batch_size, embed_size+hidden_size)</span></span><br><span class="line"><span class="comment">#             print("rnn",x.size(), len(hidden_state))</span></span><br><span class="line">            out, hidden_state = self.rnn(x.transpose(<span class="number">0</span>,<span class="number">1</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.transpose(<span class="number">0</span>, <span class="number">1</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                        enc_valid_len]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>,</span><br><span class="line">                            num_hiddens=<span class="number">16</span>, num_layers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># encoder.initialize()</span></span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>,</span><br><span class="line">                                  num_hiddens=<span class="number">16</span>, num_layers=<span class="number">2</span>)</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>),dtype=torch.long)</span><br><span class="line">print(<span class="string">"batch size=4\nseq_length=7\nhidden dim=16\nnum_layers=2\n"</span>)</span><br><span class="line">print(<span class="string">'encoder output size:'</span>, encoder(X)[<span class="number">0</span>].size())</span><br><span class="line">print(<span class="string">'encoder hidden size:'</span>, encoder(X)[<span class="number">1</span>][<span class="number">0</span>].size())</span><br><span class="line">print(<span class="string">'encoder memory size:'</span>, encoder(X)[<span class="number">1</span>][<span class="number">1</span>].size())</span><br><span class="line">state = decoder.init_state(encoder(X), <span class="literal">None</span>)</span><br><span class="line">out, state = decoder(X, state)</span><br><span class="line">out.shape, len(state), state[<span class="number">0</span>].shape, len(state[<span class="number">1</span>]), state[<span class="number">1</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span> <span class="comment"># This class is saved in d2l.</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">    <span class="comment"># sort by frequency and token</span></span><br><span class="line">    counter = collections.Counter(tokens)</span><br><span class="line">    token_freqs = sorted(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    token_freqs.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">      <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">      self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">      tokens = [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.unk = <span class="number">0</span></span><br><span class="line">      tokens = [<span class="string">''</span>]</span><br><span class="line">    tokens += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs <span class="keyword">if</span> freq &gt;= min_freq]</span><br><span class="line">    self.idx_to_token = []</span><br><span class="line">    self.token_to_idx = dict()</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">      self.idx_to_token.append(token)</span><br><span class="line">      self.token_to_idx[token] = len(self.idx_to_token) - <span class="number">1</span></span><br><span class="line">      </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">      <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">      <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span><span class="params">(batch_size, max_len, num_examples=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Download an NMT dataset, return its vocabulary and data iterator."""</span></span><br><span class="line">    <span class="comment"># Download and preprocess</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_raw</span><span class="params">(text)</span>:</span></span><br><span class="line">        text = text.replace(<span class="string">'\u202f'</span>, <span class="string">' '</span>).replace(<span class="string">'\xa0'</span>, <span class="string">' '</span>)</span><br><span class="line">        out = <span class="string">''</span></span><br><span class="line">        <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(text.lower()):</span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">in</span> (<span class="string">','</span>, <span class="string">'!'</span>, <span class="string">'.'</span>) <span class="keyword">and</span> text[i<span class="number">-1</span>] != <span class="string">' '</span>:</span><br><span class="line">                out += <span class="string">' '</span></span><br><span class="line">            out += char</span><br><span class="line">        <span class="keyword">return</span> out </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'/home/kesci/input/fraeng6506/fra.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">      raw_text = f.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    text = preprocess_raw(raw_text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(text.split(<span class="string">'\n'</span>)):</span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">if</span> len(parts) &gt;= <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">' '</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build vocab</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(tokens)</span>:</span></span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">        <span class="keyword">return</span> Vocab(tokens, min_freq=<span class="number">3</span>, use_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert to index arrays</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pad</span><span class="params">(line, max_len, padding_token)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(line) &gt; max_len:</span><br><span class="line">            <span class="keyword">return</span> line[:max_len]</span><br><span class="line">        <span class="keyword">return</span> line + [padding_token] * (max_len - len(line))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_array</span><span class="params">(lines, vocab, max_len, is_source)</span>:</span></span><br><span class="line">        lines = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_source:</span><br><span class="line">            lines = [[vocab.bos] + line + [vocab.eos] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        array = torch.tensor([pad(line, max_len, vocab.pad) <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line">        valid_len = (array != vocab.pad).sum(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line">    src_array, src_valid_len = build_array(source, src_vocab, max_len, <span class="literal">True</span>)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, <span class="literal">False</span>)</span><br><span class="line">    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    train_iter = data.DataLoader(train_data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> src_vocab, tgt_vocab, train_iter</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.0</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, ctx = <span class="number">0.005</span>, <span class="number">500</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = d2l.Seq2SeqEncoder(</span><br><span class="line">    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(</span><br><span class="line">    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">model = d2l.EncoderDecoder(encoder, decoder)</span><br></pre></td></tr></table></figure>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_s2s_ch9(model, train_iter, lr, num_epochs, ctx)</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> [<span class="string">'Go .'</span>, <span class="string">'Good Night !'</span>, <span class="string">"I'm OK ."</span>, <span class="string">'I won !'</span>]:</span><br><span class="line">    print(sentence + <span class="string">' =&gt; '</span> + d2l.predict_s2s_ch9(</span><br><span class="line">        model, sentence, src_vocab, tgt_vocab, num_steps, ctx))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>MLP</tag>
        <tag>Seq2seq模型</tag>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title>机器翻译及其相关技术介绍</title>
    <url>/2020/02/28/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%8F%8A%E5%85%B6%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h1 id="机器翻译-MT-实践"><a href="#机器翻译-MT-实践" class="headerlink" title="机器翻译(MT)_实践"></a>机器翻译(MT)_实践</h1><blockquote>
<p>将一段文本从一种语言自动翻译为另一种语言<br/><br>用神经网络解决这个问题通常称为神经机器翻译（NMT）。</p>
</blockquote>
<p>主要特征：输出是单词序列而不是单个单词。 输出序列的长度可能与源序列的长度不同。</p>
<center><b>实现一个从英语到法语的机器翻译</b></center><br/>

<p>首先准备一个数据集，汇总一些常见单词和日用句子，数据集中有足够且保证正确的对应数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># For Example</span></span><br><span class="line">Go.	Va !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#2877272 (CM) &amp; #1158250 (Wittydev)</span></span><br><span class="line">Hi.	Salut !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#538123 (CM) &amp; #509819 (Aiji)</span></span><br><span class="line">Hi.	Salut.	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#538123 (CM) &amp; #4320462 (gillux)</span></span><br><span class="line">Run!	Cours !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#906328 (papabear) &amp; #906331 (sacredceltic)</span></span><br><span class="line">Run!	Courez !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#906328 (papabear) &amp; #906332 (sacredceltic)</span></span><br><span class="line">Who?	Qui ?	CC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) &amp; #4366796 (gillux)</span><br><span class="line">Wow!	Ça alors !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#52027 (Zifre) &amp; #374631 (zmoo)</span></span><br><span class="line">Fire!	Au feu !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#1829639 (Spamster) &amp; #4627939 (sacredceltic)</span></span><br><span class="line">Help!	À l  aide !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#435084 (lukaszpp) &amp; #128430 (sysko)</span></span><br><span class="line">Jump.	Saute.	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#631038 (Shishir) &amp; #2416938 (Phoenix)</span></span><br><span class="line">Stop!	Ça suffit !	CC-BY <span class="number">2.0</span> (France) Attribution: tato</span><br></pre></td></tr></table></figure>

<h3 id="导入包和模块以及数据文件"><a href="#导入包和模块以及数据文件" class="headerlink" title="导入包和模块以及数据文件"></a>导入包和模块以及数据文件</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import dataset</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.listdir(<span class="string">'path to storaged file of dataset'</span>)</span><br><span class="line"><span class="comment"># import package and module</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'path to file storge d2lzh1981'</span>)</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> d2l</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">from</span> d2l.data.base <span class="keyword">import</span> Vocab</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br></pre></td></tr></table></figure>

<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>将数据集清洗、转化为神经网络的输入minbatch</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data file'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">      raw_text = f.read()</span><br><span class="line">print(raw_text[<span class="number">0</span>:<span class="number">1000</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">针对上边的example data 进行处理</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 去掉乱码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_raw</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="comment"># 去掉法文中的空格</span></span><br><span class="line">    text = text.replace(<span class="string">'\u202f'</span>, <span class="string">' '</span>).replace(<span class="string">'\xa0'</span>, <span class="string">' '</span>)</span><br><span class="line">    out = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 大小写归一</span></span><br><span class="line">    <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(text.lower()):</span><br><span class="line">        <span class="comment"># 在单词和标点符号之间加上空格</span></span><br><span class="line">        <span class="keyword">if</span> char <span class="keyword">in</span> (<span class="string">','</span>, <span class="string">'!'</span>, <span class="string">'.'</span>) <span class="keyword">and</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> text[i<span class="number">-1</span>] != <span class="string">' '</span>:</span><br><span class="line">            out += <span class="string">' '</span></span><br><span class="line">        out += char</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">text = preprocess_raw(raw_text)</span><br><span class="line">print(text[<span class="number">0</span>:<span class="number">1000</span>])</span><br></pre></td></tr></table></figure>
<p>&ensp;&ensp;&ensp;&ensp;字符在计算机里是以编码的形式存在，我们通常所用的空格是 \x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。</p>
<p>&ensp;&ensp;&ensp;&ensp;而 \xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。再数据预处理的过程中，我们首先需要对数据进行清洗。</p>
<h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><blockquote>
<p>字符串：单词组成的列表</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_examples = <span class="number">50000</span></span><br><span class="line">source, target = [], []</span><br><span class="line"><span class="comment"># 分开每个样本</span></span><br><span class="line"><span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(text.split(<span class="string">'\n'</span>)):</span><br><span class="line">    <span class="keyword">if</span> i &gt; num_examples:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 取元素</span></span><br><span class="line">    parts = line.split(<span class="string">'\t'</span>)</span><br><span class="line">    <span class="keyword">if</span> len(parts) &gt;= <span class="number">2</span>:</span><br><span class="line">        source.append(parts[<span class="number">0</span>].split(<span class="string">' '</span>))</span><br><span class="line">        target.append(parts[<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"># test     </span></span><br><span class="line"><span class="string">source[0:3], target[0:3]</span></span><br><span class="line"><span class="string"># result</span></span><br><span class="line"><span class="string">([['go', '.'], ['hi', '.'], ['hi', '.']],</span></span><br><span class="line"><span class="string"> [['va', '!'], ['salut', '!'], ['salut', '.']])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">d2l.set_figsize()</span><br><span class="line">d2l.plt.hist([[len(l) <span class="keyword">for</span> l <span class="keyword">in</span> source], [len(l) <span class="keyword">for</span> l <span class="keyword">in</span> target]],label=[<span class="string">'source'</span>, <span class="string">'target'</span>])</span><br><span class="line">d2l.plt.legend(loc=<span class="string">'upper right'</span>);</span><br></pre></td></tr></table></figure>

<h3 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h3><blockquote>
<p>此处利用 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104297072" target="_blank" rel="noopener"><strong>文本预处理Text Preprocessing</strong></a>中的 Vocab 类</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(tokens)</span>:</span></span><br><span class="line">    <span class="comment"># 取出单词连成列表</span></span><br><span class="line">    tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">return</span> d2l.data.base.Vocab(tokens, min_freq=<span class="number">3</span>, use_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">src_vocab = build_vocab(source)</span><br><span class="line">len(src_vocab)</span><br></pre></td></tr></table></figure>

<h3 id="载入数据集得到数据生成器"><a href="#载入数据集得到数据生成器" class="headerlink" title="载入数据集得到数据生成器"></a>载入数据集得到数据生成器</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad</span><span class="params">(line, max_len, padding_token)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(line) &gt; max_len:</span><br><span class="line">        <span class="keyword">return</span> line[:max_len]</span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (max_len - len(line))</span><br><span class="line">pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab.pad)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># is_source a判断是否是法语</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_array</span><span class="params">(lines, vocab, max_len, is_source)</span>:</span></span><br><span class="line">    lines = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_source:</span><br><span class="line">        lines = [[vocab.bos] + line + [vocab.eos] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([pad(line, max_len, vocab.pad) <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line">    <span class="comment"># 有效长度：保留句子的有效长度</span></span><br><span class="line">    valid_len = (array != vocab.pad).sum(<span class="number">1</span>) <span class="comment">#第一个维度</span></span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据生成器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span><span class="params">(batch_size, max_len)</span>:</span> <span class="comment"># This function is saved in d2l.</span></span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line">    src_array, src_valid_len = build_array(source, src_vocab, max_len, <span class="literal">True</span>)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, <span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 验证四个参数是否都相同</span></span><br><span class="line">    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    train_iter = data.DataLoader(train_data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> src_vocab, tgt_vocab, train_iter</span><br></pre></td></tr></table></figure>

<blockquote>
<p>机器翻译</p>
<blockquote>
<p>困难：输入和输出不等价</p>
</blockquote>
</blockquote>
<h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>Encoder经常用循环神经网络，Decoder通过判断对后一个输出是不是eos来判断翻译是否结束</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjYXQzYzhtLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<blockquote>
<p>应用</p>
<blockquote>
<p>Encoder-Decoder常应用于输入序列和输出序列的长度是可变的，而分类问题的输出是固定的类别，不需要使用Encoder-Decoder</p>
</blockquote>
<p>机器翻译 、语音识别任务、对话机器人【属于】<br>文本分类任务【不属于】</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, *args)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, *args)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, **kwargs)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, enc_X, dec_X, *args)</span>:</span></span><br><span class="line">        <span class="comment"># 类似于H_&#123;-1&#125;</span></span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure>
<h2 id="Sequence-to-Sequence模型"><a href="#Sequence-to-Sequence模型" class="headerlink" title="Sequence to Sequence模型"></a>Sequence to Sequence模型</h2><h3 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h3><p>训练<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjN2E1M3B0LnBuZw?x-oss-process=image/format,png" alt="Image Name"><br>预测</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjZWN4Y2JhLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<h3 id="具体结构：-LSTM"><a href="#具体结构：-LSTM" class="headerlink" title="具体结构：(LSTM)"></a>具体结构：(LSTM)</h3><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjY2poa2lpLnBuZw?x-oss-process=image/format,png" /></center>

<h4 id="Encoder-–-state"><a href="#Encoder-–-state" class="headerlink" title="Encoder – state"></a>Encoder – state</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqEncoder</span><span class="params">(d2l.Encoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens=num_hiddens</span><br><span class="line">        self.num_layers=num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span><span class="params">(self, batch_size, device)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),</span><br><span class="line">                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, *args)</span>:</span></span><br><span class="line">        X = self.embedding(X) <span class="comment"># X shape: (batch_size, seq_len, embed_size)</span></span><br><span class="line">        <span class="comment"># 第0维和第1维之间调换</span></span><br><span class="line">        X = X.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># RNN needs first axes to be time</span></span><br><span class="line">        <span class="comment"># state = self.begin_state(X.shape[1], device=X.device)</span></span><br><span class="line">        out, state = self.rnn(X)</span><br><span class="line">        <span class="comment"># The shape of out is (seq_len, batch_size, num_hiddens).</span></span><br><span class="line">        <span class="comment"># state contains the hidden state and the memory cell</span></span><br><span class="line">        <span class="comment"># of the last time step, the shape is (num_layers, batch_size, num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> out, state</span><br></pre></td></tr></table></figure>
<h4 id="Decoder-–-out"><a href="#Decoder-–-out" class="headerlink" title="Decoder – out"></a>Decoder – out</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">        <span class="comment"># 输出的全连接层 映射翻译结果</span></span><br><span class="line">        self.dense = nn.Linear(num_hiddens,vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, *args)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        X = self.embedding(X).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        out, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># Make the batch to be the first dimension to simplify loss computation.</span></span><br><span class="line">        out = self.dense(out).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out, state</span><br></pre></td></tr></table></figure>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">0</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    mask = torch.arange(maxlen)[<span class="literal">None</span>, :].to(X_len.device) &lt; X_len[:, <span class="literal">None</span>]   </span><br><span class="line">    X[~mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 继承交叉损失熵函数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskedSoftmaxCELoss</span><span class="params">(nn.CrossEntropyLoss)</span>:</span></span><br><span class="line">    <span class="comment"># pred shape: (batch_size, seq_len, vocab_size)</span></span><br><span class="line">    <span class="comment"># label shape: (batch_size, seq_len)</span></span><br><span class="line">    <span class="comment"># valid_length shape: (batch_size, )</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, pred, label, valid_length)</span>:</span></span><br><span class="line">        <span class="comment"># the sample weights shape should be (batch_size, seq_len)</span></span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        weights = SequenceMask(weights, valid_length).float()</span><br><span class="line">        self.reduction=<span class="string">'none'</span></span><br><span class="line">        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(<span class="number">1</span>,<span class="number">2</span>), label)</span><br><span class="line">        <span class="keyword">return</span> (output*weights).mean(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch7</span><span class="params">(model, data_iter, lr, num_epochs, device)</span>:</span>  <span class="comment"># Saved in d2l</span></span><br><span class="line">    model.to(device)</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    tic = time.time()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, num_epochs+<span class="number">1</span>):</span><br><span class="line">        l_sum, num_tokens_sum = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_vlen, Y, Y_vlen = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">            <span class="comment"># bos word eos</span></span><br><span class="line">            Y_input, Y_label, Y_vlen = Y[:,:<span class="number">-1</span>], Y[:,<span class="number">1</span>:], Y_vlen<span class="number">-1</span></span><br><span class="line">            </span><br><span class="line">            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)</span><br><span class="line">            <span class="comment"># 评估训练好坏</span></span><br><span class="line">            l = loss(Y_hat, Y_label, Y_vlen).sum()</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                d2l.grad_clipping_nn(model, <span class="number">5</span>, device)</span><br><span class="line">            num_tokens = Y_vlen.sum().item()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.sum().item()</span><br><span class="line">            num_tokens_sum += num_tokens</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"epoch &#123;0:4d&#125;,loss &#123;1:.3f&#125;, time &#123;2:.1f&#125; sec"</span>.format( </span><br><span class="line">                  epoch, (l_sum/num_tokens_sum), time.time()-tic))</span><br><span class="line">            tic = time.time()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.0</span></span><br><span class="line">batch_size, num_examples, max_len = <span class="number">64</span>, <span class="number">1e3</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, ctx = <span class="number">0.005</span>, <span class="number">300</span>, d2l.try_gpu()</span><br><span class="line">src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(</span><br><span class="line">    batch_size, max_len,num_examples)</span><br><span class="line">encoder = Seq2SeqEncoder(</span><br><span class="line">    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqDecoder(</span><br><span class="line">    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">model = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">train_ch7(model, train_iter, lr, num_epochs, ctx)</span><br></pre></td></tr></table></figure>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate_ch7</span><span class="params">(model, src_sentence, src_vocab, tgt_vocab, max_len, device)</span>:</span></span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">' '</span>)]</span><br><span class="line">    src_len = len(src_tokens)</span><br><span class="line">    <span class="keyword">if</span> src_len &lt; max_len:</span><br><span class="line">        src_tokens += [src_vocab.pad] * (max_len - src_len)</span><br><span class="line">    enc_X = torch.tensor(src_tokens, device=device)</span><br><span class="line">    enc_valid_length = torch.tensor([src_len], device=device)</span><br><span class="line">    <span class="comment"># use expand_dim to add the batch_size dimension.</span></span><br><span class="line">    enc_outputs = model.encoder(enc_X.unsqueeze(dim=<span class="number">0</span>), enc_valid_length)</span><br><span class="line">    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)</span><br><span class="line">    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line">    predict_tokens = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(max_len):</span><br><span class="line">        Y, dec_state = model.decoder(dec_X, dec_state)</span><br><span class="line">        <span class="comment"># The token with highest score is used as the next time step input.</span></span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        py = dec_X.squeeze(dim=<span class="number">0</span>).int().item()</span><br><span class="line">        <span class="keyword">if</span> py == tgt_vocab.eos:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        predict_tokens.append(py)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join(tgt_vocab.to_tokens(predict_tokens))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> [<span class="string">'Go .'</span>, <span class="string">'Wow !'</span>, <span class="string">"I'm OK ."</span>, <span class="string">'I love you !'</span>]:</span><br><span class="line">    print(sentence + <span class="string">' =&gt; '</span> + translate_ch7(</span><br><span class="line">        model, sentence, src_vocab, tgt_vocab, max_len, ctx))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Result</span></span><br><span class="line">Go . =&gt; va !</span><br><span class="line">Wow ! =&gt; &lt;unk&gt; !</span><br><span class="line">I<span class="string">'m OK . =&gt; je vais bien .</span></span><br><span class="line"><span class="string">I love you ! =&gt; reste &lt;unk&gt; !</span></span><br></pre></td></tr></table></figure>
<h2 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h2><ul>
<li>简单 贪心搜索（greedy search）：</li>
</ul>
<blockquote>
<p>针对每一个 out 取最大概率<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjaHFvcHBuLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
</blockquote>
<ul>
<li>维特比算法：选择整体分数最高的句子（搜索空间太大）</li>
<li>集束搜索：<blockquote>
<p>集束搜索是维特比算法的贪心形式，所以集束搜索得到的并非是全局最优解<br/><br>集束搜索使用 beam size 参数来限制在每一步保留下来的可能性词的数量</p>
</blockquote>
</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjaWE4NnoxLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>MT</tag>
        <tag>encoder-decoder</tag>
        <tag>Seq2seq模型</tag>
        <tag>集束搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>学而后思,方能发展;思而立行,终将卓越</title>
    <url>/2020/02/28/%E5%AD%A6%E8%80%8C%E5%90%8E%E6%80%9D,%E6%96%B9%E8%83%BD%E5%8F%91%E5%B1%95;%E6%80%9D%E8%80%8C%E7%AB%8B%E8%A1%8C,%E7%BB%88%E5%B0%86%E5%8D%93%E8%B6%8A/</url>
    <content><![CDATA[<center><b><font size=5>学而后思</font></b></center>

<blockquote>
<p>梯度爆炸和梯度衰减问题</p>
</blockquote>
<p>深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。</p>
<p><strong>当神经网络的层数较多时，模型的数值稳定性容易变差。</strong></p>
<p>假设一个层数为 $L$ 的多层感知机的第 $l$ 层 $\boldsymbol{H}^{(l)}$ 的权重参数为$\boldsymbol{W}^{(l)}$，输出层 $\boldsymbol{H}^{(L)}$ 的权重参数为 $\boldsymbol{W}^{(L)}$。</p>
<p>为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为<code>恒等映射</code>（identity mapping）$\phi(x) = x$。</p>
<p>给定输入$\boldsymbol{X}$，多层感知机的第$l$层的输出$\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。</p>
<p>For Example：假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\boldsymbol{X}$分别与$0.2^{30} \approx 1 \times 10^{-21}$（消失）和$5^{30} \approx 9 \times 10^{20}$（爆炸）的乘积。当层数较多时，梯度的计算也容易出现消失或爆炸。</p>
<hr>
<p>解决方案：</p>
<ul>
<li>梯度爆炸：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">裁剪梯度</a></li>
<li>梯度衰减：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104340035" target="_blank" rel="noopener">GRU</a></li>
</ul>
<hr>
<blockquote>
<p>过拟合和欠拟合问题</p>
</blockquote>
<ul>
<li><a href="https://blog.csdn.net/RokoBasilisk/article/details/104344237" target="_blank" rel="noopener">从模型训练中认知拟合现象</a></li>
</ul>
<blockquote>
<p>随机初始化模型参数</p>
<blockquote>
<p>如何占据神经网络中不可或缺的位置？</p>
</blockquote>
</blockquote>
<p>话起 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104287595" target="_blank" rel="noopener">多层感知机 [ Multilayer Perceptron ]</a> </p>
<p>假设输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。</p>
<p>如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。</p>
<p>在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。</p>
<p>在这种情况下，无论隐藏单元有多少，<code>隐藏层本质上只有1个隐藏单元在发挥作用</code>。因此通常将神经网络的模型参数，特别是权重参数，进行随机初始化。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpnNzZrbG95LnBuZw?x-oss-process=image/format,png" /></center>

<blockquote>
<blockquote>
<p>列举两种随机初始化方式</p>
</blockquote>
</blockquote>
<h4 id="PyTorch的默认随机初始化"><a href="#PyTorch的默认随机初始化" class="headerlink" title="PyTorch的默认随机初始化"></a>PyTorch的默认随机初始化</h4><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104269986" target="_blank" rel="noopener">Design and Realization of Linear Regression</a> 中，使用 torch.nn.init.normal_() 使模型 net 的权重参数采用正态分布的随机初始化方式。</p>
<p>PyTorch中 nn.Module 的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考<a href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules" target="_blank" rel="noopener">源代码</a>），因此一般不用我们考虑。</p>
<h4 id="Xavier随机初始化"><a href="#Xavier随机初始化" class="headerlink" title="Xavier随机初始化"></a>Xavier随机初始化</h4><p>假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p>
<p>$$<br>U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).<br>$$</p>
<p>模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。</p>
<blockquote>
<p>环境因素带来的问题</p>
<blockquote>
<p>1.协变量偏移<br/><br>2.标签偏移<br/><br>3.概念偏移</p>
</blockquote>
</blockquote>
<blockquote>
<p>协变量偏移</p>
</blockquote>
<p><strong>For Example : 一个在冬季部署的物品推荐系统在夏季的物品推荐列表中出现了圣诞礼物</strong></p>
<p>我们假设，虽然输入的分布P(x)可能随时间而改变，但是标记函数，即条件分布P（y∣x）不会改变。【注意：容易忽视】</p>
<p>对于区分猫和狗，假设我们的训练数据使用的是猫和狗的真实的照片，但是在测试时，我们被要求对猫和狗的卡通图片进行分类。显然，这不太可能奏效。</p>
<p>训练集由照片组成，而测试集只包含卡通。在一个看起来与测试集有着本质不同的数据集上进行训练，而不考虑如何适应新的情况。</p>
<p>问题的根源在于特征分布的变化 ( 即协变量的变化 ) , 统计学家称这种协变量变化。</p>
<blockquote>
<p>标签偏移</p>
</blockquote>
<p>如果数据量足够的情况下，确保训练数据集和测试集中的数据取自同一个数据集，可以防止协变量偏移和标签偏移是正确的。如果数据量很少，少到测试集中存在训练集中未包含的标签，就会发生标签偏移。</p>
<p>当我们认为导致偏移的是标签P（y）上的边缘分布的变化，但类条件分布是不变的P（x∣y）时，就会出现相反的问题。当我们认为y导致x时，标签偏移是一个合理的假设。</p>
<p>For Example：</p>
<ul>
<li>通常我们希望根据其表现来预测诊断结果。在这种情况下，我们认为诊断引起的表现，即疾病引起的症状。<br/><br>病因（要预测的诊断结果）导致 症状（观察到的结果）。  训练数据集，数据很少只包含流感p(y)的样本。  而测试数据集有流感p(y)和流感q(y)，其中不变的是流感症状p(x|y)。</li>
<li>有时标签偏移和协变量移位假设可以同时成立。例如，当真正的标签函数是确定的和不变的。</li>
</ul>
<p><font color=#FF0000 >在概念转换中，有一种标签本身的定义发生变化的情况：<br></font></p>
<blockquote>
<p>概念偏移</p>
</blockquote>
<p>如果我们要建立一个机器翻译系统，分布P（y∣x）可能因我们的位置而异。这个问题很难发现。另一个可取之处是P（y∣x）通常只是逐渐变化。</p>
<p>换句话说就是：<strong>概念偏移可以根据其缓慢变化的特点缓解。</strong></p>
<center><b><font size=5>思而立行</font></b></center><br/>

<p>Advanced Regression Techniques：House Prices</p>
<h3 id="导入数据包和模块"><a href="#导入数据包和模块" class="headerlink" title="导入数据包和模块"></a>导入数据包和模块</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">torch.set_default_tensor_type(torch.FloatTensor)</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span>  norm</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model, svm, gaussian_process</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>

<h3 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_data = pd.read_csv(<span class="string">"path to test.csv"</span>)</span><br><span class="line">train_data = pd.read_csv(<span class="string">"path to train.csv"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="分析数据变化趋势"><a href="#分析数据变化趋势" class="headerlink" title="分析数据变化趋势"></a>分析数据变化趋势</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># analyze SalePrice of the train_data by describe</span></span><br><span class="line">train_data[<span class="string">'SalePrice'</span>].describe()</span><br><span class="line"><span class="comment"># show trend of SalePrice in train_data</span></span><br><span class="line">sns.distplot(train_data[<span class="string">'SalePrice'</span>])</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200216221220752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p><strong>根据SalePrice变化趋势分析为正态分布，设定两个图像特征峰度(Kurtosis)和偏度(Skewness)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#skewness and kurtosis</span></span><br><span class="line">print(<span class="string">"Skewness: %f"</span> % train_data[<span class="string">'SalePrice'</span>].skew())</span><br><span class="line">print(<span class="string">"Kurtosis: %f"</span> % train_data[<span class="string">'SalePrice'</span>].kurt())</span><br></pre></td></tr></table></figure>
<h3 id="提取有效特征"><a href="#提取有效特征" class="headerlink" title="提取有效特征"></a>提取有效特征</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">corrmat = train_data.corr()</span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">20</span>, <span class="number">9</span>))</span><br><span class="line">sns.heatmap(corrmat, vmax=<span class="number">0.8</span>, square=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200216220930264.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70"  width=500/></center>

<h3 id="特征取舍和离散值参与分析"><a href="#特征取舍和离散值参与分析" class="headerlink" title="特征取舍和离散值参与分析"></a>特征取舍和离散值参与分析</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">f_names = [<span class="string">'CentralAir'</span>, <span class="string">'Neighborhood'</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> f_names:</span><br><span class="line">    label = preprocessing.LabelEncoder()</span><br><span class="line">    train_data[x] = label.fit_transform(train_data[x])</span><br><span class="line">corrmat = train_data.corr()</span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">20</span>, <span class="number">9</span>))</span><br><span class="line">sns.heatmap(corrmat, vmax=<span class="number">0.8</span>, square=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><strong>由相关性图可得：’CentralAir’, ‘Neighborhood’这两个特征对房价的影响并不大,舍去特征</strong></p>
<h3 id="列出关系矩阵"><a href="#列出关系矩阵" class="headerlink" title="列出关系矩阵"></a>列出关系矩阵</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k  = <span class="number">10</span> <span class="comment"># 关系矩阵中将显示10个特征</span></span><br><span class="line">cols = corrmat.nlargest(k, <span class="string">'SalePrice'</span>)[<span class="string">'SalePrice'</span>].index</span><br><span class="line">cm = np.corrcoef(train_data[cols].values.T)</span><br><span class="line">sns.set(font_scale=<span class="number">1.25</span>)</span><br><span class="line">hm = sns.heatmap(cm, cbar=<span class="literal">True</span>, annot=<span class="literal">True</span>, \</span><br><span class="line">                 square=<span class="literal">True</span>, fmt=<span class="string">'.2f'</span>, annot_kws=&#123;<span class="string">'size'</span>: <span class="number">10</span>&#125;, yticklabels=cols.values, xticklabels=cols.values)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200216221404494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="列出散点关系图"><a href="#列出散点关系图" class="headerlink" title="列出散点关系图"></a>列出散点关系图</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.set()</span><br><span class="line">cols = [<span class="string">'SalePrice'</span>,<span class="string">'OverallQual'</span>,<span class="string">'GrLivArea'</span>, <span class="string">'GarageCars'</span>,<span class="string">'TotalBsmtSF'</span>, <span class="string">'FullBath'</span>, <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">sns.pairplot(train_data[cols], size = <span class="number">2.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="数据模拟"><a href="#数据模拟" class="headerlink" title="数据模拟"></a>数据模拟</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">cols = [<span class="string">'OverallQual'</span>,<span class="string">'GrLivArea'</span>, <span class="string">'GarageCars'</span>,<span class="string">'TotalBsmtSF'</span>, <span class="string">'FullBath'</span>, <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">x = train_data[cols].values</span><br><span class="line">y = train_data[<span class="string">'SalePrice'</span>].values</span><br><span class="line">x_scaled = preprocessing.StandardScaler().fit_transform(x)</span><br><span class="line">y_scaled = preprocessing.StandardScaler().fit_transform(y.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">X_train,X_test, y_train, y_test = train_test_split(x_scaled, y_scaled, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<h3 id="随机森林回归"><a href="#随机森林回归" class="headerlink" title="随机森林回归"></a>随机森林回归</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机森林回归</span></span><br><span class="line">clfs = &#123;</span><br><span class="line">        <span class="string">'svm'</span>:svm.SVR(), </span><br><span class="line">        <span class="string">'RandomForestRegressor'</span>:RandomForestRegressor(n_estimators=<span class="number">400</span>),</span><br><span class="line">        <span class="string">'BayesianRidge'</span>:linear_model.BayesianRidge()</span><br><span class="line">       &#125;</span><br><span class="line"><span class="keyword">for</span> clf <span class="keyword">in</span> clfs:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        clfs[clf].fit(X_train, y_train)</span><br><span class="line">        y_pred = clfs[clf].predict(X_test)</span><br><span class="line">        print(clf + <span class="string">" cost:"</span> + str(np.sum(y_pred-y_test)/len(y_pred)) )</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(clf + <span class="string">" Error:"</span>)</span><br><span class="line">        print(str(e))</span><br></pre></td></tr></table></figure>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 归一化数据的预测结果</span></span><br><span class="line">cols = [<span class="string">'OverallQual'</span>,<span class="string">'GrLivArea'</span>, <span class="string">'GarageCars'</span>,<span class="string">'TotalBsmtSF'</span>, <span class="string">'FullBath'</span>, <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">x = train_data[cols].values</span><br><span class="line">y = train_data[<span class="string">'SalePrice'</span>].values</span><br><span class="line">X_train,X_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">clf = RandomForestRegressor(n_estimators=<span class="number">400</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line">print(y_pred)</span><br><span class="line">print(y_test)</span><br><span class="line">print(sum(abs(y_pred - y_test))/len(y_pred))</span><br></pre></td></tr></table></figure>

<h3 id="检验"><a href="#检验" class="headerlink" title="检验"></a>检验</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># clf:训练模型</span></span><br><span class="line">rfr = clf</span><br><span class="line">test_data[cols].isnull().sum()</span><br><span class="line">test_data[<span class="string">'GarageCars'</span>].describe()</span><br><span class="line">test_data[<span class="string">'TotalBsmtSF'</span>].describe()</span><br><span class="line"></span><br><span class="line">cols2 = [<span class="string">'OverallQual'</span>,<span class="string">'GrLivArea'</span>, <span class="string">'FullBath'</span>, <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">cars = test_data[<span class="string">'GarageCars'</span>].fillna(<span class="number">1.766118</span>)</span><br><span class="line">bsmt = test_data[<span class="string">'TotalBsmtSF'</span>].fillna(<span class="number">1046.117970</span>)</span><br><span class="line">data_test_x = pd.concat( [test_data[cols2], cars, bsmt] ,axis=<span class="number">1</span>)</span><br><span class="line">data_test_x.isnull().sum()</span><br><span class="line"></span><br><span class="line">x = data_test_x.values</span><br><span class="line">y_te_pred = rfr.predict(x)</span><br><span class="line">print(y_te_pred)</span><br><span class="line"></span><br><span class="line">print(y_te_pred.shape)</span><br><span class="line">print(x.shape)</span><br><span class="line">print(data_test_x)</span><br></pre></td></tr></table></figure>
<p><strong>输出数据表格式：1459 rows × 7 columns</strong></p>
<h3 id="输出结果到文件"><a href="#输出结果到文件" class="headerlink" title="输出结果到文件"></a>输出结果到文件</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prediction = pd.DataFrame(y_te_pred, columns=[<span class="string">'SalePrice'</span>])</span><br><span class="line">result = pd.concat([ test_data[<span class="string">'Id'</span>], prediction], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># result = result.drop(resultlt.columns[0], 1)</span></span><br><span class="line">result.columns</span><br><span class="line"><span class="comment"># 保存预测结果</span></span><br><span class="line">result.to_csv(<span class="string">'./Predictions.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
        <tag>梯度问题</tag>
        <tag>拟合现象</tag>
        <tag>环境因素</tag>
      </tags>
  </entry>
  <entry>
    <title>从模型训练中认知拟合现象</title>
    <url>/2020/02/28/%E4%BB%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%AD%E8%AE%A4%E7%9F%A5%E6%8B%9F%E5%90%88%E7%8E%B0%E8%B1%A1/</url>
    <content><![CDATA[<p>机器学习中模型训练是必需的，在模型训练中存在两类典型的问题：</p>
<blockquote>
<p>欠拟合 (underfitting) </p>
<blockquote>
<p>模型无法得到较低的训练误差</p>
</blockquote>
<p>过拟合 (overfitting)</p>
<blockquote>
<p>模型的训练误差远小于它在测试数据集上的误差</p>
</blockquote>
</blockquote>
<p>实际训练过程中可能会出现两类问题的并发症，而且会有多种因素直接或间接地导致这种情况出现</p>
<h2 id="影响因素"><a href="#影响因素" class="headerlink" title="影响因素"></a>影响因素</h2><center>介绍其中两个因素：模型复杂度和训练数据集大小。</center>

<h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><p>以多项式函数拟合为例。给定一个由标量数据特征$x$和对应的标量标签$y$组成的训练数据集，多项式函数拟合的目标是找一个$K$阶多项式函数</p>
<p>$$<br> \hat{y} = b + \sum_{k=1}^K x^k w_k<br>$$</p>
<p>来近似 $y$。</p>
<p>在上式中，$w_k$是模型的权重参数，$b$是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。</p>
<p>给定训练数据集，模型复杂度和误差之间的关系：</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjMjd3eG9qLnBuZw?x-oss-process=image/format,png"></center>

<h3 id="训练数据集大小"><a href="#训练数据集大小" class="headerlink" title="训练数据集大小"></a>训练数据集大小</h3><p>一般来说，如果训练数据集中<strong>样本数过少</strong>，特别是比模型参数数量（按元素计）更少时，<strong>过拟合</strong>更容易发生。</p>
<p>此外，<strong>泛化误差</strong>不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p>
<hr>
<center>训练误差和泛化误差</center><br/>

<p>&ensp;&ensp;&ensp;&ensp;通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。</p>
<p>&ensp;&ensp;&ensp;&ensp;计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。</p>
<p>&ensp;&ensp;&ensp;&ensp;机器学习模型应关注降低泛化误差。</p>
<hr>
<h2 id="多项式函数拟合实验"><a href="#多项式函数拟合实验" class="headerlink" title="多项式函数拟合实验"></a>多项式函数拟合实验</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_train, n_test, true_w, true_b = <span class="number">100</span>, <span class="number">100</span>, [<span class="number">1.2</span>, <span class="number">-3.4</span>, <span class="number">5.6</span>], <span class="number">5</span></span><br><span class="line">features = torch.randn((n_train + n_test, <span class="number">1</span>))</span><br><span class="line">poly_features = torch.cat((features, torch.pow(features, <span class="number">2</span>), torch.pow(features, <span class="number">3</span>)), <span class="number">1</span>) </span><br><span class="line">labels = (true_w[<span class="number">0</span>] * poly_features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * poly_features[:, <span class="number">1</span>]</span><br><span class="line">          + true_w[<span class="number">2</span>] * poly_features[:, <span class="number">2</span>] + true_b)</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float)</span><br></pre></td></tr></table></figure>

<h3 id="定义和训练参数模型"><a href="#定义和训练参数模型" class="headerlink" title="定义和训练参数模型"></a>定义和训练参数模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">semilogy</span><span class="params">(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,</span></span></span><br><span class="line"><span class="function"><span class="params">             legend=None, figsize=<span class="params">(<span class="number">3.5</span>, <span class="number">2.5</span>)</span>)</span>:</span></span><br><span class="line">    <span class="comment"># d2l.set_figsize(figsize)</span></span><br><span class="line">    d2l.plt.xlabel(x_label)</span><br><span class="line">    d2l.plt.ylabel(y_label)</span><br><span class="line">    d2l.plt.semilogy(x_vals, y_vals)</span><br><span class="line">    <span class="keyword">if</span> x2_vals <span class="keyword">and</span> y2_vals:</span><br><span class="line">        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=<span class="string">':'</span>)</span><br><span class="line">        d2l.plt.legend(legend)</span><br><span class="line"></span><br><span class="line">num_epochs, loss = <span class="number">100</span>, torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot</span><span class="params">(train_features, test_features, train_labels, test_labels)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化网络模型</span></span><br><span class="line">    net = torch.nn.Linear(train_features.shape[<span class="number">-1</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 通过Linear文档可知，pytorch已经将参数初始化了，所以我们这里就不手动初始化了</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置批量大小</span></span><br><span class="line">    batch_size = min(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])    </span><br><span class="line">     <span class="comment"># 设置数据集</span></span><br><span class="line">    dataset = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">    <span class="comment"># 设置获取数据方式     </span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>) </span><br><span class="line">    <span class="comment"># 设置优化函数，使用的是随机梯度下降优化</span></span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)                      </span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    	<span class="comment"># 取一个批量的数据</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:               </span><br><span class="line">        	<span class="comment"># 输入到网络中计算输出，并和标签比较求得损失函数                                  </span></span><br><span class="line">            l = loss(net(X), y.view(<span class="number">-1</span>, <span class="number">1</span>)) </span><br><span class="line">            <span class="comment"># 梯度清零，防止梯度累加干扰优化                                   </span></span><br><span class="line">            optimizer.zero_grad()                                               </span><br><span class="line">            l.backward() <span class="comment"># 求梯度</span></span><br><span class="line">            optimizer.step() <span class="comment"># 迭代优化函数，进行参数优化</span></span><br><span class="line">        train_labels = train_labels.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        test_labels = test_labels.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将训练损失保存到train_ls中</span></span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).item())</span><br><span class="line">        <span class="comment"># 将测试损失保存到test_ls中         </span></span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).item())            </span><br><span class="line">    print(<span class="string">'final epoch: train loss'</span>, train_ls[<span class="number">-1</span>], <span class="string">'test loss'</span>, test_ls[<span class="number">-1</span>])    </span><br><span class="line">    semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">             range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'weight:'</span>, net.weight.data,</span><br><span class="line">          <span class="string">'\nbias:'</span>, net.bias.data)</span><br></pre></td></tr></table></figure>

<h3 id="对比三种拟合现象"><a href="#对比三种拟合现象" class="headerlink" title="对比三种拟合现象"></a>对比三种拟合现象</h3><blockquote>
<p>正常</p>
<blockquote>
<p>fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:])<br><img src="https://img-blog.csdnimg.cn/20200216165618535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></p>
</blockquote>
</blockquote>
<blockquote>
<p>欠拟合</p>
<blockquote>
<p>fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train], labels[n_train:])<br><img src="https://img-blog.csdnimg.cn/20200216172452546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" width=400 /></p>
</blockquote>
</blockquote>
<blockquote>
<p>过拟合</p>
<blockquote>
<p>fit_and_plot(poly_features[0:2, :], poly_features[n_train:, :], labels[0:2], labels[n_train:])<br><img src="https://img-blog.csdnimg.cn/20200216172416864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" width=400/></p>
</blockquote>
</blockquote>
<h2 id="针对两类拟合问题的解决办法"><a href="#针对两类拟合问题的解决办法" class="headerlink" title="针对两类拟合问题的解决办法"></a>针对两类拟合问题的解决办法</h2><h3 id="权重衰减–过拟合"><a href="#权重衰减–过拟合" class="headerlink" title="权重衰减–过拟合"></a>权重衰减–过拟合</h3><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>&ensp;&ensp;&ensp;&ensp;权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。</p>
<h4 id="L2-范数正则化（regularization）"><a href="#L2-范数正则化（regularization）" class="headerlink" title="L2 范数正则化（regularization）"></a>L2 范数正则化（regularization）</h4><p>&ensp;&ensp;&ensp;&ensp;$L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以线性回归中的线性回归损失函数为例</p>
<p>$$<br> \ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2<br>$$</p>
<p>其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为</p>
<p>$$<br>\ell(w_1, w_2, b) + \frac{\lambda}{2n} |\boldsymbol{w}|^2,<br>$$</p>
<p>其中超参数$\lambda &gt; 0$。当权重参数均为0时，惩罚项最小。当$\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。</p>
<p>&ensp;&ensp;&ensp;&ensp;有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104269986" target="_blank" rel="noopener">Design and Realization of Linear Regression</a> 中权重$w_1$和$w_2$的迭代方式更改为</p>
<p>$$<br> \begin{aligned} w_1 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\ w_2 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}<br>$$</p>
<p>可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。</p>
<p>因此，<code>$L_2$范数正则化又叫权重衰减</code>。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，<code>这可能对过拟合有效</code>。</p>
<h4 id="应用【剑指高维线性回归带来的过拟合】"><a href="#应用【剑指高维线性回归带来的过拟合】" class="headerlink" title="应用【剑指高维线性回归带来的过拟合】"></a>应用【剑指高维线性回归带来的过拟合】</h4><p>&ensp;&ensp;&ensp;&ensp;设数据样本特征的维度为$p$。对于训练数据集和测试数据集中特征为$x_1, x_2, \ldots, x_p$的任一样本，使用如下的线性函数来生成该样本的标签：</p>
<p>$$<br> y = 0.05 + \sum_{i = 1}^p 0.01x_i + \epsilon<br>$$</p>
<p>其中噪声项$\epsilon$服从均值为0、标准差为0.01的正态分布。为了较容易地观察过拟合，我们考虑高维线性回归问题，如设维度$p=200$；同时，我们特意把训练数据集的样本数设低，如20。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h5 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_train, n_test, num_inputs = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span></span><br><span class="line">true_w, true_b = torch.ones(num_inputs, <span class="number">1</span>) * <span class="number">0.01</span>, <span class="number">0.05</span></span><br><span class="line"></span><br><span class="line">features = torch.randn((n_train + n_test, num_inputs))</span><br><span class="line">labels = torch.matmul(features, true_w) + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float)</span><br><span class="line">train_features, test_features = features[:n_train, :], features[n_train:, :]</span><br><span class="line">train_labels, test_labels = labels[:n_train], labels[n_train:]</span><br><span class="line"><span class="comment"># 定义参数初始化函数，初始化模型参数并且附上梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_params</span><span class="params">()</span>:</span></span><br><span class="line">    w = torch.randn((num_inputs, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure>
<h5 id="定义L2范数惩罚项"><a href="#定义L2范数惩罚项" class="headerlink" title="定义L2范数惩罚项"></a>定义L2范数惩罚项</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l2_penalty</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (w**<span class="number">2</span>).sum() / <span class="number">2</span></span><br></pre></td></tr></table></figure>

<h5 id="训练和测试"><a href="#训练和测试" class="headerlink" title="训练和测试"></a>训练和测试</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size, num_epochs, lr = <span class="number">1</span>, <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">net, loss = d2l.linreg, d2l.squared_loss</span><br><span class="line"></span><br><span class="line">dataset = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot</span><span class="params">(lambd)</span>:</span></span><br><span class="line">    w, b = init_params()</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="comment"># 添加了L2范数惩罚项</span></span><br><span class="line">            l = loss(net(X, w, b), y) + lambd * l2_penalty(w)</span><br><span class="line">            l = l.sum()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> w.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                w.grad.data.zero_()</span><br><span class="line">                b.grad.data.zero_()</span><br><span class="line">            l.backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())</span><br><span class="line">        test_ls.append(loss(net(test_features, w, b), test_labels).mean().item())</span><br><span class="line">    d2l.semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">                 range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'L2 norm of w:'</span>, w.norm().item())</span><br></pre></td></tr></table></figure>
<h5 id="对比过拟合与权重衰减处理"><a href="#对比过拟合与权重衰减处理" class="headerlink" title="对比过拟合与权重衰减处理"></a>对比过拟合与权重衰减处理</h5><blockquote>
<p>过拟合</p>
<blockquote>
<p>fit_and_plot(lambd=0)<br><img src="https://img-blog.csdnimg.cn/20200216172039167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></p>
</blockquote>
</blockquote>
<blockquote>
<p>权重衰减处理</p>
<blockquote>
<p>fit_and_plot(lambd=3)<br><img src="https://img-blog.csdnimg.cn/20200216172215405.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></p>
</blockquote>
</blockquote>
<h4 id="权重衰减的简化"><a href="#权重衰减的简化" class="headerlink" title="权重衰减的简化"></a>权重衰减的简化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot_pytorch</span><span class="params">(wd)</span>:</span></span><br><span class="line">    <span class="comment"># 对权重参数衰减。权重名称一般是以weight结尾</span></span><br><span class="line">    net = nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.weight, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.bias, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) <span class="comment"># 对权重参数衰减</span></span><br><span class="line">    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  <span class="comment"># 不对偏差参数衰减</span></span><br><span class="line">    </span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X), y).mean()</span><br><span class="line">            optimizer_w.zero_grad()</span><br><span class="line">            optimizer_b.zero_grad()</span><br><span class="line">            </span><br><span class="line">            l.backward()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差</span></span><br><span class="line">            optimizer_w.step()</span><br><span class="line">            optimizer_b.step()</span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).mean().item())</span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).mean().item())</span><br><span class="line">    d2l.semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">                 range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'L2 norm of w:'</span>, net.weight.data.norm().item())</span><br></pre></td></tr></table></figure>
<blockquote>
<p>训练结果与手动实现基本一致</p>
</blockquote>
<h3 id="丢弃法–过拟合"><a href="#丢弃法–过拟合" class="headerlink" title="丢弃法–过拟合"></a>丢弃法–过拟合</h3><blockquote>
<p>只能用于模型训练</p>
</blockquote>
<p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104287595" target="_blank" rel="noopener">Multilayer Perceptron &amp; Classify image</a> 中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \ldots, 5$）的计算表达式为</p>
<p>$$<br> h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right)<br>$$</p>
<p>这里$\phi$是激活函数，$x_1, \ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \ldots, w_{4i}$，偏差参数为$b_i$。</p>
<p>当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。</p>
<p>丢弃概率是丢弃法的超参数。具体来说，设随机变量$\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i’$</p>
<p>$$<br> h_i’ = \frac{\xi_i}{1-p} h_i<br>$$</p>
<p>由于$E(\xi_i) = 1-p$，因此</p>
<p>$$<br> E(h_i’) = \frac{E(\xi_i)}{1-p}h_i = h_i<br>$$</p>
<p>即丢弃法不改变其输入的期望值。</p>
<p>对之前多层感知机的神经网络中的隐藏层使用丢弃法，一种可能的结果如图所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \ldots, h_5$中的任一个。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpkNjlpbjNtLnBuZw?x-oss-process=image/format,png" /></center>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h4 id="丢弃函数"><a href="#丢弃函数" class="headerlink" title="丢弃函数"></a>丢弃函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">‘’‘</span><br><span class="line">:param:drop_prob:丢失率</span><br><span class="line">:param:keep_prob:保存率</span><br><span class="line">’‘’</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(X, drop_prob)</span>:</span></span><br><span class="line">    X = X.float()</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= drop_prob &lt;= <span class="number">1</span></span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    <span class="comment"># 这种情况下把全部元素都丢弃 keep_prob 是保存率</span></span><br><span class="line">    <span class="keyword">if</span> keep_prob == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    mask = (torch.rand(X.shape) &lt; keep_prob).float()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mask * X / keep_prob</span><br></pre></td></tr></table></figure>

<h4 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 参数的初始化</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b1 = torch.zeros(num_hiddens1, requires_grad=<span class="literal">True</span>)</span><br><span class="line">W2 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b2 = torch.zeros(num_hiddens2, requires_grad=<span class="literal">True</span>)</span><br><span class="line">W3 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b3 = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2, W3, b3]</span><br></pre></td></tr></table></figure>

<h4 id="添加丢弃层"><a href="#添加丢弃层" class="headerlink" title="添加丢弃层"></a>添加丢弃层</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">drop_prob1, drop_prob2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X, is_training=True)</span>:</span></span><br><span class="line">    X = X.view(<span class="number">-1</span>, num_inputs)</span><br><span class="line">    H1 = (torch.matmul(X, W1) + b1).relu()</span><br><span class="line">    <span class="keyword">if</span> is_training:  <span class="comment"># 只在训练模型时使用丢弃法</span></span><br><span class="line">        H1 = dropout(H1, drop_prob1)  <span class="comment"># 在第一层全连接后添加丢弃层</span></span><br><span class="line">    H2 = (torch.matmul(H1, W2) + b2).relu()</span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">        H2 = dropout(H2, drop_prob2)  <span class="comment"># 在第二层全连接后添加丢弃层</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(H2, W3) + b3</span><br></pre></td></tr></table></figure>

<h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net)</span>:</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">if</span> isinstance(net, torch.nn.Module):</span><br><span class="line">            net.eval()  <span class="comment"># 评估模式, 这会关闭dropout</span></span><br><span class="line">            acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">            net.train()  <span class="comment"># 改回训练模式</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 自定义的模型</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="string">'is_training'</span> <span class="keyword">in</span> net.__code__.co_varnames):  <span class="comment"># 如果有is_training这个参数</span></span><br><span class="line">                <span class="comment"># 将is_training设置成False</span></span><br><span class="line">                acc_sum += (net(X, is_training=<span class="literal">False</span>).argmax(dim=<span class="number">1</span>)</span><br><span class="line">                            == y).float().sum().item()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br></pre></td></tr></table></figure>
<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs, lr, batch_size = <span class="number">5</span>, <span class="number">100.0</span>, <span class="number">256</span>  <span class="comment"># 这里的学习率设置的很大，原因与之前相同。</span></span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(</span><br><span class="line">    batch_size, root=<span class="string">'path to FashionMNIST2065'</span>)</span><br><span class="line">d2l.train_ch3(</span><br><span class="line">    net,</span><br><span class="line">    train_iter,</span><br><span class="line">    test_iter,</span><br><span class="line">    loss,</span><br><span class="line">    num_epochs,</span><br><span class="line">    batch_size,</span><br><span class="line">    params,</span><br><span class="line">    lr)</span><br></pre></td></tr></table></figure>
<h4 id="丢弃法的简化"><a href="#丢弃法的简化" class="headerlink" title="丢弃法的简化"></a>丢弃法的简化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens1),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob1),</span><br><span class="line">        nn.Linear(num_hiddens1, num_hiddens2), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob2),</span><br><span class="line">        nn.Linear(num_hiddens2, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    nn.init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>

<center><b>综上<b/><center/><br/>

<p>对于训练误差较低但是泛化误差依然较高，二者相差较大的过拟合 </p>
<ul>
<li>权重衰减</li>
<li>丢弃法</li>
<li>更加全面的方案【<a href="https://www.cnblogs.com/XDU-Lakers/p/10536101.html" target="_blank" rel="noopener">https://www.cnblogs.com/XDU-Lakers/p/10536101.html</a>】</li>
</ul>
<p>对于模型无法达到一个较低的误差的欠拟合</p>
<ul>
<li><p>增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间</p>
</li>
<li><p>添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强</p>
</li>
<li><p>减少正则化参数，正则化的目的是用来防止过拟合的，但是模型出现了欠拟合，则需要减少正则化参数</p>
</li>
<li><p>使用非线性模型，比如核SVM 、决策树、深度学习等模型</p>
</li>
<li><p>调整模型的容量(capacity)，通俗地，模型的容量是指其拟合各种函数的能力</p>
</li>
<li><p>容量低的模型可能很难拟合训练集；使用集成学习方法，如Bagging ,将多个弱学习器Bagging</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>拟合现象</tag>
        <tag>训练误差</tag>
        <tag>泛化误差</tag>
        <tag>L2范数正则化</tag>
      </tags>
  </entry>
  <entry>
    <title>ModernRNN</title>
    <url>/2020/02/28/ModernRNN/</url>
    <content><![CDATA[<p>在循环神经网络的基础上进行了 RNN 的改进，我将介绍四种进化版的循环神经网络</p>
<ol>
<li>GRU</li>
<li>LSTM</li>
<li>深度循环神经网络</li>
<li>双向循环神经网络</li>
</ol>
<blockquote>
<p>循环神经网络初识：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104307813</a><br/><br>RNN 出现的梯度爆炸和梯度衰减问题</p>
<blockquote>
<p>解决梯度爆炸的裁剪梯度方法：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104307813</a></p>
</blockquote>
</blockquote>
<blockquote>
<p>解决梯度衰减问题</p>
</blockquote>
<h2 id="GRU-RNN"><a href="#GRU-RNN" class="headerlink" title="GRU RNN"></a>GRU RNN</h2><p>称为 [ 门控循环神经网络 ] ：通过捕捉时间序列中时间步较大的依赖关系</p>
<p>对比  <code>普通神经网络</code> 与 <code>GRU</code></p>
<center>
    <img src="https://img-blog.csdnimg.cn/20200216134327427.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70"/>
</center>
<br/>
<center>
    <img src="https://img-blog.csdnimg.cn/20200216134348404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70"/>
</center>

<p>• 重置⻔ : 有助于捕捉时间序列⾥短期的依赖关系;<br/><br>• 更新⻔ : 有助于捕捉时间序列⾥⻓期的依赖关系。  </p>
<hr>
<p><strong>参数详释</strong><br/><br>根据参数理解需要初始化的参数，首先表达式中的权重和偏置，6个W和3个b，是更新门、重置门和候选隐藏状态的初始化，紧接着作为下一个 GRU 的 $H_{t-1}$ ,此阶段输出时需要初始化输出层参数。</p>
<p>那么假如是第一层这样没有再上一层的$H_{t-1}$输入,就需要初始化最初的状态</p>
<hr>
<h3 id="实践中理解设计"><a href="#实践中理解设计" class="headerlink" title="实践中理解设计"></a>实践中理解设计</h3><blockquote>
<p>尽管一个 nn.GRU 包揽全盘，但是为了理解 GRU 的设计…</p>
</blockquote>
<h4 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line"><span class="comment"># print('will use', device)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32) <span class="comment">#正态分布</span></span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line">     </span><br><span class="line">    W_xz, W_hz, b_z = _three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = _three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = _three()  <span class="comment"># 候选隐藏状态参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_gru_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span>   <span class="comment">#隐藏状态初始化</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>
<h4 id="GRU-模型"><a href="#GRU-模型" class="headerlink" title="GRU 模型"></a>GRU 模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gru</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh(torch.matmul(X, W_xh) + R * torch.matmul(H, W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H,)</span><br></pre></td></tr></table></figure>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><blockquote>
<p>较 GRUB，LSTM 多了记忆功能，也就是结构上多了个记忆细胞</p>
</blockquote>
<p>包含三个门，引入了记忆细胞，和隐藏状态相似，用来记忆额外的信息</p>
<center>
<img src="https://img-blog.csdnimg.cn/20200216140544366.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" />
</center>

<center>长短期记忆 ( long short-term memory )</center><br />

<ul>
<li>遗忘门:控制上一时间步的记忆细胞 </li>
<li>输入门:控制当前时间步的输入  </li>
<li>输出门:控制从记忆细胞到隐藏状态  </li>
<li>记忆细胞：⼀种特殊的隐藏状态的信息的流动  </li>
</ul>
<h3 id="实践中理解设计-1"><a href="#实践中理解设计-1" class="headerlink" title="实践中理解设计"></a>实践中理解设计</h3><h4 id="初始化参数-1"><a href="#初始化参数-1" class="headerlink" title="初始化参数"></a>初始化参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">print(<span class="string">'will use'</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    W_xi, W_hi, b_i = _three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = _three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = _three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = _three()  <span class="comment"># 候选记忆细胞参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), </span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure>

<h4 id="LSTM-模型"><a href="#LSTM-模型" class="headerlink" title="LSTM 模型"></a>LSTM 模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">print(<span class="string">'will use'</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    W_xi, W_hi, b_i = _three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = _three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = _three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = _three()  <span class="comment"># 候选记忆细胞参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), </span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure>

<h2 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h2><blockquote>
<p>深度代表高度，对于神经网络隐藏层来说，并非如此，层数的加深会导致收敛困难</p>
</blockquote>
<p>对比循环神经网络：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104307813</a></p>
<div align=center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWprbTB2NDRpLnBuZw?x-oss-process=image/format,png" /></div>
深度循环神经网络就是多了隐藏层
<center><img src="https://img-blog.csdnimg.cn/20200216141700404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h4 id="实现方式相同，改变的是-num-layer"><a href="#实现方式相同，改变的是-num-layer" class="headerlink" title="实现方式相同，改变的是 num_layer"></a>实现方式相同，改变的是 num_layer</h4><h2 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h2><blockquote>
<p>常用于 NLP</p>
<blockquote>
<p>特点：预测不再仅依赖于前面的元素，而是同时结合了前后元素，一个词：content <br/><br>两层隐藏层之间的连接采用了concat的方式</p>
</blockquote>
</blockquote>
<center><img src="https://img-blog.csdnimg.cn/20200216143253637.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h4 id="改变的是-bidirectional-参数"><a href="#改变的是-bidirectional-参数" class="headerlink" title="改变的是 bidirectional 参数"></a>改变的是 bidirectional 参数</h4>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>梯度现象</tag>
        <tag>GRU</tag>
        <tag>LSTM</tag>
        <tag>深度循环神经网络</tag>
        <tag>双向循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Language Model &amp; Data Sampling</title>
    <url>/2020/02/28/Language%20Model%20&amp;%20Data%20Sampling/</url>
    <content><![CDATA[<h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><p>一段自然语言文本可以看作是一个离散时间序列，给定一个长度为$T$的词的序列$w_1, w_2, \ldots, w_T$。</p>
<p>语言模型的目标就是评估该序列是否合理，即计算该序列的概率：</p>
<p>$$<br>P(w_1, w_2, \ldots, w_T).<br>$$<br>假设序列$w_1, w_2, \ldots, w_T$中的每个词是依次生成的，则有</p>
<p>$$<br>\begin{aligned}<br>P(w_1, w_2, \ldots, w_T)<br>&amp;= \prod_{t=1}^T P(w_t \mid w_1, \ldots, w_{t-1})\\<br>&amp;= P(w_1)P(w_2 \mid w_1) \cdots P(w_T \mid w_1w_2\cdots w_{T-1})<br>\end {aligned}<br>$$<br>例如，一段含有4个词的文本序列的概率</p>
<p>$$<br>P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3).<br>$$</p>
<p>语言模型的参数就是词的概率以及给定前几个词情况下的条件概率。设训练数据集为一个大型文本语料库，如维基百科的所有条目.</p>
<p>词的概率可以通过该词在训练数据集中的相对词频来计算，例如，$w_1$的概率可以计算为：</p>
<p>$$<br>\hat P(w_1) = \frac{n(w_1)}{n}<br>$$</p>
<blockquote>
<p>其中$n(w_1)$为语料库中以$w_1$作为第一个词的文本的数量，$n$为语料库中文本的总数量。</p>
</blockquote>
<p>类似的，给定$w_1$情况下，$w_2$的条件概率可以计算为：</p>
<p>$$<br>\hat P(w_2 \mid w_1) = \frac{n(w_1, w_2)}{n(w_1)}<br>$$</p>
<blockquote>
<p>其中$n(w_1, w_2)$为语料库中以$w_1$作为第一个词，$w_2$作为第二个词的文本的数量。</p>
</blockquote>
<hr>
<center>加点料 < 统计学知识 ></center>

<h2 id="n-元语法"><a href="#n-元语法" class="headerlink" title="n 元语法"></a>n 元语法</h2><p>序列长度增加，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设简化模型，马尔科夫假设是指一个词的出现只与前面$n$个词相关，即$n$阶马尔可夫链（Markov chain of order $n$），如果$n=1$，那么有$P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)$。基于$n-1$阶马尔可夫链，我们可以将语言模型改写为</p>
<p>$$<br>P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1}) .<br>$$</p>
<p>以上也叫$n$元语法（$n$-grams），它是基于$n - 1$阶马尔可夫链的概率语言模型。例如，当$n=2$时，含有4个词的文本序列的概率就可以改写为:</p>
<p>$$<br>\begin{aligned}<br>P(w_1, w_2, w_3, w_4)<br>&amp;= P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3)\\<br>&amp;= P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3)<br>\end{aligned}<br>$$</p>
<p>当$n$分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列$w_1, w_2, w_3, w_4$在一元语法、二元语法和三元语法中的概率分别为</p>
<p>$$<br>\begin{aligned}<br>P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\<br>P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3) ,\\<br>P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_2, w_3) .<br>\end{aligned}<br>$$</p>
<p>当$n$较小时，$n$元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当$n$较大时，$n$元语法需要计算并存储大量的词频和多词相邻频率。</p>
<hr>
<blockquote>
<p>深入理解元语法的缺陷</p>
</blockquote>
<ul>
<li><p>参数空间过大 </p>
<p>  n 元语法当 n 足够大的时候词频和使用频率的计算会越来越大</p>
</li>
<li><p>数据稀疏 </p>
<p>  齐夫定律：按频率递减顺序排列的频率词表中，单词的频率与它的序号之间存在“幂律”(power law)关系，即如果把单词按使用频率排序，那么使用频率与序号之间几乎恰好成反比。</p>
</li>
</ul>
<blockquote>
<p>在缺陷的基础上寻找问题的解决办法</p>
<blockquote>
<p>数据采样</p>
</blockquote>
<p>随机采样 &amp;&amp; 相邻采样</p>
</blockquote>
<h3 id="引入数据集"><a href="#引入数据集" class="headerlink" title="引入数据集"></a>引入数据集</h3><blockquote>
<p>利用周杰伦的歌词作为数据集 jaychou_lyrics.txt</p>
<blockquote>
<p><a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/data/jaychou_lyrics.txt.zip?raw=true" target="_blank" rel="noopener">下载地址</a></p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># read data</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'path to jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    corpus_chars = f.read()</span><br><span class="line">print(len(corpus_chars))</span><br><span class="line">print(corpus_chars[: <span class="number">40</span>])</span><br><span class="line">corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">corpus_chars = corpus_chars[: <span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># build character index</span></span><br><span class="line">idx_to_char = list(set(corpus_chars)) <span class="comment"># 去重，得到索引到字符的映射</span></span><br><span class="line">char_to_idx = &#123;char: i <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)&#125; <span class="comment"># 字符到索引的映射 enumerate枚举</span></span><br><span class="line">vocab_size = len(char_to_idx)</span><br><span class="line">print(vocab_size)</span><br><span class="line">corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]  <span class="comment"># 将每个字符转化为索引，得到一个索引的序列</span></span><br><span class="line">sample = corpus_indices[: <span class="number">20</span>]</span><br><span class="line">print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample])) <span class="comment">#join 进行字符的拼接</span></span><br><span class="line">print(<span class="string">'indices:'</span>, sample)</span><br></pre></td></tr></table></figure>

<h3 id="数据采样"><a href="#数据采样" class="headerlink" title="数据采样"></a>数据采样</h3><p>在训练中我们需要每次随机读取小批量样本和标签。时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”，即$X$=“想要有直升”，$Y$=“要有直升机”。</p>
<p>现在我们考虑序列“想要有直升机，想要和你飞到宇宙去”，如果时间步数为5，有以下可能的样本和标签：</p>
<ul>
<li>$X$：“想要有直升”，$Y$：“要有直升机”</li>
<li>$X$：“要有直升机”，$Y$：“有直升机，”</li>
<li>$X$：“有直升机，”，$Y$：“直升机，想”</li>
<li>…</li>
<li>$X$：“要和你飞到”，$Y$：“和你飞到宇”</li>
<li>$X$：“和你飞到宇”，$Y$：“你飞到宇宙”</li>
<li>$X$：“你飞到宇宙”，$Y$：“飞到宇宙去”</li>
</ul>
<p>可以看到，如果序列的长度为$T$，时间步数为$n$，那么一共有$T-n$个合法的样本，但是这些样本有大量的重合，我们通常采用更加高效的采样方式。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。</p>
<h3 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h3><p>下面的代码每次从数据里随机采样一个小批量。其中批量大小<code>batch_size</code>是每个小批量的样本数，<code>num_steps</code>是每个样本所包含的时间步数。<br>在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。</p>
<div align=center>
<img src= "https://img-blog.csdnimg.cn/20200214095637385.png" />
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, device=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为对于长度为n的序列，X最多只有包含其中的前n - 1个字符</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps  <span class="comment"># 下取整，得到不重叠情况下的样本个数</span></span><br><span class="line">    example_indices = [i * num_steps <span class="keyword">for</span> i <span class="keyword">in</span> range(num_examples)]  <span class="comment"># 每个样本的第一个字符在corpus_indices中的下标</span></span><br><span class="line">    random.shuffle(example_indices) <span class="comment">#因为做随机采样，shuffle进行捣乱</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(i)</span>:</span></span><br><span class="line">        <span class="comment"># 返回从i开始的长为num_steps的序列</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[i: i + num_steps]</span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        <span class="comment"># 每次选出batch_size个随机样本</span></span><br><span class="line">        batch_indices = example_indices[i: i + batch_size]  <span class="comment"># 当前batch的各个样本的首字符的下标</span></span><br><span class="line">        X = [_data(j) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X, device=device), torch.tensor(Y, device=device)</span><br></pre></td></tr></table></figure>

<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_seq = list(range(<span class="number">30</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">6</span>):</span><br><span class="line">    print(<span class="string">'X: '</span>, X, <span class="string">'\nY:'</span>, Y, <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">X:  tensor([[ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">        [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">        [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]]) </span><br><span class="line"></span><br><span class="line">X:  tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [<span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="相邻采样"><a href="#相邻采样" class="headerlink" title="相邻采样"></a>相邻采样</h3><p>在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。</p>
<div align=center> <img src="https://img-blog.csdnimg.cn/20200214101159177.png" /></div>

<blockquote>
<p>三部分堆叠构成二维的 tensor</p>
</blockquote>
<div align=center><img src="https://img-blog.csdnimg.cn/20200214101954759.png" /></div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, device=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">    corpus_len = len(corpus_indices) // batch_size * batch_size  <span class="comment"># 保留下来的序列的长度</span></span><br><span class="line">    corpus_indices = corpus_indices[: corpus_len]  <span class="comment"># 仅保留前corpus_len个字符</span></span><br><span class="line">    indices = torch.tensor(corpus_indices, device=device)</span><br><span class="line">    indices = indices.view(batch_size, <span class="number">-1</span>)  <span class="comment"># resize成(batch_size, )</span></span><br><span class="line">    batch_num = (indices.shape[<span class="number">1</span>] - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_num):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i: i + num_steps] <span class="comment">#构建索引 Ｘ是样本</span></span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>: i + num_steps + <span class="number">1</span>]　<span class="comment"># Y 是标签</span></span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure>

<h4 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter_consecutive(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">6</span>):</span><br><span class="line">    print(<span class="string">'X: '</span>, X, <span class="string">'\nY:'</span>, Y, <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">X:  tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>]]) </span><br><span class="line"></span><br><span class="line">X:  tensor([[ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">        [<span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">        [<span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>]])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>语言模型</tag>
        <tag>概率论</tag>
        <tag>元语法</tag>
        <tag>数据采样</tag>
      </tags>
  </entry>
  <entry>
    <title>Text Preprocessing</title>
    <url>/2020/02/28/Text%20Preprocessing/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>打开 Google， 输入搜索关键词，显示上百条搜索结果</p>
</blockquote>
<blockquote>
<p>打开 Google Translate， 输入待翻译文本，翻译结果框中显示出翻译结果</p>
</blockquote>
<p>以上二者的共同点便是文本预处理 Pre-Processing</p>
</blockquote>
<p>在 NLP 项目中，文本预处理占据了超过半数的时间，其重要性不言而喻。</p>
<hr>
<div align=center>
当然<br>
也可以利用完备且效率可观的工具可以快速完成项目
</div>
For Example： 我一直在使用的由 哈工大社会计算与信息检索研究中心开发的 （LTP，Language Technology Platform ）语言技术平台

<hr>
<h1 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h1><p>文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：</p>
<p>【较为简单的步骤，如果需要专门做NLP相关的时候，要进行特殊的预处理】</p>
<ol>
<li>读入文本</li>
<li>分词</li>
<li>建立字典，将每个词映射到一个唯一的索引（index）</li>
<li>将文本从词的序列转换为索引的序列，方便输入模型</li>
</ol>
<h2 id="中英文文本预处理的特点"><a href="#中英文文本预处理的特点" class="headerlink" title="中英文文本预处理的特点"></a>中英文文本预处理的特点</h2><ul>
<li><p>中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般我们需要用分词算法来完成分词。</p>
</li>
<li><p>英文文本的预处理特殊在拼写问题，很多时候，对英文预处理要包括拼写检查，比如“Helo World”这样的错误，我们不能在分析的时候再去纠错。还有就是词干提取(stemming)和词形还原(lemmatization)，主要是因为英文中一个词会存在不同的形式，这个步骤有点像孙悟空的火眼金睛，直接得到单词的原始形态。For Example：” faster “、” fastest “ -&gt; “ fast “；“ leafs ”、“ leaves ” -&gt;  “ leaf “ 。</p>
</li>
</ul>
<blockquote>
<p>本文进行简要的介绍和实现<br>详细可参考：<a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/</a></p>
</blockquote>
<h3 id="读入文本"><a href="#读入文本" class="headerlink" title="读入文本"></a>读入文本</h3><p>引入数据源：<a href="http://www.gutenberg.org/ebooks/35" target="_blank" rel="noopener">http://www.gutenberg.org/ebooks/35</a>【小说 Time Machine】</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'path to timemachine.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f: <span class="comment">#每次处理一行</span></span><br><span class="line">        lines = [re.sub(<span class="string">'[^a-z]+'</span>, <span class="string">' '</span>, line.strip().lower()) <span class="keyword">for</span> line <span class="keyword">in</span> f]  <span class="comment">#正则表达式</span></span><br><span class="line">    <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(<span class="string">'# sentences %d'</span> % len(lines))</span><br></pre></td></tr></table></figure>

<h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>对每个句子进行分词，也就是将一个句子划分成若干个词（token），转换为一个词的序列。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentences, token=<span class="string">'word'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Split sentences into word or char tokens"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'word'</span>:</span><br><span class="line">        <span class="keyword">return</span> [sentence.split(<span class="string">' '</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>:</span><br><span class="line">        <span class="keyword">return</span> [list(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'ERROR: unkown token type '</span>+token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">tokens[<span class="number">0</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#分词结果:</span></span><br><span class="line">[[<span class="string">'the'</span>, <span class="string">'time'</span>, <span class="string">'machine'</span>, <span class="string">'by'</span>, <span class="string">'h'</span>, <span class="string">'g'</span>, <span class="string">'wells'</span>, <span class="string">''</span>], [<span class="string">''</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="建立字典"><a href="#建立字典" class="headerlink" title="建立字典"></a>建立字典</h3><p>为了方便模型处理，将字符串转换为数字。因此先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 构建Vocab类时注意：句子长度统计与构建字典无关 | 所以不必要进行句子长度统计</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">        counter = count_corpus(tokens)  <span class="comment">#&lt;key,value&gt;:&lt;词，词频&gt; </span></span><br><span class="line">        self.token_freqs = list(counter.items()) <span class="comment">#去重并统计词频</span></span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">            <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">            self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            self.idx_to_token += [<span class="string">'&lt;pad&gt;'</span>, <span class="string">'&lt;bos&gt;'</span>, <span class="string">'&lt;eos&gt;'</span>, <span class="string">'&lt;unk&gt;'</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.unk = <span class="number">0</span></span><br><span class="line">            self.idx_to_token += [<span class="string">'&lt;unk&gt;'</span>]</span><br><span class="line">        self.idx_to_token += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> self.token_freqs</span><br><span class="line">                        <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.idx_to_token]</span><br><span class="line">        self.token_to_idx = dict()</span><br><span class="line">        <span class="keyword">for</span> idx, token <span class="keyword">in</span> enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] = idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span> <span class="comment"># 返回字典的大小</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span>  <span class="comment"># 词到索引的映射</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span>  <span class="comment">#索引到词的映射</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    tokens = [tk <span class="keyword">for</span> st <span class="keyword">in</span> sentences <span class="keyword">for</span> tk <span class="keyword">in</span> st]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)  <span class="comment"># 返回一个字典，记录每个词的出现次数</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>&lt; unk &gt;    较为特殊，表示为登录词，无论 use_special_token 参数是否为真，都会用到</p>
</blockquote>
<h3 id="将词转为索引"><a href="#将词转为索引" class="headerlink" title="将词转为索引"></a>将词转为索引</h3><p>使用字典，我们可以将原文本中的句子从单词序列转换为索引序列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'words:'</span>, tokens[i])</span><br><span class="line">    print(<span class="string">'indices:'</span>, vocab[tokens[i]])</span><br></pre></td></tr></table></figure>
<hr>
<p>到此，按照常见步骤已经介绍完了</p>
<p>我们对于分词这一重要关卡需要考虑的更多，上边实现的简要分词许多情形还未完善，只能实现部分特定的分词</p>
<ol>
<li>标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了</li>
<li>类似“shouldn’t”, “doesn’t”这样的词会被错误地处理</li>
<li>类似”Mr.”, “Dr.”这样的词会被错误地处理</li>
</ol>
<p>一者，我们可以通过引入更复杂的规则来解决这些问题，但是事实上，有一些现有的工具可以很好地进行分词，在这里简单介绍其中的两个：<a href="https://spacy.io/" target="_blank" rel="noopener">spaCy</a>和<a href="https://www.nltk.org/" target="_blank" rel="noopener">NLTK</a>。</p>
<p>For Example :</p>
<blockquote>
<p>text = “Mr. Chen doesn’t agree with my suggestion.”</p>
</blockquote>
<h2 id="spaCy"><a href="#spaCy" class="headerlink" title="spaCy"></a>spaCy</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">doc = nlp(text)</span><br><span class="line">print([token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">[<span class="string">'Mr.'</span>, <span class="string">'Chen'</span>, <span class="string">'does'</span>, <span class="string">"n't"</span>, <span class="string">'agree'</span>, <span class="string">'with'</span>, <span class="string">'my'</span>, <span class="string">'suggestion'</span>, <span class="string">'.'</span>]</span><br></pre></td></tr></table></figure>

<h2 id="NLTK"><a href="#NLTK" class="headerlink" title="NLTK"></a>NLTK</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> data</span><br><span class="line">data.path.append(<span class="string">'/home/kesci/input/nltk_data3784/nltk_data'</span>)</span><br><span class="line">print(word_tokenize(text))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">[<span class="string">'Mr.'</span>, <span class="string">'Chen'</span>, <span class="string">'does'</span>, <span class="string">"n't"</span>, <span class="string">'agree'</span>, <span class="string">'with'</span>, <span class="string">'my'</span>, <span class="string">'suggestion'</span>, <span class="string">'.'</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>文本预处理</tag>
        <tag>spaCy</tag>
        <tag>NLTK</tag>
      </tags>
  </entry>
  <entry>
    <title>Multilayer Perceptron &amp; Classify image</title>
    <url>/2020/02/28/Multilayer%20Perceptron%20&amp;%20Classify%20image/</url>
    <content><![CDATA[<h2 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h2><p>以多层感知机为例，概述多层神经网络</p>
<h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>此图为多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。</p>
<div align=center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWhvNjg0am1oLnBuZw?x-oss-process=image/format,png"/> </div>

<h3 id="表达公式"><a href="#表达公式" class="headerlink" title="表达公式"></a>表达公式</h3><p>具体来说，给定一个小批量样本$\boldsymbol{X} \in \mathbb{R}^{n \times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层【其中隐藏单元个数为$h$】记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\boldsymbol{H}$，有$\boldsymbol{H} \in \mathbb{R}^{n \times h}$。</p>
<p>因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\boldsymbol{W}_h \in \mathbb{R}^{d \times h}$和 $\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，输出层的权重和偏差参数分别为$\boldsymbol{W}_o \in \mathbb{R}^{h \times q}$和$\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}$。</p>
<p>含单隐藏层的多层感知机的设计。其输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算为</p>
<p>$$<br> \begin{aligned} \boldsymbol{H} &amp;= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\\ \boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}<br>$$</p>
<p>也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到</p>
<p>$$<br> \boldsymbol{O} = (\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X} \boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o.<br>$$</p>
<blockquote>
<blockquote>
<p>存在的问题</p>
</blockquote>
<ul>
<li>虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络</li>
<li>其中输出层权重参数为$\boldsymbol{W}_h\boldsymbol{W}_o$，偏差参数为$\boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o$、</li>
</ul>
</blockquote>
<blockquote>
<p>结论：隐藏层未起到作用</p>
</blockquote>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><blockquote>
<blockquote>
<p>问题解释</p>
</blockquote>
<p>全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。</p>
</blockquote>
<blockquote>
<blockquote>
<p>解决方法</p>
</blockquote>
<p>引入非线性变换<br>Example:对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。<br>这个非线性函数被称为激活函数（activation function）。</p>
</blockquote>
<ul>
<li><h4 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h4>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为</li>
</ul>
<p>$$<br>\text{ReLU}(x) = \max(x, 0).<br>$$</p>
<p>可以看出，ReLU函数只保留正数元素，并将负数元素清零。</p>
<div align=center><img src=" https://img-blog.csdnimg.cn/20200228104905655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70 " /></div>


<ul>
<li><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4>sigmoid函数可以将元素的值变换到0和1之间：</li>
</ul>
<p>$$<br>\text{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.<br>$$</p>
<div align=center><img src=" https://img-blog.csdnimg.cn/20200228104942978.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70 " /> </div>

<ul>
<li><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h4>tanh（双曲正切）函数可以将元素的值变换到-1和1之间：</li>
</ul>
<p>$$<br>\text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.<br>$$</p>
<p>我们接着绘制tanh函数。当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。</p>
<div align=center><img src="https://img-blog.csdnimg.cn/20200228105017679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></div>

<h4 id="激活函数的选择"><a href="#激活函数的选择" class="headerlink" title="激活函数的选择"></a>激活函数的选择</h4><blockquote>
<blockquote>
<p>ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</p>
</blockquote>
<blockquote>
<p>用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。  </p>
</blockquote>
<blockquote>
<p>在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</p>
</blockquote>
<blockquote>
<p>在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</p>
</blockquote>
</blockquote>
<p>那么之前表达式中输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算变为：</p>
<p>$$<br> \begin{aligned} \boldsymbol{H} &amp;= \phi(\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h),\\ \boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}<br>$$</p>
<p>其中$\phi$表示激活函数。</p>
<h2 id="多层感知机实现"><a href="#多层感知机实现" class="headerlink" title="多层感知机实现"></a>多层感知机实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure>
<h3 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,</span><br><span class="line">			root=<span class="string">'path to FashionMNIST.zip'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="定义模型参数"><a href="#定义模型参数" class="headerlink" title="定义模型参数"></a>定义模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, num_hiddens)), dtype=torch.float)</span><br><span class="line">b1 = torch.zeros(num_hiddens, dtype=torch.float)</span><br><span class="line">W2 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_hiddens, num_outputs)), dtype=torch.float)</span><br><span class="line">b2 = torch.zeros(num_outputs, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="定义激活函数"><a href="#定义激活函数" class="headerlink" title="定义激活函数"></a>定义激活函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.max(input=X, other=torch.tensor(<span class="number">0.0</span>))</span><br><span class="line"><span class="comment">#进行0和Ｘ的大小比较</span></span><br></pre></td></tr></table></figure>

<h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    X = X.view((<span class="number">-1</span>, num_inputs))</span><br><span class="line">    H = relu(torch.matmul(X, W1) + b1)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(H, W2) + b2</span><br></pre></td></tr></table></figure>

<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">100.0</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="多层感知机Pytorch简化"><a href="#多层感知机Pytorch简化" class="headerlink" title="多层感知机Pytorch简化"></a>多层感知机Pytorch简化</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init model and param</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">    </span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(num_hiddens, num_outputs), </span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> params <span class="keyword">in</span> net.parameters():</span><br><span class="line">    init.normal_(params, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>

<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=<span class="string">'path to FashionMNIST.zip'</span>)</span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>MLP</tag>
        <tag>Network</tag>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>Softmax &amp; 分类模型</title>
    <url>/2020/02/28/Softmax%20&amp;%20%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h1><blockquote>
<p>与候选采样相对</p>
</blockquote>
<p>Softmax function, a wonderful activation function that turns numbers aka logits into probabilities that sum to one. Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes.</p>
<p>一种函数，可提供多类别分类模型中每个可能类别的概率。这些概率的总和正好为 1.0。</p>
<p>Example: softmax 可能会得出某个图像是狗、猫和马的概率分别是 0.9、0.08 和 0.02。（也称为完整 softmax。）<img src="https://img-blog.csdnimg.cn/20200212195050635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="softmax的基本概念"><a href="#softmax的基本概念" class="headerlink" title="softmax的基本概念"></a>softmax的基本概念</h2><ul>
<li><p>分类问题<br>一个简单的图像分类问题，输入图像的高和宽均为2像素，色彩为灰度。<br>图像中的4像素分别记为$x_1, x_2, x_3, x_4$。<br>假设真实标签为狗、猫或者鸡，这些标签对应的离散值为$y_1, y_2, y_3$。<br>我们通常使用离散的数值来表示类别，例如$y_1=1, y_2=2, y_3=3$。</p>
</li>
<li><p>权重矢量  </p>
</li>
</ul>
<p>$$<br> \begin{aligned} o_1 &amp;= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1 \end{aligned}<br>$$</p>
<p>$$<br> \begin{aligned} o_2 &amp;= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2 \end{aligned}<br>$$</p>
<p>$$<br> \begin{aligned} o_3 &amp;= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3 \end{aligned}<br>$$</p>
<ul>
<li>神经网络图<br>下图用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$o_1, o_2, o_3$的计算都要依赖于所有的输入$x_1, x_2, x_3, x_4$，softmax回归的输出层也是一个全连接层。</li>
</ul>
<div align=center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWhteW1lem9nLnBuZw?x-oss-process=image/format,png" /></div> 

<p>$$<br>\begin{aligned}softmax回归是一个单层神经网络\end{aligned}<br>$$</p>
<p>既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$当作预测类别是$i$的置信度，并将值最大的输出所对应的类作为预测输出，即输出 $\underset{i}{\arg\max} o_i$。例如，如果$o_1,o_2,o_3$分别为$0.1,10,0.1$，由于$o_2$最大，那么预测类别为2，其代表猫。</p>
<ul>
<li>输出问题<br>直接使用输出层的输出有两个问题：<ol>
<li>一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果$o_1=o_3=10^3$，那么输出值10却又表示图像类别为猫的概率很低。</li>
<li>另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。</li>
</ol>
</li>
</ul>
<p>softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：</p>
<p>$$<br> \hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3)<br>$$</p>
<p>其中</p>
<p>$$<br> \hat{y}1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.<br>$$</p>
<p>容易看出$\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$且$0 \leq \hat{y}_1, \hat{y}_2, \hat{y}_3 \leq 1$，因此$\hat{y}_1, \hat{y}_2, \hat{y}_3$是一个合法的概率分布。这时候，如果$\hat{y}_2=0.8$，不管$\hat{y}_1$和$\hat{y}_3$的值是多少，我们都知道图像类别为猫的概率是80%。此外，我们注意到</p>
<p>$$<br> \underset{i}{\arg\max} o_i = \underset{i}{\arg\max} \hat{y}_i<br>$$</p>
<p>因此softmax运算不改变预测类别输出。</p>
<ul>
<li>计算效率<ul>
<li>单样本矢量计算表达式<br>为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为</li>
</ul>
</li>
</ul>
<p>$$<br> \boldsymbol{W} = \begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} \\ w_{21} &amp; w_{22} &amp; w_{23} \\ w_{31} &amp; w_{32} &amp; w_{33} \\ w_{41} &amp; w_{42} &amp; w_{43} \end{bmatrix},\quad \boldsymbol{b} = \begin{bmatrix} b_1 &amp; b_2 &amp; b_3 \end{bmatrix},<br>$$</p>
<p>设高和宽分别为2个像素的图像样本$i$的特征为</p>
<p>$$<br>\boldsymbol{x}^{(i)} = \begin{bmatrix}x_1^{(i)} &amp; x_2^{(i)} &amp; x_3^{(i)} &amp; x_4^{(i)}\end{bmatrix},<br>$$</p>
<p>输出层的输出为</p>
<p>$$<br>\boldsymbol{o}^{(i)} = \begin{bmatrix}o_1^{(i)} &amp; o_2^{(i)} &amp; o_3^{(i)}\end{bmatrix},<br>$$</p>
<p>预测为狗、猫或鸡的概率分布为</p>
<p>$$<br>\boldsymbol{\hat{y}}^{(i)} = \begin{bmatrix}\hat{y}_1^{(i)} &amp; \hat{y}_2^{(i)} &amp; \hat{y}_3^{(i)}\end{bmatrix}.<br>$$</p>
<p>softmax回归对样本$i$分类的矢量计算表达式为</p>
<p>$$<br> \begin{aligned} \boldsymbol{o}^{(i)} &amp;= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{y}}^{(i)} &amp;= \text{softmax}(\boldsymbol{o}^{(i)}). \end{aligned}<br>$$</p>
<ul>
<li>小批量矢量计算表达式<br>  为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为$n$，输入个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$\boldsymbol{X} \in \mathbb{R}^{n \times d}$。假设softmax回归的权重和偏差参数分别为$\boldsymbol{W} \in \mathbb{R}^{d \times q}$和$\boldsymbol{b} \in \mathbb{R}^{1 \times q}$。softmax回归的矢量计算表达式为</li>
</ul>
<p>$$<br> \begin{aligned} \boldsymbol{O} &amp;= \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{Y}} &amp;= \text{softmax}(\boldsymbol{O}), \end{aligned}<br>$$</p>
<p>其中的加法运算使用了广播机制，$\boldsymbol{O}, \boldsymbol{\hat{Y}} \in \mathbb{R}^{n \times q}$且这两个矩阵的第$i$行分别为样本$i$的输出$\boldsymbol{o}^{(i)}$和概率分布$\boldsymbol{\hat{y}}^{(i)}$。</p>
<hr>
<p>两种操作对比</p>
<blockquote>
<p>numpy 操作：np.exp(x) / np.sum(np.exp(x), axis=0)<br>pytorch 操作：torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1,1)</p>
</blockquote>
<h2 id="引入Fashion-MNIST"><a href="#引入Fashion-MNIST" class="headerlink" title="引入Fashion-MNIST"></a>引入Fashion-MNIST</h2><blockquote>
<p>为方便介绍 Softmax， 为了更加直观的观察到算法之间的差异<br>引入较为复杂的多分类图像分类数据集</p>
<blockquote>
<p>导入：torchvision 包【构建计算机视觉模型】</p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import needed package</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(torch.__version__)</span></span><br><span class="line"><span class="comment"># print(torchvision.__version__)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get dataset</span></span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">'path to file storge FashionMNIST.zip'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">'path to file storge FashionMNIST.zip'</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fashion_mnist_labels</span><span class="params">(labels)</span>:</span></span><br><span class="line">    text_labels = [<span class="string">'t-shirt'</span>, <span class="string">'trouser'</span>, <span class="string">'pullover'</span>, <span class="string">'dress'</span>, <span class="string">'coat'</span>,</span><br><span class="line">                   <span class="string">'sandal'</span>, <span class="string">'shirt'</span>, <span class="string">'sneaker'</span>, <span class="string">'bag'</span>, <span class="string">'ankle boot'</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[int(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_fashion_mnist</span><span class="params">(images, labels)</span>:</span></span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    <span class="comment"># 这里的_表示我们忽略（不使用）的变量</span></span><br><span class="line">    _, figs = plt.subplots(<span class="number">1</span>, len(images), figsize=(<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> f, img, lbl <span class="keyword">in</span> zip(figs, images, labels):</span><br><span class="line">        f.imshow(img.view((<span class="number">28</span>, <span class="number">28</span>)).numpy())</span><br><span class="line">        f.set_title(lbl)</span><br><span class="line">        f.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        f.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">X, y = [], []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    X.append(mnist_train[i][<span class="number">0</span>]) <span class="comment"># 将第i个feature加到X中</span></span><br><span class="line">    y.append(mnist_train[i][<span class="number">1</span>]) <span class="comment"># 将第i个label加到y中</span></span><br><span class="line">show_fashion_mnist(X, get_fashion_mnist_labels(y))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># read data</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">num_workers = <span class="number">4</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line">print(<span class="string">'%.2f sec'</span> % (time.time() - start))</span><br></pre></td></tr></table></figure>

<h2 id="Softmax-手动实现"><a href="#Softmax-手动实现" class="headerlink" title="Softmax 手动实现"></a>Softmax 手动实现</h2><h3 id="import-package-and-module"><a href="#import-package-and-module" class="headerlink" title="import package and module"></a>import package and module</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br><span class="line">print(torchvision.__version__)</span><br></pre></td></tr></table></figure>
<h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>

<h3 id="模型参数初始化"><a href="#模型参数初始化" class="headerlink" title="模型参数初始化"></a>模型参数初始化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init module param </span></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">print(<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, num_outputs)), dtype=torch.float)</span><br><span class="line">b = torch.zeros(num_outputs, dtype=torch.float)</span><br><span class="line">W.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line">b.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Sofmax-定义"><a href="#Sofmax-定义" class="headerlink" title="Sofmax 定义"></a>Sofmax 定义</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define softmax function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></span><br><span class="line">    X_exp = X.exp()</span><br><span class="line">    partition = X_exp.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># print("X size is ", X_exp.size())</span></span><br><span class="line">    <span class="comment"># print("partition size is ", partition, partition.size())</span></span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure>

<h3 id="softmax-回归模型"><a href="#softmax-回归模型" class="headerlink" title="softmax 回归模型"></a>softmax 回归模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define regression model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> softmax(torch.mm(X.view((<span class="number">-1</span>, num_inputs)), W) + b)</span><br></pre></td></tr></table></figure>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat.gather(<span class="number">1</span>, y.view(<span class="number">-1</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="准确率"><a href="#准确率" class="headerlink" title="准确率"></a>准确率</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat.argmax(dim=<span class="number">1</span>) == y).float().mean().item()</span><br></pre></td></tr></table></figure>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch3</span><span class="params">(net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">              params=None, lr=None, optimizer=None)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y).sum()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">            <span class="keyword">elif</span> params <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line">            </span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                d2l.sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                optimizer.step() </span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            train_l_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).sum().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)</span><br></pre></td></tr></table></figure>
<h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = iter(test_iter).next()</span><br><span class="line"></span><br><span class="line">true_labels = d2l.get_fashion_mnist_labels(y.numpy())</span><br><span class="line">pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=<span class="number">1</span>).numpy())</span><br><span class="line">titles = [true + <span class="string">'\n'</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> zip(true_labels, pred_labels)]</span><br><span class="line"></span><br><span class="line">d2l.show_fashion_mnist(X[<span class="number">0</span>:<span class="number">9</span>], titles[<span class="number">0</span>:<span class="number">9</span>])</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Pytorch-改进"><a href="#Pytorch-改进" class="headerlink" title="Pytorch 改进"></a>Pytorch 改进</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<h3 id="初始化参数和获取数据"><a href="#初始化参数和获取数据" class="headerlink" title="初始化参数和获取数据"></a>初始化参数和获取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init param and get data</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<h3 id="定义网络模型"><a href="#定义网络模型" class="headerlink" title="定义网络模型"></a>定义网络模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_inputs, num_outputs)</span>:</span></span><br><span class="line">        super(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(num_inputs, num_outputs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># x 的形状: (batch, 1, 28, 28)</span></span><br><span class="line">        y = self.linear(x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line"><span class="comment"># net = LinearNet(num_inputs, num_outputs)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FlattenLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(FlattenLayer, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># x 的形状: (batch, *, *, ...)</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        <span class="comment"># FlattenLayer(),</span></span><br><span class="line">        <span class="comment"># LinearNet(num_inputs, num_outputs) </span></span><br><span class="line">        OrderedDict([</span><br><span class="line">           (<span class="string">'flatten'</span>, FlattenLayer()),</span><br><span class="line">           (<span class="string">'linear'</span>, nn.Linear(num_inputs, num_outputs))]) <span class="comment"># 或者写成我们自己定义的 LinearNet(num_inputs, num_outputs) 也可以</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init module param</span></span><br><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net.linear.bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss() <span class="comment"># 下面是他的函数原型</span></span><br><span class="line"><span class="comment"># class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')</span></span><br></pre></td></tr></table></figure>

<h3 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>) <span class="comment"># 下面是函数原型</span></span><br><span class="line"><span class="comment"># class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)</span></span><br></pre></td></tr></table></figure>

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>

<h3 id="训练结果分析"><a href="#训练结果分析" class="headerlink" title="训练结果分析"></a>训练结果分析</h3><p><img src="https://img-blog.csdnimg.cn/20200212213623439.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>最开始：训练数据集上的准确率低于测试数据集上的准确率</p>
</blockquote>
<blockquote>
<blockquote>
<p>原因</p>
</blockquote>
<p>训练集上的准确率是在一个epoch的过程中计算得到的<br>测试集上的准确率是在一个epoch结束后计算得到的<br>Result: 后者的模型参数更优</p>
</blockquote>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Softmax</tag>
        <tag>classify</tag>
        <tag>Fashion-MNIST</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title>Design and Realization of Linear Regression</title>
    <url>/2020/02/28/Design%20and%20Realization%20of%20Linear%20Regression/</url>
    <content><![CDATA[<h2 id="线性回归的基本要素"><a href="#线性回归的基本要素" class="headerlink" title="线性回归的基本要素"></a>线性回归的基本要素</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>$$<br>\mathrm{y} = w \cdot \mathrm{x} + b<br>$$</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们在收集到的数据中寻找合适的模型参数来使模型的预测价格与真实价格的误差最小。被训练的数据的集合称为训练数据集（training data set）或训练集（training set），每一条数据的主体作为一个样本（sample），被预测值称作标签（label），用来预测标签的因素叫作特征（feature）。特征用来表征样本的特点。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为 $i$ 的样本误差的表达式为</p>
<p>$$<br>l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,<br>$$</p>
<p>$$<br>L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.<br>$$</p>
<h3 id="优化函数-随机梯度下降"><a href="#优化函数-随机梯度下降" class="headerlink" title="优化函数 - 随机梯度下降"></a>优化函数 - 随机梯度下降</h3><p>鉴于大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值，在这种求数值解的优化算法中，通常使用小批量随机梯度下降（mini-batch stochastic gradient descent）。</p>
<blockquote>
<p>算法</p>
</blockquote>
<p>先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。   </p>
<p>$$<br>(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)<br>$$</p>
<p>学习率: $\eta$代表在每次优化中，能够学习的步长的大小<br>批量大小: $\mathcal{B}$是小批量计算中的批量大小batch size   </p>
<p>总结一下，优化函数的有以下两个步骤：</p>
<ul>
<li>初始化模型参数，一般使用随机初始化；</li>
<li>在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。</li>
</ul>
<h2 id="线性回归模型的实现"><a href="#线性回归模型的实现" class="headerlink" title="线性回归模型的实现"></a>线性回归模型的实现</h2><blockquote>
<p>以房屋为样本，价格为标签，面积和房龄为特征<br>$$<br>\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b<br>$$</p>
</blockquote>
<h3 id="导入包和模块"><a href="#导入包和模块" class="headerlink" title="导入包和模块"></a>导入包和模块</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import packages and modules</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>使用线性模型来生成数据集，生成一个 1000 个样本的数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set input feature number </span></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line"><span class="comment"># set example number</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set true weight and bias in order to generate corresponded label</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line"></span><br><span class="line">features = torch.randn(num_examples, num_inputs,</span><br><span class="line">                      dtype=torch.float32) <span class="comment">#1000*2 vector</span></span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()),</span><br><span class="line">                       dtype=torch.float32) <span class="comment">#偏差通过正态分布随机生成</span></span><br></pre></td></tr></table></figure>
<h3 id="通过图像观察生成的数据合适程度"><a href="#通过图像观察生成的数据合适程度" class="headerlink" title="通过图像观察生成的数据合适程度"></a>通过图像观察生成的数据合适程度</h3><p><img src="https://img-blog.csdnimg.cn/20200211213555899.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(batch_size, features, labels)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(indices)  <span class="comment"># random read 10 samples</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) <span class="comment"># the last time may be not enough for a whole batch</span></span><br><span class="line">        <span class="keyword">yield</span>  features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span> <span class="comment"># read 10 samples</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels): <span class="comment"># input features and labels</span></span><br><span class="line">    print(X, <span class="string">'\n'</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init parameter</span></span><br><span class="line">w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, <span class="number">1</span>)), dtype=torch.float32)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">w.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line">b.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linreg</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.mm(X, w) + b</span><br></pre></td></tr></table></figure>
<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><blockquote>
<p>均方误差损失函数<br>$$<br>l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,<br>$$</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span><span class="params">(y_hat, y)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.view(y_hat.size())) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><blockquote>
<p>小批量随机梯度下降优化</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define optimization function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span> </span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size <span class="comment"># ues .data to operate param without gradient track</span></span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># super parameters init</span></span><br><span class="line">lr = <span class="number">0.03</span> <span class="comment">#学习率</span></span><br><span class="line">num_epochs = <span class="number">5</span> <span class="comment">#训练周期</span></span><br><span class="line"></span><br><span class="line">net = linreg <span class="comment">#单层网络</span></span><br><span class="line">loss = squared_loss <span class="comment">#均方误差损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):  <span class="comment"># training repeats num_epochs times</span></span><br><span class="line">    <span class="comment"># in each epoch, all the samples in dataset will be used once</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># X is the feature and y is the label of a batch sample</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y).sum()  </span><br><span class="line">        <span class="comment"># calculate the gradient of batch sample loss </span></span><br><span class="line">        l.backward()  </span><br><span class="line">        <span class="comment"># using small batch random gradient descent to iter model parameters</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  </span><br><span class="line">        <span class="comment"># reset parameter gradient</span></span><br><span class="line">        w.grad.data.zero_()<span class="comment">#参数梯度清零，为防止叠加</span></span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, train_l.mean().item()))</span><br></pre></td></tr></table></figure>
<h3 id="检验训练结果"><a href="#检验训练结果" class="headerlink" title="检验训练结果"></a>检验训练结果</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># output result of trainning </span></span><br><span class="line">w, true_w, b, true_b</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="使用-torch-简化代码"><a href="#使用-torch-简化代码" class="headerlink" title="使用 torch 简化代码"></a>使用 torch 简化代码</h2><blockquote>
<p>未单独重写的步骤默认与上一致</p>
</blockquote>
<h3 id="读取数据集-1"><a href="#读取数据集-1" class="headerlink" title="读取数据集"></a>读取数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># combine featues and labels of dataset</span></span><br><span class="line">dataset = Data.TensorDataset(features, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># put dataset into DataLoader</span></span><br><span class="line">data_iter = Data.DataLoader(</span><br><span class="line">    dataset=dataset,            <span class="comment"># torch TensorDataset format</span></span><br><span class="line">    batch_size=batch_size,      <span class="comment"># mini batch size</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,               <span class="comment"># whether shuffle the data or not</span></span><br><span class="line">    num_workers=<span class="number">2</span>,              <span class="comment"># read data in multithreading</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">    print(X, <span class="string">'\n'</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature)</span>:</span></span><br><span class="line">        super(LinearNet, self).__init__()      <span class="comment"># call father function to init </span></span><br><span class="line">        self.linear = nn.Linear(n_feature, <span class="number">1</span>)  <span class="comment"># function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line">net = LinearNet(num_inputs) </span><br><span class="line">print(net) <span class="comment">#单层线性网络</span></span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line">init.normal_(net[<span class="number">0</span>].weight, mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net[<span class="number">0</span>].bias, val=<span class="number">0.0</span>)  <span class="comment"># or you can use `net[0].bias.data.fill_(0)` to modify it directly</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    print(param)</span><br></pre></td></tr></table></figure>

<h3 id="定义损失函数-1"><a href="#定义损失函数-1" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.MSELoss()    <span class="comment"># nn built-in squared loss function</span></span><br><span class="line">                       <span class="comment"># function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`</span></span><br></pre></td></tr></table></figure>

<h3 id="定义优化函数-1"><a href="#定义优化函数-1" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)   <span class="comment"># built-in random gradient descent function</span></span><br><span class="line">print(optimizer)  <span class="comment"># function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`</span></span><br></pre></td></tr></table></figure>
<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)   <span class="comment"># built-in random gradient descent function</span></span><br><span class="line">print(optimizer)  <span class="comment"># function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`</span></span><br></pre></td></tr></table></figure>

<h3 id="检验训练结果-1"><a href="#检验训练结果-1" class="headerlink" title="检验训练结果"></a>检验训练结果</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># result comparision</span></span><br><span class="line">dense = net[<span class="number">0</span>]</span><br><span class="line">print(true_w, dense.weight.data)</span><br><span class="line">print(true_b, dense.bias.data)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Linear Regression</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer(Google 机器翻译模型)</title>
    <url>/2020/02/26/Transformer(Goole%20%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B)/</url>
    <content><![CDATA[<center><b><font size=5>双壁合一</font></b></center>

<p><strong>卷积神经网络(CNNS)</strong></p>
<ul>
<li><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104380762" target="_blank" rel="noopener">Fundamentals of Convolutional Neural Networks</a></p>
</li>
<li><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104381637" target="_blank" rel="noopener">LeNet &amp;&amp; ModernCNN</a></p>
</li>
</ul>
<blockquote>
<p>CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。</p>
</blockquote>
<p><strong>循环神经网络(RNNS)</strong></p>
<ul>
<li><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">Fundamentals of Recurrent Neural Network</a></p>
</li>
<li><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104340035" target="_blank" rel="noopener">ModernRNN</a></p>
</li>
</ul>
<blockquote>
<p>RNNs 适合捕捉长距离变长序列的依赖，但是自身的recurrent特性却难以实现并行化处理序列。</p>
</blockquote>
<p><strong>整合CNN和RNN的优势，Vaswani et al., 2017 创新性地使用注意力机制设计了 Transformer 模型。</strong></p>
<hr>
<p>该模型利用 attention 机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的 tokens ，上述优势使得 Transformer 模型在性能优异的同时大大减少了训练时间。</p>
<hr>
<p>如图展示了 Transformer 模型的架构，与<a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">机器翻译及其相关技术介绍</a>中介绍的 seq2seq <strong>相似</strong>，Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：</p>
<ol>
<li>Transformer blocks：$循环网络_{seq2seq模型}$–&gt; Transformer Blocks<br/><br>Transform Blocks模块包含一个多头注意力层（Multi-head Attention Layers）以及两个 position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。</li>
<li>Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理<br>该层包含残差结构以及<font color=gree>层归一化</font>。</li>
<li>Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。</li>
</ol>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwYmoyY2o1LnBuZw?x-oss-process=image/format,png" alt="Fig. 10.3.1 The Transformer architecture."></p>
<p>$$<br>Transformer 架构.<br>$$</p>
<p>鉴于新子块第一次出现，在此前 CNNS 和 RNNS 的基础上，实现 Transform 子模块，并且就<a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">机器翻译及其相关技术介绍</a>中的英法翻译数据集实现一个新的机器翻译模型。</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'path to file storge d2lzh1981'</span>)</span><br><span class="line"><span class="keyword">import</span> d2l</span><br></pre></td></tr></table></figure>

<h2 id="masked-softmax"><a href="#masked-softmax" class="headerlink" title="masked softmax"></a>masked softmax</h2><blockquote>
<p>参考<a href="https://blog.csdn.net/RokoBasilisk/article/details/104369799" target="_blank" rel="noopener">$注意力机制和Seq2seq模型_{工具1}$</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">-1e6</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    X_len = X_len.to(X.device)</span><br><span class="line">    <span class="comment">#print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\n',X_len[:, None] )</span></span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float, device=X.device)</span><br><span class="line">    mask = mask[<span class="literal">None</span>, :] &lt; X_len[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment">#print(mask)</span></span><br><span class="line">    X[~mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span><span class="params">(X, valid_length)</span>:</span></span><br><span class="line">    <span class="comment"># X: 3-D tensor, valid_length: 1-D or 2-D tensor</span></span><br><span class="line">    softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> softmax(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_length.dim() == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = valid_length.reshape((<span class="number">-1</span>,))</span><br><span class="line">        <span class="comment"># fill masked elements with a large negative, whose exp is 0</span></span><br><span class="line">        X = SequenceMask(X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>])), valid_length)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> softmax(X).reshape(shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># query: (batch_size, #queries, d)</span></span><br><span class="line">    <span class="comment"># key: (batch_size, #kv_pairs, d)</span></span><br><span class="line">    <span class="comment"># value: (batch_size, #kv_pairs, dim_v)</span></span><br><span class="line">    <span class="comment"># valid_length: either (batch_size, ) or (batch_size, xx)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length=None)</span>:</span></span><br><span class="line">        d = query.shape[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># set transpose_b=True to swap the last two dimensions of key</span></span><br><span class="line">        scores = torch.bmm(query, key.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure>
<h2 id="多头注意力层"><a href="#多头注意力层" class="headerlink" title="多头注意力层"></a>多头注意力层</h2><hr>
<p>引入:<strong>自注意力（self-attention）</strong></p>
<p>自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。</p>
<p>与循环神经网络相比，自注意力对每个元素输出的<strong>计算是并行</strong>的，所以我们可以<strong>高效</strong>的实现这个模块。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwY2t2MzhxLnBuZw?x-oss-process=image/format,png" alt="Fig. 10.3.2 自注意力结构"></p>
<p>$$<br>自注意力结构<br>$$</p>
<p>$$<br>输出了一个与输入长度相同的表征序列<br>$$</p>
<hr>
<p>多头注意力层包含$h$个<strong>并行的自注意力层</strong>，每一个这种层被成为一个head。</p>
<p>对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，这$h$个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwY3NvemlkLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>多头注意力<br>$$</p>
<p>假设query，key和value的维度分别是$d_q$、$d_k$和$d_v$。那么对于每一个头$i=1,\ldots,h$，我们可以训练相应的模型权重$W_q^{(i)} \in \mathbb{R}^{p_q\times d_q}$、$W_k^{(i)} \in \mathbb{R}^{p_k\times d_k}$和$W_v^{(i)} \in \mathbb{R}^{p_v\times d_v}$，以得到每个头的输出：</p>
<p>$$<br>o^{(i)} = attention(W_q^{(i)}q, W_k^{(i)}k, W_v^{(i)}v)<br>$$</p>
<p>这里的attention可以是任意的attention function，之后我们将所有head对应的输出拼接起来，送入最后一个线性层进行整合，这个层的权重可以表示为$W_o\in \mathbb{R}^{d_0 \times hp_v}$</p>
<p>$$<br>o = W_o[o^{(1)}, \ldots, o^{(h)}]<br>$$</p>
<p>接下来实现多头注意力，假设有h个头，隐藏层权重 $hidden_size = p_q = p_k = p_v$ 与query，key，value的维度一致。除此之外，因为多头注意力层保持输入与输出张量的维度不变，所以输出feature 的维度也设置为 $d_0 = hidden_size$。</p>
<h3 id="MultiHeadAttention-class"><a href="#MultiHeadAttention-class" class="headerlink" title="MultiHeadAttention class"></a><div id ="MultiHeadAttention">MultiHeadAttention class</div></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, num_heads, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_k = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_v = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_o = nn.Linear(hidden_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length)</span>:</span></span><br><span class="line">        <span class="comment"># query, key, and value shape: (batch_size, seq_len, dim),</span></span><br><span class="line">        <span class="comment"># where seq_len is the length of input sequence</span></span><br><span class="line">        <span class="comment"># valid_length shape is either (batch_size, )</span></span><br><span class="line">        <span class="comment"># or (batch_size, seq_len).</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Project and transpose query, key, and value from</span></span><br><span class="line">        <span class="comment"># (batch_size, seq_len, hidden_size * num_heads) to</span></span><br><span class="line">        <span class="comment"># (batch_size * num_heads, seq_len, hidden_size).</span></span><br><span class="line">        </span><br><span class="line">        query = transpose_qkv(self.W_q(query), self.num_heads)</span><br><span class="line">        key = transpose_qkv(self.W_k(key), self.num_heads)</span><br><span class="line">        value = transpose_qkv(self.W_v(value), self.num_heads)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Copy valid_length by num_heads times</span></span><br><span class="line">            device = valid_length.device</span><br><span class="line">            valid_length = valid_length.cpu().numpy() <span class="keyword">if</span> valid_length.is_cuda <span class="keyword">else</span> valid_length.numpy()</span><br><span class="line">            <span class="keyword">if</span> valid_length.ndim == <span class="number">1</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">            valid_length = valid_length.to(device)</span><br><span class="line">            </span><br><span class="line">        output = self.attention(query, key, value, valid_length)</span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure>
<h3 id="转置函数"><a href="#转置函数" class="headerlink" title="转置函数"></a>转置函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_qkv</span><span class="params">(X, num_heads)</span>:</span></span><br><span class="line">    <span class="comment"># Original X shape: (batch_size, seq_len, hidden_size * num_heads),</span></span><br><span class="line">    <span class="comment"># -1 means inferring its value, after first reshape, X shape:</span></span><br><span class="line">    <span class="comment"># (batch_size, seq_len, num_heads, hidden_size)</span></span><br><span class="line">    X = X.view(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)</span></span><br><span class="line">    X = X.transpose(<span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Merge the first two dimensions. Use reverse=True to infer shape from</span></span><br><span class="line">    <span class="comment"># right to left.</span></span><br><span class="line">    <span class="comment"># output shape: (batch_size * num_heads, seq_len, hidden_size)</span></span><br><span class="line">    output = X.view(<span class="number">-1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Saved in the d2l package for later use</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_output</span><span class="params">(X, num_heads)</span>:</span></span><br><span class="line">    <span class="comment"># A reversed version of transpose_qkv</span></span><br><span class="line">    X = X.view(<span class="number">-1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.transpose(<span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">    <span class="keyword">return</span> X.view(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], <span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="测试输出"><a href="#测试输出" class="headerlink" title="测试输出"></a>测试输出</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cell = MultiHeadAttention(<span class="number">5</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">0.5</span>)</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">valid_length = torch.FloatTensor([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">cell(X, X, X, valid_length).shape</span><br></pre></td></tr></table></figure>

<h2 id="Position-Wise-Feed-Forward-Networks"><a href="#Position-Wise-Feed-Forward-Networks" class="headerlink" title="Position-Wise Feed-Forward Networks"></a>Position-Wise Feed-Forward Networks</h2><p>Transformer 模块另一个非常重要的部分就是<strong>基于位置的前馈网络（FFN）</strong>，它接受一个形状为（batch_size，seq_length, feature_size）的三维张量。</p>
<p>Position-wise FFN 由两个全连接层组成，它们<strong>作用在最后一维</strong>上。因为序列的每个位置的状态都会被单独地更新，所以我们称为 position-wise，其等效于一个 1x1 的卷积。</p>
<h3 id="Position-wise-FFN-class"><a href="#Position-wise-FFN-class" class="headerlink" title="Position-wise FFN  class"></a>Position-wise FFN  class</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionWiseFFN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, ffn_hidden_size, hidden_size_out, **kwargs)</span>:</span></span><br><span class="line">        super(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.ffn_1 = nn.Linear(input_size, ffn_hidden_size)</span><br><span class="line">        self.ffn_2 = nn.Linear(ffn_hidden_size, hidden_size_out)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.ffn_2(F.relu(self.ffn_1(X)))</span><br></pre></td></tr></table></figure>
<p>与<a href="#MultiHeadAttention">多头注意力层</a>相似，FFN层同样只会对最后一维的大小进行改变；除此之外，对于两个完全相同的输入，FFN层的输出也将相等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">out = ffn(torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line">print(out, out.shape)</span><br></pre></td></tr></table></figure>
<h2 id="Add-and-Norm"><a href="#Add-and-Norm" class="headerlink" title="Add and Norm"></a>Add and Norm</h2><blockquote>
<p>Transformer还有一个重要的<strong>相加归一化层</strong></p>
</blockquote>
<p>它可以平滑地整合输入和其他层的输出，因此我们在每个多头注意力层和 FFN 层后面都添加一个含残差连接的Layer Norm层。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener">Layer Norm 相似于 Batch Norm</a> </p>
</blockquote>
<p><strong>唯一的区别</strong></p>
<blockquote>
<ul>
<li>Batch Norm是对于batch size这个维度进行计算均值和方差的，</li>
<li>Layer Norm则是对最后一维进行计算。<br/></li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">layernorm = nn.LayerNorm(normalized_shape=<span class="number">2</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">batchnorm = nn.BatchNorm1d(num_features=<span class="number">2</span>, affine=<span class="literal">True</span>)</span><br><span class="line">X = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">print(<span class="string">'layer norm:'</span>, layernorm(X))</span><br><span class="line">print(<span class="string">'batch norm:'</span>, batchnorm(X))</span><br></pre></td></tr></table></figure>

<p><font color=gree>层归一化</font>可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能。</p>
<h3 id="AddNorm-class"><a href="#AddNorm-class" class="headerlink" title="AddNorm class"></a>AddNorm class</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.norm = nn.LayerNorm(hidden_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>【注意】：由于残差连接，X和Y需要有相同的维度。</p>
</blockquote>
<h3 id="模块测试"><a href="#模块测试" class="headerlink" title="模块测试"></a>模块测试</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_norm = AddNorm(<span class="number">4</span>, <span class="number">0.5</span>)</span><br><span class="line">add_norm(torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)), torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))).shape</span><br></pre></td></tr></table></figure>

<hr>
<p>以上是Transformer 模型的三个模块，还记得 Transformer 模型高效并行的特性，得益于：</p>
<blockquote>
<p>多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新</p>
</blockquote>
<p>但是在这种特性下，却丢失了重要的序列顺序的信息，为了更好的捕捉序列信息，需要一种可以保持序列元素位置的模块，因而引入位置编码。</p>
<h1 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h1><p>假设输入序列的嵌入表示 $X\in \mathbb{R}^{l\times d}$, 序列长度为$l$嵌入向量维度为$d$，则其位置编码为$P \in \mathbb{R}^{l\times d}$ ，输出的向量就是二者相加 $X + P$。</p>
<p>位置编码是一个二维的矩阵，i对应着序列中的顺序，j对应其embedding vector内部的维度索引。我们可以通过以下等式计算位置编码：</p>
<p>$$<br>P_{i,2j} = sin(i/10000^{2j/d})<br>$$</p>
<p>$$<br>P_{i,2j+1} = cos(i/10000^{2j/d})<br>$$</p>
<p>$$<br>for\ i=0,\ldots, l-1\ and\ j=0,\ldots,\lfloor (d-1)/2 \rfloor<br>$$</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwZTBsdTM4LnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>位置编码<br>$$</p>
<h2 id="PositionalEncoding-class"><a href="#PositionalEncoding-class" class="headerlink" title="PositionalEncoding class"></a>PositionalEncoding class</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, dropout, max_len=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.P = np.zeros((<span class="number">1</span>, max_len, embedding_size))</span><br><span class="line">        X = np.arange(<span class="number">0</span>, max_len).reshape(<span class="number">-1</span>, <span class="number">1</span>) / np.power(</span><br><span class="line">            <span class="number">10000</span>, np.arange(<span class="number">0</span>, embedding_size, <span class="number">2</span>)/embedding_size)</span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = np.sin(X)</span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = np.cos(X)</span><br><span class="line">        self.P = torch.FloatTensor(self.P)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> X.is_cuda <span class="keyword">and</span> <span class="keyword">not</span> self.P.is_cuda:</span><br><span class="line">            self.P = self.P.cuda()</span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br></pre></td></tr></table></figure>
<h2 id="模块测试-1"><a href="#模块测试-1" class="headerlink" title="模块测试"></a>模块测试</h2><p>可视化其中四个维度，可以看到，第 4 维和第 5 维有相同的频率但偏置不同。第 6 维和第 7 维具有更低的频率；因此 positional encoding 对于不同维度具有可区分性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">Y = pe(torch.zeros((<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))).numpy()</span><br><span class="line">d2l.plot(np.arange(<span class="number">100</span>), Y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].T, figsize=(<span class="number">6</span>, <span class="number">2.5</span>),</span><br><span class="line">         legend=[<span class="string">"dim %d"</span> % p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200219115650513.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>$$<br>Result-to-Test<br>$$</p>
<h1 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h1><p>有了组成Transformer的各个模块，可以搭建一个编码器。</p>
<p>编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层。</p>
<p>对于attention模型以及FFN模型，由于残差连接导致输出维度都是与 embedding 维度一致的，因此要将前一层的输出与原始输入相加并归一化。</p>
<h2 id="Encoder-Block基础块"><a href="#Encoder-Block基础块" class="headerlink" title="Encoder Block基础块"></a>Encoder Block基础块</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, ffn_hidden_size, num_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, **kwargs)</span>:</span></span><br><span class="line">        super(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_1 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)</span><br><span class="line">        self.addnorm_2 = AddNorm(embedding_size, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, valid_length)</span>:</span></span><br><span class="line">        Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm_2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># batch_size = 2, seq_len = 100, embedding_size = 24</span></span><br><span class="line"><span class="comment"># ffn_hidden_size = 48, num_head = 8, dropout = 0.5</span></span><br><span class="line"></span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">encoder_blk = EncoderBlock(<span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder_blk(X, valid_length).shape</span><br></pre></td></tr></table></figure>
<p>整个编码器由 n 个 Encoder Block 堆叠而成，利用 Encoder Block 基础块实现 Transformer 编码器。</p>
<p><strong>两个注意点：</strong></p>
<ul>
<li>残差连接的缘故，中间状态的维度始终与嵌入向量的维度 d 一致；</li>
<li>同时注意到我们把嵌入向量乘以 $\sqrt{d}$ 以防止其值过小。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span><span class="params">(d2l.Encoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_size, ffn_hidden_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads, num_layers, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size)</span><br><span class="line">        self.pos_encoding = PositionalEncoding(embedding_size, dropout)</span><br><span class="line">        self.blks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.blks.append(</span><br><span class="line">                EncoderBlock(embedding_size, ffn_hidden_size,</span><br><span class="line">                             num_heads, dropout))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, valid_length, *args)</span>:</span></span><br><span class="line">        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X = blk(X, valid_length)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test encoder</span></span><br><span class="line">encoder = TransformerEncoder(<span class="number">200</span>, <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>)).long(), valid_length).shape</span><br></pre></td></tr></table></figure>
<h1 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h1><p>Transformer 模型的解码器与编码器结构类似，然而，除了之前介绍的几个模块之外，解码器部分有另一个子模块。该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。</p>
<p>与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和<font color=gree>层归一化</font>将各个子层的输出相连。</p>
<p>在第t个时间步，当前输入$x_t$是query，那么self attention接受了第t步以及前t-1步的所有输入$x_1,\ldots, x_{t-1}$。在训练时，由于第t位置的输入可以观测到全部的序列，这与预测阶段的情形项矛盾，所以我们要通过将第t个时间步所对应的可观测长度设置为t，以消除不需要看到的未来的信息。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwZWZoY3lnLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<h2 id="Decoder-Block-基础块"><a href="#Decoder-Block-基础块" class="headerlink" title="Decoder Block 基础块"></a>Decoder Block 基础块</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, ffn_hidden_size, num_heads,dropout,i,**kwargs)</span>:</span></span><br><span class="line">        super(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention_1 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_1 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.attention_2 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_2 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)</span><br><span class="line">        self.addnorm_3 = AddNorm(embedding_size, dropout)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        enc_outputs, enc_valid_length = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Example Demo:</span></span><br><span class="line">        <span class="comment"># love dogs ! [EOS]</span></span><br><span class="line">        <span class="comment">#  |    |   |   |</span></span><br><span class="line">        <span class="comment">#   Transformer </span></span><br><span class="line">        <span class="comment">#    Decoder</span></span><br><span class="line">        <span class="comment">#  |   |   |   |</span></span><br><span class="line">        <span class="comment">#  I love dogs !</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># shape of key_values = (batch_size, t, hidden_size)</span></span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), dim=<span class="number">1</span>) </span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, seq_len, _ = X.shape</span><br><span class="line">            <span class="comment"># Shape: (batch_size, seq_len), the values in the j-th column are j+1</span></span><br><span class="line">            valid_length = torch.FloatTensor(np.tile(np.arange(<span class="number">1</span>, seq_len+<span class="number">1</span>), (batch_size, <span class="number">1</span>))) </span><br><span class="line">            valid_length = valid_length.to(X.device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        X2 = self.attention_1(X, key_values, key_values, valid_length)</span><br><span class="line">        Y = self.addnorm_1(X, X2)</span><br><span class="line">        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length)</span><br><span class="line">        Z = self.addnorm_2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm_3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(<span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_length), valid_length, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>
<p>对于 Transformer 解码器来说，构造方式与编码器一样，除了最后一层添加一个 dense layer 以获得输出的置信度分数。</p>
<p><strong>Transformer Decoder 参数设置：</strong></p>
<ul>
<li>编码器的输出 enc_outputs </li>
<li>句子有效长度 enc_valid_length</li>
<li>常规的超参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_size, ffn_hidden_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads, num_layers, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size)</span><br><span class="line">        self.pos_encoding = PositionalEncoding(embedding_size, dropout)</span><br><span class="line">        self.blks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.blks.append(</span><br><span class="line">                DecoderBlock(embedding_size, ffn_hidden_size, num_heads,</span><br><span class="line">                             dropout, i))</span><br><span class="line">        self.dense = nn.Linear(embedding_size, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, enc_valid_length, *args)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_length, [<span class="literal">None</span>]*self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br></pre></td></tr></table></figure>

<h1 id="机器翻译模型-Transformer-训练"><a href="#机器翻译模型-Transformer-训练" class="headerlink" title="机器翻译模型 Transformer 训练"></a>机器翻译模型 Transformer 训练</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">    <span class="comment"># sort by frequency and token</span></span><br><span class="line">    counter = collections.Counter(tokens)</span><br><span class="line">    token_freqs = sorted(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    token_freqs.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">      <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">      self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">      tokens = [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.unk = <span class="number">0</span></span><br><span class="line">      tokens = [<span class="string">''</span>]</span><br><span class="line">    tokens += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs <span class="keyword">if</span> freq &gt;= min_freq]</span><br><span class="line">    self.idx_to_token = []</span><br><span class="line">    self.token_to_idx = dict()</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">      self.idx_to_token.append(token)</span><br><span class="line">      self.token_to_idx[token] = len(self.idx_to_token) - <span class="number">1</span></span><br><span class="line">      </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">      <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">      <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span><span class="params">(batch_size, max_len, num_examples=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Download an NMT dataset, return its vocabulary and data iterator."""</span></span><br><span class="line">    <span class="comment"># Download and preprocess</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_raw</span><span class="params">(text)</span>:</span></span><br><span class="line">        text = text.replace(<span class="string">'\u202f'</span>, <span class="string">' '</span>).replace(<span class="string">'\xa0'</span>, <span class="string">' '</span>)</span><br><span class="line">        out = <span class="string">''</span></span><br><span class="line">        <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(text.lower()):</span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">in</span> (<span class="string">','</span>, <span class="string">'!'</span>, <span class="string">'.'</span>) <span class="keyword">and</span> text[i<span class="number">-1</span>] != <span class="string">' '</span>:</span><br><span class="line">                out += <span class="string">' '</span></span><br><span class="line">            out += char</span><br><span class="line">        <span class="keyword">return</span> out </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'path to data dile'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">      raw_text = f.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    text = preprocess_raw(raw_text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(text.split(<span class="string">'\n'</span>)):</span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">if</span> len(parts) &gt;= <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">' '</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build vocab</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(tokens)</span>:</span></span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">        <span class="keyword">return</span> Vocab(tokens, min_freq=<span class="number">3</span>, use_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert to index arrays</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pad</span><span class="params">(line, max_len, padding_token)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(line) &gt; max_len:</span><br><span class="line">            <span class="keyword">return</span> line[:max_len]</span><br><span class="line">        <span class="keyword">return</span> line + [padding_token] * (max_len - len(line))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_array</span><span class="params">(lines, vocab, max_len, is_source)</span>:</span></span><br><span class="line">        lines = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_source:</span><br><span class="line">            lines = [[vocab.bos] + line + [vocab.eos] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        array = torch.tensor([pad(line, max_len, vocab.pad) <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line">        valid_len = (array != vocab.pad).sum(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line">    src_array, src_valid_len = build_array(source, src_vocab, max_len, <span class="literal">True</span>)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, <span class="literal">False</span>)</span><br><span class="line">    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    train_iter = data.DataLoader(train_data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> src_vocab, tgt_vocab, train_iter</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> d2l</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"1"</span></span><br><span class="line"></span><br><span class="line">embed_size, embedding_size, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.05</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, ctx = <span class="number">0.005</span>, <span class="number">250</span>, d2l.try_gpu()</span><br><span class="line">print(ctx)</span><br><span class="line">num_hiddens, num_heads = <span class="number">64</span>, <span class="number">4</span></span><br><span class="line"></span><br><span class="line">src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    len(src_vocab), embedding_size, num_hiddens, num_heads, num_layers,</span><br><span class="line">    dropout)</span><br><span class="line">decoder = TransformerDecoder(</span><br><span class="line">    len(src_vocab), embedding_size, num_hiddens, num_heads, num_layers,</span><br><span class="line">    dropout)</span><br><span class="line">model = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_s2s_ch9(model, train_iter, lr, num_epochs, ctx)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.eval()</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> [<span class="string">'Go .'</span>, <span class="string">'Wow !'</span>, <span class="string">"I'm OK ."</span>, <span class="string">'I won !'</span>]:</span><br><span class="line">    print(sentence + <span class="string">' =&gt; '</span> + d2l.predict_s2s_ch9(</span><br><span class="line">        model, sentence, src_vocab, tgt_vocab, num_steps, ctx))</span><br></pre></td></tr></table></figure>
<hr>
<p>以上是利用 Transformer 机器翻译模型实现翻译 Demo 的全部分块，针对其中的层归一化进行总结：</p>
<p>层归一化</p>
<ol>
<li>层归一化有利于加快收敛，减少训练时间成本</li>
<li>层归一化对一个中间层的所有神经元进行归一化 </li>
<li>层归一化的效果不会受到batch大小的影响</li>
</ol>
<p>补充：</p>
<p>&ensp;&ensp;&ensp;批归一化</p>
<p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;每个神经元的输入数据以mini-batch为单位进行汇总</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>masked softmax</tag>
        <tag>多头注意</tag>
        <tag>位置编码</tag>
        <tag>编/译码器</tag>
      </tags>
  </entry>
</search>
