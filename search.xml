<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Feature Engineering for Data Mining</title>
    <url>/2020/03/26/Feature%20Engineering%20for%20Data%20Mining/</url>
    <content><![CDATA[<p><strong>重要性：</strong></p>
<p>特征工程是比赛中重要的一部分，调参带来的效益往往是有限的，特征工程选取的好坏决定最终的结果。</p>
<p><strong>必要性：</strong></p>
<p>特征工程是和模型结合在一起的，其能更好地表示潜在问题的特征，从而提高机器学习的性能。</p>
<p><strong>特征工程是很低的，容易入门，但是若想精通就需要花费较大的功夫了，正所谓师傅领进门，修行靠个人。</strong></p>
<p><strong>目标：</strong></p>
<p>学习特征工程我们的目的是：对于特征进行进一步分析，并对于数据进行处理。</p>
<p><strong>内容：</strong></p>
<blockquote>
<p>本文我将从以下几点介绍我认识的特征工程</p>
</blockquote>
<ol>
<li><a href="#1_title">异常处理；</a></li>
<li><a href="#2_title">特征归一化/标准化；</a></li>
<li><a href="#3_title">数据分桶；</a></li>
<li><a href="#4_title">缺失值处理；</a></li>
<li><a href="#5_title">特征构造；</a></li>
<li><a href="#6_title">特征筛选；</a></li>
<li><a href="#7_title">降维。</a></li>
</ol>
<h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a><div id="1_title">异常处理</div></h2><blockquote>
<p>通常有以下几种方法</p>
</blockquote>
<ul>
<li><div id="return"><font color=sky-blue>通过箱线图（或 3-Sigma）分析删除异常值；</font></div></li>
<li><a href="#box_cox">BOX-COX 转换（处理有偏分布）；</a></li>
<li><a href="#tile_cut">长尾截断；</a></li>
</ul>
<p><strong>异常值处理：</strong></p>
<blockquote>
<p>通过箱线图，分析并删去异常值</p>
</blockquote>
<ul>
<li>上四分位数Q3 是指数据从小到大排列，取其3/4处位置的分位数</li>
<li>下四分位数Q1 是指数据从小到大排列，取其1/4处位置的分位数</li>
<li>IQR = Q3-Q1指的是四分位距，即上四分位数和下四分位数的差值</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 给出一个数据异常值处理的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outliers_proc</span><span class="params">(data, col_name, scale=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用于清洗异常值，默认用 box_plot（scale=3）进行清洗</span></span><br><span class="line"><span class="string">    :param data: 接收 pandas 数据格式</span></span><br><span class="line"><span class="string">    :param col_name: pandas 列名</span></span><br><span class="line"><span class="string">    :param scale: 尺度</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">box_plot_outliers</span><span class="params">(data_ser, box_scale)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        利用箱线图去除异常值</span></span><br><span class="line"><span class="string">        :param data_ser: 接收 pandas.Series 数据格式</span></span><br><span class="line"><span class="string">        :param box_scale: 箱线图尺度，</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        iqr = box_scale * (data_ser.quantile(<span class="number">0.75</span>) - data_ser.quantile(<span class="number">0.25</span>))</span><br><span class="line">        val_low = data_ser.quantile(<span class="number">0.25</span>) - iqr</span><br><span class="line">        val_up = data_ser.quantile(<span class="number">0.75</span>) + iqr</span><br><span class="line">        rule_low = (data_ser &lt; val_low)</span><br><span class="line">        rule_up = (data_ser &gt; val_up)</span><br><span class="line">        <span class="keyword">return</span> (rule_low, rule_up), (val_low, val_up)</span><br><span class="line"></span><br><span class="line">    data_n = data.copy()</span><br><span class="line">    data_series = data_n[col_name]</span><br><span class="line">    rule, value = box_plot_outliers(data_series, box_scale=scale)</span><br><span class="line">    index = np.arange(data_series.shape[<span class="number">0</span>])[rule[<span class="number">0</span>] | rule[<span class="number">1</span>]]</span><br><span class="line">    print(<span class="string">"Delete number is: &#123;&#125;"</span>.format(len(index)))</span><br><span class="line">    data_n = data_n.drop(index)</span><br><span class="line">    data_n.reset_index(drop=<span class="literal">True</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    print(<span class="string">"Now column number is: &#123;&#125;"</span>.format(data_n.shape[<span class="number">0</span>]))</span><br><span class="line">    index_low = np.arange(data_series.shape[<span class="number">0</span>])[rule[<span class="number">0</span>]]</span><br><span class="line">    outliers = data_series.iloc[index_low]</span><br><span class="line">    print(<span class="string">"Description of data less than the lower bound is:"</span>)</span><br><span class="line">    print(pd.Series(outliers).describe())</span><br><span class="line">    index_up = np.arange(data_series.shape[<span class="number">0</span>])[rule[<span class="number">1</span>]]</span><br><span class="line">    outliers = data_series.iloc[index_up]</span><br><span class="line">    print(<span class="string">"Description of data larger than the upper bound is:"</span>)</span><br><span class="line">    print(pd.Series(outliers).describe())</span><br><span class="line">    </span><br><span class="line">    fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">    sns.boxplot(y=data[col_name], data=data, palette=<span class="string">"Set1"</span>, ax=ax[<span class="number">0</span>])</span><br><span class="line">    sns.boxplot(y=data_n[col_name], data=data_n, palette=<span class="string">"Set1"</span>, ax=ax[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> data_n</span><br></pre></td></tr></table></figure>
<p><a href="#return">返回..</a></p>
<div id="box_cox"><b>BOX-COX 转换（处理有偏分布）</b></div><br/>

<blockquote>
<p>将数据转化为正态分布</p>
</blockquote>
<p>我们都知道 <a href="https://www.cnblogs.com/my-love-is-python/p/10322080.html" target="_blank" rel="noopener">log 变换</a> 可以使数据在一定程度上符合正态分布，特别是有尾数据分布。</p>
<p>但是…采用这种方法，</p>
<ul>
<li><p>较小数据之间的差异将会变大(因为对数函数的斜率很小)，而较大数据之间的差异将减少(可能某分布中较大数据的斜率很小)。</p>
</li>
<li><p>如果你拓展了左尾的差异，减少了右尾的差异，结果将是方差恒定、形状对称的正态分布(无论均值大小如何)。</p>
</li>
</ul>
<p>参见【Reference】：<a href="http://blog.sina.com.cn/s/blog_839b4f640102vqen.html" target="_blank" rel="noopener">使用Box-Cox转换的益处</a> 、<a href="https://wenku.baidu.com/view/96140c8376a20029bd642de3.html" target="_blank" rel="noopener">BoxCox 变换方法及其实现运用</a></p>
<p><a href="#return">返回..</a></p>
<div id="tile_cut"><b>长尾截断</b></div><br/>

<p>对于连续型数值特征，有时精度太高可能只是噪声，并不具备太多的信息，也使得特征维度急剧上升。因此可以保留一定的精度，之后当作类别特征进行处理。对于长尾的数据，可以先进行对数缩放，再进行精度截断，之后可以当做类别变量做二值化处理。</p>
<p><img src="https://img-blog.csdnimg.cn/20200326112942490.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="特征归一化-标准化"><a href="#特征归一化-标准化" class="headerlink" title="特征归一化 / 标准化"></a><div id="2_title">特征归一化 / 标准化</div></h2><p><strong>思想：</strong></p>
<ul>
<li>标准化（转换为标准正态分布）；</li>
<li>归一化（映射到 [0,1] 区间）； - Sigmoid 函数</li>
<li>针对幂律分布，可以采用公式：$log( \frac{1+x}{1+median})$</li>
</ul>
<p><strong>算法：</strong></p>
<ol>
<li>Z-score</li>
<li>min-max</li>
<li>行归一化</li>
</ol>
<p>特征尺度不一致需要标准化，以下算法会受尺度影响</p>
<ol start="4">
<li>KNN，依赖距离</li>
<li>K-meas</li>
<li>逻辑回归，支持向量机，神经网络等如果使用梯度下降来学习权重；</li>
<li>主成分分析，特征向量偏向于较大的列</li>
</ol>
<p>在机器学习 Pipeline 中使用归一化</p>
<h2 id="数据分桶"><a href="#数据分桶" class="headerlink" title="数据分桶"></a><div id="3_title">数据分桶</div></h2><p><strong>Q:</strong></p>
<p>为什么要做数据分桶呢？</p>
<p><strong>A:</strong></p>
<ol>
<li>离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；</li>
<li>离散后的特征对异常值更具鲁棒性，如 age&gt;30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；</li>
<li>LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；</li>
<li>离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；</li>
<li>特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化</li>
<li>e.t.c 当然还有很多原因，LightGBM 在改进 XGBoost 时就增加了数据分桶，增强了模型的泛化性</li>
</ol>
<ul>
<li>等频分桶；</li>
<li>等距分桶；</li>
<li>Best-KS 分桶（类似利用基尼指数进行二分类）；</li>
<li>卡方分桶；</li>
</ul>
<p>参见【Reference】：<a href="https://flashgene.com/archives/81897.html" target="_blank" rel="noopener">浅谈微视推荐系统中的特征工程</a></p>
<h2 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a><div id="4_title">缺失值处理</div></h2><ul>
<li>不处理（针对类似 XGBoost 等树模型）；</li>
<li>删除（缺失数据太多）；</li>
<li>插值补全，包括均值/中位数/众数/建模预测/多重插补/压缩感知补全/矩阵补全等；</li>
<li>分箱，缺失值一个箱；</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200326151308471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>参见【Reference】：<a href="https://www.zhihu.com/question/26639110" target="_blank" rel="noopener">机器学习中如何处理缺失数据？</a>、<a href="http://cda.pinggu.org/view/20394.html" target="_blank" rel="noopener">数据缺失值的4种处理方法</a></p>
<h2 id="特征构造"><a href="#特征构造" class="headerlink" title="特征构造"></a><div id="5_title">特征构造</div></h2><ul>
<li>构造统计量特征，报告计数、求和、比例、标准差等；</li>
<li>时间特征，包括相对时间和绝对时间，节假日，双休日等；</li>
<li>地理信息，包括分箱，分布编码等方法；</li>
<li>非线性变换，包括 log/ 平方/ 根号等；</li>
<li>特征组合，特征交叉；</li>
</ul>
<blockquote>
<p>训练集和测试集放在一起，方便构造特征 【contact】</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Train_data[<span class="string">'train'</span>]=<span class="number">1</span></span><br><span class="line">Test_data[<span class="string">'train'</span>]=<span class="number">0</span></span><br><span class="line">data = pd.concat([Train_data, Test_data], ignore_index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>检查空数据</p>
</blockquote>
<p>可以选择删除也可以留下</p>
<ul>
<li>删除缺失数据占总样本量过大，不建议删去；</li>
<li>当要交到xgboost模型中处理时，不必要。</li>
</ul>
<blockquote>
<p>特征的构造视模型数量而定</p>
</blockquote>
<p>不同模型对数据集的要求不同，需要分开构造</p>
<blockquote>
<p>特征归一化 / 标准化和数据分桶通常体现在特征构造的过程中</p>
</blockquote>
<h2 id="特征筛选"><a href="#特征筛选" class="headerlink" title="特征筛选"></a><div id="6_title">特征筛选</div></h2><ul>
<li>过滤式（filter）：先对数据进行特征选择，然后在训练学习器，常见的方法有 Relief / 方差选择法 / 相关系数法 / 卡方检验法 / 互信息法；</li>
<li>包裹式（wrapper）：直接把最终将要使用的学习器的性能作为特征子集的评价准则，常见方法有 LVM（Las Vegas Wrapper） ；</li>
<li>嵌入式（embedding）：结合过滤式和包裹式，学习器训练过程中自动进行了特征选择，常见的有 lasso 回归；</li>
</ul>
<p>参见【Reference】：<a href="https://baijiahao.baidu.com/s?id=1604074325918456186&wfr=spider&for=pc" target="_blank" rel="noopener">如何进行特征选择（理论篇）</a></p>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a><div id="7_title">降维</div></h2><blockquote>
<p>采用某种映射方法，将原高维空间中的数据点映射到低维度的空间中</p>
</blockquote>
<p>降维本质上即是学习一个映射函数 $f : x-&gt;y$【其中x是原始数据点的表达，目前最多使用向量表达形式。 y是数据点映射后的低维向量表达，通常y的维度小于x的维度（当然提高维度也是可以的）】</p>
<p>$f$ 可能是显式的或隐式的、线性的或非线性的</p>
<p><strong>目的：</strong></p>
<ul>
<li>减少冗余信息所造成的误差,提高识别（或其他应用）的精度；</li>
<li>希望通过降维算法来寻找数据内部的本质结构特征</li>
</ul>
<p><strong>方法：</strong></p>
<ul>
<li>PCA / LDA / ICA；</li>
<li>特征选择也是一种降维。</li>
</ul>
<p>参见【Reference】:<a href="https://blog.csdn.net/rosenor1/article/details/52278116" target="_blank" rel="noopener">机器学习四大降维方法</a></p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>EDA-数据探索性分析</title>
    <url>/2020/03/24/EDA-%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>EDA (Exploratory Data Analysis)，也就是对数据进行探索性的分析，从而为之后的数据预处理和特征工程提供必要的结论。</p>
<p>针对数据集导入和数据的读取不再赘述，是数据集格式而定。</p>
<blockquote>
<p>格式：.csv / .json / .txt .e.t.c</p>
</blockquote>
<p>参见：<a href="https://blog.csdn.net/weixin_42297855/article/details/97501680" target="_blank" rel="noopener" title="关于 EDA 的 Blog">关于 EDA 的 Blog</a></p>
<p>我想就以下几个问题谈谈我认识的和学到的 EDA。</p>
<ol>
<li><a href="#data_overview">数据总览；</a></li>
<li><a href="#judge">判断数据缺失和异常；</a></li>
<li><a href="#location">了解预测值的分布；</a></li>
<li><a href="#num_analy">数字特征分析；</a></li>
<li><a href="#category_analy">类型特征分析。</a></li>
</ol>
<h2 id="数据总览"><a href="#数据总览" class="headerlink" title="数据总览"></a><div id="data_overview">数据总览</div></h2><p>为了熟悉数据的相关统计量，此前我一直使用的是 describe( )。</p>
<p><strong>优点：</strong> 其统计结果中有每列的统计量【个数 count、平均值 mean、方差 std、最小值 min、中位数25% 50% 75% 、以及最大值】</p>
<blockquote>
<p>方便瞬间掌握数据的大概的范围以及每个值的异常值的判断</p>
</blockquote>
<p><strong>e.g.</strong> 999 9999 -1 等值这些其实都是 nan 的另外一种表达方式</p>
<p>我想说的是还有另外一种数据总览的方法 info( )。</p>
<p><strong>优点：</strong> 了解数据每列的type，有助于了解是否存在除了 nan 以外的特殊符号异常。</p>
<p>describe() 的结果就不展示了，比较直观，针对 info() 的结果展示一个范例。</p>
<center><a  data-fancybox="gallery" href="https://img-blog.csdnimg.cn/2020032323381238.png" target="_blank" rel="noopener">
    <img src="https://img-blog.csdnimg.cn/2020032323381238.png">
</a></center>

<h2 id="判断数据缺失和异常"><a href="#判断数据缺失和异常" class="headerlink" title="判断数据缺失和异常"></a><div id="judge">判断数据缺失和异常</div></h2><blockquote>
<p>说白了还是针对 nan 类型的</p>
</blockquote>
<p>如果 nan 的数量很小一般选择填充，如果使用 lgb 等树模型可以直接空缺，让树自己去优化，但如果 nan 存在的过多、可以考虑删掉</p>
<hr>
<p>补充：</p>
<p>&ensp;&ensp;nan，即非数值，不等于任何数，也不等于 nan 本身</p>
<p>&ensp;&ensp;是否为 nan 类型的判断方式：isnan( )  <strong>or</strong>  isnull( )</p>
<hr>
<p>面对缺省值的三类处理方法：</p>
<ol>
<li>用平均值、中值、分位数、众数、随机值等替代。</li>
</ol>
<blockquote>
<p>效果一般，因为等于人为增加了噪声。</p>
</blockquote>
<ol start="2">
<li>用其他变量做预测模型来算出缺失变量。</li>
</ol>
<blockquote>
<p>效果比方法1略好，<br>但有一个根本缺陷</p>
<blockquote>
<p>如果其他变量和缺失变量无关，则预测的结果无意义。</p>
</blockquote>
<blockquote>
<p>如果预测结果相当准确，则又说明这个变量是没必要加入建模的。一般情况下，介于两者之间。</p>
</blockquote>
</blockquote>
<ol start="3">
<li>最精确的做法，把变量映射到高维空间。</li>
</ol>
<blockquote>
<p>For Example: 性别，有男、女、缺失三种情况，则映射成3个变量：是否男、是否女、是否缺失。</p>
</blockquote>
<p>连续型变量也可以这样处理。比如Google、百度的CTR预估模型，预处理时会把所有变量都这样处理，达到几亿维。</p>
<p>这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值、不用考虑线性不可分之类的问题。缺点是计算量大大提升。</p>
<p>而且只有在样本量非常大的时候效果才好，否则会因为过于稀疏，效果很差。</p>
<h2 id="了解预测值的分布"><a href="#了解预测值的分布" class="headerlink" title="了解预测值的分布"></a><div id="location">了解预测值的分布</div></h2><ol>
<li>总体分布概况（无界约翰逊分布等）</li>
</ol>
<blockquote>
<p>针对预测值的特征寻找最佳拟合曲线，对数 / 正态 / 无界 Johnson SU</p>
</blockquote>
<ol start="2">
<li>查看 skewness and kurtosis</li>
</ol>
<blockquote>
<p>分别对应数据的偏度 .skew( )  &amp;&amp; 峰度  .kurt( )</p>
<blockquote>
<p>参见：<a href="https://www.cnblogs.com/wyy1480/p/10474046.html" target="_blank" rel="noopener">偏度峰度简介及实现</a></p>
</blockquote>
</blockquote>
<ol start="3">
<li>查看预测值的具体频数</li>
</ol>
<blockquote>
<p>通过直方图可视化预测数据，可以得到频数</p>
</blockquote>
<p><strong>处理方法：</strong> 把值较少的部分填充或者删掉</p>
<p>处理过后的数据比较集中，可以在预测之前先进行变换，一般进行 log 变换，使得数据均匀化分布</p>
<h2 id="特征分析"><a href="#特征分析" class="headerlink" title="特征分析"></a>特征分析</h2><ol>
<li>特征 nunique 分布</li>
<li>数据可视化</li>
</ol>
<h3 id="数字特征分析"><a href="#数字特征分析" class="headerlink" title="数字特征分析"></a><div id="data_analy">数字特征分析</div></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">numeric_features = Train_data.select_dtypes(include=[np.number])</span><br><span class="line">numeric_features.columns</span><br></pre></td></tr></table></figure>

<h3 id="类型特征分析"><a href="#类型特征分析" class="headerlink" title="类型特征分析"></a><div id="category_analy">类型特征分析</div></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">categorical_features = Train_data.select_dtypes(include=[np.object])</span><br><span class="line">categorical_features.columns</span><br></pre></td></tr></table></figure>

<h3 id="nunique-分布"><a href="#nunique-分布" class="headerlink" title="nunique 分布"></a>nunique 分布</h3><blockquote>
<p>针对 object 类型字段特征</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> categorical_feas:</span><br><span class="line">    print(feature + <span class="string">"的特征分布如下："</span>)</span><br><span class="line">    print(data_train[feature].value_counts())</span><br><span class="line">    <span class="keyword">if</span> feature != <span class="string">'communityName'</span>:</span><br><span class="line">        plt.hist(data_all[feature], bins=<span class="number">3</span>)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>根据结果找到分类结果的分布特征情况</p>
</blockquote>
<h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><blockquote>
<p>作为一种工具，旨在展现数据或者特征之间的关系</p>
</blockquote>
<p>参见：<a href="https://www.jianshu.com/p/6e18d21a4cad" target="_blank" rel="noopener">数据可视化参数调整</a></p>
<p>数据关系可以分为四种类型：</p>
<ol>
<li><p><strong>比较：</strong> 比较数据间各类别的关系，或者是它们随着时间的变化趋势，比如折线图；</p>
</li>
<li><p><strong>联系：</strong> 查看两个或两个以上变量之间的关系，比如散点图；</p>
</li>
<li><p><strong>构成：</strong> 每个部分占整体的百分比，或者是随着时间的百分比变化，比如饼图；</p>
</li>
<li><p><strong>分布：</strong> 关注单个变量，或者多个变量的分布情况，比如直方图。</p>
</li>
</ol>
<p>参见：<a href="https://www.jianshu.com/p/1b4f351013d3" target="_blank" rel="noopener">数据可视化是一种技能</a></p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>EDA</tag>
      </tags>
  </entry>
  <entry>
    <title>涉猎线程</title>
    <url>/2020/03/19/%E6%B6%89%E7%8C%8E%E7%BA%BF%E7%A8%8B/</url>
    <content><![CDATA[<blockquote>
<p>在Java程序设计语言中，并发程序主要集中于线程</p>
</blockquote>
<blockquote>
<p>而且随着计算机系统拥有多个处理器或带有多个执行内核，线程的系统能力得到了很大的增强</p>
</blockquote>
<blockquote>
<p>其中并发程序设计是指由若干个可在同一时间段执行的程序模块组成程序的程序设计方法。</p>
</blockquote>
<p>这种可并发执行的程序模块称为<strong>进程</strong>，进程由数据和机器指令和堆栈组成，存储在磁盘的程序运行起来就是一个进程，进程在内存中，所以<strong>在一个操作系统中能开启的进程是有限的</strong>。</p>
<h3 id="线程"><a href="#线程" class="headerlink" title="线程"></a><strong>线程</strong></h3><blockquote>
<p>它是进程中的一个独立运行单位，是程序执行流的最小单位。</p>
</blockquote>
<p>线程（Thread）是并发编程的基础，也是程序执行的最小单元，它依托进程而存在。一个进程中可以包含多个线程，多线程可以共享一块内存空间和一组系统资源，因此线程之间的切换更加节省资源、更加轻量化，也因此被称为轻量进程，</p>
<p>线程自己不拥有系统资源，只拥有在运行中必不可少的资源，一个线程可以创建和撤销另一个线程，同一个进程中多个线程之间可以并发执行。</p>
<p>由于线程之间的相互制约，可以说线程有暂停，等待，休眠，停止等状态，从你创建一个线程，到执行任务完毕，这是线程的一个生命周期。</p>
<center id="return"><img src="https://img-blog.csdnimg.cn/20200319094140271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p><a href="#transfer">线程工作模式  详述…</a></p>
<p><strong>线程在CPU上执行</strong>，（CPU多少核多少线程，简单来说意味着在这台电脑上同一时间能执行多少个线程），<strong>线程执行需要消耗CPU（运行），高速缓存（缓存线程运行时所需的数据），内存（存储数据）</strong>，CPU的读取速度是最快的，为达到最大效率，将数据提前提取到高速缓存中便于CPU在运行时能尽量体现其高速的特性。</p>
<h3 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h3><p>线程是程序中一个单一的顺序控制流程，在耽搁程序中同时运行多个线程完成不同的工作，即为多线程。</p>
<p>引入线程是因为<strong>创建一个新线程花费时间少，两个线程（在同一进程中的）的切换时间少,</strong> 线程之间互相通信不必调用内核，线程能独立执行；</p>
<p>而使用多线程就是最大限度的利用硬件资源来提升程序的效率，（这也就是说在一个进程中不能开启N个线程，因为硬件资源会限制执行效率）。</p>
<h3 id="Java实现多线程有两种方式"><a href="#Java实现多线程有两种方式" class="headerlink" title="Java实现多线程有两种方式"></a>Java实现多线程有两种方式</h3><ol>
<li>Runnable接口：此接口中有一个run（）方法，是线程的运行方法（线程要执行的任务），当run结束时，线程也就结束了；</li>
<li>Thread类：（Thread类是Runnable接口的子类）所以run方法仍然保持，此外这个类中还有start( );和sleep(long time); start方法是线程的启动方法，如果想要JVM把你的这个类当作一个线程，就要使用start方法来启动线程，sleep是线程的休眠方法，单位是毫秒</li>
</ol>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>新线程态(New Thread)</td>
<td>可运行态(Runnable)</td>
</tr>
<tr>
<td>非运行态(Not Runnable)</td>
<td>死亡态(Dead)</td>
</tr>
</tbody></table>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Runnable 是接口 其内部有run()方法</span></span><br><span class="line"><span class="comment"> * Thread 是类  继承了Runnable的接口</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span></span>&#123; <span class="comment">//implements Runnable&#123; // extends Thread&#123;</span></span><br><span class="line"> </span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">int</span> num;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="title">Test</span><span class="params">(<span class="keyword">int</span> num)</span></span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.num = num;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  num++;</span><br><span class="line">  System.out.println(Thread.currentThread()+<span class="string">" "</span>+<span class="string">"num ="</span>+num);</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadTest</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line"> </span><br><span class="line"> <span class="keyword">public</span> Test test;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ThreadTest</span><span class="params">(Test test)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.test = test;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(test.num&lt;<span class="number">20</span>) &#123;</span><br><span class="line">  test.print();</span><br><span class="line">  <span class="comment">//异常事件处理</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="comment">//设置休眠时间，以免太快而无法观察</span></span><br><span class="line">   Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">   e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">  Test tes = <span class="keyword">new</span> Test(<span class="number">1</span>);</span><br><span class="line">  <span class="comment">//启动多线程</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=<span class="number">10</span>;i++) &#123;</span><br><span class="line">  ThreadTest thr = <span class="keyword">new</span> ThreadTest(tes);</span><br><span class="line">  <span class="comment">//构造方法转换thr对象类型</span></span><br><span class="line">  Thread t = <span class="keyword">new</span> Thread(thr);</span><br><span class="line">  t.start();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//输出当前的线程</span></span><br><span class="line">  System.out.println(Thread.currentThread());</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Thread[Thread-<span class="number">1</span>,<span class="number">5</span>,main] num =<span class="number">2</span></span><br><span class="line">Thread[Thread-<span class="number">13</span>,<span class="number">5</span>,main] num =<span class="number">3</span></span><br><span class="line">Thread[Thread-<span class="number">9</span>,<span class="number">5</span>,main] num =<span class="number">4</span></span><br><span class="line">Thread[Thread-<span class="number">11</span>,<span class="number">5</span>,main] num =<span class="number">5</span></span><br><span class="line">Thread[main,<span class="number">5</span>,main]</span><br><span class="line">Thread[Thread-<span class="number">7</span>,<span class="number">5</span>,main] num =<span class="number">6</span></span><br><span class="line">Thread[Thread-<span class="number">3</span>,<span class="number">5</span>,main] num =<span class="number">7</span></span><br><span class="line">Thread[Thread-<span class="number">5</span>,<span class="number">5</span>,main] num =<span class="number">8</span></span><br><span class="line">Thread[Thread-<span class="number">21</span>,<span class="number">5</span>,main] num =<span class="number">9</span></span><br><span class="line">Thread[Thread-<span class="number">19</span>,<span class="number">5</span>,main] num =<span class="number">10</span></span><br><span class="line">Thread[Thread-<span class="number">15</span>,<span class="number">5</span>,main] num =<span class="number">11</span></span><br><span class="line">Thread[Thread-<span class="number">17</span>,<span class="number">5</span>,main] num =<span class="number">12</span></span><br><span class="line">Thread[Thread-<span class="number">7</span>,<span class="number">5</span>,main] num =<span class="number">13</span></span><br><span class="line">Thread[Thread-<span class="number">5</span>,<span class="number">5</span>,main] num =<span class="number">14</span></span><br><span class="line">Thread[Thread-<span class="number">15</span>,<span class="number">5</span>,main] num =<span class="number">15</span></span><br><span class="line">Thread[Thread-<span class="number">11</span>,<span class="number">5</span>,main] num =<span class="number">16</span></span><br><span class="line">Thread[Thread-<span class="number">13</span>,<span class="number">5</span>,main] num =<span class="number">17</span></span><br><span class="line">Thread[Thread-<span class="number">1</span>,<span class="number">5</span>,main] num =<span class="number">18</span></span><br><span class="line">Thread[Thread-<span class="number">21</span>,<span class="number">5</span>,main] num =<span class="number">19</span></span><br><span class="line">Thread[Thread-<span class="number">19</span>,<span class="number">5</span>,main] num =<span class="number">20</span></span><br><span class="line">Thread[Thread-<span class="number">17</span>,<span class="number">5</span>,main] num =<span class="number">21</span></span><br><span class="line">Thread[Thread-<span class="number">3</span>,<span class="number">5</span>,main] num =<span class="number">22</span></span><br><span class="line">Thread[Thread-<span class="number">9</span>,<span class="number">5</span>,main] num =<span class="number">23</span></span><br><span class="line"><span class="comment">//在这里发现运行结果并非全部的num都变成了20，这就是多线程控制同一变量变化时出现的问题</span></span><br></pre></td></tr></table></figure>
<p>出现了问题，下边就来解决：</p>
<p>共用代码块是我想到的第一种方法，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span></span>&#123; <span class="comment">//implements Runnable&#123; // extends Thread&#123;</span></span><br><span class="line"> </span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">int</span> num;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="title">Test</span><span class="params">(<span class="keyword">int</span> num)</span></span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.num = num;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  num++;</span><br><span class="line">  System.out.println(Thread.currentThread()+<span class="string">" "</span>+<span class="string">"num ="</span>+num);</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadTest</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line"> </span><br><span class="line"> <span class="keyword">public</span> Test test;</span><br><span class="line"> <span class="comment">//为解决多线程控制同一变量时的问题</span></span><br><span class="line"> <span class="keyword">public</span> Object obj = <span class="keyword">new</span> Object();</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="title">ThreadTest</span><span class="params">(Test test)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.test = test;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">synchronized</span>(obj) &#123;<span class="comment">//共用代码块</span></span><br><span class="line">  <span class="keyword">while</span>(test.num&lt;<span class="number">20</span>) &#123;</span><br><span class="line">  test.print();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">   Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">   e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">  Test tes = <span class="keyword">new</span> Test(<span class="number">1</span>);</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=<span class="number">10</span>;i++) &#123;</span><br><span class="line">  ThreadTest thr = <span class="keyword">new</span> ThreadTest(tes);</span><br><span class="line">  Thread t = <span class="keyword">new</span> Thread(thr);</span><br><span class="line">  t.start();</span><br><span class="line">  &#125;</span><br><span class="line">  System.out.println(Thread.currentThread());</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但貌似并没有起到预想的效果，再接再厉……</p>
<p>之后在实现界面上直线等的动画时再次见到了多线程，这次与之不同的是，我在网上查找到了多线程怎么控制并发数，有两种方法，第一种是用join方法，第二种则是利用线程池</p>
<p>过了好久终于解决了～～～～～</p>
<p>首先明白一个问题，创建一个线程的时间远远小于启动一个线程去运行的时间，所以应该将创建和启动线程分开写</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=<span class="number">10</span>;i++) &#123;</span><br><span class="line">			ThreadTest thr = <span class="keyword">new</span> ThreadTest(tes);</span><br><span class="line">			list.add(thr);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">for</span>(ThreadTest tt : list) &#123;</span><br><span class="line">			tt.start();</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure>
<p>第二点，自加时间足够短，加休眠有点多此一举了，所以run方法就只剩下了一行代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">test.print();</span><br></pre></td></tr></table></figure>
<p>最终的运行结构发现终于没有重复的值了</p>
<p>面对一个问题的时候，从问题的本质入手，简单代码上手，理解多线程同步是一个巨大的工程：</p>
<blockquote>
<p>顺便列举一下了解到的多线程同步的七种方法吧</p>
</blockquote>
<ol>
<li>同步方法</li>
<li>同步代码块（此上两种方法都是涉及到synchronized关键字的）</li>
<li>wait与notify</li>
<li>使用特殊域变量（volatile）实现</li>
<li>重入锁（）java.util.concurrennt包</li>
<li>使用局部变量</li>
<li>使用阻塞队列实现<strong>原子操作</strong></li>
</ol>
<hr>
<p><font size=4><b>补充：</b></font></p>
<div align=center><b>线程状态</b></div><br/>

<p>线程的状态在 JDK 1.5 之后以枚举的方式被定义在 Thread 的源码中，它总共包含以下 6 个状态：</p>
<ul>
<li><strong>NEW</strong>，新建状态，线程被创建出来，但尚未启动时的线程状态；</li>
<li><strong>RUNNABLE</strong>，就绪状态，表示可以运行的线程状态，它可能正在运行，或者是在排队等待操作系统给它分配 CPU 资源；</li>
<li><strong>BLOCKED</strong>，阻塞等待锁的线程状态，表示处于阻塞状态的线程正在等待监视器锁，比如等待执行 synchronized 代码块或者使用 synchronized 标记的方法；  </li>
<li><strong>WAITING</strong>，等待状态，一个处于等待状态的线程正在等待另一个线程执行某个特定的动作，比如，一个线程调用了Object.wait()方法，那它就在等待另一个线程调用Object.notify()…</li>
<li><strong>TIMED_WAITING</strong>，计时等待状态，和等待状态（WAITING）类似，它只是多了超时时间，比如调用了有超时时间设置的方法Object.wait(longtimeout)和Thread.j…</li>
<li><strong>TERMINATED</strong>，终止状态，表示线程已经执行完成。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> State &#123; </span><br><span class="line">	<span class="comment">/** </span></span><br><span class="line"><span class="comment">	* 新建状态，线程被创建出来，但尚未启动时的线程状态 </span></span><br><span class="line"><span class="comment">	*/</span> </span><br><span class="line">	</span><br><span class="line">	NEW,</span><br><span class="line">	 </span><br><span class="line">	<span class="comment">/** </span></span><br><span class="line"><span class="comment">	* 就绪状态，表示可以运行的线程状态，但它在排队等待来自操作系统的 CPU 资源 </span></span><br><span class="line"><span class="comment">	*/</span> </span><br><span class="line">	</span><br><span class="line">	RUNNABLE, </span><br><span class="line">	</span><br><span class="line">	<span class="comment">/** </span></span><br><span class="line"><span class="comment">	* 阻塞等待锁的线程状态，表示正在处于阻塞状态的线程 </span></span><br><span class="line"><span class="comment">	* 正在等待监视器锁，比如等待执行 synchronized 代码块或者 </span></span><br><span class="line"><span class="comment">	* 使用 synchronized 标记的方法 </span></span><br><span class="line"><span class="comment">	*/</span></span><br><span class="line">	</span><br><span class="line">	BLOCKED, </span><br><span class="line">	</span><br><span class="line">	<span class="comment">/** </span></span><br><span class="line"><span class="comment">	* 等待状态，一个处于等待状态的线程正在等待另一个线程执行某个特定的动作。 </span></span><br><span class="line"><span class="comment">	* 例如，一个线程调用了 Object.wait() 它在等待另一个线程调用 </span></span><br><span class="line"><span class="comment">	* Object.notify() 或 Object.notifyAll() </span></span><br><span class="line"><span class="comment">	*/</span> </span><br><span class="line"></span><br><span class="line">	WAITING, </span><br><span class="line">	</span><br><span class="line">	<span class="comment">/** </span></span><br><span class="line"><span class="comment">	* 计时等待状态，和等待状态 (WAITING) 类似，只是多了超时时间，比如 </span></span><br><span class="line"><span class="comment">	* 调用了有超时时间设置的方法 Object.wait(long timeout) 和 </span></span><br><span class="line"><span class="comment">	* Thread.join(long timeout) 就会进入此状态 </span></span><br><span class="line"><span class="comment">	*/</span> </span><br><span class="line"></span><br><span class="line">	TIMED_WAITING, </span><br><span class="line"></span><br><span class="line">	<span class="comment">/** </span></span><br><span class="line"><span class="comment">	* 终止状态，表示线程已经执行完成 </span></span><br><span class="line"><span class="comment">	*/</span></span><br><span class="line"></span><br><span class="line">	TERMINATED</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><a href="#return">返回…</a><div  id="transfer"  align=center><b>线程的工作模式</b></div><br/></p>
<ol>
<li><p>首先先要创建线程并指定线程需要执行的业务方法，然后再调用线程的start()方法，</p>
</li>
<li><p>此时线程就从NEW（新建）状态变成了RUNNABLE（就绪）状态，</p>
</li>
<li><p>此时线程会判断要执行的方法中有没有 synchronized 同步代码块，</p>
</li>
<li><p>如果有并且其他线程也在使用此锁，那么线程就会变为 BLOCKED（阻塞等待）状态，</p>
</li>
<li><p>当其他线程使用完此锁之后，线程会继续执行剩余的方法。</p>
</li>
</ol>
<p>当遇到Object.wait()或Thread.join()方法时，线程会变为WAITING（等待状态）状态，如果是带了超时时间的等待方法，那么线程会进入TIMED_WAITING（计时等待）状态，当有其他线程执行了 notify() 或 notifyAll() 方法之后，线程被唤醒继续执行剩余的业务方法，直到方法执行完成为止，此时整个线程的流程就执行完了。</p>
<hr>
<h3 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h3><p>线程一般会作为并发编程的起始问题，用于引出更多的关于并发编程的面试问题。当然对于线程的掌握程度也决定了你对并发编程的掌握程度。</p>
<ol>
<li>BLOCKED（阻塞等待）和 WAITING（等待）有什么区别？ </li>
<li>start() 方法和 run() 方法有什么区别？ </li>
<li>线程的优先级有什么用？该如何设置？ </li>
<li>线程的常用方法有哪些？</li>
</ol>
<h4 id="BLOCKED-和-WAITING-的区别"><a href="#BLOCKED-和-WAITING-的区别" class="headerlink" title="BLOCKED 和 WAITING 的区别"></a>BLOCKED 和 WAITING 的区别</h4><blockquote>
<p>虽然 BLOCKED 和 WAITING 都有等待的含义，但二者有着本质的区别。</p>
</blockquote>
<p>首先它们状态形成的调用方法不同，</p>
<p>其次 BLOCKED 可以理解为当前线程还处于活跃状态，只是在阻塞等待其他线程使用完某个锁资源；</p>
<p>而 WAITING 则是因为自身调用了 Object.wait() 或着是 Thread.join() 又或者是 LockSupport.park() 而进入等待状态，只能等待其他线程执行某个特定的动作才能被继续唤醒。</p>
<p><strong>For Example：</strong></p>
<p>比如当线程因为调用了 Object.wait() 而进入 WAITING 状态之后，则需要等待另一个线程执行 Object.notify() 或 Object.notifyAll() 才能被唤醒。</p>
<h4 id="start-和-run-的区别"><a href="#start-和-run-的区别" class="headerlink" title="start() 和 run() 的区别"></a>start() 和 run() 的区别</h4><p>首先从 Thread 源码来看，start() 方法属于 Thread 自身的方法，并且使用了 synchronized 来保证线程安全，源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123; </span><br><span class="line">	<span class="comment">// 状态验证，不等于 NEW 的状态会抛出异常 </span></span><br><span class="line">	<span class="keyword">if</span> (threadStatus != <span class="number">0</span>) </span><br><span class="line">		<span class="keyword">throw</span> <span class="keyword">new</span> IllegalThreadStateException(); </span><br><span class="line">	<span class="comment">// 通知线程组，此线程即将启动 </span></span><br><span class="line">	group.add(<span class="keyword">this</span>); </span><br><span class="line">	<span class="keyword">boolean</span> started = <span class="keyword">false</span>; </span><br><span class="line">	<span class="keyword">try</span> &#123; </span><br><span class="line">		start0(); </span><br><span class="line">		started = <span class="keyword">true</span>; </span><br><span class="line">	&#125; <span class="keyword">finally</span> &#123; </span><br><span class="line">		<span class="keyword">try</span> &#123; </span><br><span class="line">			<span class="keyword">if</span> (!started) &#123; </span><br><span class="line">				group.threadStartFailed(<span class="keyword">this</span>); </span><br><span class="line">			&#125; </span><br><span class="line">		&#125; <span class="keyword">catch</span> (Throwable ignore) &#123; </span><br><span class="line">		<span class="comment">// 不处理任何异常，如果 start0 抛出异常，则它将被传递到调用堆栈上 </span></span><br><span class="line">		&#125; </span><br><span class="line"> 	&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>run() 方法为 Runnable 的抽象方法，必须由调用类重写此方法，重写的 run() 方法其实就是此线程要执行的业务方法，源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Thread</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123; </span><br><span class="line">	<span class="comment">// 忽略其他方法...... </span></span><br><span class="line">	<span class="keyword">private</span> Runnable target; </span><br><span class="line">	<span class="meta">@Override</span> </span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123; </span><br><span class="line">		<span class="keyword">if</span> (target != <span class="keyword">null</span>) &#123; </span><br><span class="line">			target.run(); </span><br><span class="line">		&#125; </span><br><span class="line">	&#125; </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="meta">@FunctionalInterface</span> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Runnable</span> </span>&#123; </span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从执行的效果来说，</p>
<p>start() 方法可以开启多线程，让线程从 NEW 状态转换成 RUNNABLE 状态，而 run() 方法只是一个普通的方法。 其次，它们可调用的次数不同，start() 方法不能被多次调用，否则会抛出 java.lang.IllegalStateException；</p>
<p>而 run() 方法可以进行多次调用，因为它只是一个普通的方法而已。</p>
<h4 id="线程优先级"><a href="#线程优先级" class="headerlink" title="线程优先级"></a>线程优先级</h4><p>在 Thread 源码中和线程优先级相关的属性有 3 个：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 线程可以拥有的最小优先级</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> MIN_PRIORITY = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 线程默认优先级</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> NORM_PRIORITY = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 线程可以拥有的最大优先级</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> MAX_PRIORITY = <span class="number">10</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>线程的<strong>优先级</strong>可以理解为<strong>线程抢占 CPU 时间片的概率</strong></p>
</blockquote>
<p>优先级越高的线程优先执行的概率就越大，但并不能保证优先级高的线程一定先执行。</p>
<p>在程序中我们可以通过 Thread.setPriority() 来设置优先级，setPriority() 源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">setPriority</span><span class="params">(<span class="keyword">int</span> newPriority)</span> </span>&#123;</span><br><span class="line">    ThreadGroup g;</span><br><span class="line">    checkAccess();</span><br><span class="line">    <span class="comment">// 先验证优先级的合理性</span></span><br><span class="line">    <span class="keyword">if</span> (newPriority &gt; MAX_PRIORITY || newPriority &lt; MIN_PRIORITY) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>((g = getThreadGroup()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// 优先级如果超过线程组的最高优先级，则把优先级设置为线程组的最高优先级</span></span><br><span class="line">        <span class="keyword">if</span> (newPriority &gt; g.getMaxPriority()) &#123;</span><br><span class="line">            newPriority = g.getMaxPriority();</span><br><span class="line">        &#125;</span><br><span class="line">        setPriority0(priority = newPriority);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="线程的常用方法"><a href="#线程的常用方法" class="headerlink" title="线程的常用方法"></a>线程的常用方法</h4><ol>
<li>join()</li>
</ol>
<p>在一个线程中调用 other.join() ，这时候当前线程会让出执行权给 other 线程，直到 other 线程执行完或者过了超时时间之后再继续执行当前线程，</p>
<p>join() 源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">join</span><span class="params">(<span class="keyword">long</span> millis)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> base = System.currentTimeMillis();</span><br><span class="line">    <span class="keyword">long</span> now = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 超时时间不能小于 0</span></span><br><span class="line">    <span class="keyword">if</span> (millis &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"timeout value is negative"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 等于 0 表示无限等待，直到线程执行完为之</span></span><br><span class="line">    <span class="keyword">if</span> (millis == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// 判断子线程 (其他线程) 为活跃线程，则一直等待</span></span><br><span class="line">        <span class="keyword">while</span> (isAlive()) &#123;</span><br><span class="line">            wait(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 循环判断</span></span><br><span class="line">        <span class="keyword">while</span> (isAlive()) &#123;</span><br><span class="line">            <span class="keyword">long</span> delay = millis - now;</span><br><span class="line">            <span class="keyword">if</span> (delay &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            wait(delay);</span><br><span class="line">            now = System.currentTimeMillis() - base;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从源码中可以看出 join() 方法底层还是通过 wait() 方法来实现的。 </p>
<p>例如，在未使用 join() 时，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread thread = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">6</span>; i++) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(<span class="string">"子线程睡眠："</span> + i + <span class="string">"秒。"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        thread.start(); <span class="comment">// 开启线程</span></span><br><span class="line">        <span class="comment">// 主线程执行</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(<span class="string">"主线程睡眠："</span> + i + <span class="string">"秒。"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">主线程睡眠：<span class="number">1</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">1</span>秒。</span><br><span class="line">主线程睡眠：<span class="number">2</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">2</span>秒。</span><br><span class="line">主线程睡眠：<span class="number">3</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">3</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">4</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">5</span>秒。</span><br></pre></td></tr></table></figure>

<p>从结果可以看出，在未使用 join() 时主子线程会交替执行。</p>
<p>然后我们再把 join() 方法加入到代码中，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread thread = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">6</span>; i++) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(<span class="string">"子线程睡眠："</span> + i + <span class="string">"秒。"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        thread.start(); <span class="comment">// 开启线程</span></span><br><span class="line">        thread.join(<span class="number">2000</span>); <span class="comment">// 等待子线程先执行 2 秒钟</span></span><br><span class="line">        <span class="comment">// 主线程执行</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(<span class="string">"主线程睡眠："</span> + i + <span class="string">"秒。"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">子线程睡眠：<span class="number">1</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">2</span>秒。</span><br><span class="line">主线程睡眠：<span class="number">1</span>秒。 <span class="comment">// thread.join(2000); 等待 2 秒之后，主线程和子线程再交替执行</span></span><br><span class="line">子线程睡眠：<span class="number">3</span>秒。</span><br><span class="line">主线程睡眠：<span class="number">2</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">4</span>秒。</span><br><span class="line">子线程睡眠：<span class="number">5</span>秒。</span><br><span class="line">主线程睡眠：<span class="number">3</span>秒。</span><br></pre></td></tr></table></figure>

<p>从执行结果可以看出，添加 join() 方法之后，主线程会先等子线程执行 2 秒之后才继续执行。</p>
<ol start="2">
<li>yield()</li>
</ol>
<p>看 Thread 的源码可以知道 yield() 为本地方法，也就是说 yield() 是由 C 或 C++ 实现的，源码如下：</p>
<blockquote>
<p>public static native void yield();</p>
</blockquote>
<p>yield() 方法表示给线程调度器一个当前线程愿意出让 CPU 使用权的暗示，但是线程调度器可能会忽略这个暗示。</p>
<p>比如我们执行这段包含了 yield() 方法的代码，如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    Runnable runnable = <span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                System.out.println(<span class="string">"线程："</span> +</span><br><span class="line">                        Thread.currentThread().getName() + <span class="string">" I："</span> + i);</span><br><span class="line">                <span class="keyword">if</span> (i == <span class="number">5</span>) &#123;</span><br><span class="line">                    Thread.yield();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    Thread t1 = <span class="keyword">new</span> Thread(runnable, <span class="string">"T1"</span>);</span><br><span class="line">    Thread t2 = <span class="keyword">new</span> Thread(runnable, <span class="string">"T2"</span>);</span><br><span class="line">    t1.start();</span><br><span class="line">    t2.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当我们把这段代码执行多次之后会发现，每次执行的结果都不相同，这是因为 yield() 执行非常不稳定，线程调度器不一定会采纳 yield() 出让 CPU 使用权的建议，从而导致了这样的结果。</p>
]]></content>
      <categories>
        <category>Operating Systems</category>
      </categories>
      <tags>
        <tag>(多)线程与进程</tag>
      </tags>
  </entry>
  <entry>
    <title>10亿数据找目标，众里寻他之红黑树</title>
    <url>/2020/03/14/10%E4%BA%BF%E6%95%B0%E6%8D%AE%E6%89%BE%E7%9B%AE%E6%A0%87%EF%BC%8C%E4%BC%97%E9%87%8C%E5%AF%BB%E4%BB%96%E4%B9%8B%E7%BA%A2%E9%BB%91%E6%A0%91/</url>
    <content><![CDATA[<p>众里寻他，10亿数据中只需要进行10几次比较就能查找到目标，这便是红黑树的魅力所在。</p>
<p><strong><font color=blue>Q:</font></strong></p>
<ol>
<li>红黑树是一种比较难的数据结构，要完全搞懂非常耗时耗力，红黑树怎么自平衡？</li>
<li>什么时候需要左旋或右旋？</li>
<li>插入和删除破坏了树的平衡后怎么处理？</li>
</ol>
<hr>
<center><b><font size=4>先修知识点</font></b></center>

<ul>
<li>二叉查找树</li>
<li>完美平衡二叉树</li>
</ul>
<hr>
<p>红黑树也是二叉查找树，我们知道，二叉查找树这一数据结构并不难，而红黑树之所以难是难在它是自平衡的二叉查找树，在进行插入和删除等可能会破坏树的平衡的操作时，需要重新自处理达到平衡状态。想想场景就有点烦心。</p>
<h2 id="红黑树定义和性质"><a href="#红黑树定义和性质" class="headerlink" title="红黑树定义和性质"></a>红黑树定义和性质</h2><p>红黑树是一种含有红黑结点并能自平衡的二叉查找树。它必须满足下面性质：</p>
<ul>
<li>性质1：每个节点要么是黑色，要么是红色。</li>
<li>性质2：根节点是黑色。</li>
<li>性质3：每个叶子节点（NIL）是黑色。</li>
<li>性质4：每个红色结点的两个子结点一定都是黑色。</li>
<li>性质5：任意一结点到每个叶子结点的路径都包含数量相同的黑结点。</li>
</ul>
<p>从性质5又可以推出：</p>
<ul>
<li>性质5.1：如果一个结点存在黑子结点，那么该结点肯定有两个子结点</li>
</ul>
<p>图中就是一颗简单的红黑树。其中Nil为叶子结点，并且它是黑色的。(值得提醒注意的是，在Java中，叶子结点是为null的结点。)</p>
<center><img src="https://img-blog.csdnimg.cn/20200314232111332.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>红黑树并不是一个完美平衡二叉查找树，从图中可以看到，根结点 P 的左子树显然比右子树高，但左子树和右子树的黑结点的层数是相等的。</p>
<p>亦即任意一个结点到到每个叶子结点的路径都包含数量相同的黑结点(性质5)。所以我们叫红黑树这种平衡为黑色完美平衡。</p>
<p>接着我们还来约定下红黑树一些结点的叫法，如图所示</p>
<center><img src="https://img-blog.csdnimg.cn/20200314232616270.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>


<p>前面讲到红黑树能自平衡，它靠的是什么？三种操作：左旋、右旋和变色。</p>
<ul>
<li><strong>左旋：</strong> 以某个结点作为支点(旋转结点)，其右子结点变为旋转结点的父结点，右子结点的左子结点变为旋转结点的右子结点，左子结点保持不变。</li>
<li><strong>右旋：</strong> 以某个结点作为支点(旋转结点)，其左子结点变为旋转结点的父结点，左子结点的右子结点变为旋转结点的左子结点，右子结点保持不变。</li>
<li><strong>变色：</strong> 结点的颜色由红变黑或由黑变红。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200314233239573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>$$<br>左旋<br>$$</p>
<p><img src="https://img-blog.csdnimg.cn/20200314233311668.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>$$<br>右旋<br>$$</p>
<p>上面所说的旋转结点也即旋转的支点，也即图中的P结点。</p>
<p>我们先忽略颜色，可以看到旋转操作不会影响旋转结点的父结点，父结点以上的结构还是保持不变的。</p>
<ul>
<li><strong>左旋</strong>只影响旋转结点和<strong>其右子树</strong>的结构，把右子树的结点往左子树挪了。</li>
<li><strong>右旋</strong>只影响旋转结点和<strong>其左子树</strong>的结构，把左子树的结点往右子树挪了。</li>
</ul>
<p>旋转操作是局部的。当一边子树的结点少了，那么向另外一边子树“借”一些结点；当一边子树的结点多了，那么向另外一边子树“租”一些结点。</p>
<p><strong>注意：</strong></p>
<p>但要保持红黑树的性质，结点不能乱挪，还得靠变色了。怎么变？具体情景又不同变法，后面会具体讲到，现在只需要记住<strong>红黑树总是通过旋转和变色达到自平衡</strong>。</p>
<p><strong><font color=blue>Q:</font></strong></p>
<p>&ensp;&ensp;黑结点可以同时包含一个红子结点和一个黑子结点吗？ </p>
<h2 id="红黑树查找"><a href="#红黑树查找" class="headerlink" title="红黑树查找"></a>红黑树查找</h2><p>因为红黑树是一颗二叉平衡树，并且查找不会破坏树的平衡，所以查找跟二叉平衡树的查找无异：</p>
<ol>
<li>从根结点开始查找，把根结点设置为当前结点；</li>
<li>若当前结点为空，返回null；</li>
<li>若当前结点不为空，用当前结点的 key 跟查找 key 作比较；</li>
<li>若当前结点 key 等于查找 key，那么该 key 就是查找目标，返回当前结点；</li>
<li>若当前结点 key 大于查找 key，把当前结点的左子结点设置为当前结点，重复步骤2；</li>
<li>若当前结点 key 小于查找 key，把当前结点的右子结点设置为当前结点，重复步骤2；</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20200314235257779.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>非常简单，但简单不代表它效率不好。</p>
<p>正由于红黑树总保持黑色完美平衡，所以它的查找最坏时间复杂度为 $O(2lgN)$，也即整颗树刚好红黑相隔的时候。能有这么好的查找效率得益于红黑树自平衡的特性。</p>
<h2 id="红黑树插入"><a href="#红黑树插入" class="headerlink" title="红黑树插入"></a>红黑树插入</h2><p>插入操作包括两部分工作</p>
<ul>
<li>一、查找插入的<strong>位置</strong>；</li>
<li>二、插入后<strong>自平衡</strong>。</li>
</ul>
<p>查找插入的父结点很简单，跟查找操作区别不大：</p>
<ol>
<li>从根结点开始查找；</li>
<li>若根结点为空，那么插入结点作为根结点，结束。</li>
<li>若根结点不为空，那么把根结点作为当前结点；</li>
<li>若当前结点为null，返回当前结点的父结点，结束。</li>
<li>若当前结点key等于查找key，那么该key所在结点就是插入结点，更新结点的值，结束。</li>
<li>若当前结点key大于查找key，把当前结点的左子结点设置为当前结点，重复步骤4；</li>
<li>若当前结点key小于查找key，把当前结点的右子结点设置为当前结点，重复步骤4；</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20200314235812841.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>插入位置已经找到，把插入结点放到正确的位置就可以啦。</p>
<p><strong>Q:</strong><br>但插入结点是应该是什么颜色呢？</p>
<p><strong>A:</strong><br>红色。</p>
<p><strong>理由：</strong></p>
<p>红色在父结点（如果存在）为黑色结点时，红黑树的黑色平衡没被破坏，不需要做自平衡操作。但如果插入结点是黑色，那么插入位置所在的子树黑色结点总是多1，必须做自平衡。</p>
<h3 id="插入场景"><a href="#插入场景" class="headerlink" title="插入场景"></a>插入场景</h3><p><img src="https://img-blog.csdnimg.cn/20200315224334183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>根据二叉树的性质，除了情景2，所有插入操作都是在叶子结点进行的。这点应该不难理解，因为查找插入位置时，我们就是在找子结点为空的父结点的。</p>
<p>还是先做个约定：</p>
<center><img src="https://img-blog.csdnimg.cn/20200315224508315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="插入情景1：红黑树为空树"><a href="#插入情景1：红黑树为空树" class="headerlink" title="插入情景1：红黑树为空树"></a>插入情景1：红黑树为空树</h3><p>最简单的一种情景，直接把插入结点作为根结点就行，但注意，根据红黑树性质2：根节点是黑色。还需要把插入结点设为黑色。</p>
<p><strong>处理：</strong> 把插入结点作为根结点，并把结点设置为黑色。</p>
<h3 id="插入情景2：插入结点的Key已存在"><a href="#插入情景2：插入结点的Key已存在" class="headerlink" title="插入情景2：插入结点的Key已存在"></a>插入情景2：插入结点的Key已存在</h3><p>插入结点的Key已存在，既然红黑树总保持平衡，在插入前红黑树已经是平衡的，那么把插入结点设置为将要替代结点的颜色，再把结点的值更新就完成插入。</p>
<p><strong>处理：</strong></p>
<ul>
<li>把I设为当前结点的颜色</li>
<li>更新当前结点的值为插入结点的值</li>
</ul>
<h3 id="插入情景3：插入结点的父结点为黑结点"><a href="#插入情景3：插入结点的父结点为黑结点" class="headerlink" title="插入情景3：插入结点的父结点为黑结点"></a>插入情景3：插入结点的父结点为黑结点</h3><p>由于插入的结点是红色的，当插入结点的黑色时，并不会影响红黑树的平衡，直接插入即可，无需做自平衡。</p>
<p><strong>处理：</strong> 直接插入。</p>
<h3 id="插入情景4：插入结点的父结点为红结点"><a href="#插入情景4：插入结点的父结点为红结点" class="headerlink" title="插入情景4：插入结点的父结点为红结点"></a>插入情景4：插入结点的父结点为红结点</h3><blockquote>
<p>回想下红黑树的性质2：根结点是黑色。</p>
</blockquote>
<p>如果插入的父结点为红结点，那么该父结点不可能为根结点，所以插入结点总是存在祖父结点。这点很重要，因为后续的旋转操作肯定需要祖父结点的参与。<br>情景4又分为很多子情景，下面将进入重点部分，各位看官请留神了。</p>
<h4 id="插入情景4-1：叔叔结点存在并且为红结点"><a href="#插入情景4-1：叔叔结点存在并且为红结点" class="headerlink" title="插入情景4.1：叔叔结点存在并且为红结点"></a>插入情景4.1：叔叔结点存在并且为红结点</h4><p>从红黑树性质4可以，祖父结点肯定为黑结点，因为不可以同时存在两个相连的红结点。那么此时该插入子树的红黑层数的情况是：黑红红。显然最简单的处理方式是把其改为：红黑红。如图所示。</p>
<p><strong>处理：</strong></p>
<ul>
<li>将P和S设置为黑色</li>
<li>将PP设置为红色</li>
<li>把PP设置为当前插入结点</li>
</ul>
<center><img src="https://img-blog.csdnimg.cn/20200316110338633.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<center><img src="https://img-blog.csdnimg.cn/20200316110410528.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>可以看到，我们把PP结点设为红色了，如果PP的父结点是黑色，那么无需再做任何处理；但如果PP的父结点是红色，根据性质4，此时红黑树已不平衡了，所以还需要把PP当作新的插入结点，继续做插入操作自平衡处理，直到平衡为止。</p>
<p>试想下PP刚好为根结点时，那么根据性质2，我们必须把PP重新设为黑色，那么树的红黑结构变为：黑黑红。换句话说，从根结点到叶子结点的路径中，黑色结点增加了。这也是唯一一种会增加红黑树黑色结点层数的插入情景。</p>
<p>我们还可以总结出另外一个经验：红黑树的生长是自底向上的。这点不同于普通的二叉查找树，普通的二叉查找树的生长是自顶向下的。</p>
<h4 id="插入情景4-2：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的左子结点"><a href="#插入情景4-2：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的左子结点" class="headerlink" title="插入情景4.2：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的左子结点"></a>插入情景4.2：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的左子结点</h4><p>单纯从插入前来看，也即不算情景 4.1 自底向上处理时的情况，叔叔结点非红即为叶子结点(Nil)。因为如果叔叔结点为黑结点，而父结点为红结点，那么叔叔结点所在的子树的黑色结点就比父结点所在子树的多了，这不满足红黑树的性质5。后续情景同样如此，不再多做说明了。</p>
<p>前文说了，需要旋转操作时，肯定一边子树的结点多了或少了，需要租或借给另一边。插入显然是多的情况，那么把多的结点租给另一边子树就可以了。</p>
<h5 id="插入情景4-2-1：插入结点是其父结点的左子结点"><a href="#插入情景4-2-1：插入结点是其父结点的左子结点" class="headerlink" title="插入情景4.2.1：插入结点是其父结点的左子结点"></a>插入情景4.2.1：插入结点是其父结点的左子结点</h5><p><strong>处理：</strong></p>
<ul>
<li>将P设为黑色</li>
<li>将PP设为红色</li>
<li>对PP进行右旋</li>
</ul>
<center><img src="https://img-blog.csdnimg.cn/20200316133830860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>左边两个红结点，右边不存在，那么一边一个刚刚好，并且因为为红色，肯定不会破坏树的平衡。</p>
<p><strong>Q:</strong><br>&ensp;可以把P设为红色，I和PP设为黑色吗？</p>
<p><strong>A:</strong><br>&ensp;可以！看过《算法：第4版》的同学可能知道，书中讲解的就是把P设为红色，I和PP设为黑色。但把P设为红色，显然又会出现情景4.1的情况，需要自底向上处理，做多了无谓的操作。</p>
<h5 id="插入情景4-2-2：插入结点是其父结点的右子结点"><a href="#插入情景4-2-2：插入结点是其父结点的右子结点" class="headerlink" title="插入情景4.2.2：插入结点是其父结点的右子结点"></a>插入情景4.2.2：插入结点是其父结点的右子结点</h5><blockquote>
<p>这种情景显然可以转换为情景4.2.1.</p>
</blockquote>
<p><strong>处理：</strong></p>
<ul>
<li>对P进行左旋</li>
<li>把P设置为插入结点，得到情景4.2.1</li>
<li>进行情景4.2.1的处理</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200316134059375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="插入情景4-3：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的右子结点"><a href="#插入情景4-3：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的右子结点" class="headerlink" title="插入情景4.3：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的右子结点"></a>插入情景4.3：叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的右子结点</h4><p>该情景对应情景4.2，只是方向反转，不做过多说明了，直接看图。</p>
<h5 id="插入情景4-3-1：插入结点是其父结点的右子结点"><a href="#插入情景4-3-1：插入结点是其父结点的右子结点" class="headerlink" title="插入情景4.3.1：插入结点是其父结点的右子结点"></a>插入情景4.3.1：插入结点是其父结点的右子结点</h5><p><strong>处理：</strong></p>
<ul>
<li>将P设为黑色</li>
<li>将PP设为红色</li>
<li>对PP进行左旋</li>
</ul>
<center><img src="https://img-blog.csdnimg.cn/2020031613421548.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h5 id="插入情景4-3-2：插入结点是其父结点的右子结点"><a href="#插入情景4-3-2：插入结点是其父结点的右子结点" class="headerlink" title="插入情景4.3.2：插入结点是其父结点的右子结点"></a>插入情景4.3.2：插入结点是其父结点的右子结点</h5><p><strong>处理：</strong></p>
<ul>
<li>对P进行右旋</li>
<li>把P设置为插入结点，得到情景4.3.1</li>
<li>进行情景4.3.1的处理</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200316134345104.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>Q:</strong><br>&ensp;上面的情景举例的都是第一次插入而不包含自底向上处理的情况，那么上面所说的情景都适合自底向上的情况吗？</p>
<p><strong>A:</strong><br>&ensp;答案是肯定的。理由很简单，但每棵子树都能自平衡，那么整棵树最终总是平衡的。</p>
]]></content>
      <categories>
        <category>Feedback</category>
      </categories>
      <tags>
        <tag>红黑树</tag>
      </tags>
  </entry>
  <entry>
    <title>我亦未曾饶过岁月</title>
    <url>/2020/03/12/%E6%88%91%E4%BA%A6%E6%9C%AA%E6%9B%BE%E9%A5%B6%E8%BF%87%E5%B2%81%E6%9C%88/</url>
    <content><![CDATA[<p><strong>岁月不饶人，我亦未曾绕过岁月，总结反思，重新来过。</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">后端开发面试总结</span><br></pre></td></tr></table></figure>
<p>从效率方面考虑，数组的检索效率较好，但是插入和删除效率低下；对于链表，插入和删除的效率较好，但是检索的效率低下，然而 HashMap 合理的继承了上述两位的所有优点，意味着完美。</p>
<p>针对哈希函数的构造</p>
<ol>
<li>直接定址法</li>
<li>平方取中法</li>
<li>除数余数法</li>
</ol>
<p>为几种主要的方法，具体不展开论述，对于 HashMap 神奇存储展示拙见</p>
<blockquote>
<p>HashMap部分源码分析</p>
<ul>
<li>感触</li>
</ul>
</blockquote>
<p> HashMap 的数据是存储在 Table 数组中的，是一个 Entry 数组，二维处理的话，纵观为数组，横向为链表，每当建立一个 HashMap 的时候，就初始化一个数组</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">transient</span> Entry[] table;</span><br></pre></td></tr></table></figure>

<p>【解释】transient关键字，为了使其修饰的对象不参与序列化<br>【实质】此 table 对象无法持久化</p>
<center><img src="https://img-blog.csdnimg.cn/20200110180103273.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>【总结】对于HashMap的工作原理的简要阐述（可作为面试参考回答）</p>
<p>HashMap类有一个叫做Entry的内部类。这个Entry类包含了key-value作为实例变量。 每当往hashmap        里面存放key-value对的时候，都会为它们实例化一个Entry对象，这个Entry对象就会存储在前面提到    的Entry数组table中。Entry具体存在table的那个位置是 根据key的hashcode()方法计算出来的hash值    （来决定）。</p>
<ul>
<li>定义数组中的链表</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Basic hash bin node, used for most entries.  (See below for</span></span><br><span class="line"><span class="comment">* TreeNode subclass, and in LinkedHashMap for its Entry subclass.)</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Map</span>.<span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123; <span class="comment">//Entry数组中的链表</span></span><br><span class="line">	<span class="keyword">final</span> <span class="keyword">int</span> hash;</span><br><span class="line">    <span class="keyword">final</span> K key;</span><br><span class="line">    V value;</span><br><span class="line">    Node&lt;K,V&gt; next;</span><br><span class="line"></span><br><span class="line">    Node(<span class="keyword">int</span> hash, K key, V value, Node&lt;K,V&gt; next) &#123;</span><br><span class="line">    	<span class="keyword">this</span>.hash = hash;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">        <span class="keyword">this</span>.next = next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>取 Entry ，判断方法为【如果 key==null ，直接取数组的第一个元素，如果不是，先计算出 key 的 hashcode 找到下标，再判断是否相等，如果相等，则返回对应的 entry ，如果不相等，则返回 null 】</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt; e;</span><br><span class="line">    <span class="keyword">return</span> (e = getNode(hash(key), key)) == <span class="keyword">null</span> ? <span class="keyword">null</span> : e.value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>扩容</li>
</ul>
<p>其本质是先新创建一个2倍于原来长度的 table 数组，通过重新遍历将旧的 table 中的数据复制 到新的 table 中，在新的 table 中，索引 hash 值重新计算，并且更新扩容后的阈值。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> ((size &gt;= threshold) &amp;&amp; (<span class="keyword">null</span> != table[bucketIndex])) &#123;</span><br><span class="line"><span class="comment">// 将table表的长度增加到之前的两倍</span></span><br><span class="line">resize(<span class="number">2</span> * table.length);</span><br><span class="line"><span class="comment">// 重新计算哈希值</span></span><br><span class="line">hash = (<span class="keyword">null</span> != key) ? hash(key) : <span class="number">0</span>;</span><br><span class="line"><span class="comment">// 从新计算新增元素在扩容后的table中应该存放的index</span></span><br><span class="line">bucketIndex = indexFor(hash, table.length);</span><br></pre></td></tr></table></figure>
<p>其中 resize 起到了创建一个新的 table 数组的作用</p>
<p>针对HashMap的数据存储无序性及其他特性</p>
<ul>
<li>附上一个较为完善的解析：<a href="https://www.jianshu.com/p/dde9b12343c1" target="_blank" rel="noopener">https://www.jianshu.com/p/dde9b12343c1</a></li>
</ul>
<blockquote>
<p>HashCode</p>
<ul>
<li>与 equals 的协调合作</li>
</ul>
</blockquote>
<p>hashcode 和 equals 通常是同时出现用来获取值的【在 HashMap 中使用 hashcode() 和 equals() 方法确定键值对的索引】</p>
<p><strong>用法：</strong><br>初步了解了HashMap的工作原理，在此基础上，如果要在 HashMap 中新增加元素的时候保证不重复，只用 equals 显然不如结合 hashcode方便，可以直接利用此方法将新的数据存储而不进行任何比较，这也就是查找效率高的本质，降低了比较次数。</p>
<p><strong>注意点：</strong><br>在Java中相等（相同）的对象必须具有相等的哈希码（或者散列码），但是如果两个对象的hashcode相同，它们并不一定相同。没有正确使用的时候会导致相同 hash 值的结果。</p>
<blockquote>
<ul>
<li>方法详解</li>
</ul>
</blockquote>
<p>先看 hashcode 的定义<code>public native int  hashcode();</code> – 证明 hashcode 是一个本地方法【前提是原生的方法而非经过自定义覆盖过的】</p>
<p>hashcode() 方法给对象返回一个 hash code 值，性质有以下几点：</p>
<ol>
<li>在一个Java应用执行期间，如果一个对象提供给 equals() 做比较的信息没有被修改的话，该对象多次调用hashCode()方法，该方法返回的 integer 必须相同；</li>
<li>如果两个对象依 equals() 方法判断是相等的，分别调用 hashcode() 方法产生的值必须相同；</li>
<li>并不要求依 equals() 方法判断不相等的两个对象，分别调用各自的 hashCode() 方法必须产生不同的 integer 值。然而，对于不同的对象产生不同的integer结果，有可能会提高 hash table 的性能。</li>
</ol>
<blockquote>
<p>LinkedHashMap</p>
</blockquote>
<p>我们知道 HashMap 的存储是无序的，那么为了有顺序存储键值对，就需要引入LinedHashMap。<del>【当然检验此句话可以尝试分别输出 HashMap 中存储的值和 LinkedHashMap 中的值】</del> </p>
<p>同样的在 Map 旗下，而且 LinkedHashMap 继承了 HashMap ，在定义的时候稍微变动类名就OK：<code>Map&lt;String, String&gt; = new HashMap&lt;&gt;();</code><br><code>Map&lt;String, String&gt; = new LinkedHashMap&lt;&gt;();</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LinkedHashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">HashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Map</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>所以在用法上，LinkedHashMap 和 HashMap 的用法基本一致，通过 put 和 get 的方法进行 key-value 的存储和读取。</p>
<blockquote>
<ul>
<li>区别点</li>
</ul>
</blockquote>
<p>LinkedHashMap 的有序性到底体现在什么地方？</p>
<p>其本质为 HashMap + 双向链表的组合，即通过 HashMap 的构造函数初始化 Entry 数组【图左】，然后通过LinkedHashMap 自身的 init  的初始化方法初始化了一个只有头节点的双向链表【图右】。</p>
<center><img src="https://img-blog.csdnimg.cn/20200111114222993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>有了双向链表的存在，那么在进行 put /扩容等操作时就需要考虑到双向链表的事。</p>
<ol>
<li>在 put 元素时，不但要把它加入到HashMap中去，还要加入到双向链表中；【记住一点：header 并不能存储数据】</li>
<li>扩容时，数据的再散列和HashMap是不一样。</li>
</ol>
<p>① HashMap 是先遍历旧 table ，再遍历旧 table 中每个元素的单向链表，取得 Entry 以后，重新计算 hash 值，然后存放到新 table 的对应位置；</p>
<p>② LinkedHashMap 是遍历的双向链表，取得每一个 Entry ，然后重新计算hash值并且存放到新 table 的对应位置。</p>
<p>附上一个较为详细的解释：<a href="https://www.jianshu.com/p/8f4f58b4b8ab" target="_blank" rel="noopener">https://www.jianshu.com/p/8f4f58b4b8ab</a></p>
<blockquote>
<p>ArrayList</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ArrayList</span>&lt;<span class="title">E</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractList</span>&lt;<span class="title">E</span>&gt; <span class="keyword">implements</span> <span class="title">List</span>&lt;<span class="title">E</span>&gt;, <span class="title">RandomAccess</span>, <span class="title">Cloneable</span>, <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span></span></span><br></pre></td></tr></table></figure>
<p>ArrayList 是可以动态增长和缩减的索引序列，它是基于数组实现的List类</p>
<p>ArrayList 的应用在于其的可变长性，本质是类对象内部定义了 capacity 属性，封装了一个动态再分配的 Object[ ] 数组，当数组的元素数量增加时，属性的值会自动增加。</p>
<p>如果想ArrayList中添加大量元素，可使用ensureCapacity方法一次性增加capcacity，可以减少增加重分配的次数提高性能 。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Default initial capacity.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> 	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_CAPACITY = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Shared empty array instance used for empty instances.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Object[] EMPTY_ELEMENTDATA = &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Shared empty array instance used for default sized empty instances. We</span></span><br><span class="line"><span class="comment"> * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when</span></span><br><span class="line"><span class="comment"> * first element is added.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Increases the capacity of this &lt;tt&gt;ArrayList&lt;/tt&gt; instance, if</span></span><br><span class="line"><span class="comment"> * necessary, to ensure that it can hold at least the number of elements</span></span><br><span class="line"><span class="comment"> * specified by the minimum capacity argument.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span>   minCapacity   the desired minimum capacity</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ensureCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA)</span><br><span class="line">            <span class="comment">// any size if not default element table</span></span><br><span class="line">            <span class="comment">// larger than default for default empty table. It's already</span></span><br><span class="line">            <span class="comment">// supposed to be at default size.</span></span><br><span class="line">            <span class="comment">//DEFAULT_CAPACITY;//这里的值为10</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (minCapacity &gt; minExpand) &#123;</span><br><span class="line">            ensureExplicitCapacity(minCapacity);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>ArrayList 和 vector 的区别</p>
<blockquote>
<p>线程</p>
</blockquote>
<p>ArrayList 线程是不安全的, vector 的线程是安全的<br>当多线程访问一个 ArrayList 时,需要手动保持同步性</p>
</blockquote>
<blockquote>
<p>ArrayList 底层实现</p>
</blockquote>
<p>底层的数据结构就是数组，本质是 elementData，数组元素类型为Object类型，即可以存放所有类型数据【包括 null 】，所有的操作都是基于数组的。</p>
<p>ArrayList 继承了 AbstractList，AbstractList 继承了 AbstractCollection</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">grow</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">	<span class="comment">// overflow-conscious code</span></span><br><span class="line">    <span class="comment">//将扩充前的elementData大小给oldCapacity</span></span><br><span class="line">    <span class="keyword">int</span> oldCapacity = elementData.length;</span><br><span class="line">    <span class="comment">//newCapacity就是1.5倍的oldCapacity，因为向右移1位代表除以2</span></span><br><span class="line">    <span class="keyword">int</span> newCapacity = oldCapacity + (oldCapacity &gt;&gt; <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">if</span> (newCapacity - minCapacity &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    *这句话就是适应elementData空数组的时候，length=0</span></span><br><span class="line"><span class="comment">    *那么oldCapacity=0,newCapacity=0,所以这个判断成立</span></span><br><span class="line"><span class="comment">    *在这里就是真正的初始化elementData的大小了，前面的工作都是准备工作。</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">     	newCapacity = minCapacity;</span><br><span class="line">    <span class="keyword">if</span> (newCapacity - MAX_ARRAY_SIZE &gt; <span class="number">0</span>)</span><br><span class="line">    <span class="comment">//如果newCapacity超过了最大的容量的限制，就调用hugeCapacity，也就是能给的最大值给newCapacity</span></span><br><span class="line">        newCapacity = hugeCapacity(minCapacity);</span><br><span class="line">    <span class="comment">// minCapacity is usually close to size, so this is a win:</span></span><br><span class="line">    <span class="comment">//新的容量大小已经确定好了，就copy数组，改变容量大小</span></span><br><span class="line">    elementData = Arrays.copyOf(elementData, newCapacity);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>grow() 方法是 ArrayList 的核心，该方法保障了<strong>变长</strong>特性。</p>
<p>附上一个底层方法个详解：<a href="https://blog.csdn.net/weixin_42036647/article/details/100709820" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42036647/article/details/100709820</a></p>
<blockquote>
<p>ArrayList 和数组的区别</p>
</blockquote>
<p><strong>数组</strong></p>
<p>优点：在内存中的存储时连续的，索引速度快，赋值和修改也较为方便</p>
<p>缺点：数组中插入元素比较繁琐，而且在定义时需要指定元素类型和数组长度</p>
<p><strong>ArrayList</strong></p>
<p>优点：解决了数组的缺点</p>
<p>缺点：在插入元素时，可以插入任意元素，都被处理为 Object 类，但是不保证数据安全，在数据利用的时候会报类型不匹配的错误</p>
<p>从时间复杂度的角度分析 ArrayList：<a href="https://blog.csdn.net/weixin_33939380/article/details/87975097" target="_blank" rel="noopener">https://blog.csdn.net/weixin_33939380/article/details/87975097</a></p>
<blockquote>
<p>+= 是如何实现的</p>
</blockquote>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">C/C++ code</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt; </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">const</span> <span class="title">T</span> <span class="title">operator</span>+=(<span class="title">const</span> <span class="title">T</span>&amp; <span class="title">rhs</span>) </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span> + rhs;                </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以对于 += 运算符，在使用过程中不会申请新的内存。</p>
<p><strong>由于运算符优先级不同，i = i + 1 的运算速度要低于 i += 1</strong><br>【优先级标准：优先级从上到下依次递减，最上面具有最高的优先级，逗号操作符具有最低的优先级。表达式的结合次序取决于表达式中各种运算符的优先级。优先级高的运算符先结合，优先级低的运算符后结合，同一行中的运算符的优先级相同。】</p>
<blockquote>
<p>字符串</p>
</blockquote>
<p>在 Java 中字符串 String 是不可改变的，通过操作运算符改变后返回的是新对象，并不能将原有字符串改变</p>
<p><strong>原因：</strong></p>
<p>1)字符串变量是存放栈内存中的，而其所对应的字符串是存放在堆内存中的。</p>
<p>2)某个字符串变量一旦赋值，实际上就是在栈内存中的这个字符串变量通过内存地址指向到堆内存中的某个字符串。</p>
<p>3)而如果这个字符串变量所对应的字符串发生改变，在堆内存中就会新款开辟一块空间出来存放这新字符串，并使得原先的内存地址指向发生改变</p>
<p>对于之前的字符串，如果不再有其他字符串变量所指向，那么将会变成垃圾，交由 Java 中的 GC 机制进行处理。</p>
<p>【注意：无论对字符串变量进行重新赋值、截取、追加等操作其实改变的都不是字符串本身，而是指向该字符串的内存地址。】</p>
<blockquote>
<p>字符串 string 转 Int</p>
</blockquote>
<p>在 Java 中 Integer 类型下有名为 parseInt() 的方法，专门用于将字符串参数作为有符号的十进制整数进行解析【如果方法有两个参数， 使用第二个参数指定的基数，将字符串参数解析为有符号的整数。】</p>
<p>实现 parseInt() 方法：</p>
<ol>
<li>使用 map 字典的形式存储从 0~9 十位数字分别对应的字母，进行字符串的切割组合</li>
<li>直接进行每一位字符的转换组合</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">parseInt</span><span class="params">(String num)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (num.startsWith(<span class="string">"-"</span>)) &#123;</span><br><span class="line">		index++;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">while</span> (index &lt;= num.length() - <span class="number">1</span>) &#123;</span><br><span class="line">		<span class="keyword">int</span> n = num.charAt(index);</span><br><span class="line">		<span class="keyword">if</span> (n &lt;= <span class="number">57</span> &amp;&amp; n &gt;= <span class="number">48</span>) &#123;</span><br><span class="line">			result *= <span class="number">10</span>;</span><br><span class="line">			result += n - <span class="number">48</span>;</span><br><span class="line">		&#125; </span><br><span class="line">		<span class="keyword">else</span> &#123;</span><br><span class="line">			System.err.println(<span class="string">"args is not a interger"</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		index++;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (num.startsWith(<span class="string">"-"</span>)) &#123;</span><br><span class="line">		result = -result;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Feedback</category>
      </categories>
      <tags>
        <tag>面试总结</tag>
      </tags>
  </entry>
  <entry>
    <title>GC in HotSpot JVM</title>
    <url>/2020/03/12/GC%20in%20HotSpot%20JVM/</url>
    <content><![CDATA[<p>今天我们来聊 GC ( Garbage Collection ),  Java 与 C++ 之间有一堵由<strong>内存动态分配</strong>和<strong>垃圾收集技术</strong>所围城的 <font color=blue>“高墙”</font>。</p>
<center>外面的人想进去，里面的人想出来</center>

<h2 id="GC-是什么？"><a href="#GC-是什么？" class="headerlink" title="GC 是什么？"></a>GC 是什么？</h2><blockquote>
<p>Java垃圾回收机制</p>
</blockquote>
<p>目前主流的 JVM（HotSpot）采用的是分代收集算法。与 C++ 不同的是，Java 采用的是类似于树形结构的可达性分析法来判断对象是否还存在引用。</p>
<p>即：从 GC Root 开始，把所有可以搜索得到的对象标记为存活对象。</p>
<h2 id="JVM-内存结构"><a href="#JVM-内存结构" class="headerlink" title="JVM 内存结构"></a>JVM 内存结构</h2><blockquote>
<p>GC主要是针对运行的数据区的</p>
</blockquote>
<p>做应用程序开发，只需要大方向关注 5 块区域</p>
<ol>
<li><a href="#method_area">方法区(Method Area)；</a></li>
<li><a href="#jvm_stack">Java栈(Java stack)；</a></li>
<li><a href="#native_method_stacks">本地方法栈(Native Method Stack)；</a></li>
<li><a href="#heap">堆(Heap)；</a></li>
<li><a href="#program_counter_register">程序计数器(Program Counter Register)</a></li>
</ol>
<center><img src="https://img-blog.csdnimg.cn/20200312115043330.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<div id="method_area"><b>方法区 (Method Area)</b></div><br/>

<p>&ensp;&ensp;&ensp;&ensp;与Heap一样，也是各个线程共享的内存域，这块区域主要是用来存储类加载器加载的类信息，常量，静态变量，通俗的讲就是编译后的class文件信息。</p>
<div id="jvm_stack"><b>Jvm栈</b></div><br/>

<p>&ensp;&ensp;&ensp;&ensp;与程序计数器一样，它是每个线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。</p>
<div id="native_method_stacks"><b>本地方法栈（Native Method Stacks）</b></div><br/>

<p>&ensp;&ensp;&ensp;&ensp;本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。</p>
<div id="heap"><b>堆 (Heap)</b></div><br/>

<p>&ensp;&ensp;&ensp;&ensp;是Jvm管理的内存中最大的一块。程序的主要数据也都是存放在堆内存中的，这一块区域被所有的线程所共享，通常出现线程安全问题的一般都是这个区域的数据出现的问题。</p>
<div id="program_counter_register"><b>程序计数器 (Program Counter Register)</b></div><br/>

<p>&ensp;&ensp;&ensp;&ensp;个人感觉的他就是为多线程准备的，程序计数器是每个线程独有的，所以是线程安全的。它主要用于记录每个线程的执行情况。</p>
<h2 id="GC实现机制-我们为什么要去了解GC和内存分配？"><a href="#GC实现机制-我们为什么要去了解GC和内存分配？" class="headerlink" title="GC实现机制-我们为什么要去了解GC和内存分配？"></a>GC实现机制-我们为什么要去了解GC和内存分配？</h2><p><strong>A:</strong></p>
<ul>
<li>内存溢出</li>
<li>内存泄漏</li>
<li>并发处理</li>
<li>当垃圾收集成为系统达到更高并发量的瓶颈时，我们需要监控和调节</li>
</ul>
<blockquote>
<p>GC 主要是针对 Java Heap 这块区域，其次是方法区</p>
</blockquote>
<p>JVM的内存空间，从大的层面上来分析包含：新生代空间( Young )和老年代空间（ Old ）。新生代空间（Young）又被分为2个部分（Eden区域、Survivous区域）和3个板块（1个Eden区域和2个Survivous区域）</p>
<center><img src="https://img-blog.csdnimg.cn/2020031212013163.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p><b>Eden(伊甸园)区域</b>：用来存放使用new或者newInstance等方式创建的对象，默认这些对象都是存放在Eden区，除非这个对象太大，或者超出了设定的阈值-XX:PretenureSizeThresold，这样的对象会被直接分配到Old区域。</p>
<p>2个<b>Survivous(幸存)区域</b>：一般称为S0，S1，理论上他们一样大。</p>
<p><strong>Q:</strong><br><strong>S0和S1一般多大，靠什么参数来控制，有什么变化？</strong></p>
<p><strong>A:</strong></p>
<p>一般来说很小，我们大概知道它与Young差不多相差一倍的比例，设置的参数主要有两个：</p>
<blockquote>
<p>-XX:SurvivorRatio=8<br>-XX:InitialSurvivorRatio=8</p>
</blockquote>
<p><strong>第一个参数</strong> :是 Eden 和 Survivous 区域比重（注意 Survivous 一般包含两个区域 $S_{0}$ 和 $S_{1}$ ，这里是一个Survivous的大小）。如果将 -XX:SurvivorRatio=8 设置为8，则说明Eden区域是一个Survivous区的8倍，换句话说 $S_{0}$ 或 $S_{1}$ 空间是整个Young空间的1/10，剩余的8/10由Eden区域来使用。</p>
<p><strong>第二个参数</strong> :是 $Young/S0$ 的比值，当其设置为 8 时，表示 $S_{0}$ 或 $S_{1}$ 占整个 Young 空间的1/8（或12.5%）</p>
<h2 id="GC的实现"><a href="#GC的实现" class="headerlink" title="GC的实现"></a>GC的实现</h2><ol>
<li>Minor GC</li>
<li>Full GC</li>
</ol>
<p><font color=red>通过 Minor GC 后进入旧生代的平均大小大于旧生代的可用内存。如果发现统计数据说之前 Minor GC 的平均晋升大小比目前旧生代剩余的空间大，则不会触发 Minor GC 而是转为触发 Full GC。</font></p>
<h3 id="Minor-GC"><a href="#Minor-GC" class="headerlink" title="Minor GC"></a>Minor GC</h3><h4 id="First-GC"><a href="#First-GC" class="headerlink" title="First GC"></a>First GC</h4><p>在不断创建对象的过程中，当Eden区域被占满，此时会开始做Young GC也叫Minor GC</p>
<ol>
<li><p>第一次GC时Survivous中S0区和S1区都为空，将其中一个作为To Survivous(用来存储Eden区域执行GC后不能被回收的对象)。比如：将S0作为To Survivous，则S1为From Survivous。</p>
</li>
<li><p>将Eden区域经过GC不能被回收的对象存储到To Survivous（S0）区域（此时Eden区域的内存会在垃圾回收的过程中全部释放），但如果To Survivous（S0）被占满了，Eden中剩下不能被回收对象只能存放到Old区域。</p>
</li>
<li><p>将Eden区域空间清空，此时From Survivous区域（S1）也是空的。</p>
</li>
<li><p>S0与S1互相切换标签，S0为From Survivous，S1为To Survivous。</p>
</li>
</ol>
<center><img src="https://img-blog.csdnimg.cn/20200312121238749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h4 id="Second-GC"><a href="#Second-GC" class="headerlink" title="Second GC"></a>Second GC</h4><p>当第二次Eden区域被占满时，此时开始做GC</p>
<ol>
<li><p>将Eden和From Survivous(S0)中经过GC未被回收的对象迁移到To Survivous(S1)，如果To Survious(S1)区放不下，将剩下的不能回收对象放入Old区域；</p>
</li>
<li><p>将Eden区域空间和From Survivous（S0）区域空间清空；</p>
</li>
<li><p>S0与S1互相切换标签，S0为To Survivous，S1为From Survivous。</p>
</li>
</ol>
<center><img src="https://img-blog.csdnimg.cn/2020031213042674.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>。。。以此类推</p>
<h4 id="第三次、第四次…"><a href="#第三次、第四次…" class="headerlink" title="第三次、第四次…"></a>第三次、第四次…</h4><blockquote>
<p>反反复复多次没有被淘汰的对象，将会被放入Old区域中</p>
</blockquote>
<h3 id="究竟会经过多少次？"><a href="#究竟会经过多少次？" class="headerlink" title="究竟会经过多少次？"></a>究竟会经过多少次？</h3><p>设置次数的参数：由计数器记录【计数器会在对象的头部记录它的交换次数】 </p>
<blockquote>
<p>–XX:MaxTenuringThreshold=15</p>
</blockquote>
<h3 id="Full-GC"><a href="#Full-GC" class="headerlink" title="Full GC"></a>Full GC</h3><p>我们来捋捋 Full GC 的触发条件：</p>
<ol>
<li>System.gc()方法的调用；<br>此方法的调用是建议JVM进行Full GC,虽然只是建议而非一定，但很多情况下它会触发 Full GC,从而增加Full GC的频率，也即增加了间歇性停顿的次数。强烈影响系建议能不使用此方法就别使用，让虚拟机自己去管理它的内存，可通过通过-XX:+ DisableExplicitGC来禁止RMI（Java远程方法调用）调用System.gc。</li>
<li>旧生代空间不足;<br>旧生代空间只有在新生代对象转入及创建为大对象、大数组时才会出现不足的现象，当执行Full GC后空间仍然不足，则抛出错误：java.lang.OutOfMemoryError: Java heap space 。为避免以上两种状况引起的FullGC，调优时应尽量做到让对象在Minor GC阶段被回收、让对象在新生代多存活一段时间及不要创建过大的对象及数组。</li>
<li>由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小</li>
</ol>
<h2 id="GC实现机制-Java虚拟机如何实现垃圾回收机制"><a href="#GC实现机制-Java虚拟机如何实现垃圾回收机制" class="headerlink" title="GC实现机制-Java虚拟机如何实现垃圾回收机制"></a>GC实现机制-Java虚拟机如何实现垃圾回收机制</h2><ol>
<li><a href="#reference_counting">引用计数算法（Reference Counting）</a></li>
<li><a href="#reachability_analysis">可达性分析算法（Reachability Analysis）</a></li>
</ol>
<h3 id="引用计数算法（Reference-Counting）"><a href="#引用计数算法（Reference-Counting）" class="headerlink" title="引用计数算法（Reference Counting）"></a><div id="reference_counting">引用计数算法（Reference Counting）</div></h3><p>给对象添加一个引用计数器，每当有一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的，这就是引用计数算法的核心。</p>
<p>客观来讲，引用计数算法实现简单，判定效率也很高，在大部分情况下都是一个不错的算法。但是Java虚拟机并没有采用这个算法来判断何种对象为死亡对象，因为它很难解决对象之间相互循环引用的问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReferenceCountingGC</span> </span>&#123;</span><br><span class="line">   <span class="keyword">public</span> Object object = <span class="keyword">null</span>;</span><br><span class="line">  </span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> OenM = <span class="number">1024</span> * <span class="number">1024</span>;</span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">byte</span>[] bigSize = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">2</span> * OneM];</span><br><span class="line"> </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">testCG</span><span class="params">()</span></span>&#123;</span><br><span class="line">      ReferenceCountingGC objA = <span class="keyword">new</span> ReferenceCountingGC(); </span><br><span class="line">       ReferenceCountingGC objB = <span class="keyword">new</span> ReferenceCountingGC(); </span><br><span class="line">      </span><br><span class="line">       objA.object = <span class="keyword">null</span>;</span><br><span class="line">       objB.object = <span class="keyword">null</span>;</span><br><span class="line"> </span><br><span class="line">      System.gc();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上述代码段中，objA与objB互相循环引用，没有结束循环的判断条件，运行结果显示Full GC，就说明当Java虚拟机并不是使用引用计数算法来判断对象是否存活的。</p>
<h3 id="可达性分析算法（Reachability-Analysis）"><a href="#可达性分析算法（Reachability-Analysis）" class="headerlink" title="可达性分析算法（Reachability Analysis）"></a><div id="reachability_analysis">可达性分析算法（Reachability Analysis）</div></h3><blockquote>
<p>这是Java虚拟机采用的判定对象是否存活的算法。</p>
</blockquote>
<p>通过一系列的称为“GC Roots”的对象作为起始点，从这些结点开始向下搜索，搜索所走过的路径称为饮用链（Reference Chain），当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。</p>
<p>可作为GC Roots的对象包括：</p>
<ol>
<li>虚拟机栈中引用的对象;</li>
<li>方法区中类静态属性引用的对象;</li>
<li>方法区中常量引用的对象;</li>
<li>本地方法栈JNI引用的对象</li>
</ol>
<h2 id="GC实现机制-何为死亡对象？"><a href="#GC实现机制-何为死亡对象？" class="headerlink" title="GC实现机制-何为死亡对象？"></a>GC实现机制-何为死亡对象？</h2><blockquote>
<p>从根引用开始，对象的内部属性可能也是引用，只要能级联到的都被认为是活着的对象。</p>
</blockquote>
<blockquote>
<p>Java虚拟机在进行死亡对象判定时，会经历两个过程。</p>
</blockquote>
<p>如果对象在进行可达性分析后没有与 GC Roots 相关联的引用链，则该对象会被 JVM 进行第一次标记并且进行一次筛选</p>
<blockquote>
<p>筛选的条件是此对象是否有必要执行 finalize() 方法，</p>
</blockquote>
<ul>
<li><p>如果当前对象没有覆盖该方法，或者 finalize() 方法已经被 JVM 调用过都会被虚拟机判定为“没有必要执行”。</p>
</li>
<li><p>如果该对象被判定为没有必要执行，那么该对象将会被放置在一个叫做 F-Queue 的队列当中，并在稍后由一个虚拟机自动建立的、低优先级的 Finalizer 线程去执行它，在执行过程中JVM可能不会等待该线程执行完毕，因为如果一个对象在 finalize() 方法中执行缓慢，或者发生死循环，将很有可能导致 F-Queue 队列中其他对象永久处于等待状态，甚至导致整个内存回收系统崩溃。</p>
</li>
<li><p>如果在 finalize() 方法中该对象重新与引用链上的任何一个对象建立了关联，即该对象连上了任何一个对象的引用链，例如 this 关键字，那么该对象就会逃脱垃圾回收系统；</p>
</li>
<li><p>如果该对象在 finalize() 方法中没有与任何一个对象进行关联操作，那么该对象会被虚拟机进行第二次标记，该对象就会被垃圾回收系统回收。值得注意的是 finaliza() 方法JVM系统只会自动调用一次.</p>
</li>
<li><p>如果对象面临下一次回收，它的 finalize() 方法不会被再次执行。</p>
</li>
</ul>
<h2 id="GC实现机制-垃圾收集算法"><a href="#GC实现机制-垃圾收集算法" class="headerlink" title="GC实现机制-垃圾收集算法"></a>GC实现机制-垃圾收集算法</h2><ol>
<li>标记-清除算法（Mark-Sweep）</li>
<li>复制算法（Copying）</li>
<li>分代收集(Generational Collection)算法</li>
<li>标记-整理(Mark-Compat)算法</li>
</ol>
<p>新生代中，每次垃圾收集时都有大批对象死去，只有少量存活，就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集；</p>
<p>老年代中，其存活率较高、没有额外空间对它进行分配担保，就应该使用“标记-整理”或“标记-清理”算法进行回收。</p>
<h2 id="常用的垃圾回收器"><a href="#常用的垃圾回收器" class="headerlink" title="常用的垃圾回收器"></a>常用的垃圾回收器</h2><ol>
<li>Serial收集器</li>
<li>ParNew收集器</li>
<li>Parallel Scavenge收集器</li>
<li>CMS(Concurrent Mark Sweep) 收集器</li>
<li>G1(Garbage First) 收集器</li>
</ol>
<p><strong>Q:</strong><br><strong>对象进入Old区域有什么坏处？</strong></p>
<p><strong>A:</strong><br>&ensp;&ensp;&ensp;&ensp;&ensp;Old区域一般称为老年代，老年代与新生代不一样。新生代，我们可以认为存活下来的对象很少，而老年代则相反，存活下来的对象很多，所以JVM的堆内存，才是我们通常关注的主战场，因为这里面活着的对象非常多，所以发生一次FULL GC，来找出来所有存活的对象是非常耗时的，因此，我们应该避免FULL GC的发生。</p>
<p><strong>Q:</strong><br><strong>为什么发生FULL GC会带来很大的危害？</strong></p>
<p><strong>A:</strong><br>&ensp;&ensp;&ensp;&ensp;在发生FULL GC的时候，意味着JVM会安全的暂停所有正在执行的线程（Stop The World），来回收内存空间，在这个时间内，所有除了回收垃圾的线程外，其他有关JAVA的程序，代码都会静止，反映到系统上，就会出现系统响应大幅度变慢，卡机等状态。</p>
]]></content>
      <categories>
        <category>Feedback</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title>人=波,颠倒因果</title>
    <url>/2020/03/05/%E4%BA%BA=%E6%B3%A2%EF%BC%8C%E9%A2%A0%E5%80%92%E5%9B%A0%E6%9E%9C/</url>
    <content><![CDATA[<p>忆中学时期的物理课，物理科学使我受益匪浅，特别是那一假设：</p>
<hr>
<p>人说可能也是以一种波的形式存在，当你观察的时候就以实物的形式存在，当你不观察的时候就以波的形式存在，因为每当我们闭上眼睛根本不知道周围真正发生了什么。</p>
<hr>
<p>这个假设深埋心底，迟迟未能解决，到了大学，有幸在物理实验中接触到双缝干涉实验，并受一篇物理学文章的启发，终于算是解决了这个埋藏已久的问题。</p>
<p><strong>引入：</strong></p>
<blockquote>
<p>双缝干涉实验：颠倒因果</p>
</blockquote>
<ul>
<li><p>牛顿的经典力学曾被认为适用于宇宙中所有物体，但自从波尔的量子力学出现，经典力学总会失效，微观世界与宏观世界并不同。</p>
</li>
<li><p>双缝干涉实验中，让光源的光线先通过第一个圆孔，这样产生的光源变成相干光源。</p>
</li>
<li><p>再让相干光源通过两条细缝的纸板，最后用一块屏幕承接光源，发现屏幕上出现了明暗相间的条纹。</p>
</li>
</ul>
<blockquote>
<p>这个实验证明了光不仅是一种粒子还具有波的性质</p>
</blockquote>
<p><strong>双缝干涉中粒子是光子，不知对于其他粒子可否能行呢？</strong></p>
<blockquote>
<p>电子</p>
</blockquote>
<p>实验：</p>
<p>&ensp;&ensp;当科学家用电子枪做电子的双缝干涉实验时，电子枪连续向双缝中的其中一条缝发射电子，屏幕上也出现了明暗相间的条纹，这就说明电子和光子一样，具有波粒二象性。后来逐渐证明了质子中子都具有波粒二象性。</p>
<p>测量：</p>
<p>&ensp;&ensp;为了弄清电子穿过双缝时到底是粒子还是波的形态，科学家就在双缝后摆了量子观显示察仪</p>
<p>结果：<br>&ensp;&ensp;当电子通过左缝时显示1，当电子通过右缝时显示2，除此以外的状态，仪器显示0。当再次对电子进行观测时，干涉条纹消失了，屏幕上出现了明亮的电子亮斑，仪器显示此时电子只经过了一条缝隙，而当关闭观察仪的时候再次出现了干涉条纹。</p>
<blockquote>
<p>这就说明了<strong>观察</strong>这个动作能对粒子的形态产生影响</p>
<blockquote>
<p>进一步猜测<strong>不观察电子就是波，观察电子就是粒子</strong></p>
</blockquote>
</blockquote>
<p><strong>这点正是和我刚开始的问题一致</strong>，物理学家认为这或许只是观察仪器发出的光子影响了电子运行中的状态，使电子吸收了光的能量从而产生变化。</p>
<p>其实，这种现象类似于<code>海森堡不确定性原理</code>，微观世界中你无法同时精准得知道粒子的动量和位置，动量越精确位置就越不准。</p>
<p>【注意：涉及到相近于原子内部电子绕原子核转动的问题，电子实际并非绕原子核做规则的圆周运动，而是以一种概率性的出现，一定概率一定时间点出现在某个位置，下一刻的位置并不确定也没有规律可言】</p>
<hr>
<p><strong>补充：</strong></p>
<center><b>量子延迟选择实验</b></center>

<p>$$<br>这是双缝干涉实验的另一个版本<br>$$</p>
<p>当电子枪向双缝发射电子时，这时观察仪没有开启，电子以波的形式同时通过两条细缝，</p>
<p>但当电子通过细缝之后立刻开启观察仪，记录电子的运行轨迹时，<strong>发现电子竟然是从其中的一条细缝通过</strong>,在微观世界中这个动作在发生之前已经对电子的形态产生了影响</p>
<p>明确说明这种现象在现实世界或者宏观条件下时不可能存在的，既说明了宏观与微观的区别，更加说明了微观世界中可以颠倒因果。</p>
<hr>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>双缝干涉</tag>
        <tag>不确定性</tag>
        <tag>波粒二象性</tag>
      </tags>
  </entry>
  <entry>
    <title>CPU Sched</title>
    <url>/2020/03/05/CPU%20Sched/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文将介绍 CPU 高级策略之 CPU 调度，针对一系列调度策略展开。</p>
<center><b>THE CRUX</b></center>

<p><strong>Q：</strong><br>&ensp;如何制定调度策略？</p>
<ol>
<li>如何开发一个基本框架考虑调度策略？</li>
<li>需要做哪些关键的假设？</li>
<li>哪些指标是最重要的？</li>
<li>最早的计算机使用的基本方法？</li>
</ol>
<h2 id="工作量假设-Workload-Assumptions"><a href="#工作量假设-Workload-Assumptions" class="headerlink" title="工作量假设 [Workload Assumptions]"></a>工作量假设 [Workload Assumptions]</h2><p>在开始介绍策略之前，首先对系统中运行的进程 (或称为工作量) 做一些假设。</p>
<blockquote>
<p>确定工作量是建立策略的步骤中最重要的部分</p>
</blockquote>
<div id="presume"><font size=4><b>假设如下：</b></font></div><br/>

<p>&ensp;<strong>Hint：</strong> 这些看似不可行的假设，在之后的介绍中会放宽要求，简化了全业务调度的开发</p>
<ol>
<li>所有进程的运行时间相同;</li>
<li>所有进程同时到达等待启动；</li>
<li>一旦开始，所有进程终将进行完整的运行流程；</li>
<li>所有的进程均只利用 CPU（不执行任何 I/O）</li>
<li>每个进程的运行时间已知</li>
</ol>
<h3 id="调度指标-Scheduling-Metrics"><a href="#调度指标-Scheduling-Metrics" class="headerlink" title="调度指标 [Scheduling Metrics]"></a>调度指标 [Scheduling Metrics]</h3><blockquote>
<p>我们需要一样东西来衡量不同的调度策略，即调度指标</p>
</blockquote>
<p>设定一个简单的指标：Turnaround Time (理解为 “周转时间” )， 其定义为进程完成的时刻减去到达等待开始的时刻：</p>
<p>$$<br>T_{turnaround } = T_{completion} - T_{arrival}<br>$$</p>
<p>在工作量假设中有一条假设：所有进程同时等待启动，所以  $T_{arrival} = 0$ ，进一步，$T_{turnaround } = T_{completion}$。也就是说放宽假设的要求，这个观点就会改变。</p>
<blockquote>
<p>生活不总是完美的</p>
</blockquote>
<p><strong>注意到</strong>，周转时间是一个性能指标，由此不得不再设定一个指标，Fairness (理解为公平性)</p>
<hr>
<p>在调度中，<strong>性能</strong>和<strong>公平性</strong>常常是<strong>不一致</strong>的</p>
<p>For Example：调度程序可以优化性能，但代价是阻止一些作业运行，从而降低公平性。</p>
<hr>
<h3 id="先进先出（FIFO）"><a href="#先进先出（FIFO）" class="headerlink" title="先进先出（FIFO）"></a>先进先出（FIFO）</h3><blockquote>
<p>FIFO 的优点在于简单易实现</p>
</blockquote>
<p>我们可以实现的最基本的算法是<strong>先进先出调度</strong> ( 亦被称为 <strong>First Come, First Served</strong> - FCFS )</p>
<p><strong>For Example:</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;假设系统中有三个进程，A、B和C，大致同时到达（$T_{arrival}= 0$）。因为 FIFO 必须把某个进程放在第一位执行。假设当它们同时到达时，A在B之前到达，而B在C之前到达。同时假设每个进程运行10秒。这些工作的<strong>平均周转时间</strong>将是多少？</p>
<!-- <center><img src="https://img-blog.csdnimg.cn/20200304141601408.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center> -->

<a data-fancybox="gallery" href="https://img-blog.csdnimg.cn/20200304141601408.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener">
    <img src="https://img-blog.csdnimg.cn/20200304141601408.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70">
</a>

<p>图中模拟了进程执行时间，A 在10 的位置完成，B 在20 的位置完成，C 在30 的位置完成</p>
<p>那么，平均周转时间计算为：</p>
<p>$$<br>{\frac{10+20+30}{3}} = 20<br>$$</p>
<p><strong>问题来了：</strong></p>
<p>如果我们放宽第一个假设的要求，不再假设每个进程的运行时间相同。</p>
<ol>
<li>这种情形下，FIFO 如何运行？</li>
<li>如何构建工作量，会导致 FIFO 性能崩溃？</li>
</ol>
<p><strong>我们继续通过例子模拟进程的运行时间，以及不同运行时间的进程如何搞垮 FIFO</strong></p>
<p><strong>For Example：</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;接着之前的三个进程 A、B、C，与上次不同的是 A 运行时间为100，B、C均为10</p>
<p><strong>模拟结果：</strong></p>
<center><img src="https://img-blog.csdnimg.cn/20200304143751298.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>B、C 在运行之前先等待 A 运行 100，这样，平均周转时间计算为：</p>
<p>$$<br>{\frac{100+110+120}{3}} = 110<br>$$</p>
<p>一些资源较短的潜在消费者排在一些重量级资源消费者的后面，这便是经典的<strong>护航效应 (convoy effect)</strong>。</p>
<p><strong>Q:</strong></p>
<p>&ensp;&ensp;怎样才能开发出一个更好的算法来处理运行时间不同的进程呢？</p>
<h3 id="Shortest-Job-First-SJF"><a href="#Shortest-Job-First-SJF" class="headerlink" title="Shortest Job First - SJF"></a>Shortest Job First - SJF</h3><blockquote>
<p>是一个通用的调度原则，它可以应用于任何系统</p>
<blockquote>
<p>特点是：每个进程的周转时间平等重要</p>
</blockquote>
</blockquote>
<p>忆上个学期<strong>运筹学</strong>中的<strong>运输问题</strong>解决办法$-$&gt;<strong>最短作业法</strong>事实上，这个方法可以应用于计算机系统中的工作调度。</p>
<p>这个新的调度规则称为最短作业优先 (最短作业优先)，其名称应该很容易记住，因为它非常完整地描述了策略: 首先运行最短作业，然后运行下一个最短作业，以此类推。</p>
<center><img src="https://img-blog.csdnimg.cn/20200304150420976.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>接着前面的例子重新模拟，平均周转时间计算为：</p>
<p>$$<br>{\frac{10+20+120}{3}} = 50<br>$$</p>
<p>$$<br>[110-&gt;50 ]<br>$$</p>
<p>理论上，如果假设进程同时到达成立，那么可以证明 SJF 是一个最优调度算法【实际中，非也】</p>
<p><strong>问题来了：</strong></p>
<p>&ensp;&ensp;针对假设2，对其要求再进一步放宽，让进程随机到达，而不是同时，会出现什么状况？</p>
<p><strong>For Example:</strong></p>
<p>&ensp;&ensp;假设 A 在 t = 0 时刻到达，需要运行 100 ，而 B 和 C 在 t = 10 到达，每个需要运行 10 。</p>
<p><strong>模拟结果：</strong></p>
<center><img src="https://img-blog.csdnimg.cn/20200304152513501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>B、C 在 A 开始运行后到达，出现了和之前相同的护航效应，此时，平均周转时间计算为：</p>
<p>$$<br>{\frac{100+(110-10)+(120-10)}{3}} = 103.33…<br>$$</p>
<p><strong>Q:</strong><br>&ensp;&ensp;调度究竟能做什么？<br><strong>A:</strong><br>&ensp;&ensp;为了解决新出现的问题，接下来考虑放宽假设 3 的要求</p>
<hr>
<p><b><font size=4>补充：</font></b></p>
<center><b>抢占式调度 Preemptive Schedulers</b></center><br/>

<p>&ensp;&ensp;&ensp;&ensp;在以前的批量计算时代，开发了许多非抢占式调度程序，这种系统将正在运行的进程运行完毕才考虑要不要运行下一个新的进程，几乎所有的现代调度程序都是抢占式的，并且会为了运行另一个进程而停止一个进程的运行。</p>
<p>&ensp;&ensp;&ensp;&ensp;这意味着调度程序采用了我们之前了解到的机制；特别是，调度程序可以执行上下切换，暂时停止一个正在运行的进程，并恢复（或启动）另一个进程。</p>
<hr>
<p>根据调度的本身机制，考虑到计时器中断和上下切换，当 B 和C到达时，调度器会做一些其他的事情：抢占 A 的运行时间并运行其他进程，或者决定先暂停稍后再运行。而 <strong>SJF 是一个非抢占式调度程序</strong>，因此<strong>无法改变平均周转时间</strong>仍然很<strong>长</strong>的现状。</p>
<p>那么可否将抢占机制加到 SJF 中呢，这样就可以完善了？答案是可以的，添加后的结果称为最短完成时间优先（STCF）或抢占最短作业优先（PSJF）调度程序</p>
<center>SJF</center>
<center>+</center>
<center>抢占机制</center>
<center>||</center>
<center>STCF</center>

<h3 id="Shortest-Time-to-Completion-First-STCF"><a href="#Shortest-Time-to-Completion-First-STCF" class="headerlink" title="Shortest Time-to-Completion First (STCF)"></a>Shortest Time-to-Completion First (STCF)</h3><blockquote>
<p>放宽假设3的要求，进程不必要一旦开始就定要执行完毕</p>
</blockquote>
<p>每当新进程进入系统时，STCF调度器确定剩余进程 (包括新进程) 中剩余时间最少的进程，并调度该进程。</p>
<center><img src="https://img-blog.csdnimg.cn/20200304160421216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>STCF将抢占 A 并运行 B 和 C 直到完成；只有当它们完成时，才会安排 A 的剩余时间。如此，平均周转时间计算为：</p>
<p>$$<br>{\frac{(120-0)+(20-10)+(30-10)}{3}} = 50<br>$$</p>
<p><strong>和之前一样，考虑到我们的新假设，STCF是可证明最优的；考虑到如果所有工作同时到达，SJF是最优的。</strong></p>
<p><strong>However：</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;如果我们知道进程运行时间长度、进程仅需要 CPU，并且<strong>唯一的度量指标</strong>是周转时间，可以确定 STCF 是最优的调度策略。但是随着<strong>分时机</strong>的引入，用户在终端中也可以与系统进行交互。</p>
<p>&ensp;&ensp;&ensp;&ensp;出现了<strong>交互</strong>，我们需要引入一个<strong>新的度量指标</strong>：响应时间。</p>
<h3 id="响应时间-A-New-Metric-Response-Time"><a href="#响应时间-A-New-Metric-Response-Time" class="headerlink" title="响应时间 [A New Metric: Response Time]"></a>响应时间 [A New Metric: Response Time]</h3><blockquote>
<p>从进程到达系统到它第一次被调度的时间</p>
</blockquote>
<p>$$<br>T_{response} = T_{firstrun} - T_{arrival}<br>$$</p>
<p><strong>利用之前的调度例子：</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;不同的是 A 在时间 0 时到达，B 和 C 在时间 10 时到达，每个进程的响应时间如下 : A为0，B为0，C为10，响应时间平均值为 3.33</p>
<blockquote>
<p>可以看出：STCF及其相关并不利于优化响应时间。</p>
</blockquote>
<p><strong>For Example：</strong></p>
<p>如果三个作业同时到达，那么第三个作业必须等待前两个作业全部运行，然后才进行一次调度。虽然这种方法对于周转时间很好，但是对于响应时间和交互性却很差。</p>
<p>实际上，现实生活中，终端前打字，不得不等上 10 秒钟才能看到系统的响应，因为在这之前有其他的进程已经被调度。</p>
<p><strong>Q:</strong></p>
<p>那么我们该如何构建一个对响应时间敏感的调度器？</p>
<p><strong>A:</strong></p>
<p>引入新的调度算法$-$&gt; Round Robin</p>
<h3 id="轮询法-Round-Robin"><a href="#轮询法-Round-Robin" class="headerlink" title="轮询法 [Round Robin]"></a>轮询法 [Round Robin]</h3><blockquote>
<p>负载均衡算法【简称 RR】</p>
</blockquote>
<p>RR 不再是一旦运行开始就必须运行完毕，而是运行一个时间片( 亦称为调度量 )，接着切换到运行队列中的下一个进程，反复这样做，直到所有工作完成。所以有时候 RR 冠之以时间切片的称号。</p>
<p><font color=skyblue><strong>注意：</strong> 时间片的长度必须是计时器中断周期的倍数；因此，如果计时器每10毫秒中断一次，则时间片可以是10、20或任何其他10毫秒的倍数。</font></p>
<p>还是通过例子以达到对 RR 的彻底理解：</p>
<p><strong><font size=4>For Example：</font></strong></p>
<p>假设A、B、C三个进程同时到达，并且每一个进程都希望运行 5 个时间单位。</p>
<blockquote>
<p>一个 SJF 调度器就像之前介绍的一样在运行一个新的进程之前保证正在运行的进程运行完毕。</p>
<center><img src="https://img-blog.csdnimg.cn/20200304170743309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>
平均响应时间：(0+5+10) / 3 = 5
</blockquote>
<blockquote>
<p>相比之下，RR 按照 1 个时间单位的时间片可以很高效的完成所有任务。</p>
<center><img src="https://img-blog.csdnimg.cn/20200304170827977.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>
平均响应时间：(0+1+2) / 3 = 1
</blockquote>
<ul>
<li><p><font size=4><b>如上对比:</b></font> 时间片的长度对于 RR 来说是至关重要的。它<strong>越短</strong>，在响应时间度量下 RR 的<strong>性能越好</strong>。</p>
</li>
<li><p><font size=4><b>但是</b></font> ，盲目得让时间片太短是有问题的，突发的上下文切换带来的开销将主导整体性能。</p>
</li>
<li><p><font size=4><b>因此</b></font>，<strong>决定时间片的长度</strong>对系统设计人员来说是一种<strong>权衡</strong>。目的是<font color=red>使其有足够长的时间来摊销切换的成本，同时又不会使其太长而导致系统不再响应</font></p>
</li>
</ul>
<p>【<strong>注意：</strong> 上下文切换的开销并不仅仅来自于OS保存和恢复几个寄存器的操作。当程序运行时，它们会在CPU缓存、缓存器、分支预测器和其他片上硬件中建立大量的状态。切换到另一个作业会导致刷新此状态并引入与当前正在运行的进程相关的新状态，这可能会导致显著的性能损失】</p>
<hr>
<p><strong>补充：</strong></p>
<center><b>摊销可以降低成本 [Amortization can Reduce Costs]</b></center><br/>
&ensp;&ensp;&ensp;&ensp;某项固定的开销产生时，往往伴随着系统中的摊销技术的体现。通过减少成本（即减少操作次数），降低了系统的总成本。<br/>

<p><strong>For Example:</strong> </p>
<ul>
<li><p>如果时间片设置为10毫秒，而上下文切换成本为1毫秒，则大约10%的时间用于上下文切换，均被浪费；</p>
</li>
<li><p>如果我们想分摊这个成本，可以增加时间片到 100 毫秒。在这种情况下，不到 1% 的时间用于上下文切换，这样一来，时间成本得到了摊销。</p>
</li>
</ul>
<hr>
<center><b><font size=4>没有对比就没有伤害</b></center><br/>

<ul>
<li><p>如果响应时间是我们唯一的度量，那么具有合理时间片的RR就是一个非常好的调度器。</p>
</li>
<li><p>还记得之前我们以周转时间为唯一度量，那么在 RR 的例子的基础上，运行时间分别为 5 秒的 A、B和 C 同时到达，RR 是具有 1 个单位长时间片的调度器。从上面的图中我们可以看到，A结束于13,B结束于14,C结束于15，<strong>平均周转时间</strong>为14。</p>
</li>
</ul>
<p><strong>因此</strong>，如果周转时间是我们的度量标准，那么 RR 确实是最糟糕的策略之一也就不足为奇了。</p>
<p><strong>从直觉上看：</strong> RR所做的就是尽可能地延长每个作业的执行时间，在执行下一个作业之前，只运行一小段时间。因为周转时间只关心作业何时完成，所以 RR 在特定情况下比 FIFO 的效果都差。</p>
<center><b><font size=4>鱼与熊掌不可兼得</b></center><br/>

<blockquote>
<p>说白了就是：权衡</p>
</blockquote>
<p>任何公平的策略（如 RR），即在小时间尺度上将 CPU 平均分配给活动进程的策略，相应的在周转时间等指标上都会表现不佳。</p>
<p><strong>事实上，这是一种内在的权衡</strong> 。当然，你也可以运行更短的作业到完成，但代价是响应时间；如果你重视公平，则响应时间会降低，但代价是周转时间。</p>
<hr>
<p><strong>补充：</strong></p>
<center><b>重叠操作可提高系统的利用率</b></center><br/>

<p>&ensp;&ensp;&ensp;&ensp;在尽可能的情况下，重叠操作得以最大限度地利用系统。重叠在许多不同的域中都很有用，包括在执行磁盘 I/O 或向远程计算机发送消息时；</p>
<p>&ensp;&ensp;&ensp;&ensp;在这两种情况下，启动操作然后切换到另一个工作空间，可以提高系统的总体利用率和效率。</p>
<hr>
<p>至此，需要进行总结…</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>我们认知了两种类型的调度策略：【就两个度量指标进行对比】</p>
<ul>
<li><p>第一种类型（SJF，STCF）优化了周转时间，但对响应时间不利。</p>
</li>
<li><p>第二种类型（RR）优化了响应时间，但不利于周转。</p>
</li>
</ul>
<p>试回想 <a href="#presume">5 个假设</a>，我们已经涉及到了前三个假设，并且都进行了需求更改。</p>
<p>接下来，需要进一步面对剩余两个假设…</p>
<h3 id="Incorporating-I-O"><a href="#Incorporating-I-O" class="headerlink" title="Incorporating I/O"></a>Incorporating I/O</h3><blockquote>
<p>针对假设 4 ：所有的进程均只利用 CPU（不执行任何 I/O）</p>
</blockquote>
<p>对于假设 4 ，当然正常情况下所有程序都执行I/O。</p>
<ul>
<li><p>设想一个不接受任何输入的程序，它每次都将会产生相同的输出；</p>
</li>
<li><p>再想象一个没有输出的程序，那么它有没有在运行似乎已经变得无关紧要。</p>
</li>
</ul>
<p>当一个进程启动一个I/O请求时，调度器显然具有决策权。因为当前运行的进程在 I/O 期间不会使用CPU，它在等待 I/O 完成时被阻塞。</p>
<p>如果 I/O 是输出到硬盘驱动器，进程可能会被阻塞几毫秒甚至更长时间，具体取决于驱动器的当前 I/O 负载。从而，调度器可能在 CPU 上调度另一个进程。</p>
<p>具有决策权的调度器还必须在 I/O 完成时做出决定。做决定时会引发中断，操作系统运行并将请求 I/O 的进程从阻塞状态移回就绪状态。当然，它甚至也可以决定在那时进行这个任务。</p>
<p><strong>Q:</strong><br><strong>那么，对待每一个进程，可以重新开始也可以继续进行，操作系统应该如何对待每项工作？</strong><br><strong>A:</strong><br><strong>e.g.</strong> 假设有两个进程，A 和 B，每个进程都需要50毫秒的计算时间。区别在于，A 运行10毫秒，然后发出一个I/O请求（ 这里假设每个I/O需要10毫秒 ），而 B 只使用50毫秒计算时间，不执行I/O。这种情形下，调度器会先运行A，然后运行B。</p>
<center><img src="https://img-blog.csdnimg.cn/20200305092834284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p><strong>Q:</strong><br>&ensp;假设我们用的调度器是STCF调度程序。那么应该如何解释？<br><strong>A:</strong><br>&ensp;a 被分解成5个10毫秒的子作业，而B只是一个50毫秒的 CPU 需求，很显然，只需要运行一个进程，对于另一个没有把 I/O 考虑在内的进程是没有意义的。</p>
<blockquote>
<p>另一种常见的方法是将 A 的每个 10ms 子作业视为独立作业。</p>
</blockquote>
<p>因此，当系统启动时，其选择是调度 10 ms A还是 50 ms B。对于STCF，显然是选择较短的一个。当 Ａ 的第一个子作业完成时，只剩下 B，它开始运行。然后提交一个新的 A 子作业，它抢占 B 并运行 10 ms。这样做允许重叠，一个进程使用CPU，同时等待另一个进程的 I/O 完成；<strong>高效的利用</strong>了 CPU。</p>
<center><img src="https://img-blog.csdnimg.cn/20200305093935132.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p><strong>Summary：</strong></p>
<p>&ensp;&ensp;&ensp;通过这种新的方法，我们可以看到调度器如何合并 I/O。将每个 CPU 突发当作一个作业来处理，调度器确保 <strong>“交互式”</strong> 运行。当这些交互作业执行 I/O 时，其他CPU密集型作业也会运行，从而更好地利用处理器。</p>
<h3 id="No-More-Oracle"><a href="#No-More-Oracle" class="headerlink" title="No More Oracle"></a>No More Oracle</h3><p>基于 合并 I/O 方法，我们就可以得到最后的假设，调度器知道每个作业的长度。正如我们之前所说，这可能是我们能做出的最不切实际的假设。事实上，在一个通用操作系统（就像我们关心的那些操作系统）中，操作系统通常对每个作业的长度知之甚少。</p>
<h1 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h1><p>此文介绍了调度背后的基本思想，并开发了两种方法。第一个运行剩余的最短作业，从而优化周转时间；第二个在所有作业之间交替，从而优化响应时间。内在均衡问题导致无法判别某一种方法的绝对好坏。</p>
<p>我们也看到了如何将I/O集成到图片中，但仍然没有解决操作系统无法预见未来的问题。</p>
<p><strong>经典 Q:</strong><br>&ensp;&ensp;&ensp;&ensp;因此，如果没有这样的先验知识，我们怎么能建立一个像SJF/STCF这样的方法呢？此外，我们如何将我们看到的一些想法与RR调度器结合起来，从而使响应时间也相当好？</p>
<p><strong>A:</strong><br>&ensp;&ensp;&ensp;&ensp;通过构建一个调度程序，使用最近的过去来预测未来。这个调度程序称为<strong>多级反馈队列</strong>，见下回分晓…</p>
]]></content>
      <categories>
        <category>Operating Systems</category>
      </categories>
      <tags>
        <tag>Incorporating I/O</tag>
        <tag>调度策略</tag>
        <tag>周转/响应时间</tag>
      </tags>
  </entry>
  <entry>
    <title>自增主键的前世今生</title>
    <url>/2020/02/29/%E8%87%AA%E5%A2%9E%E4%B8%BB%E9%94%AE%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/</url>
    <content><![CDATA[<p>以MySQL(Innodb存储)为例介绍自增主键，介入场景分析主键的目的，从面试题下手，深入理解主键机制</p>
<a id="more"></a>

<p><font size=5><strong>引入：</strong></font></p>
<hr>
<p>使用MySQL建表时，我们通常会创建一个自增字段(AUTO_INCREMENT)，并以此字段作为主键</p>
<hr>
<p>本文将分三点阐述：</p>
<ol>
<li><a href="#title1">你可能不知道的自增主键</a></li>
<li><a href="#title2">应对变化的自增主键</a></li>
<li><a href="#title3"><font color=red>[坑]</font>如果自增主键用完怎么办</a></li>
</ol>
<h2 id="1-你可能不知道的自增主键"><a href="#1-你可能不知道的自增主键" class="headerlink" title="1.你可能不知道的自增主键"></a><div id="title1">1.你可能不知道的自增主键</div></h2><blockquote>
<p>使用自增主键可以提高数据存储效率</p>
</blockquote>
<p>在MySQL中(Innodb 存储引擎)，数据记录本身被存于主索引（B+Tree）的叶子节点上<br><font color=blue>*补充：【要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放】</font></p>
<hr>
<p>针对索引，</p>
<ul>
<li>如果我们<strong>定义了主键</strong>(PRIMARY KEY)，那么InnoDB会选择主键作为聚集索引</li>
<li>如果<strong>没有显式定义</strong>主键，则InnoDB会选择第一个<strong>不包含有NULL值的唯一索引</strong>作为主键索引</li>
<li>如果也<strong>没有这样的唯一索引</strong>，则InnoDB会选择内置6字节长的<strong>ROWID</strong>作为隐含的聚集索</li>
</ul>
<p><font color=blue>*补充：【ROWID随着行记录的写入而主键递增，这个ROWID不像ORACLE的ROWID那样可引用，是隐含的】</font></p>
<hr>
<p><strong>Q：</strong></p>
<p>每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）。</p>
<p><strong>A：</strong></p>
<ul>
<li><p>如果表<strong>使用自增主键</strong>。每次插入新的记录，会顺序添加到当前索引节点的后续位置，一页写满，自动开辟一个新的页</p>
</li>
<li><p>如果<strong>使用非自增主键</strong>（For Example:身份证号或学号等）【每次插入主键的值近似于随机】，每次新纪录都要被插到现有索引页中间某个位置，MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来<br/><br>这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。</p>
</li>
</ul>
<blockquote>
<p>自增id是增长的，不一定连续</p>
</blockquote>
<p><strong>原因有以下几点：</strong></p>
<ul>
<li>唯一键冲突</li>
<li>事务回滚</li>
<li>insert … select语句批量申请</li>
</ul>
<p><strong>针对自增值的保存策略：</strong></p>
<p>InnoDB 引擎中，自增值保存在了内存中，继 MySQL 8.0 之后，出现了<strong>自增值持久化</strong>，自增值的变更记录存储在 redo log 中，重启时可以依靠其恢复之前的值</p>
<p><font color=blue>*补充：【自增值持久化：如果发生重启，表的自增值可以恢复为 MySQL 重启前的值】</font></p>
<h2 id="2-应对变化的自增主键"><a href="#2-应对变化的自增主键" class="headerlink" title="2.应对变化的自增主键"></a><div id="title2">2.应对变化的自增主键</div></h2><p><strong>导入：</strong></p>
<blockquote>
<p>在设计数据库时不需要费尽心思去考虑设置哪个字段为主键</p>
</blockquote>
<p> 但是应用到实际场景，自增主键的主要目的还是应对变化。</p>
<p>设计一个场景：</p>
<p>&ensp;&ensp;&ensp;&ensp;<a href="https://mp.weixin.qq.com/s/rh0tg4L9Ffj1fy32rKCkHw" target="_blank" rel="noopener">维护商业账号的资质相关信息</a></p>
<p><strong>最初设计：</strong> 账号是由全局唯一且自增的分布式ID生成器生成的，很显然这个时候我们把账号作为主键这就天然合理</p>
<p><strong>业务迭代一定时间:</strong>  提出了新的需求，一个账号，在不同业务线，需要享有不同资质</p>
<p><strong>比较：</strong> accountId 较之前不唯一，因为，同一个账号，不同业务线，资质是不一样的【无法像最初那样作为主键】</p>
<p><strong>解决方式：</strong> <a href="https://mp.weixin.qq.com/s/rh0tg4L9Ffj1fy32rKCkHw" target="_blank" rel="noopener">见场景中</a></p>
<h2 id="3-如果自增主键用完怎么办"><a href="#3-如果自增主键用完怎么办" class="headerlink" title="3.如果自增主键用完怎么办"></a><div id="title3">3.如果自增主键用完怎么办</div></h2><p><strong>老掉牙但经典：</strong>【面试题】</p>
<blockquote>
<p>面试官:”用过mysql吧，你们是用自增主键还是UUID？”<br>你:”用的是自增主键”<br>面试官:”为什么是自增主键？”<br>你:”因为采用自增主键，数据在物理结构上是顺序存储，性能最好，blabla…”<br>面试官:”那自增主键达到最大值了，用完了怎么办？”<br>你:”what，没复习啊！！”<br>( 然后，你就可以回去等通知了！)</p>
</blockquote>
<p><strong>说明：</strong> 自增 id 是整型字段，常用 int 类型来定义增长 id ，而 int 类型有上限 即增长 id 也是有上限的。</p>
<p>既然 int 不够了，首先想到的是改为 BigInt 类型【做个对比】</p>
<center><img src="https://img-blog.csdnimg.cn/20200229113709135.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<center><img src="https://img-blog.csdnimg.cn/20200229113551427.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<p>如果回答：把自增主键的类型改为BigInt类型就好了<br/><br>面试官:”你在线上怎么修改列的数据类型的？”</p>
</blockquote>
<p>修改方法：</p>
<ul>
<li>使用mysql5.6+提供的在线修改功能</li>
<li>借助第三方工具</li>
<li>改从库表结构，然后主从切换</li>
</ul>
<p><strong>差不多就算结束了这个问题了。但是回过头想一想，是不是一条路走到黑了，或许从头开始就错了呢！！！</strong></p>
<hr>
<p><strong>插入一条生存力测试，形象生动：</strong></p>
<p>&ensp;&ensp;假如女朋友问：我刚才吃药时看窗外，你猜我看到了什么？</p>
<p>&ensp;&ensp;&ensp;&ensp;<strong>歧途：</strong> 白云？你为什么不看我呢。</p>
<p>&ensp;&ensp;&ensp;&ensp;<strong>正解：</strong> 你怎么要吃药呢</p>
<hr>
<p>那么<strong>正解</strong>应该是什么呢？</p>
<blockquote>
<p>这问题没遇到过，因为自增主键一般用int类型，一般达不到最大值，我们就分库分表了，所以不曾遇见过！</p>
</blockquote>
<p><a href="https://mp.weixin.qq.com/s/kVqj4VdZewuvR_OsXgY_RQ" target="_blank" rel="noopener">具体介绍…</a></p>
]]></content>
      <categories>
        <category>Database Systems</category>
      </categories>
      <tags>
        <tag>自增主键</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Fundamentals of Recurrent Neural Network</title>
    <url>/2020/02/29/Fundamentals%20of%20Recurrent%20Neural%20Network/</url>
    <content><![CDATA[<h2 id="基于循环神经网络实现语言模型。"><a href="#基于循环神经网络实现语言模型。" class="headerlink" title="基于循环神经网络实现语言模型。"></a>基于循环神经网络实现语言模型。</h2><blockquote>
<p>对于语言模型的介绍</p>
<blockquote>
<p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104303197" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104303197</a></p>
</blockquote>
</blockquote>
<p>我们的目的是基于当前的输入与过去的输入序列，预测序列的下一个字符。循环神经网络引入一个隐藏变量$H$，用$H_{t}$表示$H$在时间步$t$的值。$H_{t}$的计算基于$X_{t}$和$H_{t-1}$，可以认为$H_{t}$记录了到当前字符为止的序列信息，利用$H_{t}$对序列的下一个字符进行预测。</p>
<div align=center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWprbTB2NDRpLnBuZw?x-oss-process=image/format,png" /></div>

<h2 id="构造-Structure"><a href="#构造-Structure" class="headerlink" title="构造(Structure)"></a>构造(Structure)</h2><p>我们先看循环神经网络的具体构造。假设 $\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$ 是时间步 $t$ 的小批量输入，$\boldsymbol{H}_t  \in \mathbb{R}^{n \times h}$ 是该时间步的隐藏变量，则：</p>
<center>【广播机制】</center>

<p>$$<br>\boldsymbol{H}_t = \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}  + \boldsymbol{b}_h).<br>$$</p>
<p>其中，$\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}$，$\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}$, $\boldsymbol{b}_{h} \in \mathbb{R}^{1 \times h}$, $\phi$ 函数是非线性激活函数。</p>
<p>由于引入了 $\boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}$，$H_{t}$ 能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。</p>
<p>由于$H_{t}$的计算基于$H_{t-1}$，上式的计算是循环的，使用循环计算的网络即循环神经网络（recurrent neural network）。</p>
<p>在时间步$t$，输出层的输出为：</p>
<p>$$<br>\boldsymbol{O}_t = \boldsymbol{H}_t \boldsymbol{W}_{hq} + \boldsymbol{b}_q.<br>$$<br>其中$\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}$，$\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}$。</p>
<h2 id="手动实现"><a href="#手动实现" class="headerlink" title="手动实现"></a>手动实现</h2><blockquote>
<p>实现一个基于字符级循环神经网络的语言模型，仍然使用周杰伦的歌词作为语料</p>
<blockquote>
<p>下载地址：<a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/data/jaychou_lyrics.txt.zip?raw=true" target="_blank" rel="noopener">见语言模型一章</a>【点击可直接下载】</p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2l_jay9460 <span class="keyword">as</span> d2l</span><br><span class="line">(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="one-hot向量"><a href="#one-hot向量" class="headerlink" title="one-hot向量"></a>one-hot向量</h3><blockquote>
<p>在此采用one-hot向量将字符表示成向量</p>
</blockquote>
<p>假设词典大小是$N$，每次字符对应一个从$0$到$N-1$的唯一的索引，则该字符的向量是一个长度为$N$的向量，若字符的索引是$i$，则该向量的第$i$个位置为$1$，其他位置为$0$。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span><span class="params">(x, n_class, dtype=torch.float32)</span>:</span></span><br><span class="line">    result = torch.zeros(x.shape[<span class="number">0</span>], n_class, dtype=dtype, device=x.device)  <span class="comment"># shape: (n, n_class)</span></span><br><span class="line">    result.scatter_(<span class="number">1</span>, x.long().view(<span class="number">-1</span>, <span class="number">1</span>), <span class="number">1</span>)  <span class="comment"># result[i, x[i, 0]] = 1</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">    </span><br><span class="line">x = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">x_one_hot = one_hot(x, vocab_size)</span><br><span class="line">print(x_one_hot)</span><br><span class="line">print(x_one_hot.shape)</span><br><span class="line">print(x_one_hot.sum(axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>每次采样的小批量的形状是（批量大小, 时间步数）。我们将其变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。也就是说，时间步$t$的输入为</p>
<p>$$<br>\boldsymbol{X}_t \in \mathbb{R}^{n \times d}<br>$$</p>
<p>其中$n$为批量大小，$d$为词向量大小，即one-hot向量长度（词典大小)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_onehot</span><span class="params">(X, n_class)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [one_hot(X[:, i], n_class) <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>])]</span><br><span class="line"></span><br><span class="line">X = torch.arange(<span class="number">10</span>).view(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">inputs = to_onehot(X, vocab_size)</span><br><span class="line">print(len(inputs), inputs[<span class="number">0</span>].shape)</span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init module param</span></span><br><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line"><span class="comment"># num_inputs: d</span></span><br><span class="line"><span class="comment"># num_hiddens: h, 隐藏单元的个数是超参数</span></span><br><span class="line"><span class="comment"># num_outputs: q</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span> <span class="comment"># 随机初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        param = torch.zeros(shape, device=device, dtype=torch.float32)</span><br><span class="line">        nn.init.normal_(param, <span class="number">0</span>, <span class="number">0.01</span>) <span class="comment"># 随机体现</span></span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(param)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = _one((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = _one((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device))<span class="comment"># 偏置参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device))<span class="comment"># 偏置参数</span></span><br><span class="line">    <span class="keyword">return</span> (W_xh, W_hh, b_h, W_hq, b_q)</span><br></pre></td></tr></table></figure>

<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>函数<code>rnn</code>用循环的方式依次完成循环神经网络每个时间步的计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn</span><span class="params">(inputs, state, params)</span>:</span> <span class="comment"># 前向计算</span></span><br><span class="line">    <span class="comment"># inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state <span class="comment"># 提供了需要维护的状态的初始值 state定义成了元组</span></span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H,) <span class="comment"># 返回新的状态Ｈ，以便于相邻采样</span></span><br></pre></td></tr></table></figure>
<p>函数init_rnn_state初始化隐藏变量，这里的返回值是一个元组。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rnn_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>
<h3 id="裁剪梯度-clip-gradient"><a href="#裁剪梯度-clip-gradient" class="headerlink" title="裁剪梯度(clip gradient)"></a>裁剪梯度(clip gradient)</h3><blockquote>
<p>针对梯度爆炸问题</p>
</blockquote>
<p>循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。假设我们把所有模型参数的梯度拼接成一个向量 $\boldsymbol{g}$，并设裁剪的阈值是$\theta$。裁剪后的梯度</p>
<p>$$<br> \min\left(\frac{\theta}{|\boldsymbol{g}|}, 1\right)\boldsymbol{g}<br>$$</p>
<p>的$L_2$范数不超过$\theta$。</p>
<p>反向传播方式：时间反向传播【DPTT】</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, device)</span>:</span> <span class="comment"># theta 预设的阈值</span></span><br><span class="line">    norm = torch.tensor([<span class="number">0.0</span>], device=device)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad.data ** <span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().item()</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad.data *= (theta / norm)</span><br></pre></td></tr></table></figure>
<h3 id="定义预测函数"><a href="#定义预测函数" class="headerlink" title="定义预测函数"></a>定义预测函数</h3><blockquote>
<p>基于前缀 <code>prefix</code>（含有数个字符的字符串）来预测接下来的 <code>num_chars</code> 个字符。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn</span><span class="params">(prefix, num_chars, rnn, params, init_rnn_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                num_hiddens, vocab_size, device, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    <span class="comment"># 模型处理前缀prefix，隐藏状态H就记录了相关信息，模型在处理prefix 最后一个字符时，就已经预测出了下一个字符，所以可以作为之后的输入</span></span><br><span class="line">    state = init_rnn_state(<span class="number">1</span>, num_hiddens, device) <span class="comment">#　构造并且初始化状态</span></span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]   <span class="comment"># output记录prefix加上预测的num_chars个字符</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一时间步的输出作为当前时间步的输入</span></span><br><span class="line">        X = to_onehot(torch.tensor([[output[<span class="number">-1</span>]]], device=device), vocab_size)</span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        (Y, state) = rnn(X, state, params)</span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(Y[<span class="number">0</span>].argmax(dim=<span class="number">1</span>).item())　<span class="comment"># 最大的一列</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br></pre></td></tr></table></figure>
<h3 id="困惑度"><a href="#困惑度" class="headerlink" title="困惑度"></a>困惑度</h3><p>我们通常使用困惑度（perplexity）来评价语言模型的好坏。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，</p>
<blockquote>
<p>交叉熵损失函数</p>
<blockquote>
<p>损失函数详解：<a href="https://zhuanlan.zhihu.com/p/35709485" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35709485</a></p>
</blockquote>
</blockquote>
<ul>
<li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；</li>
<li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li>
<li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li>
</ul>
<p>显然，任何一个有效模型的困惑度必须小于类别个数。此处困惑度必须小于词典大小<code>vocab_size</code>。</p>
<h3 id="定义模型训练函数"><a href="#定义模型训练函数" class="headerlink" title="定义模型训练函数"></a>定义模型训练函数</h3><p>跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：</p>
<ol>
<li>使用困惑度评价模型。</li>
<li>在迭代模型参数前裁剪梯度。</li>
<li>对时序数据采用不同采样方法将导致隐藏状态初始化的不同。</li>
</ol>
<p>相邻采样，开始的时候初始化隐藏状态，容易引起开销过大，通常将隐藏状态分离</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn</span><span class="params">(rnn, get_params, init_rnn_state, num_hiddens,</span></span></span><br><span class="line"><span class="function"><span class="params">                          vocab_size, device, corpus_indices, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                          char_to_idx, is_random_iter, num_epochs, num_steps,</span></span></span><br><span class="line"><span class="function"><span class="params">                          lr, clipping_theta, batch_size, pred_period,</span></span></span><br><span class="line"><span class="function"><span class="params">                          pred_len, prefixes)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_random_iter:</span><br><span class="line">        data_iter_fn = d2l.data_iter_random <span class="comment"># 随机采样</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data_iter_fn = d2l.data_iter_consecutive <span class="comment">#相邻采样</span></span><br><span class="line">    params = get_params()</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_random_iter:  <span class="comment"># 如使用相邻采样，在epoch开始时初始化隐藏状态</span></span><br><span class="line">            state = init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> is_random_iter:  <span class="comment"># 如使用随机采样，在每个小批量更新前初始化隐藏状态</span></span><br><span class="line">                state = init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 否则需要使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">            <span class="comment"># inputs是num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">            inputs = to_onehot(X, vocab_size)</span><br><span class="line">            <span class="comment"># outputs有num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">            (outputs, state) = rnn(inputs, state, params) <span class="comment">#循环神经网路的前向计算</span></span><br><span class="line">            <span class="comment"># 拼接之后形状为(num_steps * batch_size, vocab_size)</span></span><br><span class="line">            outputs = torch.cat(outputs, dim=<span class="number">0</span>) <span class="comment"># 拼接</span></span><br><span class="line">            <span class="comment"># Y的形状是(batch_size, num_steps)，转置后再变成形状为</span></span><br><span class="line">            <span class="comment"># (num_steps * batch_size,)的向量，这样跟输出的行一一对应</span></span><br><span class="line">            y = torch.flatten(Y.T)</span><br><span class="line">            <span class="comment"># 使用交叉熵损失计算平均分类误差</span></span><br><span class="line">            l = loss(outputs, y.long())</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 梯度清0</span></span><br><span class="line">            <span class="keyword">if</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(params, clipping_theta, device)  <span class="comment"># 裁剪梯度</span></span><br><span class="line">            d2l.sgd(params, lr, <span class="number">1</span>)  <span class="comment"># 因为误差已经取过均值，梯度不用再做平均</span></span><br><span class="line">            l_sum += l.item() * y.shape[<span class="number">0</span>]</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_period == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,</span><br><span class="line">                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))</span><br></pre></td></tr></table></figure>
<h3 id="训练模型并创作歌词"><a href="#训练模型并创作歌词" class="headerlink" title="训练模型并创作歌词"></a>训练模型并创作歌词</h3><blockquote>
<ul>
<li>设置超参数</li>
<li>前缀：“分开”和“不分开”</li>
<li>歌词长度：50个字符（不考虑前缀长度）</li>
<li>周期：50</li>
<li>采样方式：随机采样 &amp;&amp; 相邻采样</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set super param</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line"><span class="comment"># set prefix and recurrent </span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"><span class="comment"># training by random sampling</span></span><br><span class="line">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, <span class="literal">True</span>, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                      prefixes)</span><br><span class="line"><span class="comment"># training by adjacent sampling</span></span><br><span class="line">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, <span class="literal">False</span>, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                      prefixes)</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="简化实现"><a href="#简化实现" class="headerlink" title="简化实现"></a>简化实现</h2><h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><blockquote>
<p>使用 Pytorch 中的 nn.RNN 构造神经网络</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个基于循环神经网络的语言模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_layer, vocab_size)</span>:</span> <span class="comment">#rnn_layer 是pytorch中的一个类</span></span><br><span class="line">        super(RNNModel, self).__init__()</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.hidden_size = rnn_layer.hidden_size * (<span class="number">2</span> <span class="keyword">if</span> rnn_layer.bidirectional <span class="keyword">else</span> <span class="number">1</span>) </span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.dense = nn.Linear(self.hidden_size, vocab_size) <span class="comment">#定义一个线性层作为输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">        <span class="comment"># inputs.shape: (batch_size, num_steps)</span></span><br><span class="line">        X = to_onehot(inputs, vocab_size)</span><br><span class="line">        X = torch.stack(X)  <span class="comment"># X.shape: (num_steps, batch_size, vocab_size)</span></span><br><span class="line">        hiddens, state = self.rnn(X, state)</span><br><span class="line">        hiddens = hiddens.view(<span class="number">-1</span>, hiddens.shape[<span class="number">-1</span>])  <span class="comment"># hiddens.shape: (num_steps * batch_size, hidden_size)</span></span><br><span class="line">        output = self.dense(hiddens)</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure>

<h3 id="预测函数"><a href="#预测函数" class="headerlink" title="预测函数"></a>预测函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn_pytorch</span><span class="params">(prefix, num_chars, model, vocab_size, device, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                      char_to_idx)</span>:</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]  <span class="comment"># output记录prefix加上预测的num_chars个字符</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        X = torch.tensor([output[<span class="number">-1</span>]], device=device).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        (Y, state) = model(X, state)  <span class="comment"># 前向计算不需要传入模型参数</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(Y.argmax(dim=<span class="number">1</span>).item())</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><blockquote>
<p>采用相邻采样</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># training function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn_pytorch</span><span class="params">(model, num_hiddens, vocab_size, device,</span></span></span><br><span class="line"><span class="function"><span class="params">                                corpus_indices, idx_to_char, char_to_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                num_epochs, num_steps, lr, clipping_theta,</span></span></span><br><span class="line"><span class="function"><span class="params">                                batch_size, pred_period, pred_len, prefixes)</span>:</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=lr) <span class="comment">#优化模型参数</span></span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) <span class="comment"># 相邻采样</span></span><br><span class="line">        state = <span class="literal">None</span> <span class="comment">#构造 并初始化</span></span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">if</span> isinstance (state, tuple): <span class="comment"># LSTM, state:(h, c)  </span></span><br><span class="line">                    state[<span class="number">0</span>].detach_()</span><br><span class="line">                    state[<span class="number">1</span>].detach_()</span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    state.detach_()</span><br><span class="line">            (output, state) = model(X, state) <span class="comment"># output.shape: (num_steps * batch_size, vocab_size)</span></span><br><span class="line">            y = torch.flatten(Y.T)</span><br><span class="line">            l = loss(output, y.long())</span><br><span class="line">            </span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(model.parameters(), clipping_theta, device)</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.item() * y.shape[<span class="number">0</span>]</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_period == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn_pytorch(</span><br><span class="line">                    prefix, pred_len, model, vocab_size, device, idx_to_char,</span><br><span class="line">                    char_to_idx))</span><br><span class="line">       </span><br><span class="line">num_epochs, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">32</span>, <span class="number">1e-3</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                            corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                            num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                            batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>语言模型</tag>
        <tag>RNN</tag>
        <tag>梯度现象</tag>
        <tag>广播</tag>
        <tag>one-hot</tag>
      </tags>
  </entry>
  <entry>
    <title>词嵌入之 Word2Vec</title>
    <url>/2020/02/28/%E8%AF%8D%E5%B5%8C%E5%85%A5%E4%B9%8B%20Word2Vec/</url>
    <content><![CDATA[<h1 id="词嵌入基础"><a href="#词嵌入基础" class="headerlink" title="词嵌入基础"></a>词嵌入基础</h1><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">循环神经网络的从零开始实现</a>中使用 one-hot 向量表示单词，虽然它们构造起来很容易，但通常并不是一个好选择。</p>
<blockquote>
<p><strong>原因：</strong></p>
<blockquote>
<p>one-hot 词向量无法准确表达不同词之间的相似度，如我们常常使用的余弦相似度<br/><br>任意单词间的余弦相似度都为零。</p>
</blockquote>
</blockquote>
<p>Word2Vec 词嵌入工具的提出正是为了解决上面这个问题，它将每个词表示成一个定长的向量，并通过在语料库上的预训练使得这些向量能较好地表达不同词之间的相似和类比关系，以引入一定的语义信息。</p>
<p>基于两种概率模型的假设，介绍以下两种 Word2Vec 模型：</p>
<ol>
<li>Skip-Gram 跳字模型：假设背景词由中心词生成，即建模 $P(w_o\mid w_c)$，其中 $w_c$ 为中心词，$w_o$ 为任一背景词；</li>
</ol>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW1qc3E4NG85LnBuZw?x-oss-process=image/format,png" /></center>

<ol start="2">
<li>CBOW (continuous bag-of-words) 连续词袋模型：假设中心词由背景词生成，即建模 $P(w_c\mid \mathcal{W}_o)$，其中 $\mathcal{W}_o$ 为背景词的集合。</li>
</ol>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW1qdDRyMDJuLnBuZw?x-oss-process=image/format,png" /></center>

<p>在这里我们主要介绍 Skip-Gram 模型的实现，CBOW 实现与其类似，读者可之后自己尝试实现。后续的内容将大致从以下四个部分展开：</p>
<ol>
<li><a href="#ptb">PTB 数据集</a></li>
<li><a href="#skip-gram">Skip-Gram 跳字模型</a></li>
<li><a href="#nsa">负采样近似</a></li>
<li><a href="#train">训练模型</a></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br></pre></td></tr></table></figure>
<h2 id="PTB-数据集"><a href="#PTB-数据集" class="headerlink" title="PTB 数据集"></a><div id="ptb">PTB 数据集</div></h2><p>简单来说，Word2Vec 能从语料中学到如何将离散的词映射为连续空间中的向量，并保留其语义上的相似关系。</p>
<p>为了训练 Word2Vec 模型，我们就需要准备一个自然语言语料库，模型将从中学习各个单词间的关系，这里我们使用经典的 PTB 语料库进行训练。</p>
<h3 id="载入数据集"><a href="#载入数据集" class="headerlink" title="载入数据集"></a>载入数据集</h3><p>数据集训练文件 <code>ptb.train.txt</code> 示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">aer banknote berlitz calloway centrust cluett fromstein gitano guterman ...</span><br><span class="line">pierre  N years old will join the board as a nonexecutive director nov. N </span><br><span class="line">mr.  is chairman of  n.v. the dutch publishing group </span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'path to ptb.train.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    lines = f.readlines() <span class="comment"># 该数据集中句子以换行符为分割</span></span><br><span class="line">    raw_dataset = [st.split() <span class="keyword">for</span> st <span class="keyword">in</span> lines] <span class="comment"># st是sentence的缩写，单词以空格为分割</span></span><br><span class="line">print(<span class="string">'# sentences: %d'</span> % len(raw_dataset))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于数据集的前3个句子，打印每个句子的词数和前5个词</span></span><br><span class="line"><span class="comment"># 句尾符为 '' ，生僻词全用 '' 表示，数字则被替换成了 'N'</span></span><br><span class="line"><span class="keyword">for</span> st <span class="keyword">in</span> raw_dataset[:<span class="number">3</span>]:</span><br><span class="line">    print(<span class="string">'# tokens:'</span>, len(st), st[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line"><span class="comment"># sentences: 42068</span></span><br><span class="line"><span class="comment"># tokens: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']</span></span><br><span class="line"><span class="comment"># tokens: 15 ['pierre', '&lt;unk&gt;', 'N', 'years', 'old']</span></span><br><span class="line"><span class="comment"># tokens: 11 ['mr.', '&lt;unk&gt;', 'is', 'chairman', 'of']</span></span><br></pre></td></tr></table></figure>
<h3 id="建立词语索引"><a href="#建立词语索引" class="headerlink" title="建立词语索引"></a>建立词语索引</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">counter = collections.Counter([tk <span class="keyword">for</span> st <span class="keyword">in</span> raw_dataset <span class="keyword">for</span> tk <span class="keyword">in</span> st]) <span class="comment"># tk是token的缩写</span></span><br><span class="line">counter = dict(filter(<span class="keyword">lambda</span> x: x[<span class="number">1</span>] &gt;= <span class="number">5</span>, counter.items())) <span class="comment"># 只保留在数据集中至少出现5次的词</span></span><br><span class="line"></span><br><span class="line">idx_to_token = [tk <span class="keyword">for</span> tk, _ <span class="keyword">in</span> counter.items()]</span><br><span class="line">token_to_idx = &#123;tk: idx <span class="keyword">for</span> idx, tk <span class="keyword">in</span> enumerate(idx_to_token)&#125;</span><br><span class="line">dataset = [[token_to_idx[tk] <span class="keyword">for</span> tk <span class="keyword">in</span> st <span class="keyword">if</span> tk <span class="keyword">in</span> token_to_idx]</span><br><span class="line">           <span class="keyword">for</span> st <span class="keyword">in</span> raw_dataset] <span class="comment"># raw_dataset中的单词在这一步被转换为对应的idx</span></span><br><span class="line">num_tokens = sum([len(st) <span class="keyword">for</span> st <span class="keyword">in</span> dataset])</span><br><span class="line"><span class="string">'# tokens: %d'</span> % num_tokens</span><br></pre></td></tr></table></figure>
<h3 id="二次采样"><a href="#二次采样" class="headerlink" title="二次采样"></a>二次采样</h3><p>文本数据中一般会出现一些高频词，如英文中的“the”“a”和“in”。通常来说</p>
<p>在一个背景窗口中，一个词（如“chip”）和较低频词（如“microprocessor”）同时出现比和较高频词（如“the”）同时出现对训练词嵌入模型更有益。</p>
<p>因此，训练词嵌入模型时可以对词进行二次采样。 具体来说，数据集中每个被索引词 $w_i$ 将有一定概率被丢弃，该丢弃概率为</p>
<p>$$<br>P(w_i)=\max(1-\sqrt{\frac{t}{f(w_i)}},0)<br>$$</p>
<p>其中  $f(w_i)$  是数据集中词 $w_i$ 的个数与总词数之比，常数 $t$ 是一个超参数（实验中设为 $10^{−4}$）。</p>
<p>可见，只有当 $f(w_i)&gt;t$ 时，我们才有可能在二次采样中丢弃词 $w_i$，并且越高频的词被丢弃的概率越大。具体的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discard</span><span class="params">(idx)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        idx: 单词的下标</span></span><br><span class="line"><span class="string">    @return: True/False 表示是否丢弃该单词</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> random.uniform(<span class="number">0</span>, <span class="number">1</span>) &lt; <span class="number">1</span> - math.sqrt(</span><br><span class="line">        <span class="number">1e-4</span> / counter[idx_to_token[idx]] * num_tokens)</span><br><span class="line"></span><br><span class="line">subsampled_dataset = [[tk <span class="keyword">for</span> tk <span class="keyword">in</span> st <span class="keyword">if</span> <span class="keyword">not</span> discard(tk)] <span class="keyword">for</span> st <span class="keyword">in</span> dataset]</span><br><span class="line">print(<span class="string">'# tokens: %d'</span> % sum([len(st) <span class="keyword">for</span> st <span class="keyword">in</span> subsampled_dataset]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compare_counts</span><span class="params">(token)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'# %s: before=%d, after=%d'</span> % (token, sum(</span><br><span class="line">        [st.count(token_to_idx[token]) <span class="keyword">for</span> st <span class="keyword">in</span> dataset]), sum(</span><br><span class="line">        [st.count(token_to_idx[token]) <span class="keyword">for</span> st <span class="keyword">in</span> subsampled_dataset]))</span><br><span class="line"></span><br><span class="line">print(compare_counts(<span class="string">'the'</span>))</span><br><span class="line">print(compare_counts(<span class="string">'join'</span>))</span><br></pre></td></tr></table></figure>
<h3 id="提取中心词和背景词"><a href="#提取中心词和背景词" class="headerlink" title="提取中心词和背景词"></a>提取中心词和背景词</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_centers_and_contexts</span><span class="params">(dataset, max_window_size)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        dataset: 数据集为句子的集合，每个句子则为单词的集合，此时单词已经被转换为相应数字下标</span></span><br><span class="line"><span class="string">        max_window_size: 背景词的词窗大小的最大值</span></span><br><span class="line"><span class="string">    @return:</span></span><br><span class="line"><span class="string">        centers: 中心词的集合</span></span><br><span class="line"><span class="string">        contexts: 背景词窗的集合，与中心词对应，每个背景词窗则为背景词的集合</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    centers, contexts = [], []</span><br><span class="line">    <span class="keyword">for</span> st <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="keyword">if</span> len(st) &lt; <span class="number">2</span>:  <span class="comment"># 每个句子至少要有2个词才可能组成一对“中心词-背景词”</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        centers += st</span><br><span class="line">        <span class="keyword">for</span> center_i <span class="keyword">in</span> range(len(st)):</span><br><span class="line">            window_size = random.randint(<span class="number">1</span>, max_window_size) <span class="comment"># 随机选取背景词窗大小</span></span><br><span class="line">            indices = list(range(max(<span class="number">0</span>, center_i - window_size),</span><br><span class="line">                                 min(len(st), center_i + <span class="number">1</span> + window_size)))</span><br><span class="line">            indices.remove(center_i)  <span class="comment"># 将中心词排除在背景词之外</span></span><br><span class="line">            contexts.append([st[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> indices])</span><br><span class="line">    <span class="keyword">return</span> centers, contexts</span><br><span class="line"></span><br><span class="line">all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">tiny_dataset = [list(range(<span class="number">7</span>)), list(range(<span class="number">7</span>, <span class="number">10</span>))]</span><br><span class="line">print(<span class="string">'dataset'</span>, tiny_dataset)</span><br><span class="line"><span class="keyword">for</span> center, context <span class="keyword">in</span> zip(*get_centers_and_contexts(tiny_dataset, <span class="number">2</span>)):</span><br><span class="line">    print(<span class="string">'center'</span>, center, <span class="string">'has contexts'</span>, context)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><em>注：数据批量读取的实现需要依赖负采样近似的实现，故放于负采样近似部分进行讲解。</em></p>
</blockquote>
<h2 id="Skip-Gram-跳字模型"><a href="#Skip-Gram-跳字模型" class="headerlink" title="Skip-Gram 跳字模型"></a><div id="skip-gram">Skip-Gram 跳字模型</div></h2><p>在跳字模型中，每个词被表示成两个 $d$ 维向量，用来计算条件概率。</p>
<p>假设这个词在词典中索引为 $i$ ，当它为中心词时向量表示为 $\boldsymbol{v}_i\in\mathbb{R}^d$，而为背景词时向量表示为 $\boldsymbol{u}_i\in\mathbb{R}^d$ 。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_o$ 在词典中索引为 $o$</p>
<p>我们假设给定中心词生成背景词的条件概率满足下式：</p>
<p>$$<br>P(w_o\mid w_c)=\frac{\exp(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{\sum_{i\in\mathcal{V}}\exp(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}<br>$$</p>
<h3 id="PyTorch-预置的-Embedding-层"><a href="#PyTorch-预置的-Embedding-层" class="headerlink" title="PyTorch 预置的 Embedding 层"></a>PyTorch 预置的 Embedding 层</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed = nn.Embedding(num_embeddings=<span class="number">10</span>, embedding_dim=<span class="number">4</span>)</span><br><span class="line">print(embed.weight)</span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.long)</span><br><span class="line">print(embed(x))</span><br></pre></td></tr></table></figure>
<h3 id="PyTorch-预置的批量乘法"><a href="#PyTorch-预置的批量乘法" class="headerlink" title="PyTorch 预置的批量乘法"></a>PyTorch 预置的批量乘法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">print(torch.bmm(X, Y).shape)</span><br></pre></td></tr></table></figure>
<h3 id="Skip-Gram-模型的前向计算"><a href="#Skip-Gram-模型的前向计算" class="headerlink" title="Skip-Gram 模型的前向计算"></a>Skip-Gram 模型的前向计算</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">skip_gram</span><span class="params">(center, contexts_and_negatives, embed_v, embed_u)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        center: 中心词下标，形状为 (n, 1) 的整数张量</span></span><br><span class="line"><span class="string">        contexts_and_negatives: 背景词和噪音词下标，形状为 (n, m) 的整数张量</span></span><br><span class="line"><span class="string">        embed_v: 中心词的 embedding 层</span></span><br><span class="line"><span class="string">        embed_u: 背景词的 embedding 层</span></span><br><span class="line"><span class="string">    @return:</span></span><br><span class="line"><span class="string">        pred: 中心词与背景词（或噪音词）的内积，之后可用于计算概率 p(w_o|w_c)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    v = embed_v(center) <span class="comment"># shape of (n, 1, d)</span></span><br><span class="line">    u = embed_u(contexts_and_negatives) <span class="comment"># shape of (n, m, d)</span></span><br><span class="line">    pred = torch.bmm(v, u.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)) <span class="comment"># bmm((n, 1, d), (n, d, m)) =&gt; shape of (n, 1, m)</span></span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure>
<h2 id="负采样近似"><a href="#负采样近似" class="headerlink" title="负采样近似"></a><div id="nsa">负采样近似</div></h2><p><strong>问题：</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;由于 softmax 运算考虑了背景词可能是词典 $\mathcal{V}$ 中的任一词，对于含几十万或上百万词的较大词典，就可能导致计算的开销过大。</p>
<p>我们将<strong>以 skip-gram 模型为例</strong>，介绍负采样 (negative sampling) 的实现来尝试解决这个问题。</p>
<p>负采样方法用以下公式来近似条件概率 $P(w_o\mid w_c)=\frac{\exp(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{\sum_{i\in\mathcal{V}}\exp(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}$：</p>
<p>$$<br>P(w_o\mid w_c)=P(D=1\mid w_c,w_o)\prod_{k=1,w_k\sim P(w)}^K P(D=0\mid w_c,w_k)<br>$$</p>
<p>其中 $P(D=1\mid w_c,w_o)=\sigma(\boldsymbol{u}_o^\top\boldsymbol{v}_c)$，$\sigma(\cdot)$ 为 sigmoid 函数。对于一对中心词和背景词，我们从词典中随机采样 $K$ 个噪声词（实验中设 $K=5$）。</p>
<hr>
<p>根据 Word2Vec 论文的建议，噪声词采样概率 $P(w)$ 设为 $w$ 词频与总词频之比的 $0.75$ 次方。</p>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_negatives</span><span class="params">(all_contexts, sampling_weights, K)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        all_contexts: [[w_o1, w_o2, ...], [...], ... ]</span></span><br><span class="line"><span class="string">        sampling_weights: 每个单词的噪声词采样概率</span></span><br><span class="line"><span class="string">        K: 随机采样个数</span></span><br><span class="line"><span class="string">    @return:</span></span><br><span class="line"><span class="string">        all_negatives: [[w_n1, w_n2, ...], [...], ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    all_negatives, neg_candidates, i = [], [], <span class="number">0</span></span><br><span class="line">    population = list(range(len(sampling_weights)))</span><br><span class="line">    <span class="keyword">for</span> contexts <span class="keyword">in</span> all_contexts:</span><br><span class="line">        negatives = []</span><br><span class="line">        <span class="keyword">while</span> len(negatives) &lt; len(contexts) * K:</span><br><span class="line">            <span class="keyword">if</span> i == len(neg_candidates):</span><br><span class="line">                <span class="comment"># 根据每个词的权重（sampling_weights）随机生成k个词的索引作为噪声词。</span></span><br><span class="line">                <span class="comment"># 为了高效计算，可以将k设得稍大一点</span></span><br><span class="line">                i, neg_candidates = <span class="number">0</span>, random.choices(</span><br><span class="line">                    population, sampling_weights, k=int(<span class="number">1e5</span>))</span><br><span class="line">            neg, i = neg_candidates[i], i + <span class="number">1</span></span><br><span class="line">            <span class="comment"># 噪声词不能是背景词</span></span><br><span class="line">            <span class="keyword">if</span> neg <span class="keyword">not</span> <span class="keyword">in</span> set(contexts):</span><br><span class="line">                negatives.append(neg)</span><br><span class="line">        all_negatives.append(negatives)</span><br><span class="line">    <span class="keyword">return</span> all_negatives</span><br><span class="line"></span><br><span class="line">sampling_weights = [counter[w]**<span class="number">0.75</span> <span class="keyword">for</span> w <span class="keyword">in</span> idx_to_token]</span><br><span class="line">all_negatives = get_negatives(all_contexts, sampling_weights, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>*注：除负采样方法外，还有层序 softmax (hiererarchical softmax) 方法也可以用来解决计算量过大的问题</p>
</blockquote>
<h3 id="批量读取数据"><a href="#批量读取数据" class="headerlink" title="批量读取数据"></a>批量读取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, centers, contexts, negatives)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> len(centers) == len(contexts) == len(negatives)</span><br><span class="line">        self.centers = centers</span><br><span class="line">        self.contexts = contexts</span><br><span class="line">        self.negatives = negatives</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (self.centers[index], self.contexts[index], self.negatives[index])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.centers)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    用作DataLoader的参数collate_fn</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        data: 长为batch_size的列表，列表中的每个元素都是__getitem__得到的结果</span></span><br><span class="line"><span class="string">    @outputs:</span></span><br><span class="line"><span class="string">        batch: 批量化后得到 (centers, contexts_negatives, masks, labels) 元组</span></span><br><span class="line"><span class="string">            centers: 中心词下标，形状为 (n, 1) 的整数张量</span></span><br><span class="line"><span class="string">            contexts_negatives: 背景词和噪声词的下标，形状为 (n, m) 的整数张量</span></span><br><span class="line"><span class="string">            masks: 与补齐相对应的掩码，形状为 (n, m) 的0/1整数张量</span></span><br><span class="line"><span class="string">            labels: 指示中心词的标签，形状为 (n, m) 的0/1整数张量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    max_len = max(len(c) + len(n) <span class="keyword">for</span> _, c, n <span class="keyword">in</span> data)</span><br><span class="line">    centers, contexts_negatives, masks, labels = [], [], [], []</span><br><span class="line">    <span class="keyword">for</span> center, context, negative <span class="keyword">in</span> data:</span><br><span class="line">        cur_len = len(context) + len(negative)</span><br><span class="line">        centers += [center]</span><br><span class="line">        contexts_negatives += [context + negative + [<span class="number">0</span>] * (max_len - cur_len)]</span><br><span class="line">        masks += [[<span class="number">1</span>] * cur_len + [<span class="number">0</span>] * (max_len - cur_len)] <span class="comment"># 使用掩码变量mask来避免填充项对损失函数计算的影响</span></span><br><span class="line">        labels += [[<span class="number">1</span>] * len(context) + [<span class="number">0</span>] * (max_len - len(context))]</span><br><span class="line">        batch = (torch.tensor(centers).view(<span class="number">-1</span>, <span class="number">1</span>), torch.tensor(contexts_negatives),</span><br><span class="line">            torch.tensor(masks), torch.tensor(labels))</span><br><span class="line">    <span class="keyword">return</span> batch</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">512</span></span><br><span class="line">num_workers = <span class="number">0</span> <span class="keyword">if</span> sys.platform.startswith(<span class="string">'win32'</span>) <span class="keyword">else</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">dataset = MyDataset(all_centers, all_contexts, all_negatives)</span><br><span class="line">data_iter = Data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            collate_fn=batchify, </span><br><span class="line">                            num_workers=num_workers)</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">    <span class="keyword">for</span> name, data <span class="keyword">in</span> zip([<span class="string">'centers'</span>, <span class="string">'contexts_negatives'</span>, <span class="string">'masks'</span>,</span><br><span class="line">                           <span class="string">'labels'</span>], batch):</span><br><span class="line">        print(name, <span class="string">'shape:'</span>, data.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a><div id="train">训练模型</div></h2><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>应用负采样方法后，我们可利用最大似然估计的对数等价形式将损失函数定义为如下</p>
<p>$$<br>\sum_{t=1}^T\sum_{-m\le j\le m,j\ne 0} [-\log P(D=1\mid w^{(t)},w^{(t+j)})-\sum_{k=1,w_k\sim P(w)^K}\log P(D=0\mid w^{(t)},w_k)]<br>$$</p>
<p>根据这个损失函数的定义，我们可以直接使用<strong>二元交叉熵损失函数</strong>进行计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SigmoidBinaryCrossEntropyLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(SigmoidBinaryCrossEntropyLoss, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets, mask=None)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        @params:</span></span><br><span class="line"><span class="string">            inputs: 经过sigmoid层后为预测D=1的概率</span></span><br><span class="line"><span class="string">            targets: 0/1向量，1代表背景词，0代表噪音词</span></span><br><span class="line"><span class="string">        @return:</span></span><br><span class="line"><span class="string">            res: 平均到每个label的loss</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        inputs, targets, mask = inputs.float(), targets.float(), mask.float()</span><br><span class="line">        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=<span class="string">"none"</span>, weight=mask)</span><br><span class="line">        res = res.sum(dim=<span class="number">1</span>) / mask.float().sum(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">loss = SigmoidBinaryCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">pred = torch.tensor([[<span class="number">1.5</span>, <span class="number">0.3</span>, <span class="number">-1</span>, <span class="number">2</span>], [<span class="number">1.1</span>, <span class="number">-0.6</span>, <span class="number">2.2</span>, <span class="number">0.4</span>]])</span><br><span class="line">label = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]]) <span class="comment"># 标签变量label中的1和0分别代表背景词和噪声词</span></span><br><span class="line">mask = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]])  <span class="comment"># 掩码变量</span></span><br><span class="line">print(loss(pred, label, mask))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmd</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> - math.log(<span class="number">1</span> / (<span class="number">1</span> + math.exp(-x)))</span><br><span class="line">print(<span class="string">'%.4f'</span> % ((sigmd(<span class="number">1.5</span>) + sigmd(<span class="number">-0.3</span>) + sigmd(<span class="number">1</span>) + sigmd(<span class="number">-2</span>)) / <span class="number">4</span>)) <span class="comment"># 注意1-sigmoid(x) = sigmoid(-x)</span></span><br><span class="line">print(<span class="string">'%.4f'</span> % ((sigmd(<span class="number">1.1</span>) + sigmd(<span class="number">-0.6</span>) + sigmd(<span class="number">-2.2</span>)) / <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<h3 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed_size = <span class="number">100</span></span><br><span class="line">net = nn.Sequential(nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),</span><br><span class="line">                    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size))</span><br></pre></td></tr></table></figure>
<h3 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(net, lr, num_epochs)</span>:</span></span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">    print(<span class="string">"train on"</span>, device)</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        start, l_sum, n = time.time(), <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            center, context_negative, mask, label = [d.to(device) <span class="keyword">for</span> d <span class="keyword">in</span> batch]</span><br><span class="line">            </span><br><span class="line">            pred = skip_gram(center, context_negative, net[<span class="number">0</span>], net[<span class="number">1</span>])</span><br><span class="line">            </span><br><span class="line">            l = loss(pred.view(label.shape), label, mask).mean() <span class="comment"># 一个batch的平均loss</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.cpu().item()</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">        print(<span class="string">'epoch %d, loss %.2f, time %.2fs'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, l_sum / n, time.time() - start))</span><br><span class="line"></span><br><span class="line">train(net, <span class="number">0.01</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>*注：最好在ＧＰＵ上运行</p>
</blockquote>
<h3 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_similar_tokens</span><span class="params">(query_token, k, embed)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params:</span></span><br><span class="line"><span class="string">        query_token: 给定的词语</span></span><br><span class="line"><span class="string">        k: 近义词的个数</span></span><br><span class="line"><span class="string">        embed: 预训练词向量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    W = embed.weight.data</span><br><span class="line">    x = W[token_to_idx[query_token]]</span><br><span class="line">    <span class="comment"># 添加的1e-9是为了数值稳定性</span></span><br><span class="line">    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=<span class="number">1</span>) * torch.sum(x * x) + <span class="number">1e-9</span>).sqrt()</span><br><span class="line">    _, topk = torch.topk(cos, k=k+<span class="number">1</span>)</span><br><span class="line">    topk = topk.cpu().numpy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> topk[<span class="number">1</span>:]:  <span class="comment"># 除去输入词</span></span><br><span class="line">        print(<span class="string">'cosine sim=%.3f: %s'</span> % (cos[i], (idx_to_token[i])))</span><br><span class="line">        </span><br><span class="line">get_similar_tokens(<span class="string">'chip'</span>, <span class="number">3</span>, net[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>PTB</tag>
        <tag>Skip-Gram</tag>
        <tag>负近似采样</tag>
        <tag>CBOW</tag>
        <tag>Word2Vec</tag>
      </tags>
  </entry>
  <entry>
    <title>Advanced Optimization</title>
    <url>/2020/02/28/Advanced%20Optimization/</url>
    <content><![CDATA[<p>基于<a href="https://blog.csdn.net/RokoBasilisk/article/details/104413638" target="_blank" rel="noopener">凸优化和梯度下降优化算法</a>，进一步展开阐述 :</p>
<ol>
<li><a href="#momentum">Momentum;</a></li>
<li><a href="#adagrad">AdaGrad;</a></li>
<li><a href="#rmsprop">RMSProp;</a></li>
<li><a href="#adadelta">AdaDelta;</a></li>
<li><a href="#adam">Adam</a></li>
</ol>
<h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a><div id="momentum">Momentum</div></h1><p>在<a href="https://blog.csdn.net/RokoBasilisk/article/details/104413638" target="_blank" rel="noopener">随机梯度下降</a>中，我们提到，目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。对于noisy gradient,我们需要谨慎的选取学习率和batch size, 来控制梯度方差和收敛的结果。</p>
<p>$$<br>\mathbf{g}<em>t = \partial</em>{\mathbf{w}} \frac{1}{|\mathcal{B}<em>t|} \sum</em>{i \in \mathcal{B}<em>t} f(\mathbf{x}</em>{i}, \mathbf{w}<em>{t-1}) = \frac{1}{|\mathcal{B}_t|} \sum</em>{i \in \mathcal{B}<em>t} \mathbf{g}</em>{i, t-1}.<br>$$</p>
<h2 id="An-ill-conditioned-Problem"><a href="#An-ill-conditioned-Problem" class="headerlink" title="An ill-conditioned Problem"></a>An ill-conditioned Problem</h2><p>Condition Number of Hessian Matrix:</p>
<p>$$<br> cond_{H} = \frac{\lambda_{max}}{\lambda_{min}}<br>$$</p>
<p>where $\lambda_{max}, \lambda_{min}$ is the maximum amd minimum eignvalue of Hessian matrix.</p>
<p>让我们考虑一个输入和输出分别为二维向量$\boldsymbol{x} = [x_1, x_2]^\top$和标量的目标函数:</p>
<p>$$<br> f(\boldsymbol{x})=0.1x_1^2+2x_2^2<br>$$</p>
<p>$$<br> cond_{H} = \frac{4}{0.2} = 20 \quad \rightarrow \quad \text{ill-conditioned}<br>$$</p>
<h2 id="Supp-Preconditioning"><a href="#Supp-Preconditioning" class="headerlink" title="Supp: Preconditioning"></a>Supp: Preconditioning</h2><center><img src="https://img-blog.csdnimg.cn/2020022114561129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>在二阶优化中，我们使用Hessian matrix的逆矩阵(或者pseudo inverse)来左乘梯度向量 $i.e. \Delta_{x} = H^{-1}\mathbf{g}$，这样的做法称为precondition，相当于将 $H$ 映射为一个单位矩阵，拥有分布均匀的Spectrum，也即我们去优化的等价标函数的Hessian matrix为良好的identity matrix。</p>
<p>将$x_1^2$系数从$1$减小到了$0.1$。下面实现基于这个目标函数的梯度下降，并展示使用学习率为$0.4$时自变量的迭代轨迹。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.4</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * <span class="number">0.2</span> * x1, x2 - eta * <span class="number">4</span> * x2, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223165505355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>可以看到，同一位置上，目标函数在竖直方向（$x_2$轴方向）比在水平方向（$x_1$轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。</p>
<p>接下来将学习率 $eta$ 加大+<font size=6>+</font><font size=10>+</font></p>
<p>此时自变量在竖直方向不断越过最优解并逐渐发散。</p>
<h3 id="Solution-to-ill-condition"><a href="#Solution-to-ill-condition" class="headerlink" title="Solution to ill-condition"></a>Solution to ill-condition</h3><ul>
<li><strong>Preconditioning gradient vector</strong>: applied in Adam, RMSProp, AdaGrad, Adelta, KFC, Natural gradient and other secord-order optimization algorithms.</li>
<li><strong>Averaging history gradient</strong>: like momentum, which allows larger learning rates to accelerate convergence; applied in Adam, RMSProp, SGD momentum.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.6</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223165906709.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Momentum-Algorithm"><a href="#Momentum-Algorithm" class="headerlink" title="Momentum Algorithm"></a>Momentum Algorithm</h2><p>动量法的提出是为了解决梯度下降的上述问题。设时间步 $t$ 的自变量为 $\boldsymbol{x}_t$，学习率为 $\eta_t$。<br>在时间步 $t=0$，动量法创建速度变量 $\boldsymbol{m}_0$，并将其元素初始化成 0。在时间步 $t&gt;0$，动量法对每次迭代的步骤做如下修改：</p>
<p>$$<br>\begin{aligned}<br>\boldsymbol{m}_t &amp;\leftarrow \beta \boldsymbol{m}_{t-1} + \eta_t \boldsymbol{g}_t, \<br>\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{m}_t,<br>\end{aligned}<br>$$</p>
<p>Another version:</p>
<p>$$<br>\begin{aligned}<br>\boldsymbol{m}_t &amp;\leftarrow \beta \boldsymbol{m}_{t-1} + (1-\beta) \boldsymbol{g}_t, \<br>\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}_{t-1} - \alpha_t \boldsymbol{m}_t,<br>\end{aligned}<br>$$</p>
<p>$$<br>\alpha_t = \frac{\eta_t}{1-\beta}<br>$$</p>
<p>其中，动量超参数 $\beta$满足 $0 \leq \beta &lt; 1$。当 $\beta=0$ 时，动量法等价于小批量随机梯度下降。</p>
<p>利用梯度下降在使用动量法后的迭代轨迹更加方便理解数学推导</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">momentum_2d</span><span class="params">(x1, x2, v1, v2)</span>:</span></span><br><span class="line">    v1 = beta * v1 + eta * <span class="number">0.2</span> * x1</span><br><span class="line">    v2 = beta * v2 + eta * <span class="number">4</span> * x2</span><br><span class="line">    <span class="keyword">return</span> x1 - v1, x2 - v2, v1, v2</span><br><span class="line"></span><br><span class="line">eta, beta = <span class="number">0.4</span>, <span class="number">0.5</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223170104740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>可以看到使用较小的学习率 $\eta=0.4$ 和动量超参数 $\beta=0.5$ 时，动量法在竖直方向上的移动更加平滑，且在水平方向上更快逼近最优解。下面使用较大的学习率 $\eta=0.6$，此时自变量也不再发散。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.6</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223170204913.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="Exponential-Moving-Average"><a href="#Exponential-Moving-Average" class="headerlink" title="Exponential Moving Average"></a>Exponential Moving Average</h3><blockquote>
<p>从数学上理解动量法</p>
</blockquote>
<hr>
<p>指数加权移动平均（exponential moving average）</p>
<p>给定超参数 $0 \leq \beta &lt; 1$，当前时间步 $t$ 的变量 $y_t$ 是上一时间步 $t-1$ 的变量 $y_{t-1}$ 和当前时间步另一变量 $x_t$ 的线性组合：</p>
<p>$$<br>y_t = \beta y_{t-1} + (1-\beta) x_t.<br>$$</p>
<p>我们可以对 $y_t$ 展开：</p>
<p>$$<br>\begin{aligned}<br>y_t  &amp;= (1-\beta) x_t + \beta y_{t-1}\<br>         &amp;= (1-\beta)x_t + (1-\beta) \cdot \beta x_{t-1} + \beta^2y_{t-2}\<br>         &amp;= (1-\beta)x_t + (1-\beta) \cdot \beta x_{t-1} + (1-\beta) \cdot \beta^2x_{t-2} + \beta^3y_{t-3}\<br>         &amp;= (1-\beta) \sum_{i=0}^{t} \beta^{i}x_{t-i}<br>\end{aligned}<br>$$</p>
<p>$$<br>(1-\beta)\sum_{i=0}^{t} \beta^{i} = \frac{1-\beta^{t}}{1-\beta} (1-\beta) = (1-\beta^{t})<br>$$</p>
<h3 id="Supp-Approximate"><a href="#Supp-Approximate" class="headerlink" title="Supp Approximate"></a>Supp Approximate</h3><p>Average of $\frac{1}{1-\beta}$ Steps</p>
<p>令 $n = 1/(1-\beta)$，那么 $\left(1-1/n\right)^n = \beta^{1/(1-\beta)}$。因为</p>
<p>$$<br> \lim_{n \rightarrow \infty}  \left(1-\frac{1}{n}\right)^n = \exp(-1) \approx 0.3679,<br>$$</p>
<p>所以当 $\beta \rightarrow 1$时，$\beta^{1/(1-\beta)}=\exp(-1)$，如 $0.95^{20} \approx \exp(-1)$。如果把 $\exp(-1)$ 当作一个比较小的数，我们可以在近似中忽略所有含 $\beta^{1/(1-\beta)}$ 和比 $\beta^{1/(1-\beta)}$ 更高阶的系数的项。例如，当 $\beta=0.95$ 时，</p>
<p>$$<br>y_t \approx 0.05 \sum_{i=0}^{19} 0.95^i x_{t-i}.<br>$$</p>
<p>因此，在实际中，我们常常将 $y_t$ 看作是对最近 $1/(1-\beta)$ 个时间步的 $x_t$ 值的加权平均。例如，当 $\gamma = 0.95$ 时，$y_t$ 可以被看作对最近20个时间步的 $x_t$ 值的加权平均；当 $\beta = 0.9$ 时，$y_t$ 可以看作是对最近10个时间步的 $x_t$ 值的加权平均。而且，离当前时间步 $t$ 越近的 $x_t$ 值获得的权重越大（越接近1）。</p>
<hr>
<h3 id="由指数加权移动平均理解动量法"><a href="#由指数加权移动平均理解动量法" class="headerlink" title="由指数加权移动平均理解动量法"></a>由指数加权移动平均理解动量法</h3><p>现在，我们对动量法的速度变量做变形：</p>
<p>$$<br>\boldsymbol{m}_t \leftarrow \beta \boldsymbol{m}_{t-1} + (1 - \beta) \left(\frac{\eta_t}{1 - \beta} \boldsymbol{g}_t\right).<br>$$</p>
<p>Another version:</p>
<p>$$<br>\boldsymbol{m}_t \leftarrow \beta \boldsymbol{m}_{t-1} + (1 - \beta) \boldsymbol{g}_t.<br>$$</p>
<p>$$<br>\begin{aligned}<br>\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}_{t-1} - \alpha_t \boldsymbol{m}_t,<br>\end{aligned}<br>$$</p>
<p>$$<br>\alpha_t = \frac{\eta_t}{1-\beta}<br>$$</p>
<p>由指数加权移动平均的形式可得，速度变量 $\boldsymbol{v}_t$ 实际上对序列 $\{\eta_{t-i}\boldsymbol{g}_{t-i} /(1-\beta):i=0,\ldots,1/(1-\beta)-1\}$ 做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，动量法在每个时间步的自变量更新量近似于将前者对应的最近 $1/(1-\beta)$ 个时间步的更新量做了指数加权移动平均后再除以 $1-\beta$。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。在本节之前示例的优化问题中，所有梯度在水平方向上为正（向右），而在竖直方向上时正（向上）时负（向下）。这样，我们就可以使用较大的学习率，从而使自变量向最优解更快移动。</p>
<h2 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h2><p>相对于小批量随机梯度下降，动量法需要对每一个自变量维护一个同它一样形状的速度变量，且超参数里多了动量超参数。实现中，我们将速度变量用更广义的状态变量<code>states</code>表示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span>  </span><br><span class="line">    data = np.genfromtxt(<span class="string">'/home/kesci/input/airfoil4755/airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), \</span><br><span class="line">        torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_momentum_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32)</span><br><span class="line">    v_b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> p, v <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v.data = hyperparams[<span class="string">'momentum'</span>] * v.data + hyperparams[<span class="string">'lr'</span>] * p.grad.data</span><br><span class="line">        p.data -= v.data</span><br></pre></td></tr></table></figure>
<blockquote>
<p>令 momentum = 0.5</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(sgd_momentum, init_momentum_states(),</span><br><span class="line">              &#123;<span class="string">'lr'</span>: <span class="number">0.02</span>, <span class="string">'momentum'</span>: <span class="number">0.5</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223170543454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<p>令 momentum = 0.9</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(sgd_momentum, init_momentum_states(),</span><br><span class="line">              &#123;<span class="string">'lr'</span>: <span class="number">0.02</span>, <span class="string">'momentum'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223170642172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>可见目标函数值在后期迭代过程中的变化不够平滑。直觉上，10倍小批量梯度比2倍小批量梯度大了5倍，我们可以试着将学习率减小到原来的1/5。此时目标函数值在下降了一段时间后变化更加平滑。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(sgd_momentum, init_momentum_states(),</span><br><span class="line">              &#123;<span class="string">'lr'</span>: <span class="number">0.004</span>, <span class="string">'momentum'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223170904126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class"><a href="#Pytorch-Class" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><blockquote>
<p>在Pytorch中，torch.optim.SGD 已实现了Momentum</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.SGD, &#123;<span class="string">'lr'</span>: <span class="number">0.004</span>, <span class="string">'momentum'</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">                    features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/202002231714533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h1 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a><div id="adagrad">AdaGrad</div></h1><p>在之前介绍过的优化算法中，目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。举个例子，假设目标函数为$f$，自变量为一个二维向量$[x_1, x_2]^\top$，该向量中每一个元素在迭代时都使用相同的学习率。例如，在学习率为$\eta$的梯度下降中，元素$x_1$和$x_2$都使用相同的学习率$\eta$来自我迭代：</p>
<p>$$<br>x_1 \leftarrow x_1 - \eta \frac{\partial{f}}{\partial{x_1}}, \quad<br>x_2 \leftarrow x_2 - \eta \frac{\partial{f}}{\partial{x_2}}.<br>$$</p>
<p><a href="#momentum">动量法</a>中当 $x_1$ 和 $x_2$ 的梯度值有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。</p>
<p>动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。</p>
<p>AdaGrad 算法，它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题 。</p>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>AdaGrad算法会使用一个小批量随机梯度 $\boldsymbol{g}_t$ 按元素平方的累加变量$\boldsymbol{s}_t$。在时间步0，AdaGrad将 $\boldsymbol{s}_0$ 中每个元素初始化为0。在时间步$t$，首先将小批量随机梯度$\boldsymbol{g}_t$按元素平方后累加到变量$\boldsymbol{s}_t$：</p>
<p>$$<br>\boldsymbol{s}_t \leftarrow \boldsymbol{s}_{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t,<br>$$</p>
<p>其中$\odot$是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：</p>
<p>$$<br>\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t,<br>$$</p>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。</p>
<h2 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h2><p>需要强调的是，小批量随机梯度按元素平方的累加变量$\boldsymbol{s}_t$出现在学习率的分母项中。</p>
<ul>
<li><p>因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；</p>
</li>
<li><p>反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。</p>
</li>
<li><p>然而，由于$\boldsymbol{s}_t$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。</p>
</li>
<li><p>所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。</p>
</li>
</ul>
<p>下面我们仍然以目标函数$f(\boldsymbol{x})=0.1x_1^2+2x_2^2$为例观察AdaGrad算法对自变量的迭代轨迹。我们实现AdaGrad算法并使用和上一节实验中相同的学习率0.4。可以看到，自变量的迭代轨迹较平滑。但由于$\boldsymbol{s}_t$的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span>  <span class="comment"># 前两项为自变量梯度</span></span><br><span class="line">    s1 += g1 ** <span class="number">2</span></span><br><span class="line">    s2 += g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.4</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223171954469.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<p>下面将学习率增大到2。可以看到自变量更为迅速地逼近了最优解</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">2</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223172353999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Implement-1"><a href="#Implement-1" class="headerlink" title="Implement"></a>Implement</h2><p>同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。我们根据AdaGrad算法中的公式实现该算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span>  </span><br><span class="line">    data = np.genfromtxt(<span class="string">'path to airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), </span><br><span class="line">        torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32)</span><br><span class="line">        </span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adagrad_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32)</span><br><span class="line">    s_b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    eps = <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s.data += (p.grad.data**<span class="number">2</span>)</span><br><span class="line">        p.data -= hyperparams[<span class="string">'lr'</span>] * p.grad.data / torch.sqrt(s + eps)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>增大学习率</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(adagrad, init_adagrad_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223173629947.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class-1"><a href="#Pytorch-Class-1" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><p>通过名称为 “adagrad” 的 Trainer 实例，我们便可使用 Pytorch 提供的 AdaGrad 算法来训练模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adagrad, &#123;<span class="string">'lr'</span>: <span class="number">0.1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223173755189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h1 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a><div id="rmsprop">RMSProp</div></h1><p><a href="#adagrad">“AdaGrad算法”</a>中因为调整学习率时分母上的变量 $\boldsymbol{s}_t$ 一直在累加按元素平方的小批量随机梯度，所以目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。因此，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp算法对AdaGrad算法做了修改。该算法源自Coursera上的一门课程，即“机器学习的神经网络”。</p>
<h2 id="Algorithm-1"><a href="#Algorithm-1" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>我们在<a href="#momentum">“动量法”</a>一节里介绍过指数加权移动平均。不同于AdaGrad算法里状态变量$\boldsymbol{s}_t$是截至时间步$t$所有小批量随机梯度$\boldsymbol{g}_t$按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数$0 \leq \gamma 0$计算</p>
<p>$$<br>\boldsymbol{v}_t \leftarrow \beta \boldsymbol{v}_{t-1} + (1 - \beta) \boldsymbol{g}_t \odot \boldsymbol{g}_t.<br>$$</p>
<p>和AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量</p>
<p>$$<br>\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\alpha}{\sqrt{\boldsymbol{v}_t + \epsilon}} \odot \boldsymbol{g}_t,<br>$$</p>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。因为RMSProp算法的状态变量$\boldsymbol{s}_t$是对平方项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$的指数加权移动平均，所以可以看作是最近$1/(1-\beta)$个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。</p>
<p>照例，让我们先观察RMSProp算法对目标函数$f(\boldsymbol{x})=0.1x_1^2+2x_2^2$中自变量的迭代轨迹。回忆在<a href="#adagrad">“AdaGrad算法”</a>中使用的学习率为0.4的AdaGrad算法，自变量在迭代后期的移动幅度较小。但在同样的学习率下，RMSProp算法可以更快逼近最优解。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span></span><br><span class="line">    s1 = beta * s1 + (<span class="number">1</span> - beta) * g1 ** <span class="number">2</span></span><br><span class="line">    s2 = beta * s2 + (<span class="number">1</span> - beta) * g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= alpha / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= alpha / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">alpha, beta = <span class="number">0.4</span>, <span class="number">0.9</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/2020022317423677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Implement-2"><a href="#Implement-2" class="headerlink" title="Implement"></a>Implement</h2><blockquote>
<p>接下来按照RMSProp算法中的公式实现该算法。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span>  </span><br><span class="line">    data = np.genfromtxt(<span class="string">'path to airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), \</span><br><span class="line">        torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32)</span><br><span class="line">        </span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rmsprop_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32)</span><br><span class="line">    s_b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    gamma, eps = hyperparams[<span class="string">'beta'</span>], <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s.data = gamma * s.data + (<span class="number">1</span> - gamma) * (p.grad.data)**<span class="number">2</span></span><br><span class="line">        p.data -= hyperparams[<span class="string">'lr'</span>] * p.grad.data / torch.sqrt(s + eps)</span><br></pre></td></tr></table></figure>
<p>我们将初始学习率设为0.01，并将超参数$\gamma$设为0.9。此时，变量$\boldsymbol{s}_t$可看作是最近$1/(1-0.9) = 10$个时间步的平方项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$的加权平均。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(rmsprop, init_rmsprop_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'beta'</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">              features, labels)</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/2020022317441345.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class-2"><a href="#Pytorch-Class-2" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><p>通过名称为“rmsprop”的Trainer实例，我们便可使用Gluon提供的RMSProp算法来训练模型。注意，超参数$\gamma$通过gamma1指定。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.RMSprop, &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'alpha'</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">                    features, labels)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200223174542606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h1 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a><div id="adadelta">AdaDelta</div></h1><p>除了<a href="#rmsprop">RMSProp算法</a>以外，另一个常用优化算法AdaDelta算法也针对<a href="#adagrad">AdaGrad算法</a>在迭代后期可能较难找到有用解的问题做了改进 .</p>
<blockquote>
<p>AdaDelta算法没有学习率这一超参数。</p>
</blockquote>
<h2 id="Algorithm-2"><a href="#Algorithm-2" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>AdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度$\boldsymbol{g}_t$按元素平方的指数加权移动平均变量$\boldsymbol{s}_t$。在时间步0，它的所有元素被初始化为0。给定超参数$0 \leq \rho 0$，同RMSProp算法一样计算</p>
<p>$$<br>\boldsymbol{s}_t \leftarrow \rho \boldsymbol{s}_{t-1} + (1 - \rho) \boldsymbol{g}_t \odot \boldsymbol{g}_t.<br>$$</p>
<p>与RMSProp算法不同的是，AdaDelta算法还维护一个额外的状态变量$\Delta\boldsymbol{x}<em>t$，其元素同样在时间步0时被初始化为0。我们使用$\Delta\boldsymbol{x}</em>{t-1}$来计算自变量的变化量：</p>
<p>$$<br> \boldsymbol{g}_t’ \leftarrow \sqrt{\frac{\Delta\boldsymbol{x}_{t-1} + \epsilon}{\boldsymbol{s}_t + \epsilon}}   \odot \boldsymbol{g}_t,<br>$$</p>
<p>其中$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-5}$。接着更新自变量：</p>
<p>$$<br>\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}’_t.<br>$$</p>
<p>最后，我们使用$\Delta\boldsymbol{x}_t$来记录自变量变化量$\boldsymbol{g}’_t$按元素平方的指数加权移动平均：</p>
<p>$$<br>\Delta\boldsymbol{x}_t \leftarrow \rho \Delta\boldsymbol{x}_{t-1} + (1 - \rho) \boldsymbol{g}’_t \odot \boldsymbol{g}’_t.<br>$$</p>
<p>可以看到，如不考虑$\epsilon$的影响，AdaDelta算法与RMSProp算法的不同之处在于使用$\sqrt{\Delta\boldsymbol{x}_{t-1}}$来替代超参数$\eta$。</p>
<h2 id="Implement-3"><a href="#Implement-3" class="headerlink" title="Implement"></a>Implement</h2><p>AdaDelta算法需要对每个自变量维护两个状态变量，即$\boldsymbol{s}_t$和$\Delta\boldsymbol{x}_t$。我们按AdaDelta算法中的公式实现该算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adadelta_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w, s_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    delta_w, delta_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> ((s_w, delta_w), (s_b, delta_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adadelta</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    rho, eps = hyperparams[<span class="string">'rho'</span>], <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> p, (s, delta) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s[:] = rho * s + (<span class="number">1</span> - rho) * (p.grad.data**<span class="number">2</span>)</span><br><span class="line">        g =  p.grad.data * torch.sqrt((delta + eps) / (s + eps))</span><br><span class="line">        p.data -= g</span><br><span class="line">        delta[:] = rho * delta + (<span class="number">1</span> - rho) * g * g</span><br><span class="line">d2l.train_ch7(adadelta, init_adadelta_states(), &#123;<span class="string">'rho'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223174834831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class-3"><a href="#Pytorch-Class-3" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><p>通过名称为 “adadelta” 的 Traine r实例，我们便可使用 pytorch 提供的 AdaDelta 算法。它的超参数可以通过 rho 来指定。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adadelta, &#123;<span class="string">'rho'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/2020022317495777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a><div id="adam">Adam</div></h1><blockquote>
<p>相当于是<a href="#rmsprop">RMSProp算法</a>和<a href="#momentum">动量算法</a>的结合</p>
</blockquote>
<center>RMSProp算法</center>
<center>+</center>
<center>对小批量随机梯度也做了指数加权移动平均</center>
<center>||</center>
<center>Adam</center>

<h2 id="Algorithm-3"><a href="#Algorithm-3" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Adam算法使用了动量变量$\boldsymbol{m}_t$和RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量$\boldsymbol{v}_t$，并在时间步0将它们中每个元素初始化为0。给定超参数$0 \leq \beta_1 &lt; 1$（算法作者建议设为0.9），时间步$t$的动量变量$\boldsymbol{m}_t$即小批量随机梯度$\boldsymbol{g}_t$的指数加权移动平均：</p>
<p>$$<br>\boldsymbol{m}_t \leftarrow \beta_1 \boldsymbol{m}_{t-1} + (1 - \beta_1) \boldsymbol{g}_t.<br>$$</p>
<p>和RMSProp算法中一样，给定超参数$0 \leq \beta_2 &lt; 1$（算法作者建议设为0.999），<br>将小批量随机梯度按元素平方后的项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$做指数加权移动平均得到$\boldsymbol{v}_t$：</p>
<p>$$<br>\boldsymbol{v}_t \leftarrow \beta_2 \boldsymbol{v}_{t-1} + (1 - \beta_2) \boldsymbol{g}_t \odot \boldsymbol{g}_t.<br>$$</p>
<p>由于我们将$\boldsymbol{m}_0$和$\boldsymbol{s}_0$中的元素都初始化为0，<br>在时间步$t$我们得到$\boldsymbol{m}_t =  (1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} \boldsymbol{g}_i$。将过去各时间步小批量随机梯度的权值相加，得到 $(1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} = 1 - \beta_1^t$。需要注意的是，当$t$较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当$\beta_1 = 0.9$时，$\boldsymbol{m}_1 = 0.1\boldsymbol{g}_1$。为了消除这样的影响，对于任意时间步$t$，我们可以将$\boldsymbol{m}_t$再除以$1 - \beta_1^t$，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，我们对变量$\boldsymbol{m}_t$和$\boldsymbol{v}_t$均作偏差修正：</p>
<p>$$<br>\hat{\boldsymbol{m}}_t \leftarrow \frac{\boldsymbol{m}_t}{1 - \beta_1^t},<br>$$</p>
<p>$$<br>\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_2^t}.<br>$$</p>
<p>接下来，Adam算法使用以上偏差修正后的变量$\hat{\boldsymbol{m}}_t$和$\hat{\boldsymbol{m}}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整：</p>
<p>$$<br>\boldsymbol{g}_t’ \leftarrow \frac{\eta \hat{\boldsymbol{m}}_t}{\sqrt{\hat{\boldsymbol{v}}_t} + \epsilon},<br>$$</p>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-8}$。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用$\boldsymbol{g}_t’$迭代自变量：</p>
<p>$$<br>\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}_t’.<br>$$</p>
<h2 id="Implement-4"><a href="#Implement-4" class="headerlink" title="Implement"></a>Implement</h2><p>我们按照Adam算法中的公式实现该算法。其中时间步$t$通过 hyperparams 参数传入 adam 函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span>  </span><br><span class="line">    data = np.genfromtxt(<span class="string">'path to airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), \</span><br><span class="line">        torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32)</span><br><span class="line">        </span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adam_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w, v_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    s_w, s_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad.data</span><br><span class="line">        s[:] = beta2 * s + (<span class="number">1</span> - beta2) * p.grad.data**<span class="number">2</span></span><br><span class="line">        v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">'t'</span>])</span><br><span class="line">        s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">'t'</span>])</span><br><span class="line">        p.data -= hyperparams[<span class="string">'lr'</span>] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)</span><br><span class="line">    hyperparams[<span class="string">'t'</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_ch7(adam, init_adam_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'t'</span>: <span class="number">1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200223180150289.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="Pytorch-Class-4"><a href="#Pytorch-Class-4" class="headerlink" title="Pytorch Class"></a>Pytorch Class</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adam, &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>


<center><img src="https://img-blog.csdnimg.cn/20200223180238893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Momentum</tag>
        <tag>AdaGrad</tag>
        <tag>RMSProp</tag>
        <tag>AdaDelta</tag>
        <tag>Adam</tag>
      </tags>
  </entry>
  <entry>
    <title>Optimization including Convex Optimization and Gradient Descent</title>
    <url>/2020/02/28/Optimization%20including%20Convex%20Optimization%20and%20Gradient%20Descent/</url>
    <content><![CDATA[<p><font color=red>温馨提示：</font></p>
<p>&ensp;&ensp;&ensp;&ensp;<font color=red>本文将介绍统计学中的优化知识，凸优化和梯度下降，多为公式推导和图形化展示，较为硬核</font></p>
<h1 id="优化与深度学习"><a href="#优化与深度学习" class="headerlink" title="优化与深度学习"></a>优化与深度学习</h1><h2 id="优化与估计"><a href="#优化与估计" class="headerlink" title="优化与估计"></a>优化与估计</h2><p>尽管优化方法可以最小化深度学习中的损失函数值，但本质上优化方法达到的目标与深度学习的目标并不相同。</p>
<ul>
<li><p><strong>优化方法目标</strong>：训练集损失函数值</p>
</li>
<li><p><strong>深度学习目标</strong>：测试集损失函数值（泛化性）</p>
</li>
<li><p><strong>借助图形直观比较</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'path to file storge d2lzh1981'</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d <span class="comment"># 三维画图</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span> <span class="keyword">return</span> x * np.cos(np.pi * x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(x)</span>:</span> <span class="keyword">return</span> f(x) + <span class="number">0.2</span> * np.cos(<span class="number">5</span> * np.pi * x)</span><br><span class="line"></span><br><span class="line">d2l.set_figsize((<span class="number">5</span>, <span class="number">3</span>))</span><br><span class="line">x = np.arange(<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">0.01</span>)</span><br><span class="line">fig_f, = d2l.plt.plot(x, f(x),label=<span class="string">"train error"</span>)</span><br><span class="line">fig_g, = d2l.plt.plot(x, g(x),<span class="string">'--'</span>, c=<span class="string">'purple'</span>, label=<span class="string">"test error"</span>)</span><br><span class="line">fig_f.axes.annotate(<span class="string">'empirical risk'</span>, (<span class="number">1.0</span>, <span class="number">-1.2</span>), (<span class="number">0.5</span>, <span class="number">-1.1</span>),arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">fig_g.axes.annotate(<span class="string">'expected risk'</span>, (<span class="number">1.1</span>, <span class="number">-1.05</span>), (<span class="number">0.95</span>, <span class="number">-0.5</span>),arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'risk'</span>)</span><br><span class="line">d2l.plt.legend(loc=<span class="string">"upper right"</span>)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220165647572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="优化在深度学习中的挑战"><a href="#优化在深度学习中的挑战" class="headerlink" title="优化在深度学习中的挑战"></a>优化在深度学习中的挑战</h2><ol>
<li><a href="#Local_minimum"><strong>局部最小值</strong></a></li>
<li><a href="#saddle_point"><strong>鞍点</strong></a></li>
<li><a href="#vanishing_gradient"><strong>梯度消失</strong></a></li>
</ol>
<div id="Local_minimum"><b>局部最小值</b></div>

<p>$$<br>f(x) = x\cos \pi x<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * np.cos(np.pi * x)</span><br><span class="line"></span><br><span class="line">d2l.set_figsize((<span class="number">4.5</span>, <span class="number">2.5</span>))</span><br><span class="line">x = np.arange(<span class="number">-1.0</span>, <span class="number">2.0</span>, <span class="number">0.1</span>)</span><br><span class="line">fig,  = d2l.plt.plot(x, f(x))</span><br><span class="line">fig.axes.annotate(<span class="string">'local minimum'</span>, xy=(<span class="number">-0.3</span>, <span class="number">-0.25</span>), xytext=(<span class="number">-0.77</span>, <span class="number">-1.0</span>),</span><br><span class="line">                  arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">fig.axes.annotate(<span class="string">'global minimum'</span>, xy=(<span class="number">1.1</span>, <span class="number">-0.95</span>), xytext=(<span class="number">0.6</span>, <span class="number">0.8</span>),</span><br><span class="line">                  arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'f(x)'</span>);</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220170158309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<div id="saddle_point"><b>鞍点</b></div><br/>

<blockquote>
<p>函数在一阶导数为零处（驻点）的黑塞矩阵为不定矩阵。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">-2.0</span>, <span class="number">2.0</span>, <span class="number">0.1</span>)</span><br><span class="line">fig, = d2l.plt.plot(x, x**<span class="number">3</span>)</span><br><span class="line">fig.axes.annotate(<span class="string">'saddle point'</span>, xy=(<span class="number">0</span>, <span class="number">-0.2</span>), xytext=(<span class="number">-0.52</span>, <span class="number">-5.0</span>),</span><br><span class="line">                  arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'f(x)'</span>);</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220170416636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<hr>
<p><strong>海森矩阵</strong></p>
<p>$$<br>A=\left[\begin{array}{cccc}{\frac{\partial^{2} f}{\partial x_{1}^{2}}} &amp; {\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}}} \\ {\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}} &amp; {\frac{\partial^{2} f}{\partial x_{2}^{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{2} \partial x_{n}}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} &amp; {\frac{\partial^{2} f}{\partial x_{n} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{n}^{2}}}\end{array}\right]<br>$$</p>
<blockquote>
<p>海森矩阵特征值和鞍点还有局部极小值的点的关系</p>
<blockquote>
<p>偏导数为零的点</p>
<ul>
<li>特征值都大于零是局部极小值点</li>
<li>都为负数是局部极大指点</li>
<li>有正有负就是鞍点</li>
</ul>
</blockquote>
</blockquote>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x, y = np.mgrid[<span class="number">-1</span>: <span class="number">1</span>: <span class="number">31j</span>, <span class="number">-1</span>: <span class="number">1</span>: <span class="number">31j</span>]</span><br><span class="line">z = x**<span class="number">2</span> - y**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">d2l.set_figsize((<span class="number">6</span>, <span class="number">4</span>))</span><br><span class="line">ax = d2l.plt.figure().add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.plot_wireframe(x, y, z, **&#123;<span class="string">'rstride'</span>: <span class="number">2</span>, <span class="string">'cstride'</span>: <span class="number">2</span>&#125;)</span><br><span class="line">ax.plot([<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">0</span>], <span class="string">'ro'</span>, markersize=<span class="number">10</span>)</span><br><span class="line">ticks = [<span class="number">-1</span>,  <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">d2l.plt.xticks(ticks)</span><br><span class="line">d2l.plt.yticks(ticks)</span><br><span class="line">ax.set_zticks(ticks)</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'y'</span>);</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220171025698.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<div id="vanishing_gradient"><b>梯度消失</b></div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">-2.0</span>, <span class="number">5.0</span>, <span class="number">0.01</span>)</span><br><span class="line">fig, = d2l.plt.plot(x, np.tanh(x))</span><br><span class="line">d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">d2l.plt.ylabel(<span class="string">'f(x)'</span>)</span><br><span class="line">fig.axes.annotate(<span class="string">'vanishing gradient'</span>, (<span class="number">4</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">0.0</span>) ,arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220171233420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'path to file storge d2lzh1981'</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h3 id="一维梯度下降"><a href="#一维梯度下降" class="headerlink" title="一维梯度下降"></a>一维梯度下降</h3><hr>
<p><strong>证明：沿梯度反方向移动自变量可以减小函数值</strong></p>
<p>泰勒展开：</p>
<p>$$<br>f(x+\epsilon)=f(x)+\epsilon f^{\prime}(x)+\mathcal{O}\left(\epsilon^{2}\right)<br>$$</p>
<p>代入沿梯度方向的移动量 $\eta f^{\prime}(x)$：</p>
<p>$$<br>f\left(x-\eta f^{\prime}(x)\right)=f(x)-\eta f^{\prime 2}(x)+\mathcal{O}\left(\eta^{2} f^{\prime 2}(x)\right)<br>$$</p>
<p>$$<br>f\left(x-\eta f^{\prime}(x)\right) \lesssim f(x)<br>$$</p>
<p>$$<br>x \leftarrow x-\eta f^{\prime}(x)<br>$$</p>
<hr>
<p>e.g.</p>
<p>$$<br>f(x) = x^2<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span>  <span class="comment"># Objective function</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x  <span class="comment"># Its derivative</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd</span><span class="params">(eta)</span>:</span></span><br><span class="line">    x = <span class="number">10</span></span><br><span class="line">    results = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        <span class="comment"># eta 学习率</span></span><br><span class="line">        x -= eta * gradf(x)</span><br><span class="line">        results.append(x)</span><br><span class="line">    print(<span class="string">'epoch 20, x:'</span>, x)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">res = gd(<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<p><strong>梯度下降轨迹</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_trace</span><span class="params">(res)</span>:</span></span><br><span class="line">    n = max(abs(min(res)), abs(max(res)))</span><br><span class="line">    f_line = np.arange(-n, n, <span class="number">0.01</span>)</span><br><span class="line">    d2l.set_figsize((<span class="number">3.5</span>, <span class="number">2.5</span>))</span><br><span class="line">    d2l.plt.plot(f_line, [f(x) <span class="keyword">for</span> x <span class="keyword">in</span> f_line],<span class="string">'-'</span>)</span><br><span class="line">    d2l.plt.plot(res, [f(x) <span class="keyword">for</span> x <span class="keyword">in</span> res],<span class="string">'-o'</span>)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'f(x)'</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">show_trace(res)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220171815966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<center><font size=5>学习率</font></center>

<blockquote>
<p>学习率过小 Code：show_trace(gd(0.05))<br><img src="https://img-blog.csdnimg.cn/2020022017211762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
<blockquote>
<p>学习率过大  Code：show_trace(gd(1.1))<br><img src="https://img-blog.csdnimg.cn/20200220172154883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
</blockquote>
<blockquote>
<center><font size=4>局部极小值</font></center>
</blockquote>
<p>$$<br>f(x) = x\cos cx<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = <span class="number">0.15</span> * np.pi</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * np.cos(c * x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.cos(c * x) - c * x * np.sin(c * x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率不合适容易导致</span></span><br><span class="line">show_trace(gd(<span class="number">2</span>))</span><br><span class="line">show_trace(gd(<span class="number">0.5</span>))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220172910884.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="多维梯度下降"><a href="#多维梯度下降" class="headerlink" title="多维梯度下降"></a>多维梯度下降</h3><p>$$<br>\nabla f(\mathbf{x})=\left[\frac{\partial f(\mathbf{x})}{\partial x_{1}}, \frac{\partial f(\mathbf{x})}{\partial x_{2}}, \dots, \frac{\partial f(\mathbf{x})}{\partial x_{d}}\right]^{\top}<br>$$</p>
<p>$$<br>f(\mathbf{x}+\epsilon)=f(\mathbf{x})+\epsilon^{\top} \nabla f(\mathbf{x})+\mathcal{O}\left(|\epsilon|^{2}\right)<br>$$</p>
<p>$$<br>\mathbf{x} \leftarrow \mathbf{x}-\eta \nabla f(\mathbf{x})<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练 trainer展示x如何更新</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_2d</span><span class="params">(trainer, steps=<span class="number">20</span>)</span>:</span></span><br><span class="line">    x1, x2 = <span class="number">-5</span>, <span class="number">-2</span></span><br><span class="line">    results = [(x1, x2)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(steps):</span><br><span class="line">        x1, x2 = trainer(x1, x2)</span><br><span class="line">        results.append((x1, x2))</span><br><span class="line">    print(<span class="string">'epoch %d, x1 %f, x2 %f'</span> % (i + <span class="number">1</span>, x1, x2))</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"><span class="comment"># 垂直于等高线梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_trace_2d</span><span class="params">(f, results)</span>:</span> </span><br><span class="line">    d2l.plt.plot(*zip(*results), <span class="string">'-o'</span>, color=<span class="string">'#ff7f0e'</span>)</span><br><span class="line">    x1, x2 = np.meshgrid(np.arange(<span class="number">-5.5</span>, <span class="number">1.0</span>, <span class="number">0.1</span>), np.arange(<span class="number">-3.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>))</span><br><span class="line">    d2l.plt.contour(x1, x2, f(x1, x2), colors=<span class="string">'#1f77b4'</span>)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'x2'</span>)</span><br></pre></td></tr></table></figure>
<p>e.g.</p>
<p>$$<br>f(x) = x_1^2 + 2x_2^2<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span>  <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="keyword">return</span> x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * <span class="number">2</span> * x1, x2 - eta * <span class="number">4</span> * x2)</span><br><span class="line"></span><br><span class="line">show_trace_2d(f_2d, train_2d(gd_2d))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220214102821.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="自适应方法"><a href="#自适应方法" class="headerlink" title="自适应方法"></a>自适应方法</h2><h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><blockquote>
<p><strong>优势 :</strong><br/><br/></p>
<blockquote>
<p>梯度下降“步幅”的确定比较困难<br/><br>而牛顿法相当于可以通过Hessian矩阵来调整“步幅”。</p>
</blockquote>
<p>在牛顿法中，局部极小值也可以通过调整学习率来解决。</p>
</blockquote>
<p>在 $x + \epsilon$ 处泰勒展开：</p>
<p>$$<br>f(\mathbf{x}+\epsilon)=f(\mathbf{x})+\epsilon^{\top} \nabla f(\mathbf{x})+\frac{1}{2} \epsilon^{\top} \nabla \nabla^{\top} f(\mathbf{x}) \epsilon+\mathcal{O}\left(|\epsilon|^{3}\right)<br>$$</p>
<p>最小值点处满足: $\nabla f(\mathbf{x})=0$, 即我们希望 $\nabla f(\mathbf{x} + \epsilon)=0$, 对上式关于 $\epsilon$ 求导，忽略高阶无穷小，有：</p>
<p>$$<br>\nabla f(\mathbf{x})+\boldsymbol{H}<em>{f} \boldsymbol{\epsilon}=0 \text { and hence } \epsilon=-\boldsymbol{H}</em>{f}^{-1} \nabla f(\mathbf{x})<br>$$</p>
<blockquote>
<p>牛顿法需要计算Hessian矩阵的逆，计算量比较大。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.cosh(c * x)  <span class="comment"># Objective</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> c * np.sinh(c * x)  <span class="comment"># Derivative</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hessf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> c**<span class="number">2</span> * np.cosh(c * x)  <span class="comment"># Hessian</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hide learning rate for now</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newton</span><span class="params">(eta=<span class="number">1</span>)</span>:</span></span><br><span class="line">    x = <span class="number">10</span></span><br><span class="line">    results = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        x -= eta * gradf(x) / hessf(x)</span><br><span class="line">        results.append(x)</span><br><span class="line">    print(<span class="string">'epoch 10, x:'</span>, x)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">show_trace(newton())</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220214418174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 牛顿法对于有局部极小值的情况</span></span><br><span class="line"><span class="comment"># 和梯度下降的方法有一样的效果</span></span><br><span class="line"><span class="comment"># 正确的方法还是降低学习率</span></span><br><span class="line">c = <span class="number">0.15</span> * np.pi</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * np.cos(c * x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.cos(c * x) - c * x * np.sin(c * x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hessf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> - <span class="number">2</span> * c * np.sin(c * x) - x * c**<span class="number">2</span> * np.cos(c * x)</span><br><span class="line"></span><br><span class="line">show_trace(newton())</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220214459356.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<blockquote>
<p>show_trace(newton(0.5))<br><img src="https://img-blog.csdnimg.cn/20200220214557927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
<h3 id="收敛性分析"><a href="#收敛性分析" class="headerlink" title="收敛性分析"></a>收敛性分析</h3><p>只考虑在函数为凸函数, 且最小值点上 $f’’(x^*) &gt;0$ 时的收敛速度：</p>
<p>令 $x_k$ 为第 $k$ 次迭代后 $x$ 的值， $e_{k}:=x_{k}-x^{*}$ 表示 $x_k$ 到最小值点 $x^{*}$ 的距离，由 $f’(x^{*}) = 0$:</p>
<p>$$<br>0=f^{\prime}\left(x_{k}-e_{k}\right)=f^{\prime}\left(x_{k}\right)-e_{k} f^{\prime \prime}\left(x_{k}\right)+\frac{1}{2} e_{k}^{2} f^{\prime \prime \prime}\left(\xi_{k}\right) \text{for some } \xi_{k} \in\left[x_{k}-e_{k}, x_{k}\right]<br>$$</p>
<p>两边除以 $f’’(x_k)$, 有：</p>
<p>$$<br>e_{k}-f^{\prime}\left(x_{k}\right) / f^{\prime \prime}\left(x_{k}\right)=\frac{1}{2} e_{k}^{2} f^{\prime \prime \prime}\left(\xi_{k}\right) / f^{\prime \prime}\left(x_{k}\right)<br>$$</p>
<p>代入更新方程 $x_{k+1} = x_{k} - f^{\prime}\left(x_{k}\right) / f^{\prime \prime}\left(x_{k}\right)$, 得到：</p>
<p>$$<br>x_k - x^{*} - f^{\prime}\left(x_{k}\right) / f^{\prime \prime}\left(x_{k}\right) =\frac{1}{2} e_{k}^{2} f^{\prime \prime \prime}\left(\xi_{k}\right) / f^{\prime \prime}\left(x_{k}\right)<br>$$</p>
<p>$$<br>x_{k+1} - x^{*} = e_{k+1} = \frac{1}{2} e_{k}^{2} f^{\prime \prime \prime}\left(\xi_{k}\right) / f^{\prime \prime}\left(x_{k}\right)<br>$$</p>
<p>当 $\frac{1}{2} f^{\prime \prime \prime}\left(\xi_{k}\right) / f^{\prime \prime}\left(x_{k}\right) \leq c$ 时，有:</p>
<p>$$<br>e_{k+1} \leq c e_{k}^{2}<br>$$</p>
<h3 id="预处理-（Heissan阵辅助梯度下降）"><a href="#预处理-（Heissan阵辅助梯度下降）" class="headerlink" title="预处理 （Heissan阵辅助梯度下降）"></a>预处理 （Heissan阵辅助梯度下降）</h3><p>$$<br>\mathbf{x} \leftarrow \mathbf{x}-\eta \operatorname{diag}\left(H_{f}\right)^{-1} \nabla \mathbf{x}<br>$$</p>
<h3 id="梯度下降与线性搜索（共轭梯度法）"><a href="#梯度下降与线性搜索（共轭梯度法）" class="headerlink" title="梯度下降与线性搜索（共轭梯度法）"></a>梯度下降与线性搜索（共轭梯度法）</h3><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><h3 id="随机梯度下降参数更新"><a href="#随机梯度下降参数更新" class="headerlink" title="随机梯度下降参数更新"></a>随机梯度下降参数更新</h3><p>对于有 $n$ 个样本对训练数据集，设 $f_i(x)$ 是第 $i$ 个样本的损失函数, 则目标函数为:</p>
<p>$$<br>f(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} f_{i}(\mathbf{x})<br>$$</p>
<p>其梯度为:</p>
<p>$$<br>\nabla f(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} \nabla f_{i}(\mathbf{x})<br>$$</p>
<p>每一个样本的梯度是对整体的梯度的无偏估计</p>
<p>使用该梯度的一次更新的时间复杂度为 $\mathcal{O}(n)$</p>
<p>随机梯度下降更新公式 $\mathcal{O}(1)$:</p>
<p>$$<br>\mathbf{x} \leftarrow \mathbf{x}-\eta \nabla f_{i}(\mathbf{x})<br>$$</p>
<p>且有：</p>
<p>$$<br>\mathbb{E}<em>{i} \nabla f</em>{i}(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} \nabla f_{i}(\mathbf{x})=\nabla f(\mathbf{x})<br>$$<br>e.g. </p>
<p>$$<br>f(x_1, x_2) = x_1^2 + 2 x_2^2<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span>  <span class="comment"># Objective</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">2</span> * x1, <span class="number">4</span> * x2)  <span class="comment"># Gradient</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(x1, x2)</span>:</span>  <span class="comment"># Simulate noisy gradient</span></span><br><span class="line">    <span class="keyword">global</span> lr  <span class="comment"># Learning rate scheduler</span></span><br><span class="line">    (g1, g2) = gradf(x1, x2)  <span class="comment"># Compute gradient</span></span><br><span class="line">    (g1, g2) = (g1 + np.random.normal(<span class="number">0.1</span>), g2 + np.random.normal(<span class="number">0.1</span>))</span><br><span class="line">    eta_t = eta * lr()  <span class="comment"># Learning rate at time t</span></span><br><span class="line">    <span class="keyword">return</span> (x1 - eta_t * g1, x2 - eta_t * g2)  <span class="comment"># Update variables</span></span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.1</span></span><br><span class="line">lr = (<span class="keyword">lambda</span>: <span class="number">1</span>)  <span class="comment"># Constant learning rate</span></span><br><span class="line">show_trace_2d(f, train_2d(sgd, steps=<span class="number">50</span>))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/2020022021481780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="动态学习率"><a href="#动态学习率" class="headerlink" title="动态学习率"></a>动态学习率</h3><ul>
<li>在最开始学习率设计比较大，加速收敛</li>
<li>学习率可以设计为指数衰减或多项式衰减</li>
<li>在优化进行一段时间后可以适当减小学习率来避免振荡</li>
</ul>
<p>$$<br>\begin{array}{ll}{\eta(t)=\eta_{i} \text { if } t_{i} \leq t \leq t_{i+1}} &amp; {\text { piecewise constant }} \\ {\eta(t)=\eta_{0} \cdot e^{-\lambda t}} &amp; {\text { exponential }} \\ {\eta(t)=\eta_{0} \cdot(\beta t+1)^{-\alpha}} &amp; {\text { polynomial }}\end{array}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exponential</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ctr</span><br><span class="line">    ctr += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> math.exp(<span class="number">-0.1</span> * ctr)</span><br><span class="line"></span><br><span class="line">ctr = <span class="number">1</span></span><br><span class="line">lr = exponential  <span class="comment"># Set up learning rate</span></span><br><span class="line">show_trace_2d(f, train_2d(sgd, steps=<span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200220215551658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polynomial</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 迭代次数</span></span><br><span class="line">    <span class="keyword">global</span> ctr</span><br><span class="line">    ctr += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> + <span class="number">0.1</span> * ctr)**(<span class="number">-0.5</span>)</span><br><span class="line"></span><br><span class="line">ctr = <span class="number">1</span></span><br><span class="line">lr = polynomial  <span class="comment"># Set up learning rate</span></span><br><span class="line">show_trace_2d(f, train_2d(sgd, steps=<span class="number">50</span>))</span><br></pre></td></tr></table></figure>

<center><img src="https://img-blog.csdnimg.cn/20200220215558471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h2 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h2><h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><p><a href="https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise" target="_blank" rel="noopener">读取数据</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_ch7</span><span class="params">()</span>:</span></span><br><span class="line">    data = np.genfromtxt(<span class="string">'/home/kesci/input/airfoil4755/airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    data = (data - data.mean(axis=<span class="number">0</span>)) / data.std(axis=<span class="number">0</span>) <span class="comment"># 标准化</span></span><br><span class="line">    <span class="keyword">return</span> torch.tensor(data[:<span class="number">1500</span>, :<span class="number">-1</span>], dtype=torch.float32), \</span><br><span class="line">           torch.tensor(data[:<span class="number">1500</span>, <span class="number">-1</span>], dtype=torch.float32) <span class="comment"># 前1500个样本(每个样本5个特征)</span></span><br><span class="line"></span><br><span class="line">features, labels = get_data_ch7()</span><br><span class="line">features.shape</span><br></pre></td></tr></table></figure>
<p><strong>数据可视化</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">'path to airfoil_self_noise.dat'</span>, delimiter=<span class="string">'\t'</span>, header=<span class="literal">None</span>)</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Stochastic Gradient Descent (SGD)函数</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        p.data -= hyperparams[<span class="string">'lr'</span>] * p.grad.data</span><br></pre></td></tr></table></figure>
<p><strong>训练</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch7</span><span class="params">(optimizer_fn, states, hyperparams, features, labels,</span></span></span><br><span class="line"><span class="function"><span class="params">              batch_size=<span class="number">10</span>, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    net, loss = d2l.linreg, d2l.squared_loss</span><br><span class="line">    </span><br><span class="line">    w = torch.nn.Parameter(torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(features.shape[<span class="number">1</span>], <span class="number">1</span>)), dtype=torch.float32),</span><br><span class="line">                           requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.nn.Parameter(torch.zeros(<span class="number">1</span>, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_loss</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> loss(net(features, w, b), labels).mean().item()</span><br><span class="line"></span><br><span class="line">    ls = [eval_loss()]</span><br><span class="line">    data_iter = torch.utils.data.DataLoader(</span><br><span class="line">        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">for</span> batch_i, (X, y) <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">            l = loss(net(X, w, b), y).mean()  <span class="comment"># 使用平均损失</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> w.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                w.grad.data.zero_()</span><br><span class="line">                b.grad.data.zero_()</span><br><span class="line">                </span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer_fn([w, b], states, hyperparams)  <span class="comment"># 迭代模型参数</span></span><br><span class="line">            <span class="keyword">if</span> (batch_i + <span class="number">1</span>) * batch_size % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                ls.append(eval_loss())  <span class="comment"># 每100个样本记录下当前训练误差</span></span><br><span class="line">    <span class="comment"># 打印结果和作图</span></span><br><span class="line">    print(<span class="string">'loss: %f, %f sec per epoch'</span> % (ls[<span class="number">-1</span>], time.time() - start))</span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    d2l.plt.plot(np.linspace(<span class="number">0</span>, num_epochs, len(ls)), ls)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'loss'</span>)</span><br></pre></td></tr></table></figure>
<p><strong>测试</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_sgd</span><span class="params">(lr, batch_size, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    train_ch7(sgd, <span class="literal">None</span>, &#123;<span class="string">'lr'</span>: lr&#125;, features, labels, batch_size, num_epochs)</span><br></pre></td></tr></table></figure>
<p><strong>Result</strong></p>
<blockquote>
<ul>
<li>train_sgd(1, 1500, 6)<br><img src="https://img-blog.csdnimg.cn/20200220220207217.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li>train_sgd(0.005, 1)<br><img src="https://img-blog.csdnimg.cn/20200220220222233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li>train_sgd(0.05, 10)<br><img src="https://img-blog.csdnimg.cn/20200220220233911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
</blockquote>
<p><strong>简化模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_pytorch_ch7</span><span class="params">(optimizer_fn, optimizer_hyperparams, features, labels,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_size=<span class="number">10</span>, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Linear(features.shape[<span class="number">-1</span>], <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    loss = nn.MSELoss()</span><br><span class="line">    optimizer = optimizer_fn(net.parameters(), **optimizer_hyperparams)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_loss</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> loss(net(features).view(<span class="number">-1</span>), labels).item() / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    ls = [eval_loss()]</span><br><span class="line">    data_iter = torch.utils.data.DataLoader(</span><br><span class="line">        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">for</span> batch_i, (X, y) <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">            <span class="comment"># 除以2是为了和train_ch7保持一致, 因为squared_loss中除了2</span></span><br><span class="line">            l = loss(net(X).view(<span class="number">-1</span>), y) / <span class="number">2</span> </span><br><span class="line">            </span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">if</span> (batch_i + <span class="number">1</span>) * batch_size % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                ls.append(eval_loss())</span><br><span class="line">    <span class="comment"># 打印结果和作图</span></span><br><span class="line">    print(<span class="string">'loss: %f, %f sec per epoch'</span> % (ls[<span class="number">-1</span>], time.time() - start))</span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    d2l.plt.plot(np.linspace(<span class="number">0</span>, num_epochs, len(ls)), ls)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'loss'</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>train_pytorch_ch7(optim.SGD, {“lr”: 0.05}, features, labels, 10)<br><img src="https://img-blog.csdnimg.cn/20200220220400842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>优化</tag>
        <tag>凸优化</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title>批量归一化 &amp;&amp; 残差网络</title>
    <url>/2020/02/28/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%20&amp;&amp;%20%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>基于此前对于CNN的介绍</p>
<ul>
<li><a href="https://blog.csdn.net/RokoBasilisk/article/details/104380762" target="_blank" rel="noopener">Fundamentals of Convolutional Neural Networks</a></li>
<li><a href="https://blog.csdn.net/RokoBasilisk/article/details/104381637" target="_blank" rel="noopener">LeNet &amp;&amp; ModernCNN</a></li>
</ul>
<p>就深层次 CNN 的结构进一步探讨归一化和残差网络。</p>
<h1 id="批量归一化（BatchNormalization）"><a href="#批量归一化（BatchNormalization）" class="headerlink" title="批量归一化（BatchNormalization）"></a>批量归一化（BatchNormalization）</h1><blockquote>
<p>让网络训练归一化变得更加容易，本质是一种对数据的标准化处理</p>
</blockquote>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><ul>
<li><strong>对输入的标准化（浅层模型）</strong></li>
</ul>
<p>处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。  标准化处理输入数据使各个特征的分布相近</p>
<ul>
<li><strong>批量归一化（深度模型）随着模型参数的迭代更新，靠近输出层的数据剧烈变化</strong></li>
</ul>
<p>利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li><strong>对全连接层做批量归一化</strong><blockquote>
<p>位置：全连接层中的仿射变换和激活函数之间。  </p>
</blockquote>
</li>
</ul>
<p><strong>全连接：</strong><br>$$<br>\boldsymbol{x} = \boldsymbol{W\boldsymbol{u} + \boldsymbol{b}} \\<br> output =\phi(\boldsymbol{x})<br> $$   </p>
<p>输入是u，经过仿射变化得到x，经过激活函数得到output，size=(batch_size，输出神经元的个数)</p>
<p><strong>批量归一化：</strong><br>$$<br>output=\phi(\text{BN}(\boldsymbol{x}))$$</p>
<p>$$<br>\boldsymbol{y}^{(i)} = \text{BN}(\boldsymbol{x}^{(i)})<br>$$</p>
<p>$$<br>\boldsymbol{\mu}<em>\mathcal{B} \leftarrow \frac{1}{m}\sum</em>{i = 1}^{m} \boldsymbol{x}^{(i)},<br>$$ </p>
<p>$$<br>\boldsymbol{\sigma}_\mathcal{B}^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m}(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B})^2,<br>$$</p>
<p>$$<br>\hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B}}{\sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}},<br>$$</p>
<p>$$<br>标准化处理<br>$$</p>
<p>这⾥ϵ &gt; 0是个很小的常数，保证分母大于0</p>
<p>$$<br>{\boldsymbol{y}}^{(i)} \leftarrow \boldsymbol{\gamma} \odot<br>\hat{\boldsymbol{x}}^{(i)} + \boldsymbol{\beta}.<br>$$</p>
<p>引入可学习参数：拉伸参数γ和偏移参数β。若$\boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}$和$\boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}$，批量归一化无效。</p>
<ul>
<li><strong>对卷积层做批量归⼀化</strong><blockquote>
<p>位置：卷积计算之后、应⽤激活函数之前</p>
</blockquote>
</li>
</ul>
<p>如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数。</p>
<p>计算：对单通道，$batchsize = m,卷积计算输出 = p \times q$</p>
<p>对该通道中 $m\times p\times q$ 个元素同时做批量归一化,使用相同的均值和方差。</p>
<ul>
<li><strong>预测时的批量归⼀化</strong></li>
</ul>
<p>训练：以 batch 为单位, 对每个 batch 计算均值和方差。  </p>
<p>预测：用移动平均估算整个训练数据集的样本均值和方差。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="batch-norm-function"><a href="#batch-norm-function" class="headerlink" title="batch_norm function"></a>batch_norm function</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 放到了BatchNorm类中，使用时直接调用forward函数，此函数将成为cell函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_norm</span><span class="params">(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum)</span>:</span></span><br><span class="line">    <span class="comment"># 判断当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">        <span class="comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> len(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> len(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>) <span class="comment">#是d维的值</span></span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持</span></span><br><span class="line">            <span class="comment"># X的形状以便后面可以做广播运算</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">3</span>, keepdim=<span class="literal">True</span>) <span class="comment"># c维的值，通道有几个mean就有几个</span></span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">3</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下用当前的均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        <span class="comment"># momentum 是一个超参数</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta  <span class="comment"># 拉伸和偏移</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean, moving_var</span><br></pre></td></tr></table></figure>
<h3 id="batch-norm-class"><a href="#batch-norm-class" class="headerlink" title="batch_norm class"></a>batch_norm class</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在Batch Norm函数的基础上定义此类，作用是维护学习参数和超参数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features, num_dims)</span>:</span> </span><br><span class="line">        super(BatchNorm, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features) <span class="comment">#全连接层输出神经元 </span></span><br><span class="line">            <span class="comment"># num_features代表输出神经元的个数</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)  <span class="comment">#通道数</span></span><br><span class="line">        <span class="comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment"># 不参与求梯度和迭代的变量，全在内存上初始化成0</span></span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上</span></span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        <span class="comment"># 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false</span></span><br><span class="line">        Y, self.moving_mean, self.moving_var = batch_norm(self.training, </span><br><span class="line">            X, self.gamma, self.beta, self.moving_mean,</span><br><span class="line">            self.moving_var, eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<h3 id="基于LeNet的应用"><a href="#基于LeNet的应用" class="headerlink" title="基于LeNet的应用"></a>基于LeNet的应用</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            <span class="comment"># 直接作为一个参数加到LeNet中就行</span></span><br><span class="line">            BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 体现了仿射函数之后激活函数之前的结构</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            d2l.FlattenLayer(),</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>
<h3 id="load-data"><a href="#load-data" class="headerlink" title="load data"></a>load data</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#batch_size = 256  </span></span><br><span class="line"><span class="comment">##cpu要调小batchsize</span></span><br><span class="line">batch_size=<span class="number">16</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None, root=<span class="string">'/home/kesci/input/FashionMNIST2065'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Download the fashion mnist dataset and then load into memory."""</span></span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.append(torchvision.transforms.Resize(size=resize))</span><br><span class="line">    trans.append(torchvision.transforms.ToTensor())</span><br><span class="line">    </span><br><span class="line">    transform = torchvision.transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<h3 id="train-and-test"><a href="#train-and-test" class="headerlink" title="train and test"></a>train and test</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<h2 id="简化模型"><a href="#简化模型" class="headerlink" title="简化模型"></a>简化模型</h2><blockquote>
<p>nn中有内置的BatchNorm2d（卷积层）和BatchNorm1d（全连接层）</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">在自己应用时不需要写class和function</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">6</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            d2l.FlattenLayer(),</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="残差网络（ResNet）"><a href="#残差网络（ResNet）" class="headerlink" title="残差网络（ResNet）"></a>残差网络（ResNet）</h1><center><b>深层网络能够拟合出的映射就一定能够包含浅层网络拟合出的映射</b></center><br/>

<center><b>但 CNN 模型在建立的时候并不是越深越好</b></center><br/>

<blockquote>
<p>深度学习的问题</p>
</blockquote>
<p>深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，准确率也变得更差。</p>
<h2 id="残差块（Residual-Block）"><a href="#残差块（Residual-Block）" class="headerlink" title="残差块（Residual Block）"></a>残差块（Residual Block）</h2><p><strong>恒等映射：</strong>  </p>
<ul>
<li>左边：$f(x)=x$                                               </li>
<li>右边：$f(x)-x=0$ （易于捕捉恒等映射的细微波动 ; 易于优化）</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw4bGhub3Q0LnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>神经网络普通层(left)残差网络(right)<br>$$</p>
<blockquote>
<p>在残差块中，输⼊可通过跨层的数据线路更快 地向前传播。</p>
</blockquote>
<h3 id="残差块实现"><a href="#残差块实现" class="headerlink" title="残差块实现"></a>残差块实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span><span class="params">(nn.Module)</span>:</span>  </span><br><span class="line">    <span class="comment">#可以设定输出通道数、是否使用额外的1x1卷积层来修改通道数以及卷积层的步幅。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, use_1x1conv=False, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(Residual, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=stride)</span><br><span class="line">        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=stride)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y + X)</span><br></pre></td></tr></table></figure>
<h3 id="是否需要-1-times1-卷积层"><a href="#是否需要-1-times1-卷积层" class="headerlink" title="是否需要 $1\times1$ 卷积层"></a>是否需要 $1\times1$ 卷积层</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不需要使用1*1卷积层 输入和输出相同</span></span><br><span class="line">blk = Residual(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">X = torch.rand((<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line">blk(X).shape <span class="comment"># torch.Size([4, 3, 6, 6])</span></span><br><span class="line"><span class="comment">#需要使用</span></span><br><span class="line">blk = Residual(<span class="number">3</span>, <span class="number">6</span>, use_1x1conv=<span class="literal">True</span>, stride=<span class="number">2</span>)</span><br><span class="line">blk(X).shape <span class="comment"># torch.Size([4, 6, 3, 3])</span></span><br></pre></td></tr></table></figure>
<h2 id="ResNet模型"><a href="#ResNet模型" class="headerlink" title="ResNet模型"></a>ResNet模型</h2><h3 id="简化实现"><a href="#简化实现" class="headerlink" title="简化实现"></a>简化实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_block</span><span class="params">(in_channels, out_channels, num_residuals, first_block=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> first_block:</span><br><span class="line">        <span class="keyword">assert</span> in_channels == out_channels <span class="comment"># 第一个模块的通道数同输入通道数一致</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            <span class="comment"># 把in_channels放缩到out_channels的个数</span></span><br><span class="line">            blk.append(Residual(in_channels, out_channels, use_1x1conv=<span class="literal">True</span>, stride=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 保证输入和输出都是out_channels</span></span><br><span class="line">            blk.append(Residual(out_channels, out_channels))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把四个残差block放到net里</span></span><br><span class="line">net.add_module(<span class="string">"resnet_block1"</span>, resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">net.add_module(<span class="string">"resnet_block2"</span>, resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">"resnet_block3"</span>, resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">"resnet_block4"</span>, resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<h3 id="全局平均池化"><a href="#全局平均池化" class="headerlink" title="全局平均池化"></a>全局平均池化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.add_module(<span class="string">"global_avg_pool"</span>, d2l.GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, 512, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="train-and-test-1"><a href="#train-and-test-1" class="headerlink" title="train and test"></a>train and test</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>ResNet 的引申设计</p>
</blockquote>
<h1 id="稠密连接网络（DenseNet）"><a href="#稠密连接网络（DenseNet）" class="headerlink" title="稠密连接网络（DenseNet）"></a>稠密连接网络（DenseNet）</h1><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw4bWk3OHl6LnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<blockquote>
<p>特征：concat 连接</p>
</blockquote>
<h2 id="主要构建模块："><a href="#主要构建模块：" class="headerlink" title="主要构建模块："></a>主要构建模块：</h2><ul>
<li>稠密块（dense block）： 定义了输入和输出是如何连结的。  </li>
<li>过渡层（transition layer）：用来控制通道数，使之不过大。<h2 id="稠密块"><a href="#稠密块" class="headerlink" title="稠密块"></a>稠密块</h2></li>
<li>输出通道数=输入通道数+卷积层个数*卷积输出通道数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(in_channels, out_channels)</span>:</span></span><br><span class="line">    blk = nn.Sequential(nn.BatchNorm2d(in_channels), </span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># num_convs:用了几个卷积层</span></span><br><span class="line">    <span class="comment"># in_channels代表全部输入，但是out_channels不代表全部输出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_convs, in_channels, out_channels)</span>:</span></span><br><span class="line">        super(DenseBlock, self).__init__()</span><br><span class="line">        net = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_convs):</span><br><span class="line">            <span class="comment"># 卷积层输入的通道数</span></span><br><span class="line">            in_c = in_channels + i * out_channels</span><br><span class="line">            net.append(conv_block(in_c, out_channels))</span><br><span class="line">        self.net = nn.ModuleList(net)</span><br><span class="line">        self.out_channels = in_channels + num_convs * out_channels <span class="comment"># 计算输出通道数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            <span class="comment"># concat连接</span></span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)  <span class="comment"># 在通道维上将输入和输出连结</span></span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blk = DenseBlock(<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">Y = blk(X)</span><br><span class="line">Y.shape <span class="comment"># torch.Size([4, 23, 8, 8]) 3+2*10</span></span><br></pre></td></tr></table></figure>
<h2 id="过渡层"><a href="#过渡层" class="headerlink" title="过渡层"></a>过渡层</h2><ul>
<li>$1\times1$卷积层：来减小通道数  </li>
<li>步幅为2的平均池化层：减半高和宽</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span><span class="params">(in_channels, out_channels)</span>:</span></span><br><span class="line">    blk = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(in_channels), </span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line">blk = transition_block(<span class="number">23</span>, <span class="number">10</span>)</span><br><span class="line">blk(Y).shape <span class="comment"># torch.Size([4, 10, 4, 4])</span></span><br></pre></td></tr></table></figure>
<h3 id="DenseNet模型"><a href="#DenseNet模型" class="headerlink" title="DenseNet模型"></a>DenseNet模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 宽高减半</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    <span class="comment"># 宽高减半</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span>  <span class="comment"># num_channels为当前的通道数</span></span><br><span class="line">num_convs_in_dense_blocks = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, num_convs <span class="keyword">in</span> enumerate(num_convs_in_dense_blocks):</span><br><span class="line">    DB = DenseBlock(num_convs, num_channels, growth_rate)</span><br><span class="line">    net.add_module(<span class="string">"DenseBlosk_%d"</span> % i, DB)</span><br><span class="line">    <span class="comment"># 上一个稠密块的输出通道数</span></span><br><span class="line">    num_channels = DB.out_channels</span><br><span class="line">    <span class="comment"># 在稠密块之间加入通道数减半的过渡层</span></span><br><span class="line">    <span class="keyword">if</span> i != len(num_convs_in_dense_blocks) - <span class="number">1</span>:</span><br><span class="line">        net.add_module(<span class="string">"transition_block_%d"</span> % i, transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">        num_channels = num_channels // <span class="number">2</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.add_module(<span class="string">"BN"</span>, nn.BatchNorm2d(num_channels))</span><br><span class="line">net.add_module(<span class="string">"relu"</span>, nn.ReLU())</span><br><span class="line">net.add_module(<span class="string">"global_avg_pool"</span>, d2l.GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, num_channels, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(), nn.Linear(num_channels, <span class="number">10</span>))) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">X = torch.rand((<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"><span class="keyword">for</span> name, layer <span class="keyword">in</span> net.named_children():</span><br><span class="line">    X = layer(X)</span><br><span class="line">    print(name, <span class="string">' output shape:\t'</span>, X.shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#batch_size = 256</span></span><br><span class="line">batch_size=<span class="number">16</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line">train_iter, test_iter =load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>批量归一化</tag>
        <tag>残差网络</tag>
      </tags>
  </entry>
  <entry>
    <title>LeNet &amp;&amp; ModernCNN</title>
    <url>/2020/02/28/LeNet%20&amp;&amp;%20ModernCNN/</url>
    <content><![CDATA[<h1 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h1><blockquote>
<p>学而习之：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104380762" target="_blank" rel="noopener">Fundamentals of Convolutional Neural Networks</a></p>
</blockquote>
<p>使用全连接层的局限性：</p>
<ul>
<li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li>
<li>对于大尺寸的输入图像，使用全连接层容易导致模型过大。</li>
</ul>
<p>使用卷积层的优势：</p>
<ul>
<li>卷积层保留输入形状。</li>
<li>卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</li>
</ul>
<p><strong>卷积神经网络就是含卷积层的网络。</strong></p>
<h2 id="LeNet-模型"><a href="#LeNet-模型" class="headerlink" title="LeNet 模型"></a>LeNet 模型</h2><blockquote>
<p>90%以上的参数都在全连接层块</p>
</blockquote>
<p>LeNet分为卷积层块和全连接层块两个部分<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5kd3Ntc2FvLnBuZw?x-oss-process=image/format,png" alt="Image Name"><br><strong>解释：</strong><br>&ensp;&ensp;&ensp;&ensp;卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5 \times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。</p>
<p>&ensp;&ensp;&ensp;&ensp;全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p>
<blockquote>
<p>卷积层块里的基本单位</p>
<blockquote>
<p>是卷积层后接平均池化层</p>
</blockquote>
<p>卷积层用来识别图像里的空间模式，如线条和物体局部<br/><br>之后的平均池化层则用来降低卷积层对位置的敏感性。</p>
</blockquote>
<p><strong>LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类</strong></p>
<h3 id="通过-Sequential-类实现-LeNet-模型"><a href="#通过-Sequential-类实现-LeNet-模型" class="headerlink" title="通过 Sequential 类实现 LeNet 模型"></a>通过 Sequential 类实现 LeNet 模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#import</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to FashionMNIST2065"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#net</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Flatten</span><span class="params">(torch.nn.Module)</span>:</span>  <span class="comment">#展平操作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Reshape</span><span class="params">(torch.nn.Module)</span>:</span> <span class="comment">#将图像大小重定型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x.view(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)      <span class="comment">#(B x C x H x W)</span></span><br><span class="line"></span><br><span class="line">net = torch.nn.Sequential(     <span class="comment">#Lelet                                                  </span></span><br><span class="line">    Reshape(),  </span><br><span class="line">    <span class="comment"># 公式：[(nh-kh+ph+sh)/sh]*[(nw-kw+pw+sw)/sw]</span></span><br><span class="line">    nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), <span class="comment">#b*1*28*28  =&gt;b*6*28*28</span></span><br><span class="line">    nn.Sigmoid(),     </span><br><span class="line">    <span class="comment"># 平均池化</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),                     <span class="comment">#b*6*28*28  =&gt;b*6*14*14</span></span><br><span class="line">    nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>),  <span class="comment">#b*6*14*14  =&gt;b*16*10*10</span></span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),                     <span class="comment">#b*16*10*10  =&gt; b*16*5*5</span></span><br><span class="line">    <span class="comment"># 展平</span></span><br><span class="line">    Flatten(),                                                 <span class="comment">#b*16*5*5   =&gt; b*400</span></span><br><span class="line">    <span class="comment"># 三个全连接层</span></span><br><span class="line">    nn.Linear(in_features=<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, out_features=<span class="number">120</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(</span><br><span class="line">    batch_size=batch_size, root=<span class="string">'path to FashionMNIST2065'</span>)</span><br><span class="line"><span class="comment"># 训练集批次数</span></span><br><span class="line">print(len(train_iter))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">额外的数据表示，以图像形式显示</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment">#数据展示</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># define drawing function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_fashion_mnist</span><span class="params">(images, labels)</span>:</span></span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    <span class="comment"># 这里的_表示我们忽略（不使用）的变量</span></span><br><span class="line">    _, figs = plt.subplots(<span class="number">1</span>, len(images), figsize=(<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> f, img, lbl <span class="keyword">in</span> zip(figs, images, labels):</span><br><span class="line">        f.imshow(img.view((<span class="number">28</span>, <span class="number">28</span>)).numpy())</span><br><span class="line">        f.set_title(lbl)</span><br><span class="line">        f.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        f.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> Xdata,ylabel <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">X, y = [], []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(Xdata[i].shape,ylabel[i].numpy())</span><br><span class="line">    X.append(Xdata[i]) <span class="comment"># 将第i个feature加到X中</span></span><br><span class="line">    y.append(ylabel[i].numpy()) <span class="comment"># 将第i个label加到y中</span></span><br><span class="line">show_fashion_mnist(X, y)</span><br></pre></td></tr></table></figure>
<hr>
<p>因为卷积神经网络计算比多层感知机要复杂，建议使用GPU来加速计算。我们查看看是否可以用GPU，如果成功则使用 cuda:0，否则仍然使用 cpu。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># This function has been saved in the d2l package for future use</span></span><br><span class="line"><span class="comment">#use GPU</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""If GPU is available, return torch.device as cuda:0; else return torch.device as cpu."""</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        device = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        device = torch.device(<span class="string">'cpu'</span>)</span><br><span class="line">    <span class="keyword">return</span> device</span><br><span class="line"></span><br><span class="line">device = try_gpu()</span><br><span class="line">device</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h4 id="计算准确率"><a href="#计算准确率" class="headerlink" title="计算准确率"></a>计算准确率</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">(1). net.train()</span></span><br><span class="line"><span class="string">  启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为True</span></span><br><span class="line"><span class="string">(2). net.eval()</span></span><br><span class="line"><span class="string">不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">:param:data_iter:测试集</span></span><br><span class="line"><span class="string">:param:acc_sum:模型预测正确的总数</span></span><br><span class="line"><span class="string">:param:n:预测总数</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net,device=torch.device<span class="params">(<span class="string">'cpu'</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Evaluate accuracy of a model on the given data set."""</span></span><br><span class="line">    acc_sum,n = torch.tensor([<span class="number">0</span>],dtype=torch.float32,device=device),<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="comment"># If device is the GPU, copy the data to the GPU.</span></span><br><span class="line">        <span class="comment"># 把tensorc传到device中</span></span><br><span class="line">        X,y = X.to(device),y.to(device)</span><br><span class="line">        <span class="comment"># 网络正在进行预测</span></span><br><span class="line">        net.eval()</span><br><span class="line">        该区域涉及到的数据不需要计算梯度，也不进行反向传播</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            y = y.long()</span><br><span class="line">            <span class="comment"># 将一个批次的训练数据X通过网络模型net输出</span></span><br><span class="line">            <span class="comment"># 经过argmax得到预测值</span></span><br><span class="line">            <span class="comment"># dim维度选择为1</span></span><br><span class="line">            acc_sum += torch.sum((torch.argmax(net(X), dim=<span class="number">1</span>) == y))  <span class="comment">#[[0.2 ,0.4 ,0.5 ,0.6 ,0.8] ,[ 0.1,0.2 ,0.4 ,0.3 ,0.1]] =&gt; [ 4 , 2 ]</span></span><br><span class="line">            <span class="comment"># 预测的总数相加</span></span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 预测准确率</span></span><br><span class="line">    <span class="keyword">return</span> acc_sum.item()/n</span><br></pre></td></tr></table></figure>
<h4 id="训练函数"><a href="#训练函数" class="headerlink" title="训练函数"></a>训练函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch5</span><span class="params">(net, train_iter, test_iter,criterion, num_epochs, batch_size, device,lr=None)</span>:</span></span><br><span class="line">    <span class="string">"""Train and evaluate a model with CPU or GPU."""</span></span><br><span class="line">    print(<span class="string">'training on'</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum = torch.tensor([<span class="number">0.0</span>],dtype=torch.float32,device=device)</span><br><span class="line">        train_acc_sum = torch.tensor([<span class="number">0.0</span>],dtype=torch.float32,device=device)</span><br><span class="line">        n, start = <span class="number">0</span>, time.time()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            net.train()</span><br><span class="line">            <span class="comment"># 梯度清零 不同批次的梯度不相关</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X,y = X.to(device),y.to(device) </span><br><span class="line">            <span class="comment"># 预测值</span></span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            loss = criterion(y_hat, y)</span><br><span class="line">            <span class="comment"># 梯度回传</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                y = y.long()</span><br><span class="line">                train_l_sum += loss.float()</span><br><span class="line">                <span class="comment"># 训练集中预测正确的总数</span></span><br><span class="line">                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=<span class="number">1</span>) == y))).float()</span><br><span class="line">                n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net,device)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '</span></span><br><span class="line">              <span class="string">'time %.1f sec'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum/n, train_acc_sum/n, test_acc,</span><br><span class="line">                 time.time() - start))</span><br></pre></td></tr></table></figure>
<h4 id="训练进程"><a href="#训练进程" class="headerlink" title="训练进程"></a>训练进程</h4><p>模型参数初始化到 device 中，并使用 Xavier 随机初始化。损失函数和训练算法则依然使用交叉熵损失函数和小批量随机梯度下降。</p>
<blockquote>
<p>Xavier 随机初始化 —参考<a href="https://blog.csdn.net/RokoBasilisk/article/details/104349123" target="_blank" rel="noopener">学而后思,方能发展;思而立行,终将卓越</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练 学习率0.9</span></span><br><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear <span class="keyword">or</span> type(m) == nn.Conv2d:</span><br><span class="line">        torch.nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line">net = net.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()   <span class="comment">#交叉熵描述了两个概率分布之间的距离，交叉熵越小说明两者之间越接近</span></span><br><span class="line">train_ch5(net, train_iter, test_iter, criterion,num_epochs, batch_size,device, lr)</span><br></pre></td></tr></table></figure>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line"><span class="keyword">for</span> testdata,testlabe <span class="keyword">in</span> test_iter:</span><br><span class="line">    testdata,testlabe = testdata.to(device),testlabe.to(device)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">print(testdata.shape,testlabe.shape)</span><br><span class="line">net.eval()</span><br><span class="line">y_pre = net(testdata)</span><br><span class="line"><span class="comment"># 预测值</span></span><br><span class="line">print(torch.argmax(y_pre,dim=<span class="number">1</span>)[:<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 真实标签</span></span><br><span class="line">print(testlabe[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<h2 id="深度卷积神经网络（AlexNet）"><a href="#深度卷积神经网络（AlexNet）" class="headerlink" title="深度卷积神经网络（AlexNet）"></a>深度卷积神经网络（AlexNet）</h2><blockquote>
<p>2014年ImgNet竞赛中</p>
<blockquote>
<p>证明了学习到的特征可以超过手工设计的特征, 打破计算机视觉研究的前状</p>
</blockquote>
</blockquote>
<center><b><font size=5>知缺陷方能进步</font></b></center><br/>

<blockquote>
<p>LeNet:  在大的真实数据集上的表现并不尽如⼈意。     </p>
<blockquote>
<p>1.神经网络计算复杂。  <br/><br>2.还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。  </p>
</blockquote>
</blockquote>
<p>在此之后，针对特征的选择分为两派：</p>
<ul>
<li>机器学习的特征提取:手工定义的特征提取函数  </li>
<li>神经网络的特征提取：通过学习得到数据的多级表征，并逐级表⽰越来越抽象的概念或模式。  </li>
</ul>
<p>神经网络发展的限制:数据、硬件</p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><blockquote>
<p>设计理念核Lenet相似</p>
</blockquote>
<p><strong>特征：</strong></p>
<ol>
<li>8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。</li>
<li>将sigmoid激活函数改成了更加简单的ReLU激活函数。</li>
<li>用Dropout来控制全连接层的模型复杂度。</li>
<li>引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解泛化能力不好导致的过拟合。</li>
</ol>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWt2NGdweDg4LnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>数据集.MINIST(Left) IMAGENET(Right)<br>$$</p>
<blockquote>
<p>利用padding 的作用：使得输入和输出的形状相同<br/><br>可以参考 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104380762" target="_blank" rel="noopener">Fundamentals of Convolutional Neural Networks</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to FashionMNIST2065"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="实现-AlexNet-模型"><a href="#实现-AlexNet-模型" class="headerlink" title="实现 AlexNet 模型"></a>实现 AlexNet 模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            <span class="comment"># 默认不做padding</span></span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment">#由于使用CPU镜像，精简网络，若为GPU镜像可添加该层</span></span><br><span class="line">            <span class="comment">#nn.Linear(4096, 4096),</span></span><br><span class="line">            <span class="comment">#nn.ReLU(),</span></span><br><span class="line">            <span class="comment">#nn.Dropout(0.5),</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, img)</span>:</span></span><br><span class="line"></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], <span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h4 id="载入数据"><a href="#载入数据" class="headerlink" title="载入数据"></a>载入数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None, root=<span class="string">'/home/kesci/input/FashionMNIST2065'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Download the fashion mnist dataset and then load into memory."""</span></span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.append(torchvision.transforms.Resize(size=resize))</span><br><span class="line">    trans.append(torchvision.transforms.ToTensor())</span><br><span class="line">    </span><br><span class="line">    transform = torchvision.transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"><span class="comment">#batchsize=128</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size,<span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">    print(<span class="string">'X ='</span>, X.shape,</span><br><span class="line">        <span class="string">'\nY ='</span>, Y.type(torch.int32))</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">3</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<h2 id="使用重复元素的网络（VGG）"><a href="#使用重复元素的网络（VGG）" class="headerlink" title="使用重复元素的网络（VGG）"></a>使用重复元素的网络（VGG）</h2><center><b><font size=5>知缺陷方能进步</font></b></center><br/>

<blockquote>
<p>AlxNet</p>
<blockquote>
<p>并没有提供简单的规则来制造新的网络</p>
</blockquote>
<p>结构比较死板</p>
</blockquote>
<p><strong>所以</strong> VGG：通过重复使⽤简单的基础块来构建深度模型。  </p>
<p>Block:数个相同的填充为1、窗口形状为$3\times 3$的卷积层,接上一个步幅为2、窗口形状为$2\times 2$的最大池化层。  </p>
<p>卷积层保持输入的高和宽不变，而池化层则对其减半</p>
<p><img src="https://img-blog.csdnimg.cn/20200218210146126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="VGG11的实现"><a href="#VGG11的实现" class="headerlink" title="VGG11的实现"></a>VGG11的实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以修改的参数 e每个vgg_block结构相同但是参数可能不相同</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span><span class="params">(num_convs, in_channels, out_channels)</span>:</span> <span class="comment">#卷积层个数，输入通道数，输出通道数</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_convs):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">    blk.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)) <span class="comment"># 这里会使宽高减半</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">64</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">128</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">256</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line"><span class="comment"># 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7</span></span><br><span class="line">fc_features = <span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span> <span class="comment"># c * w * h</span></span><br><span class="line">fc_hidden_units = <span class="number">4096</span> <span class="comment"># 任意</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vgg模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg</span><span class="params">(conv_arch, fc_features, fc_hidden_units=<span class="number">4096</span>)</span>:</span></span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> i, (num_convs, in_channels, out_channels) <span class="keyword">in</span> enumerate(conv_arch):</span><br><span class="line">        <span class="comment"># 每经过一个vgg_block都会使宽高减半</span></span><br><span class="line">        net.add_module(<span class="string">"vgg_block_"</span> + str(i+<span class="number">1</span>), vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">    <span class="comment"># 全连接层部分</span></span><br><span class="line">    net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(),</span><br><span class="line">                                 nn.Linear(fc_features, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, <span class="number">10</span>)</span><br><span class="line">                                ))</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = vgg(conv_arch, fc_features, fc_hidden_units)</span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)</span></span><br><span class="line"><span class="keyword">for</span> name, blk <span class="keyword">in</span> net.named_children(): </span><br><span class="line">    X = blk(X)</span><br><span class="line">    print(name, <span class="string">'output shape: '</span>, X.shape)</span><br><span class="line">ratio = <span class="number">8</span></span><br><span class="line"><span class="comment"># 减小vgg结构，针对minist数据集较小，数据少参数多容易造成过拟合</span></span><br><span class="line">small_conv_arch = [(<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>//ratio), (<span class="number">1</span>, <span class="number">64</span>//ratio, <span class="number">128</span>//ratio), (<span class="number">2</span>, <span class="number">128</span>//ratio, <span class="number">256</span>//ratio), </span><br><span class="line">                   (<span class="number">2</span>, <span class="number">256</span>//ratio, <span class="number">512</span>//ratio), (<span class="number">2</span>, <span class="number">512</span>//ratio, <span class="number">512</span>//ratio)]</span><br><span class="line">net = vgg(small_conv_arch, fc_features // ratio, fc_hidden_units // ratio)</span><br><span class="line">print(net)</span><br><span class="line"></span><br><span class="line">batchsize=<span class="number">16</span></span><br><span class="line"><span class="comment">#batch_size = 64</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line"><span class="comment"># train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<h2 id="网络中的网络（NiN）"><a href="#网络中的网络（NiN）" class="headerlink" title="网络中的网络（NiN）"></a>网络中的网络（NiN）</h2><center><b><font size=5>知缺陷方能进步</font></b></center><br/>

<blockquote>
<p>LeNet、AlexNet和VGG</p>
<blockquote>
<p>先以由卷积层构成的模块充分抽取 空间特征<br/><br>再以由全连接层构成的模块来输出分类结果</p>
</blockquote>
</blockquote>
<blockquote>
<p>NiN</p>
<blockquote>
<p>串联多个由卷积层和 “全连接” 层构成的小⽹络来构建⼀个深层⽹络。  </p>
</blockquote>
<p>NiN去掉了全连接层，而是用平均池化层</p>
</blockquote>
<p>⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。  这样的设计显著的<strong>减少了参数尺寸防止过拟合</strong>，但是<strong>增加了训练时间</strong></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw2dTFwNXZ5LnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<hr>
<p>$1\times1$卷积核作用:</p>
<ol>
<li>放缩通道数：通过控制卷积核的数量达到通道数的放缩。  </li>
<li>增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。  </li>
<li>计算参数少   </li>
</ol>
<hr>
<h3 id="NiN-的实现"><a href="#NiN-的实现" class="headerlink" title="NiN 的实现"></a>NiN 的实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建组成模块nin_block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nin_block</span><span class="params">(in_channels, out_channels, kernel_size, stride, padding)</span>:</span></span><br><span class="line">    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU())</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalAvgPool2d</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(GlobalAvgPool2d, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.avg_pool2d(x, kernel_size=x.size()[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), </span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    GlobalAvgPool2d(), </span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小, 10)</span></span><br><span class="line">    d2l.FlattenLayer())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line"><span class="comment">#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)</span></span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.002</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>NiN</p>
<blockquote>
<ul>
<li>NiN重复使⽤由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层⽹络 ;  </li>
<li>NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数 的NiN块和全局平均池化层 ;</li>
<li>NiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计 ;</li>
</ul>
</blockquote>
</blockquote>
<h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><blockquote>
<p>牺牲了串联网络的思想</p>
</blockquote>
<ol>
<li>由 Inception 基础块组成。  </li>
<li>Inception 块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。   </li>
<li>可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 </li>
<li>使用 padding 来保证输入输出形状相同<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw2dW9ydHcucG5n?x-oss-process=image/format,png" alt="Image Name"><h3 id="Inception-基础块实现"><a href="#Inception-基础块实现" class="headerlink" title="Inception 基础块实现"></a>Inception 基础块实现</h3></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># c1 - c4为每条线路里的层的输出通道数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, c1, c2, c3, c4)</span>:</span></span><br><span class="line">        super(Inception, self).__init__()</span><br><span class="line">        <span class="comment"># 线路1，单1 x 1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1 x 1卷积层后接3 x 3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_c, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1 x 1卷积层后接5 x 5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_c, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3 x 3最大池化层后接1 x 1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)  <span class="comment"># 在通道维上连结输出</span></span><br></pre></td></tr></table></figure>

<h3 id="完整模型结构"><a href="#完整模型结构" class="headerlink" title="完整模型结构"></a>完整模型结构</h3><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWw2eDBmeXluLnBuZw?x-oss-process=image/format,png" /></center> 

<p>$$<br>input -1\times96\times96<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取特征来减小大小</span></span><br><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   d2l.GlobalAvgPool2d())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, </span><br><span class="line">                    d2l.FlattenLayer(), nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, d2l.FlattenLayer(), nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net.children(): </span><br><span class="line">    X = blk(X)</span><br><span class="line">    print(<span class="string">'output shape: '</span>, X.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#batchsize=128</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line"><span class="comment">#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)</span></span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>LeNet</tag>
        <tag>AlexNet</tag>
        <tag>VGG</tag>
        <tag>NiN</tag>
        <tag>GoogLeNet</tag>
      </tags>
  </entry>
  <entry>
    <title>Fundamentals of Convolutional Neural Networks</title>
    <url>/2020/02/28/Fundamentals%20of%20Convolutional%20Neural%20Networks/</url>
    <content><![CDATA[<h2 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h2><blockquote>
<p>常用于处理图像数据。</p>
</blockquote>
<h3 id="二维互相关运算"><a href="#二维互相关运算" class="headerlink" title="二维互相关运算"></a>二维互相关运算</h3><p>二维互相关（cross-correlation）运算的输入是一个二维输入数组和一个二维核（kernel）数组，输出也是一个二维数组，其中核数组通常称为卷积核或过滤器（filter）。卷积核的尺寸通常小于输入数组，卷积核在输入数组上滑动，在每个位置上，卷积核与该位置处的输入子数组按元素相乘并求和，得到输出数组中相应位置的元素。图1展示了一个互相关运算的例子，阴影部分分别是输入的第一个计算区域、核数组以及对应的输出。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mZGJoY3c1LnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图1-二维互相关运算<br>$$</p>
<blockquote>
<p>用 corr2d 函数实现二维互相关运算</p>
<blockquote>
<p>它接受输入数组 X 与核数组 K，并输出数组 Y。</p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 做二维互相关运算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    H, W = X.shape</span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros(H - h + <span class="number">1</span>, W - w + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<p><strong>验证：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造上图中的输入数组 X 、核数组 K 来验证二维互相关运算的输出</span></span><br><span class="line">X = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">Y = corr2d(X, K)</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure>
<p><strong>结果：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">19.</span>, <span class="number">25.</span>],</span><br><span class="line">        [<span class="number">37.</span>, <span class="number">43.</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="二维卷积层-1"><a href="#二维卷积层-1" class="headerlink" title="二维卷积层"></a>二维卷积层</h3><p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的模型参数包括卷积核和标量偏置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kernel_size)</span>:</span></span><br><span class="line">        super(Conv2D, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure>
<p><strong>For Example:</strong></p>
<p>构造一张$6 \times 8$的图像，中间4列为黑（0），其余为白（1），希望检测到颜色边缘。我们的标签是一个$6 \times 7$的二维数组，第2列是1（从1到0的边缘），第6列是-1（从0到1的边缘）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.ones(<span class="number">6</span>, <span class="number">8</span>)</span><br><span class="line">Y = torch.zeros(<span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line">X[:, <span class="number">2</span>: <span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">Y[:, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">Y[:, <span class="number">5</span>] = <span class="number">-1</span></span><br><span class="line">print(X)</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure>
<p><strong>结果：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<p>我们希望学习一个$1 \times 2$卷积层，通过卷积层来检测颜色边缘。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1*2的二维卷积层</span></span><br><span class="line">conv2d = Conv2D(kernel_size=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">step = <span class="number">30</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(step):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = ((Y_hat - Y) ** <span class="number">2</span>).sum()</span><br><span class="line">    <span class="comment"># 后向计算得到梯度</span></span><br><span class="line">    l.backward()</span><br><span class="line">    <span class="comment"># 梯度下降</span></span><br><span class="line">    <span class="comment"># 参数值减去学习率并乘以梯度</span></span><br><span class="line">    conv2d.weight.data -= lr * conv2d.weight.grad</span><br><span class="line">    conv2d.bias.data -= lr * conv2d.bias.grad</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度清零</span></span><br><span class="line">    conv2d.weight.grad.zero_()</span><br><span class="line">    conv2d.bias.grad.zero_()</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step %d, loss %.3f'</span> % (i + <span class="number">1</span>, l.item()))</span><br><span class="line"><span class="comment"># 卷积核</span></span><br><span class="line">print(conv2d.weight.data)</span><br><span class="line"><span class="comment"># 偏置</span></span><br><span class="line">print(conv2d.bias.data)</span><br></pre></td></tr></table></figure>
<h3 id="互相关运算与卷积运算"><a href="#互相关运算与卷积运算" class="headerlink" title="互相关运算与卷积运算"></a>互相关运算与卷积运算</h3><p>卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。我们将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，给定核数组，对于核数组中每一个元素，构建一个从核数组中元素到输入数组元素之间的对应关系，再进行相加求和，本质是一样的。所以使用互相关运算与使用卷积运算并无本质区别。</p>
<h3 id="特征图与感受野"><a href="#特征图与感受野" class="headerlink" title="特征图与感受野"></a>特征图与感受野</h3><p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素 $x$ 的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做 $x$ 的感受野（receptive field）。</p>
<blockquote>
<p>解释:</p>
<blockquote>
<p>输出是一个特征图，对于19来说，其感受野就是0 1 3 4</p>
</blockquote>
<blockquote>
<p>如果引入一个$2\times 2$的新的卷积核，核输出做互相关运算，得到一个$1\times1$的输出，感受野就是前面的所有元素</p>
</blockquote>
</blockquote>
<p>以图1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为$2 \times 2$的输出记为$Y$，将$Y$与另一个形状为$2 \times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。</p>
<h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>我们介绍卷积层的两个超参数，即填充和步幅，它们可以对给定形状的输入和卷积核改变输出形状。</p>
<h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），图2里我们在原输入高和宽的两侧分别添加了值为0的元素。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mbDZlank0LnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图2-在输入的高和宽两侧分别填充了0元素的二维互相关计算<br>$$</p>
<p>如果原输入的高和宽是$n_h$和$n_w$，卷积核的高和宽是$k_h$和$k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，则输出形状为：</p>
<p>$$<br>(n_h+p_h-k_h+1)\times(n_w+p_w-k_w+1)<br>$$</p>
<blockquote>
<p>在卷积神经网络中使用奇数高宽的核<br/><br>比如$3 \times 3$，$5 \times 5$的卷积核，对于高度（或宽度）为大小为 $2 k + 1$ 的核，令步幅为1，在高（或宽）两侧选择大小为$k$的填充，便可保持输入与输出尺寸相同。</p>
</blockquote>
<h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><p>在互相关运算中，卷积核在输入数组上滑动，每次滑动的行数与列数即是步幅（stride）。此前我们使用的步幅都是1，图3展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mbG9obnFnLnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图3-高和宽上步幅分别为3和2的二维互相关运算<br>$$</p>
<p>一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：</p>
<p>$$<br>\lfloor(n_h+p_h-k_h+s_h)/s_h\rfloor \times \lfloor(n_w+p_w-k_w+s_w)/s_w\rfloor<br>$$</p>
<center><img src="https://img-blog.csdnimg.cn/20200218162515171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>$$<br>解释图<br>$$</p>
<p>找后续卷积核可以覆盖到的区域只需要关注最后一个元素，看最后一个元素在输入上找到几个位置，要做的就是往下移动 $s_{h}$ ,卷积核的最后一个元素下一个位置就找到了</p>
<p>如果$p_h=k_h-1$，$p_w=k_w-1$，那么输出形状将简化为$\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h / s_h) \times (n_w/s_w)$。</p>
<p>当$p_h = p_w = p$时，我们称填充为$p$；当$s_h = s_w = s$时，我们称步幅为$s$。</p>
<h2 id="多输入通道和多输出通道"><a href="#多输入通道和多输出通道" class="headerlink" title="多输入通道和多输出通道"></a>多输入通道和多输出通道</h2><p>之前的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3 \times h \times w$的多维数组，将大小为3的这一维称为通道（channel）维。</p>
<h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p>卷积层的输入可以包含多个通道，图4展示了一个含2个输入通道的二维互相关计算的例子。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mbWRud2JxLnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图4-含2个输入通道的互相关计算<br>$$</p>
<p>假设输入数据的通道数为$c_i$，卷积核形状为$k_h\times k_w$，我们为每个输入通道各分配一个形状为$k_h\times k_w$的核数组，将$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组作为输出。我们把$c_i$个核数组在通道维上连结，即得到一个形状为$c_i\times k_h\times k_w$的卷积核。</p>
<h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3><p>卷积层的输出也可以包含多个通道，设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\times k_h\times k_w$的核数组，将它们在输出通道维上连结，卷积核的形状即$c_o\times c_i\times k_h\times k_w$。</p>
<p>对于输出通道的卷积核，我们提供这样一种理解，一个$c_i \times k_h \times k_w$的核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个这样的$c_i \times k_h \times k_w$的核数组，不同的核数组提取的是不同的特征。</p>
<h3 id="1x1卷积层"><a href="#1x1卷积层" class="headerlink" title="1x1卷积层"></a>1x1卷积层</h3><p>最后讨论形状为$1 \times 1$的卷积核，我们通常称这样的卷积运算为$1 \times 1$卷积，称包含这种卷积核的卷积层为$1 \times 1$卷积层。图5展示了使用输入通道数为3、输出通道数为2的$1\times 1$卷积核的互相关计算。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mbXE5ODByLnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>图5-1\times1卷积核的互相关计算。输入和输出具有相同的高和宽<br>$$</p>
<p>$1 \times 1$卷积核可在不改变高宽的情况下，调整通道数。$1 \times 1$卷积核不识别高和宽维度上相邻元素构成的模式，其主要计算发生在通道维上。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么$1\times 1$卷积层的作用与全连接层等价。</p>
<h2 id="卷积层与全连接层的对比"><a href="#卷积层与全连接层的对比" class="headerlink" title="卷积层与全连接层的对比"></a>卷积层与全连接层的对比</h2><ul>
<li><p>全连接层做图像分类</p>
</li>
<li><p>二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：</p>
</li>
<li><p>一是全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。</p>
</li>
<li><p>二是卷积层的参数量更少。不考虑偏置的情况下，一个形状为$(c_i, c_o, h, w)$的卷积核的参数量是$c_i \times c_o \times h \times w$，与输入图像的宽高无关。假如一个卷积层的输入和输出形状分别是$(c_1, h_1, w_1)$和$(c_2, h_2, w_2)$，如果要用全连接层进行连接，参数数量就是$c_1 \times c_2 \times h_1 \times w_1 \times h_2 \times w_2$。使用卷积层可以以较少的参数数量来处理更大的图像。</p>
</li>
</ul>
<h2 id="卷积层的简洁实现"><a href="#卷积层的简洁实现" class="headerlink" title="卷积层的简洁实现"></a>卷积层的简洁实现</h2><p>我们使用Pytorch中的 nn.Conv2d 类来实现二维卷积层</p>
<p>forward 函数的参数为一个四维张量，形状为$(N, C_{in}, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C_{out}, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。</p>
<p><strong>代码讲解：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">print(X.shape)</span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">2</span>, out_channels=<span class="number">3</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), stride=<span class="number">1</span>, padding=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">Y = conv2d(X)</span><br><span class="line">print(<span class="string">'Y.shape: '</span>, Y.shape)</span><br><span class="line">print(<span class="string">'weight.shape: '</span>, conv2d.weight.shape)</span><br><span class="line">print(<span class="string">'bias.shape: '</span>, conv2d.bias.shape)</span><br></pre></td></tr></table></figure>
<h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><h3 id="二维池化层"><a href="#二维池化层" class="headerlink" title="二维池化层"></a>二维池化层</h3><p>池化层有参与模型的正向计算，同样也会参与反向传播</p>
<p>池化层直接对窗口内的元素求最大值或平均值，并没有模型参数参与计算，所以没有模型参数</p>
<p>池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做最大池化或平均池化。图6展示了池化窗口形状为$2\times 2$的最大池化。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNW5mb2Izb2RvLnBuZw?x-oss-process=image/format,png" /></center>

<p>$$<br>池化窗口形状为 2 \times 2 的最大池化<br>$$</p>
<p>二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$p \times q$的池化层称为$p \times q$池化层，其中的池化运算叫作$p \times q$池化。</p>
<p>池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。</p>
<p>在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的输出通道数与输入通道数相等。</p>
<h3 id="池化层的简洁实现"><a href="#池化层的简洁实现" class="headerlink" title="池化层的简洁实现"></a>池化层的简洁实现</h3><p>我们使用Pytorch中的 nn.MaxPool2d 实现最大池化层</p>
<p>forward 函数的参数为一个四维张量，形状为$(N, C, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。</p>
<p>代码讲解</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">32</span>, dtype=torch.float32).view(<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">pool2d = nn.MaxPool2d(kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">Y = pool2d(X)</span><br><span class="line">print(X)</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>平均池化层使用的是 nn.AvgPool2d，使用方法与 nn.MaxPool2d 相同。</p>
</blockquote>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>卷积-池化</tag>
        <tag>特征与感受野</tag>
      </tags>
  </entry>
  <entry>
    <title>注意力机制和Seq2seq模型</title>
    <url>/2020/02/28/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8CSeq2seq%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<center><b><font size=6>Attention Mechanism</font></b></center><br/>

<blockquote>
<p>注意力机制借鉴了人类的注意力思维方式，以获得需要重点关注的目标区域</p>
</blockquote>
<p>&ensp;&ensp;&ensp;&ensp;在 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">编码器—解码器（seq2seq)</a> 中，解码器在各个时间步依赖相同的背景变量（context vector）来获取输⼊序列信息。解码器输入的语境向量(context vector)不同，每个位置都会计算各自的 attention 输出。 当编码器为循环神经⽹络时，背景变量来⾃它最终时间步的隐藏状态。将源序列输入信息以循环单位状态编码，然后将其传递给解码器以生成目标序列。</p>
<p>&ensp;&ensp;&ensp;&ensp;然而这种结构存在着问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。与此同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。例如，当把 “Hello world” 翻译成 “Bonjour le monde” 时，“Hello” 映射成 “Bonjour”，“world” 映射成  “monde”。</p>
<p>&ensp;&ensp;&ensp;&ensp;在 seq2seq 模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWttNGR3Z2Y5LlBORw?x-oss-process=image/format,png" alt="Image Name"></p>
<h2 id="注意力机制框架"><a href="#注意力机制框架" class="headerlink" title="注意力机制框架"></a>注意力机制框架</h2><p>Attention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。 $k_i∈R^{d_k}, v_i∈R^{d_v}$. Query  $q∈R^{d_q}$ ,  attention layer 得到输出与value的维度一致 $o∈R^{d_v}$.  对于一个query来说，attention layer 会与每一个 key 计算注意力分数并进行权重的归一化，输出的向量 $o$ 则是 value 的加权求和，而每个 key 计算的权重与 value 一一对应。</p>
<p>为了计算输出，我们首先假设有一个函数$\alpha$ 用于计算query和key的相似性，然后可以计算所有的 attention scores $a_1, \ldots, a_n$  by</p>
<p>$$<br>a_i = \alpha(\mathbf q, \mathbf k_i).<br>$$</p>
<p>我们使用 softmax 函数 获得注意力权重：</p>
<p>$$<br>b_1, \ldots, b_n = \textrm{softmax}(a_1, \ldots, a_n).<br>$$</p>
<p>最终的输出就是 value 的加权求和：</p>
<p>$$<br>\mathbf o = \sum_{i=1}^n b_i \mathbf v_i.<br>$$</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWttNG9veXUyLlBORw?x-oss-process=image/format,png" /></center>

<blockquote>
<p>不同的 attetion layer 的区别在于 score 函数的选择</p>
</blockquote>
<p>接下来将利用[机器翻译及其相关技术介绍]一文中的(<a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104367653</a>)</p>
<h2 id="介绍两个常用的注意层"><a href="#介绍两个常用的注意层" class="headerlink" title="介绍两个常用的注意层"></a>介绍两个常用的注意层</h2><blockquote>
<ul>
<li>Dot-product Attention <br/></li>
<li>Multilayer Perceptron Attention</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># import dataset</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.listdir(<span class="string">'path to storaged file of dataset'</span>)</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>工具1:</strong> Masked Softmax</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 排除padding位置的影响</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">-1e6</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\n',X_len[:, None] )</span></span><br><span class="line">    <span class="comment"># shape as same as X</span></span><br><span class="line">    mask = torch.arange((maxlen),dtype=torch.float)[<span class="literal">None</span>, :] &gt;= X_len[:, <span class="literal">None</span>]   </span><br><span class="line">    <span class="comment">#print(mask)</span></span><br><span class="line">    X[mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span><span class="params">(X, valid_length)</span>:</span></span><br><span class="line">    <span class="comment"># X: 3-D tensor, valid_length: 1-D or 2-D tensor</span></span><br><span class="line">    softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> softmax(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_length.dim() == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = valid_length.reshape((<span class="number">-1</span>,))</span><br><span class="line">        <span class="comment"># fill masked elements with a large negative, whose exp is 0</span></span><br><span class="line">        X = SequenceMask(X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>])), valid_length)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> softmax(X).reshape(shape)</span><br></pre></td></tr></table></figure>

<p><strong>工具2：</strong> 超出2维矩阵的乘法</p>
<p>$X$ 和 $Y$ 是维度分别为$(b,n,m)$ 和$(b, m, k)$的张量，进行 $b$ 次二维矩阵乘法后得到 $Z$, 维度为 $(b, n, k)$。</p>
<p>$$<br> Z[i,:,:] = dot(X[i,:,:], Y[i,:,:])\qquad for\ i= 1,…,n\ .<br>$$</p>
<hr>
<h3 id="Dot-Product-Attention"><a href="#Dot-Product-Attention" class="headerlink" title="Dot Product Attention"></a>Dot Product Attention</h3><p>The dot product 假设query和keys有相同的维度, 即 $\forall i, q,k_i ∈ R_d$. 通过计算 query 和 key 转置的乘积来计算 attention score ,通常还会除去 $\sqrt{d}$ 减少计算出来的 score 对维度 𝑑 的依赖性，如下</p>
<p>$$<br>α (q,k)=⟨q,k⟩/ \sqrt{d}<br>$$</p>
<p>假设 $Q∈R^{m×d}$ 有 $m$ 个query，$K∈R^{n×d}$ 有 $n$ 个 keys. 我们可以通过矩阵运算的方式计算所有 $mn$ 个 score：</p>
<p>$$<br>α (Q,K)=QK^T/\sqrt{d}<br>$$</p>
<p>它支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># query: (batch_size, #queries, d)</span></span><br><span class="line">    <span class="comment"># key: (batch_size, #kv_pairs, d)</span></span><br><span class="line">    <span class="comment"># value: (batch_size, #kv_pairs, dim_v)</span></span><br><span class="line">    <span class="comment"># valid_length: either (batch_size, ) or (batch_size, xx)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length=None)</span>:</span></span><br><span class="line">        d = query.shape[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># set transpose_b=True to swap the last two dimensions of key</span></span><br><span class="line">        </span><br><span class="line">        scores = torch.bmm(query, key.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        print(<span class="string">"attention_weight\n"</span>,attention_weights)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure>

<p><strong>测试：</strong></p>
<p>创建了两个批，每个批有一个query和10个key-values对。</p>
<p>通过valid_length指定，对于第一批，只关注前2个键-值对，而对于第二批，检查前6个键-值对</p>
<p>因此，尽管这两个批处理具有相同的查询和键值对，但我们获得的输出是不同的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">atten = DotProductAttention(dropout=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">keys = torch.ones((<span class="number">2</span>,<span class="number">10</span>,<span class="number">2</span>),dtype=torch.float)</span><br><span class="line">values = torch.arange((<span class="number">40</span>), dtype=torch.float).view(<span class="number">1</span>,<span class="number">10</span>,<span class="number">4</span>).repeat(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">atten(torch.ones((<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>),dtype=torch.float), keys, values, torch.FloatTensor([<span class="number">2</span>, <span class="number">6</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Result</span></span><br><span class="line">attention_weight</span><br><span class="line"> tensor([[[<span class="number">0.5000</span>, <span class="number">0.5000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>,</span><br><span class="line">          <span class="number">0.0000</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.1667</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>,</span><br><span class="line">          <span class="number">0.0000</span>, <span class="number">0.0000</span>]]])</span><br><span class="line">tensor([[[ <span class="number">2.0000</span>,  <span class="number">3.0000</span>,  <span class="number">4.0000</span>,  <span class="number">5.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10.0000</span>, <span class="number">11.0000</span>, <span class="number">12.0000</span>, <span class="number">13.0000</span>]]])</span><br></pre></td></tr></table></figure>

<h3 id="Multilayer-Porceptron-Attentiion"><a href="#Multilayer-Porceptron-Attentiion" class="headerlink" title="Multilayer Porceptron Attentiion"></a>Multilayer Porceptron Attentiion</h3><p>在多层感知器中，我们首先将 query and keys 投影到  $R^ℎ$ .为了更具体，我们将可以学习的参数做如下映射<br>$W_k∈R^{h×d_k}$ ,  $W_q∈R^{h×d_q}$ , and  $v∈R^h$ .  将 score 函数定义</p>
<p>$$<br>α(k,q)=v^Ttanh(W_kk+W_qq)<br>$$<br>.<br>然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为  ℎ  and 输出的size为 1 .隐层激活函数为tanh，无偏置.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLPAttention</span><span class="params">(nn.Module)</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units,ipt_dim,dropout, **kwargs)</span>:</span></span><br><span class="line">        super(MLPAttention, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># Use flatten=True to keep query's and key's 3-D shapes.</span></span><br><span class="line">        self.W_k = nn.Linear(ipt_dim, units, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(ipt_dim, units, bias=<span class="literal">False</span>)</span><br><span class="line">        self.v = nn.Linear(units, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length)</span>:</span></span><br><span class="line">        query, key = self.W_k(query), self.W_q(key)</span><br><span class="line">        <span class="comment">#print("size",query.size(),key.size())</span></span><br><span class="line">        <span class="comment"># expand query to (batch_size, #querys, 1, units), and key to</span></span><br><span class="line">        <span class="comment"># (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.</span></span><br><span class="line">        features = query.unsqueeze(<span class="number">2</span>) + key.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print("features:",features.size())  #--------------开启</span></span><br><span class="line">        scores = self.v(features).squeeze(<span class="number">-1</span>) </span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure>

<p><strong>测试：</strong></p>
<p>&ensp;&ensp;&ensp;&ensp;尽管 MLPAttention 包含一个额外的 MLP 模型，但如果给定相同的输入和相同的键，我们将获得与DotProductAttention相同的输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">atten = MLPAttention(ipt_dim=<span class="number">2</span>,units = <span class="number">8</span>, dropout=<span class="number">0</span>)</span><br><span class="line">atten(torch.ones((<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>), dtype = torch.float), keys, values, torch.FloatTensor([<span class="number">2</span>, <span class="number">6</span>]))      </span><br><span class="line"><span class="comment">#Result</span></span><br><span class="line">tensor([[[ <span class="number">2.0000</span>,  <span class="number">3.0000</span>,  <span class="number">4.0000</span>,  <span class="number">5.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10.0000</span>, <span class="number">11.0000</span>, <span class="number">12.0000</span>, <span class="number">13.0000</span>]]], grad_fn=&lt;BmmBackward&gt;)</span><br></pre></td></tr></table></figure>

<h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><blockquote>
<p>在Dot-product Attention中，key与query维度需要一致，在MLP Attention中则不需要。</p>
</blockquote>
<h2 id="Seq2seq模型"><a href="#Seq2seq模型" class="headerlink" title="Seq2seq模型"></a>Seq2seq模型</h2><blockquote>
<p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">机器翻译及其相关技术介绍</a></p>
</blockquote>
<p>seq2seq 模型的预测需人为设定终止条件，设定最长序列长度或者输出 [EOS] 结束符号，若不加以限制则可能生成无穷长度序列。</p>
<p>引出：</p>
<h2 id="引入注意力机制的Seq2seq模型"><a href="#引入注意力机制的Seq2seq模型" class="headerlink" title="引入注意力机制的Seq2seq模型"></a>引入注意力机制的Seq2seq模型</h2><p>注意力机制本身有高效的并行性，但引入注意力并不能改变seq2seq内部RNN的迭代机制，因此无法加速。</p>
<blockquote>
<p>将注意机制添加到 sequence to sequence 模型中，以显式地使用权重聚合 states。</p>
</blockquote>
<p>下图展示 encoding 和 decoding 的模型结构，在时间步为 $t$ 的时候。此刻 attention layer 保存着 encodering 看到的所有信息——即 encoding 的每一步输出。在 decoding 阶段，解码器的 $t$ 时刻的隐藏状态被当作 query，encoder 的每个时间步的 hidden states 作为 key 和 value 进行 attention 聚合. </p>
<p>Attetion model 的输出当作成上下文信息 context vector，并与解码器输入 $D_t$ 拼接起来一起送到解码器：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWttN284ejkzLlBORw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>Fig1具有注意机制的seq-to-seq模型解码的第二步<br>$$</p>
<p>下图展示了seq2seq机制的所以层的关系，下面展示了encoder和decoder的layer结构</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWttOGRpaGxyLlBORw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>Fig2具有注意机制的seq-to-seq模型中层结构<br>$$</p>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>由于带有注意机制的 seq2seq 的编码器与之前章节中的 Seq2SeqEncoder 相同，所以在此处我们只关注解码器。</p>
<p>我们添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态:</p>
<ul>
<li>the encoder outputs of all timesteps：encoder 输出的各个状态，被用于attetion layer 的 memory 部分，有相同的 key 和 values ；</li>
</ul>
<ul>
<li>the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state ；</li>
</ul>
<ul>
<li>the encoder valid length: 编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings）；</li>
</ul>
<p>在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的 query。</p>
<p>然后，将注意力模型的输出与输入嵌入向量连接起来，输入到 RNN 层。虽然 RNN 层隐藏状态也包含来自解码器的历史信息，但是 attention model 的输出显式地选择了 enc_valid_len 以内的编码器输出，这样 attention机制就会尽可能排除其他不相关的信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqAttentionDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens,vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, enc_valid_len, *args)</span>:</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line"><span class="comment">#         print("first:",outputs.size(),hidden_state[0].size(),hidden_state[1].size())</span></span><br><span class="line">        <span class="comment"># Transpose outputs to (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>), hidden_state, enc_valid_len)</span><br><span class="line">        <span class="comment">#outputs.swapaxes(0, 1)</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_len = state</span><br><span class="line">        <span class="comment">#("X.size",X.size())</span></span><br><span class="line">        X = self.embedding(X).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">#         print("Xembeding.size2",X.size())</span></span><br><span class="line">        outputs = []</span><br><span class="line">        <span class="keyword">for</span> l, x <span class="keyword">in</span> enumerate(X):</span><br><span class="line"><span class="comment">#             print(f"\n&#123;l&#125;-th token")</span></span><br><span class="line"><span class="comment">#             print("x.first.size()",x.size())</span></span><br><span class="line">            <span class="comment"># query shape: (batch_size, 1, hidden_size)</span></span><br><span class="line">            <span class="comment"># select hidden state of the last rnn layer as query</span></span><br><span class="line">            query = hidden_state[<span class="number">0</span>][<span class="number">-1</span>].unsqueeze(<span class="number">1</span>) <span class="comment"># np.expand_dims(hidden_state[0][-1], axis=1)</span></span><br><span class="line">            <span class="comment"># context has same shape as query</span></span><br><span class="line"><span class="comment">#             print("query enc_outputs, enc_outputs:\n",query.size(), enc_outputs.size(), enc_outputs.size())</span></span><br><span class="line">            context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len)</span><br><span class="line">            <span class="comment"># Concatenate on the feature dimension</span></span><br><span class="line"><span class="comment">#             print("context.size:",context.size())</span></span><br><span class="line">            x = torch.cat((context, x.unsqueeze(<span class="number">1</span>)), dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># Reshape x to (1, batch_size, embed_size+hidden_size)</span></span><br><span class="line"><span class="comment">#             print("rnn",x.size(), len(hidden_state))</span></span><br><span class="line">            out, hidden_state = self.rnn(x.transpose(<span class="number">0</span>,<span class="number">1</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.transpose(<span class="number">0</span>, <span class="number">1</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                        enc_valid_len]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>,</span><br><span class="line">                            num_hiddens=<span class="number">16</span>, num_layers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># encoder.initialize()</span></span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>,</span><br><span class="line">                                  num_hiddens=<span class="number">16</span>, num_layers=<span class="number">2</span>)</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>),dtype=torch.long)</span><br><span class="line">print(<span class="string">"batch size=4\nseq_length=7\nhidden dim=16\nnum_layers=2\n"</span>)</span><br><span class="line">print(<span class="string">'encoder output size:'</span>, encoder(X)[<span class="number">0</span>].size())</span><br><span class="line">print(<span class="string">'encoder hidden size:'</span>, encoder(X)[<span class="number">1</span>][<span class="number">0</span>].size())</span><br><span class="line">print(<span class="string">'encoder memory size:'</span>, encoder(X)[<span class="number">1</span>][<span class="number">1</span>].size())</span><br><span class="line">state = decoder.init_state(encoder(X), <span class="literal">None</span>)</span><br><span class="line">out, state = decoder(X, state)</span><br><span class="line">out.shape, len(state), state[<span class="number">0</span>].shape, len(state[<span class="number">1</span>]), state[<span class="number">1</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span> <span class="comment"># This class is saved in d2l.</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">    <span class="comment"># sort by frequency and token</span></span><br><span class="line">    counter = collections.Counter(tokens)</span><br><span class="line">    token_freqs = sorted(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    token_freqs.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">      <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">      self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">      tokens = [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.unk = <span class="number">0</span></span><br><span class="line">      tokens = [<span class="string">''</span>]</span><br><span class="line">    tokens += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs <span class="keyword">if</span> freq &gt;= min_freq]</span><br><span class="line">    self.idx_to_token = []</span><br><span class="line">    self.token_to_idx = dict()</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">      self.idx_to_token.append(token)</span><br><span class="line">      self.token_to_idx[token] = len(self.idx_to_token) - <span class="number">1</span></span><br><span class="line">      </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">      <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">      <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span><span class="params">(batch_size, max_len, num_examples=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Download an NMT dataset, return its vocabulary and data iterator."""</span></span><br><span class="line">    <span class="comment"># Download and preprocess</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_raw</span><span class="params">(text)</span>:</span></span><br><span class="line">        text = text.replace(<span class="string">'\u202f'</span>, <span class="string">' '</span>).replace(<span class="string">'\xa0'</span>, <span class="string">' '</span>)</span><br><span class="line">        out = <span class="string">''</span></span><br><span class="line">        <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(text.lower()):</span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">in</span> (<span class="string">','</span>, <span class="string">'!'</span>, <span class="string">'.'</span>) <span class="keyword">and</span> text[i<span class="number">-1</span>] != <span class="string">' '</span>:</span><br><span class="line">                out += <span class="string">' '</span></span><br><span class="line">            out += char</span><br><span class="line">        <span class="keyword">return</span> out </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'/home/kesci/input/fraeng6506/fra.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">      raw_text = f.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    text = preprocess_raw(raw_text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(text.split(<span class="string">'\n'</span>)):</span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">if</span> len(parts) &gt;= <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">' '</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build vocab</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(tokens)</span>:</span></span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">        <span class="keyword">return</span> Vocab(tokens, min_freq=<span class="number">3</span>, use_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert to index arrays</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pad</span><span class="params">(line, max_len, padding_token)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(line) &gt; max_len:</span><br><span class="line">            <span class="keyword">return</span> line[:max_len]</span><br><span class="line">        <span class="keyword">return</span> line + [padding_token] * (max_len - len(line))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_array</span><span class="params">(lines, vocab, max_len, is_source)</span>:</span></span><br><span class="line">        lines = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_source:</span><br><span class="line">            lines = [[vocab.bos] + line + [vocab.eos] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        array = torch.tensor([pad(line, max_len, vocab.pad) <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line">        valid_len = (array != vocab.pad).sum(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line">    src_array, src_valid_len = build_array(source, src_vocab, max_len, <span class="literal">True</span>)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, <span class="literal">False</span>)</span><br><span class="line">    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    train_iter = data.DataLoader(train_data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> src_vocab, tgt_vocab, train_iter</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.0</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, ctx = <span class="number">0.005</span>, <span class="number">500</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = d2l.Seq2SeqEncoder(</span><br><span class="line">    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(</span><br><span class="line">    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">model = d2l.EncoderDecoder(encoder, decoder)</span><br></pre></td></tr></table></figure>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.train_s2s_ch9(model, train_iter, lr, num_epochs, ctx)</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> [<span class="string">'Go .'</span>, <span class="string">'Good Night !'</span>, <span class="string">"I'm OK ."</span>, <span class="string">'I won !'</span>]:</span><br><span class="line">    print(sentence + <span class="string">' =&gt; '</span> + d2l.predict_s2s_ch9(</span><br><span class="line">        model, sentence, src_vocab, tgt_vocab, num_steps, ctx))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>MLP</tag>
        <tag>Seq2seq模型</tag>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title>机器翻译及其相关技术介绍</title>
    <url>/2020/02/28/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%8F%8A%E5%85%B6%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h1 id="机器翻译-MT-实践"><a href="#机器翻译-MT-实践" class="headerlink" title="机器翻译(MT)_实践"></a>机器翻译(MT)_实践</h1><blockquote>
<p>将一段文本从一种语言自动翻译为另一种语言<br/><br>用神经网络解决这个问题通常称为神经机器翻译（NMT）。</p>
</blockquote>
<p>主要特征：输出是单词序列而不是单个单词。 输出序列的长度可能与源序列的长度不同。</p>
<center><b>实现一个从英语到法语的机器翻译</b></center><br/>

<p>首先准备一个数据集，汇总一些常见单词和日用句子，数据集中有足够且保证正确的对应数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># For Example</span></span><br><span class="line">Go.	Va !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#2877272 (CM) &amp; #1158250 (Wittydev)</span></span><br><span class="line">Hi.	Salut !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#538123 (CM) &amp; #509819 (Aiji)</span></span><br><span class="line">Hi.	Salut.	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#538123 (CM) &amp; #4320462 (gillux)</span></span><br><span class="line">Run!	Cours !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#906328 (papabear) &amp; #906331 (sacredceltic)</span></span><br><span class="line">Run!	Courez !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#906328 (papabear) &amp; #906332 (sacredceltic)</span></span><br><span class="line">Who?	Qui ?	CC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) &amp; #4366796 (gillux)</span><br><span class="line">Wow!	Ça alors !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#52027 (Zifre) &amp; #374631 (zmoo)</span></span><br><span class="line">Fire!	Au feu !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#1829639 (Spamster) &amp; #4627939 (sacredceltic)</span></span><br><span class="line">Help!	À l  aide !	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#435084 (lukaszpp) &amp; #128430 (sysko)</span></span><br><span class="line">Jump.	Saute.	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#631038 (Shishir) &amp; #2416938 (Phoenix)</span></span><br><span class="line">Stop!	Ça suffit !	CC-BY <span class="number">2.0</span> (France) Attribution: tato</span><br></pre></td></tr></table></figure>

<h3 id="导入包和模块以及数据文件"><a href="#导入包和模块以及数据文件" class="headerlink" title="导入包和模块以及数据文件"></a>导入包和模块以及数据文件</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import dataset</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.listdir(<span class="string">'path to storaged file of dataset'</span>)</span><br><span class="line"><span class="comment"># import package and module</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'path to file storge d2lzh1981'</span>)</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> d2l</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">from</span> d2l.data.base <span class="keyword">import</span> Vocab</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br></pre></td></tr></table></figure>

<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>将数据集清洗、转化为神经网络的输入minbatch</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data file'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">      raw_text = f.read()</span><br><span class="line">print(raw_text[<span class="number">0</span>:<span class="number">1000</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">针对上边的example data 进行处理</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 去掉乱码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_raw</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="comment"># 去掉法文中的空格</span></span><br><span class="line">    text = text.replace(<span class="string">'\u202f'</span>, <span class="string">' '</span>).replace(<span class="string">'\xa0'</span>, <span class="string">' '</span>)</span><br><span class="line">    out = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 大小写归一</span></span><br><span class="line">    <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(text.lower()):</span><br><span class="line">        <span class="comment"># 在单词和标点符号之间加上空格</span></span><br><span class="line">        <span class="keyword">if</span> char <span class="keyword">in</span> (<span class="string">','</span>, <span class="string">'!'</span>, <span class="string">'.'</span>) <span class="keyword">and</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> text[i<span class="number">-1</span>] != <span class="string">' '</span>:</span><br><span class="line">            out += <span class="string">' '</span></span><br><span class="line">        out += char</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">text = preprocess_raw(raw_text)</span><br><span class="line">print(text[<span class="number">0</span>:<span class="number">1000</span>])</span><br></pre></td></tr></table></figure>
<p>&ensp;&ensp;&ensp;&ensp;字符在计算机里是以编码的形式存在，我们通常所用的空格是 \x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。</p>
<p>&ensp;&ensp;&ensp;&ensp;而 \xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。再数据预处理的过程中，我们首先需要对数据进行清洗。</p>
<h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><blockquote>
<p>字符串：单词组成的列表</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_examples = <span class="number">50000</span></span><br><span class="line">source, target = [], []</span><br><span class="line"><span class="comment"># 分开每个样本</span></span><br><span class="line"><span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(text.split(<span class="string">'\n'</span>)):</span><br><span class="line">    <span class="keyword">if</span> i &gt; num_examples:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 取元素</span></span><br><span class="line">    parts = line.split(<span class="string">'\t'</span>)</span><br><span class="line">    <span class="keyword">if</span> len(parts) &gt;= <span class="number">2</span>:</span><br><span class="line">        source.append(parts[<span class="number">0</span>].split(<span class="string">' '</span>))</span><br><span class="line">        target.append(parts[<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"># test     </span></span><br><span class="line"><span class="string">source[0:3], target[0:3]</span></span><br><span class="line"><span class="string"># result</span></span><br><span class="line"><span class="string">([['go', '.'], ['hi', '.'], ['hi', '.']],</span></span><br><span class="line"><span class="string"> [['va', '!'], ['salut', '!'], ['salut', '.']])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">d2l.set_figsize()</span><br><span class="line">d2l.plt.hist([[len(l) <span class="keyword">for</span> l <span class="keyword">in</span> source], [len(l) <span class="keyword">for</span> l <span class="keyword">in</span> target]],label=[<span class="string">'source'</span>, <span class="string">'target'</span>])</span><br><span class="line">d2l.plt.legend(loc=<span class="string">'upper right'</span>);</span><br></pre></td></tr></table></figure>

<h3 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h3><blockquote>
<p>此处利用 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104297072" target="_blank" rel="noopener"><strong>文本预处理Text Preprocessing</strong></a>中的 Vocab 类</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(tokens)</span>:</span></span><br><span class="line">    <span class="comment"># 取出单词连成列表</span></span><br><span class="line">    tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">return</span> d2l.data.base.Vocab(tokens, min_freq=<span class="number">3</span>, use_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">src_vocab = build_vocab(source)</span><br><span class="line">len(src_vocab)</span><br></pre></td></tr></table></figure>

<h3 id="载入数据集得到数据生成器"><a href="#载入数据集得到数据生成器" class="headerlink" title="载入数据集得到数据生成器"></a>载入数据集得到数据生成器</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad</span><span class="params">(line, max_len, padding_token)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(line) &gt; max_len:</span><br><span class="line">        <span class="keyword">return</span> line[:max_len]</span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (max_len - len(line))</span><br><span class="line">pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab.pad)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># is_source a判断是否是法语</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_array</span><span class="params">(lines, vocab, max_len, is_source)</span>:</span></span><br><span class="line">    lines = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_source:</span><br><span class="line">        lines = [[vocab.bos] + line + [vocab.eos] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([pad(line, max_len, vocab.pad) <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line">    <span class="comment"># 有效长度：保留句子的有效长度</span></span><br><span class="line">    valid_len = (array != vocab.pad).sum(<span class="number">1</span>) <span class="comment">#第一个维度</span></span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据生成器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span><span class="params">(batch_size, max_len)</span>:</span> <span class="comment"># This function is saved in d2l.</span></span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line">    src_array, src_valid_len = build_array(source, src_vocab, max_len, <span class="literal">True</span>)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, <span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 验证四个参数是否都相同</span></span><br><span class="line">    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    train_iter = data.DataLoader(train_data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> src_vocab, tgt_vocab, train_iter</span><br></pre></td></tr></table></figure>

<blockquote>
<p>机器翻译</p>
<blockquote>
<p>困难：输入和输出不等价</p>
</blockquote>
</blockquote>
<h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>Encoder经常用循环神经网络，Decoder通过判断对后一个输出是不是eos来判断翻译是否结束</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjYXQzYzhtLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<blockquote>
<p>应用</p>
<blockquote>
<p>Encoder-Decoder常应用于输入序列和输出序列的长度是可变的，而分类问题的输出是固定的类别，不需要使用Encoder-Decoder</p>
</blockquote>
<p>机器翻译 、语音识别任务、对话机器人【属于】<br>文本分类任务【不属于】</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, *args)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, *args)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, **kwargs)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, enc_X, dec_X, *args)</span>:</span></span><br><span class="line">        <span class="comment"># 类似于H_&#123;-1&#125;</span></span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure>
<h2 id="Sequence-to-Sequence模型"><a href="#Sequence-to-Sequence模型" class="headerlink" title="Sequence to Sequence模型"></a>Sequence to Sequence模型</h2><h3 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h3><p>训练<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjN2E1M3B0LnBuZw?x-oss-process=image/format,png" alt="Image Name"><br>预测</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjZWN4Y2JhLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<h3 id="具体结构：-LSTM"><a href="#具体结构：-LSTM" class="headerlink" title="具体结构：(LSTM)"></a>具体结构：(LSTM)</h3><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjY2poa2lpLnBuZw?x-oss-process=image/format,png" /></center>

<h4 id="Encoder-–-state"><a href="#Encoder-–-state" class="headerlink" title="Encoder – state"></a>Encoder – state</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqEncoder</span><span class="params">(d2l.Encoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens=num_hiddens</span><br><span class="line">        self.num_layers=num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span><span class="params">(self, batch_size, device)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),</span><br><span class="line">                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, *args)</span>:</span></span><br><span class="line">        X = self.embedding(X) <span class="comment"># X shape: (batch_size, seq_len, embed_size)</span></span><br><span class="line">        <span class="comment"># 第0维和第1维之间调换</span></span><br><span class="line">        X = X.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># RNN needs first axes to be time</span></span><br><span class="line">        <span class="comment"># state = self.begin_state(X.shape[1], device=X.device)</span></span><br><span class="line">        out, state = self.rnn(X)</span><br><span class="line">        <span class="comment"># The shape of out is (seq_len, batch_size, num_hiddens).</span></span><br><span class="line">        <span class="comment"># state contains the hidden state and the memory cell</span></span><br><span class="line">        <span class="comment"># of the last time step, the shape is (num_layers, batch_size, num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> out, state</span><br></pre></td></tr></table></figure>
<h4 id="Decoder-–-out"><a href="#Decoder-–-out" class="headerlink" title="Decoder – out"></a>Decoder – out</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">        <span class="comment"># 输出的全连接层 映射翻译结果</span></span><br><span class="line">        self.dense = nn.Linear(num_hiddens,vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, *args)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        X = self.embedding(X).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        out, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># Make the batch to be the first dimension to simplify loss computation.</span></span><br><span class="line">        out = self.dense(out).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out, state</span><br></pre></td></tr></table></figure>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">0</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    mask = torch.arange(maxlen)[<span class="literal">None</span>, :].to(X_len.device) &lt; X_len[:, <span class="literal">None</span>]   </span><br><span class="line">    X[~mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 继承交叉损失熵函数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskedSoftmaxCELoss</span><span class="params">(nn.CrossEntropyLoss)</span>:</span></span><br><span class="line">    <span class="comment"># pred shape: (batch_size, seq_len, vocab_size)</span></span><br><span class="line">    <span class="comment"># label shape: (batch_size, seq_len)</span></span><br><span class="line">    <span class="comment"># valid_length shape: (batch_size, )</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, pred, label, valid_length)</span>:</span></span><br><span class="line">        <span class="comment"># the sample weights shape should be (batch_size, seq_len)</span></span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        weights = SequenceMask(weights, valid_length).float()</span><br><span class="line">        self.reduction=<span class="string">'none'</span></span><br><span class="line">        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(<span class="number">1</span>,<span class="number">2</span>), label)</span><br><span class="line">        <span class="keyword">return</span> (output*weights).mean(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch7</span><span class="params">(model, data_iter, lr, num_epochs, device)</span>:</span>  <span class="comment"># Saved in d2l</span></span><br><span class="line">    model.to(device)</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    tic = time.time()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, num_epochs+<span class="number">1</span>):</span><br><span class="line">        l_sum, num_tokens_sum = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_vlen, Y, Y_vlen = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">            <span class="comment"># bos word eos</span></span><br><span class="line">            Y_input, Y_label, Y_vlen = Y[:,:<span class="number">-1</span>], Y[:,<span class="number">1</span>:], Y_vlen<span class="number">-1</span></span><br><span class="line">            </span><br><span class="line">            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)</span><br><span class="line">            <span class="comment"># 评估训练好坏</span></span><br><span class="line">            l = loss(Y_hat, Y_label, Y_vlen).sum()</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                d2l.grad_clipping_nn(model, <span class="number">5</span>, device)</span><br><span class="line">            num_tokens = Y_vlen.sum().item()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.sum().item()</span><br><span class="line">            num_tokens_sum += num_tokens</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"epoch &#123;0:4d&#125;,loss &#123;1:.3f&#125;, time &#123;2:.1f&#125; sec"</span>.format( </span><br><span class="line">                  epoch, (l_sum/num_tokens_sum), time.time()-tic))</span><br><span class="line">            tic = time.time()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.0</span></span><br><span class="line">batch_size, num_examples, max_len = <span class="number">64</span>, <span class="number">1e3</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, ctx = <span class="number">0.005</span>, <span class="number">300</span>, d2l.try_gpu()</span><br><span class="line">src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(</span><br><span class="line">    batch_size, max_len,num_examples)</span><br><span class="line">encoder = Seq2SeqEncoder(</span><br><span class="line">    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqDecoder(</span><br><span class="line">    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">model = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">train_ch7(model, train_iter, lr, num_epochs, ctx)</span><br></pre></td></tr></table></figure>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate_ch7</span><span class="params">(model, src_sentence, src_vocab, tgt_vocab, max_len, device)</span>:</span></span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">' '</span>)]</span><br><span class="line">    src_len = len(src_tokens)</span><br><span class="line">    <span class="keyword">if</span> src_len &lt; max_len:</span><br><span class="line">        src_tokens += [src_vocab.pad] * (max_len - src_len)</span><br><span class="line">    enc_X = torch.tensor(src_tokens, device=device)</span><br><span class="line">    enc_valid_length = torch.tensor([src_len], device=device)</span><br><span class="line">    <span class="comment"># use expand_dim to add the batch_size dimension.</span></span><br><span class="line">    enc_outputs = model.encoder(enc_X.unsqueeze(dim=<span class="number">0</span>), enc_valid_length)</span><br><span class="line">    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)</span><br><span class="line">    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line">    predict_tokens = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(max_len):</span><br><span class="line">        Y, dec_state = model.decoder(dec_X, dec_state)</span><br><span class="line">        <span class="comment"># The token with highest score is used as the next time step input.</span></span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        py = dec_X.squeeze(dim=<span class="number">0</span>).int().item()</span><br><span class="line">        <span class="keyword">if</span> py == tgt_vocab.eos:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        predict_tokens.append(py)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join(tgt_vocab.to_tokens(predict_tokens))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> [<span class="string">'Go .'</span>, <span class="string">'Wow !'</span>, <span class="string">"I'm OK ."</span>, <span class="string">'I love you !'</span>]:</span><br><span class="line">    print(sentence + <span class="string">' =&gt; '</span> + translate_ch7(</span><br><span class="line">        model, sentence, src_vocab, tgt_vocab, max_len, ctx))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Result</span></span><br><span class="line">Go . =&gt; va !</span><br><span class="line">Wow ! =&gt; &lt;unk&gt; !</span><br><span class="line">I<span class="string">'m OK . =&gt; je vais bien .</span></span><br><span class="line"><span class="string">I love you ! =&gt; reste &lt;unk&gt; !</span></span><br></pre></td></tr></table></figure>
<h2 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h2><ul>
<li>简单 贪心搜索（greedy search）：</li>
</ul>
<blockquote>
<p>针对每一个 out 取最大概率<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjaHFvcHBuLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
</blockquote>
<ul>
<li>维特比算法：选择整体分数最高的句子（搜索空间太大）</li>
<li>集束搜索：<blockquote>
<p>集束搜索是维特比算法的贪心形式，所以集束搜索得到的并非是全局最优解<br/><br>集束搜索使用 beam size 参数来限制在每一步保留下来的可能性词的数量</p>
</blockquote>
</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjaWE4NnoxLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>MT</tag>
        <tag>encoder-decoder</tag>
        <tag>Seq2seq模型</tag>
        <tag>集束搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>学而后思,方能发展;思而立行,终将卓越</title>
    <url>/2020/02/28/%E5%AD%A6%E8%80%8C%E5%90%8E%E6%80%9D,%E6%96%B9%E8%83%BD%E5%8F%91%E5%B1%95;%E6%80%9D%E8%80%8C%E7%AB%8B%E8%A1%8C,%E7%BB%88%E5%B0%86%E5%8D%93%E8%B6%8A/</url>
    <content><![CDATA[<center><b><font size=5>学而后思</font></b></center>

<blockquote>
<p>梯度爆炸和梯度衰减问题</p>
</blockquote>
<p>深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。</p>
<p><strong>当神经网络的层数较多时，模型的数值稳定性容易变差。</strong></p>
<p>假设一个层数为 $L$ 的多层感知机的第 $l$ 层 $\boldsymbol{H}^{(l)}$ 的权重参数为$\boldsymbol{W}^{(l)}$，输出层 $\boldsymbol{H}^{(L)}$ 的权重参数为 $\boldsymbol{W}^{(L)}$。</p>
<p>为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为<code>恒等映射</code>（identity mapping）$\phi(x) = x$。</p>
<p>给定输入$\boldsymbol{X}$，多层感知机的第$l$层的输出$\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。</p>
<p>For Example：假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\boldsymbol{X}$分别与$0.2^{30} \approx 1 \times 10^{-21}$（消失）和$5^{30} \approx 9 \times 10^{20}$（爆炸）的乘积。当层数较多时，梯度的计算也容易出现消失或爆炸。</p>
<hr>
<p>解决方案：</p>
<ul>
<li>梯度爆炸：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">裁剪梯度</a></li>
<li>梯度衰减：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104340035" target="_blank" rel="noopener">GRU</a></li>
</ul>
<hr>
<blockquote>
<p>过拟合和欠拟合问题</p>
</blockquote>
<ul>
<li><a href="https://blog.csdn.net/RokoBasilisk/article/details/104344237" target="_blank" rel="noopener">从模型训练中认知拟合现象</a></li>
</ul>
<blockquote>
<p>随机初始化模型参数</p>
<blockquote>
<p>如何占据神经网络中不可或缺的位置？</p>
</blockquote>
</blockquote>
<p>话起 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104287595" target="_blank" rel="noopener">多层感知机 [ Multilayer Perceptron ]</a> </p>
<p>假设输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。</p>
<p>如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。</p>
<p>在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。</p>
<p>在这种情况下，无论隐藏单元有多少，<code>隐藏层本质上只有1个隐藏单元在发挥作用</code>。因此通常将神经网络的模型参数，特别是权重参数，进行随机初始化。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpnNzZrbG95LnBuZw?x-oss-process=image/format,png" /></center>

<blockquote>
<blockquote>
<p>列举两种随机初始化方式</p>
</blockquote>
</blockquote>
<h4 id="PyTorch的默认随机初始化"><a href="#PyTorch的默认随机初始化" class="headerlink" title="PyTorch的默认随机初始化"></a>PyTorch的默认随机初始化</h4><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104269986" target="_blank" rel="noopener">Design and Realization of Linear Regression</a> 中，使用 torch.nn.init.normal_() 使模型 net 的权重参数采用正态分布的随机初始化方式。</p>
<p>PyTorch中 nn.Module 的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考<a href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules" target="_blank" rel="noopener">源代码</a>），因此一般不用我们考虑。</p>
<h4 id="Xavier随机初始化"><a href="#Xavier随机初始化" class="headerlink" title="Xavier随机初始化"></a>Xavier随机初始化</h4><p>假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p>
<p>$$<br>U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).<br>$$</p>
<p>模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。</p>
<blockquote>
<p>环境因素带来的问题</p>
<blockquote>
<p>1.协变量偏移<br/><br>2.标签偏移<br/><br>3.概念偏移</p>
</blockquote>
</blockquote>
<blockquote>
<p>协变量偏移</p>
</blockquote>
<p><strong>For Example : 一个在冬季部署的物品推荐系统在夏季的物品推荐列表中出现了圣诞礼物</strong></p>
<p>我们假设，虽然输入的分布P(x)可能随时间而改变，但是标记函数，即条件分布P（y∣x）不会改变。【注意：容易忽视】</p>
<p>对于区分猫和狗，假设我们的训练数据使用的是猫和狗的真实的照片，但是在测试时，我们被要求对猫和狗的卡通图片进行分类。显然，这不太可能奏效。</p>
<p>训练集由照片组成，而测试集只包含卡通。在一个看起来与测试集有着本质不同的数据集上进行训练，而不考虑如何适应新的情况。</p>
<p>问题的根源在于特征分布的变化 ( 即协变量的变化 ) , 统计学家称这种协变量变化。</p>
<blockquote>
<p>标签偏移</p>
</blockquote>
<p>如果数据量足够的情况下，确保训练数据集和测试集中的数据取自同一个数据集，可以防止协变量偏移和标签偏移是正确的。如果数据量很少，少到测试集中存在训练集中未包含的标签，就会发生标签偏移。</p>
<p>当我们认为导致偏移的是标签P（y）上的边缘分布的变化，但类条件分布是不变的P（x∣y）时，就会出现相反的问题。当我们认为y导致x时，标签偏移是一个合理的假设。</p>
<p>For Example：</p>
<ul>
<li>通常我们希望根据其表现来预测诊断结果。在这种情况下，我们认为诊断引起的表现，即疾病引起的症状。<br/><br>病因（要预测的诊断结果）导致 症状（观察到的结果）。  训练数据集，数据很少只包含流感p(y)的样本。  而测试数据集有流感p(y)和流感q(y)，其中不变的是流感症状p(x|y)。</li>
<li>有时标签偏移和协变量移位假设可以同时成立。例如，当真正的标签函数是确定的和不变的。</li>
</ul>
<p><font color=#FF0000 >在概念转换中，有一种标签本身的定义发生变化的情况：<br></font></p>
<blockquote>
<p>概念偏移</p>
</blockquote>
<p>如果我们要建立一个机器翻译系统，分布P（y∣x）可能因我们的位置而异。这个问题很难发现。另一个可取之处是P（y∣x）通常只是逐渐变化。</p>
<p>换句话说就是：<strong>概念偏移可以根据其缓慢变化的特点缓解。</strong></p>
<center><b><font size=5>思而立行</font></b></center><br/>

<p>Advanced Regression Techniques：House Prices</p>
<h3 id="导入数据包和模块"><a href="#导入数据包和模块" class="headerlink" title="导入数据包和模块"></a>导入数据包和模块</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">torch.set_default_tensor_type(torch.FloatTensor)</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span>  norm</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model, svm, gaussian_process</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>

<h3 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_data = pd.read_csv(<span class="string">"path to test.csv"</span>)</span><br><span class="line">train_data = pd.read_csv(<span class="string">"path to train.csv"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="分析数据变化趋势"><a href="#分析数据变化趋势" class="headerlink" title="分析数据变化趋势"></a>分析数据变化趋势</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># analyze SalePrice of the train_data by describe</span></span><br><span class="line">train_data[<span class="string">'SalePrice'</span>].describe()</span><br><span class="line"><span class="comment"># show trend of SalePrice in train_data</span></span><br><span class="line">sns.distplot(train_data[<span class="string">'SalePrice'</span>])</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200216221220752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p><strong>根据SalePrice变化趋势分析为正态分布，设定两个图像特征峰度(Kurtosis)和偏度(Skewness)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#skewness and kurtosis</span></span><br><span class="line">print(<span class="string">"Skewness: %f"</span> % train_data[<span class="string">'SalePrice'</span>].skew())</span><br><span class="line">print(<span class="string">"Kurtosis: %f"</span> % train_data[<span class="string">'SalePrice'</span>].kurt())</span><br></pre></td></tr></table></figure>
<h3 id="提取有效特征"><a href="#提取有效特征" class="headerlink" title="提取有效特征"></a>提取有效特征</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">corrmat = train_data.corr()</span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">20</span>, <span class="number">9</span>))</span><br><span class="line">sns.heatmap(corrmat, vmax=<span class="number">0.8</span>, square=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200216220930264.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70"  width=500/></center>

<h3 id="特征取舍和离散值参与分析"><a href="#特征取舍和离散值参与分析" class="headerlink" title="特征取舍和离散值参与分析"></a>特征取舍和离散值参与分析</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">f_names = [<span class="string">'CentralAir'</span>, <span class="string">'Neighborhood'</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> f_names:</span><br><span class="line">    label = preprocessing.LabelEncoder()</span><br><span class="line">    train_data[x] = label.fit_transform(train_data[x])</span><br><span class="line">corrmat = train_data.corr()</span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">20</span>, <span class="number">9</span>))</span><br><span class="line">sns.heatmap(corrmat, vmax=<span class="number">0.8</span>, square=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><strong>由相关性图可得：’CentralAir’, ‘Neighborhood’这两个特征对房价的影响并不大,舍去特征</strong></p>
<h3 id="列出关系矩阵"><a href="#列出关系矩阵" class="headerlink" title="列出关系矩阵"></a>列出关系矩阵</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k  = <span class="number">10</span> <span class="comment"># 关系矩阵中将显示10个特征</span></span><br><span class="line">cols = corrmat.nlargest(k, <span class="string">'SalePrice'</span>)[<span class="string">'SalePrice'</span>].index</span><br><span class="line">cm = np.corrcoef(train_data[cols].values.T)</span><br><span class="line">sns.set(font_scale=<span class="number">1.25</span>)</span><br><span class="line">hm = sns.heatmap(cm, cbar=<span class="literal">True</span>, annot=<span class="literal">True</span>, \</span><br><span class="line">                 square=<span class="literal">True</span>, fmt=<span class="string">'.2f'</span>, annot_kws=&#123;<span class="string">'size'</span>: <span class="number">10</span>&#125;, yticklabels=cols.values, xticklabels=cols.values)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200216221404494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h3 id="列出散点关系图"><a href="#列出散点关系图" class="headerlink" title="列出散点关系图"></a>列出散点关系图</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.set()</span><br><span class="line">cols = [<span class="string">'SalePrice'</span>,<span class="string">'OverallQual'</span>,<span class="string">'GrLivArea'</span>, <span class="string">'GarageCars'</span>,<span class="string">'TotalBsmtSF'</span>, <span class="string">'FullBath'</span>, <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">sns.pairplot(train_data[cols], size = <span class="number">2.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="数据模拟"><a href="#数据模拟" class="headerlink" title="数据模拟"></a>数据模拟</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">cols = [<span class="string">'OverallQual'</span>,<span class="string">'GrLivArea'</span>, <span class="string">'GarageCars'</span>,<span class="string">'TotalBsmtSF'</span>, <span class="string">'FullBath'</span>, <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">x = train_data[cols].values</span><br><span class="line">y = train_data[<span class="string">'SalePrice'</span>].values</span><br><span class="line">x_scaled = preprocessing.StandardScaler().fit_transform(x)</span><br><span class="line">y_scaled = preprocessing.StandardScaler().fit_transform(y.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">X_train,X_test, y_train, y_test = train_test_split(x_scaled, y_scaled, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<h3 id="随机森林回归"><a href="#随机森林回归" class="headerlink" title="随机森林回归"></a>随机森林回归</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机森林回归</span></span><br><span class="line">clfs = &#123;</span><br><span class="line">        <span class="string">'svm'</span>:svm.SVR(), </span><br><span class="line">        <span class="string">'RandomForestRegressor'</span>:RandomForestRegressor(n_estimators=<span class="number">400</span>),</span><br><span class="line">        <span class="string">'BayesianRidge'</span>:linear_model.BayesianRidge()</span><br><span class="line">       &#125;</span><br><span class="line"><span class="keyword">for</span> clf <span class="keyword">in</span> clfs:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        clfs[clf].fit(X_train, y_train)</span><br><span class="line">        y_pred = clfs[clf].predict(X_test)</span><br><span class="line">        print(clf + <span class="string">" cost:"</span> + str(np.sum(y_pred-y_test)/len(y_pred)) )</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(clf + <span class="string">" Error:"</span>)</span><br><span class="line">        print(str(e))</span><br></pre></td></tr></table></figure>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 归一化数据的预测结果</span></span><br><span class="line">cols = [<span class="string">'OverallQual'</span>,<span class="string">'GrLivArea'</span>, <span class="string">'GarageCars'</span>,<span class="string">'TotalBsmtSF'</span>, <span class="string">'FullBath'</span>, <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">x = train_data[cols].values</span><br><span class="line">y = train_data[<span class="string">'SalePrice'</span>].values</span><br><span class="line">X_train,X_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">clf = RandomForestRegressor(n_estimators=<span class="number">400</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line">print(y_pred)</span><br><span class="line">print(y_test)</span><br><span class="line">print(sum(abs(y_pred - y_test))/len(y_pred))</span><br></pre></td></tr></table></figure>

<h3 id="检验"><a href="#检验" class="headerlink" title="检验"></a>检验</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># clf:训练模型</span></span><br><span class="line">rfr = clf</span><br><span class="line">test_data[cols].isnull().sum()</span><br><span class="line">test_data[<span class="string">'GarageCars'</span>].describe()</span><br><span class="line">test_data[<span class="string">'TotalBsmtSF'</span>].describe()</span><br><span class="line"></span><br><span class="line">cols2 = [<span class="string">'OverallQual'</span>,<span class="string">'GrLivArea'</span>, <span class="string">'FullBath'</span>, <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">cars = test_data[<span class="string">'GarageCars'</span>].fillna(<span class="number">1.766118</span>)</span><br><span class="line">bsmt = test_data[<span class="string">'TotalBsmtSF'</span>].fillna(<span class="number">1046.117970</span>)</span><br><span class="line">data_test_x = pd.concat( [test_data[cols2], cars, bsmt] ,axis=<span class="number">1</span>)</span><br><span class="line">data_test_x.isnull().sum()</span><br><span class="line"></span><br><span class="line">x = data_test_x.values</span><br><span class="line">y_te_pred = rfr.predict(x)</span><br><span class="line">print(y_te_pred)</span><br><span class="line"></span><br><span class="line">print(y_te_pred.shape)</span><br><span class="line">print(x.shape)</span><br><span class="line">print(data_test_x)</span><br></pre></td></tr></table></figure>
<p><strong>输出数据表格式：1459 rows × 7 columns</strong></p>
<h3 id="输出结果到文件"><a href="#输出结果到文件" class="headerlink" title="输出结果到文件"></a>输出结果到文件</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prediction = pd.DataFrame(y_te_pred, columns=[<span class="string">'SalePrice'</span>])</span><br><span class="line">result = pd.concat([ test_data[<span class="string">'Id'</span>], prediction], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># result = result.drop(resultlt.columns[0], 1)</span></span><br><span class="line">result.columns</span><br><span class="line"><span class="comment"># 保存预测结果</span></span><br><span class="line">result.to_csv(<span class="string">'./Predictions.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
        <tag>梯度问题</tag>
        <tag>拟合现象</tag>
        <tag>环境因素</tag>
      </tags>
  </entry>
  <entry>
    <title>从模型训练中认知拟合现象</title>
    <url>/2020/02/28/%E4%BB%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%AD%E8%AE%A4%E7%9F%A5%E6%8B%9F%E5%90%88%E7%8E%B0%E8%B1%A1/</url>
    <content><![CDATA[<p>机器学习中模型训练是必需的，在模型训练中存在两类典型的问题：</p>
<blockquote>
<p>欠拟合 (underfitting) </p>
<blockquote>
<p>模型无法得到较低的训练误差</p>
</blockquote>
<p>过拟合 (overfitting)</p>
<blockquote>
<p>模型的训练误差远小于它在测试数据集上的误差</p>
</blockquote>
</blockquote>
<p>实际训练过程中可能会出现两类问题的并发症，而且会有多种因素直接或间接地导致这种情况出现</p>
<h2 id="影响因素"><a href="#影响因素" class="headerlink" title="影响因素"></a>影响因素</h2><center>介绍其中两个因素：模型复杂度和训练数据集大小。</center>

<h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><p>以多项式函数拟合为例。给定一个由标量数据特征$x$和对应的标量标签$y$组成的训练数据集，多项式函数拟合的目标是找一个$K$阶多项式函数</p>
<p>$$<br> \hat{y} = b + \sum_{k=1}^K x^k w_k<br>$$</p>
<p>来近似 $y$。</p>
<p>在上式中，$w_k$是模型的权重参数，$b$是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。</p>
<p>给定训练数据集，模型复杂度和误差之间的关系：</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpjMjd3eG9qLnBuZw?x-oss-process=image/format,png"></center>

<h3 id="训练数据集大小"><a href="#训练数据集大小" class="headerlink" title="训练数据集大小"></a>训练数据集大小</h3><p>一般来说，如果训练数据集中<strong>样本数过少</strong>，特别是比模型参数数量（按元素计）更少时，<strong>过拟合</strong>更容易发生。</p>
<p>此外，<strong>泛化误差</strong>不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p>
<hr>
<center>训练误差和泛化误差</center><br/>

<p>&ensp;&ensp;&ensp;&ensp;通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。</p>
<p>&ensp;&ensp;&ensp;&ensp;计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。</p>
<p>&ensp;&ensp;&ensp;&ensp;机器学习模型应关注降低泛化误差。</p>
<hr>
<h2 id="多项式函数拟合实验"><a href="#多项式函数拟合实验" class="headerlink" title="多项式函数拟合实验"></a>多项式函数拟合实验</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_train, n_test, true_w, true_b = <span class="number">100</span>, <span class="number">100</span>, [<span class="number">1.2</span>, <span class="number">-3.4</span>, <span class="number">5.6</span>], <span class="number">5</span></span><br><span class="line">features = torch.randn((n_train + n_test, <span class="number">1</span>))</span><br><span class="line">poly_features = torch.cat((features, torch.pow(features, <span class="number">2</span>), torch.pow(features, <span class="number">3</span>)), <span class="number">1</span>) </span><br><span class="line">labels = (true_w[<span class="number">0</span>] * poly_features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * poly_features[:, <span class="number">1</span>]</span><br><span class="line">          + true_w[<span class="number">2</span>] * poly_features[:, <span class="number">2</span>] + true_b)</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float)</span><br></pre></td></tr></table></figure>

<h3 id="定义和训练参数模型"><a href="#定义和训练参数模型" class="headerlink" title="定义和训练参数模型"></a>定义和训练参数模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">semilogy</span><span class="params">(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,</span></span></span><br><span class="line"><span class="function"><span class="params">             legend=None, figsize=<span class="params">(<span class="number">3.5</span>, <span class="number">2.5</span>)</span>)</span>:</span></span><br><span class="line">    <span class="comment"># d2l.set_figsize(figsize)</span></span><br><span class="line">    d2l.plt.xlabel(x_label)</span><br><span class="line">    d2l.plt.ylabel(y_label)</span><br><span class="line">    d2l.plt.semilogy(x_vals, y_vals)</span><br><span class="line">    <span class="keyword">if</span> x2_vals <span class="keyword">and</span> y2_vals:</span><br><span class="line">        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=<span class="string">':'</span>)</span><br><span class="line">        d2l.plt.legend(legend)</span><br><span class="line"></span><br><span class="line">num_epochs, loss = <span class="number">100</span>, torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot</span><span class="params">(train_features, test_features, train_labels, test_labels)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化网络模型</span></span><br><span class="line">    net = torch.nn.Linear(train_features.shape[<span class="number">-1</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 通过Linear文档可知，pytorch已经将参数初始化了，所以我们这里就不手动初始化了</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置批量大小</span></span><br><span class="line">    batch_size = min(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])    </span><br><span class="line">     <span class="comment"># 设置数据集</span></span><br><span class="line">    dataset = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">    <span class="comment"># 设置获取数据方式     </span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>) </span><br><span class="line">    <span class="comment"># 设置优化函数，使用的是随机梯度下降优化</span></span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)                      </span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    	<span class="comment"># 取一个批量的数据</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:               </span><br><span class="line">        	<span class="comment"># 输入到网络中计算输出，并和标签比较求得损失函数                                  </span></span><br><span class="line">            l = loss(net(X), y.view(<span class="number">-1</span>, <span class="number">1</span>)) </span><br><span class="line">            <span class="comment"># 梯度清零，防止梯度累加干扰优化                                   </span></span><br><span class="line">            optimizer.zero_grad()                                               </span><br><span class="line">            l.backward() <span class="comment"># 求梯度</span></span><br><span class="line">            optimizer.step() <span class="comment"># 迭代优化函数，进行参数优化</span></span><br><span class="line">        train_labels = train_labels.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        test_labels = test_labels.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将训练损失保存到train_ls中</span></span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).item())</span><br><span class="line">        <span class="comment"># 将测试损失保存到test_ls中         </span></span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).item())            </span><br><span class="line">    print(<span class="string">'final epoch: train loss'</span>, train_ls[<span class="number">-1</span>], <span class="string">'test loss'</span>, test_ls[<span class="number">-1</span>])    </span><br><span class="line">    semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">             range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'weight:'</span>, net.weight.data,</span><br><span class="line">          <span class="string">'\nbias:'</span>, net.bias.data)</span><br></pre></td></tr></table></figure>

<h3 id="对比三种拟合现象"><a href="#对比三种拟合现象" class="headerlink" title="对比三种拟合现象"></a>对比三种拟合现象</h3><blockquote>
<p>正常</p>
<blockquote>
<p>fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:])<br><img src="https://img-blog.csdnimg.cn/20200216165618535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></p>
</blockquote>
</blockquote>
<blockquote>
<p>欠拟合</p>
<blockquote>
<p>fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train], labels[n_train:])<br><img src="https://img-blog.csdnimg.cn/20200216172452546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" width=400 /></p>
</blockquote>
</blockquote>
<blockquote>
<p>过拟合</p>
<blockquote>
<p>fit_and_plot(poly_features[0:2, :], poly_features[n_train:, :], labels[0:2], labels[n_train:])<br><img src="https://img-blog.csdnimg.cn/20200216172416864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" width=400/></p>
</blockquote>
</blockquote>
<h2 id="针对两类拟合问题的解决办法"><a href="#针对两类拟合问题的解决办法" class="headerlink" title="针对两类拟合问题的解决办法"></a>针对两类拟合问题的解决办法</h2><h3 id="权重衰减–过拟合"><a href="#权重衰减–过拟合" class="headerlink" title="权重衰减–过拟合"></a>权重衰减–过拟合</h3><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>&ensp;&ensp;&ensp;&ensp;权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。</p>
<h4 id="L2-范数正则化（regularization）"><a href="#L2-范数正则化（regularization）" class="headerlink" title="L2 范数正则化（regularization）"></a>L2 范数正则化（regularization）</h4><p>&ensp;&ensp;&ensp;&ensp;$L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以线性回归中的线性回归损失函数为例</p>
<p>$$<br> \ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2<br>$$</p>
<p>其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为</p>
<p>$$<br>\ell(w_1, w_2, b) + \frac{\lambda}{2n} |\boldsymbol{w}|^2,<br>$$</p>
<p>其中超参数$\lambda &gt; 0$。当权重参数均为0时，惩罚项最小。当$\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。</p>
<p>&ensp;&ensp;&ensp;&ensp;有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将 <a href="https://blog.csdn.net/RokoBasilisk/article/details/104269986" target="_blank" rel="noopener">Design and Realization of Linear Regression</a> 中权重$w_1$和$w_2$的迭代方式更改为</p>
<p>$$<br> \begin{aligned} w_1 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\ w_2 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}<br>$$</p>
<p>可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。</p>
<p>因此，<code>$L_2$范数正则化又叫权重衰减</code>。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，<code>这可能对过拟合有效</code>。</p>
<h4 id="应用【剑指高维线性回归带来的过拟合】"><a href="#应用【剑指高维线性回归带来的过拟合】" class="headerlink" title="应用【剑指高维线性回归带来的过拟合】"></a>应用【剑指高维线性回归带来的过拟合】</h4><p>&ensp;&ensp;&ensp;&ensp;设数据样本特征的维度为$p$。对于训练数据集和测试数据集中特征为$x_1, x_2, \ldots, x_p$的任一样本，使用如下的线性函数来生成该样本的标签：</p>
<p>$$<br> y = 0.05 + \sum_{i = 1}^p 0.01x_i + \epsilon<br>$$</p>
<p>其中噪声项$\epsilon$服从均值为0、标准差为0.01的正态分布。为了较容易地观察过拟合，我们考虑高维线性回归问题，如设维度$p=200$；同时，我们特意把训练数据集的样本数设低，如20。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h5 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_train, n_test, num_inputs = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span></span><br><span class="line">true_w, true_b = torch.ones(num_inputs, <span class="number">1</span>) * <span class="number">0.01</span>, <span class="number">0.05</span></span><br><span class="line"></span><br><span class="line">features = torch.randn((n_train + n_test, num_inputs))</span><br><span class="line">labels = torch.matmul(features, true_w) + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float)</span><br><span class="line">train_features, test_features = features[:n_train, :], features[n_train:, :]</span><br><span class="line">train_labels, test_labels = labels[:n_train], labels[n_train:]</span><br><span class="line"><span class="comment"># 定义参数初始化函数，初始化模型参数并且附上梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_params</span><span class="params">()</span>:</span></span><br><span class="line">    w = torch.randn((num_inputs, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure>
<h5 id="定义L2范数惩罚项"><a href="#定义L2范数惩罚项" class="headerlink" title="定义L2范数惩罚项"></a>定义L2范数惩罚项</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l2_penalty</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (w**<span class="number">2</span>).sum() / <span class="number">2</span></span><br></pre></td></tr></table></figure>

<h5 id="训练和测试"><a href="#训练和测试" class="headerlink" title="训练和测试"></a>训练和测试</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size, num_epochs, lr = <span class="number">1</span>, <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">net, loss = d2l.linreg, d2l.squared_loss</span><br><span class="line"></span><br><span class="line">dataset = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot</span><span class="params">(lambd)</span>:</span></span><br><span class="line">    w, b = init_params()</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="comment"># 添加了L2范数惩罚项</span></span><br><span class="line">            l = loss(net(X, w, b), y) + lambd * l2_penalty(w)</span><br><span class="line">            l = l.sum()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> w.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                w.grad.data.zero_()</span><br><span class="line">                b.grad.data.zero_()</span><br><span class="line">            l.backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())</span><br><span class="line">        test_ls.append(loss(net(test_features, w, b), test_labels).mean().item())</span><br><span class="line">    d2l.semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">                 range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'L2 norm of w:'</span>, w.norm().item())</span><br></pre></td></tr></table></figure>
<h5 id="对比过拟合与权重衰减处理"><a href="#对比过拟合与权重衰减处理" class="headerlink" title="对比过拟合与权重衰减处理"></a>对比过拟合与权重衰减处理</h5><blockquote>
<p>过拟合</p>
<blockquote>
<p>fit_and_plot(lambd=0)<br><img src="https://img-blog.csdnimg.cn/20200216172039167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></p>
</blockquote>
</blockquote>
<blockquote>
<p>权重衰减处理</p>
<blockquote>
<p>fit_and_plot(lambd=3)<br><img src="https://img-blog.csdnimg.cn/20200216172215405.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></p>
</blockquote>
</blockquote>
<h4 id="权重衰减的简化"><a href="#权重衰减的简化" class="headerlink" title="权重衰减的简化"></a>权重衰减的简化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot_pytorch</span><span class="params">(wd)</span>:</span></span><br><span class="line">    <span class="comment"># 对权重参数衰减。权重名称一般是以weight结尾</span></span><br><span class="line">    net = nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.weight, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.bias, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) <span class="comment"># 对权重参数衰减</span></span><br><span class="line">    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  <span class="comment"># 不对偏差参数衰减</span></span><br><span class="line">    </span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X), y).mean()</span><br><span class="line">            optimizer_w.zero_grad()</span><br><span class="line">            optimizer_b.zero_grad()</span><br><span class="line">            </span><br><span class="line">            l.backward()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差</span></span><br><span class="line">            optimizer_w.step()</span><br><span class="line">            optimizer_b.step()</span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).mean().item())</span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).mean().item())</span><br><span class="line">    d2l.semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">                 range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'L2 norm of w:'</span>, net.weight.data.norm().item())</span><br></pre></td></tr></table></figure>
<blockquote>
<p>训练结果与手动实现基本一致</p>
</blockquote>
<h3 id="丢弃法–过拟合"><a href="#丢弃法–过拟合" class="headerlink" title="丢弃法–过拟合"></a>丢弃法–过拟合</h3><blockquote>
<p>只能用于模型训练</p>
</blockquote>
<p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104287595" target="_blank" rel="noopener">Multilayer Perceptron &amp; Classify image</a> 中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \ldots, 5$）的计算表达式为</p>
<p>$$<br> h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right)<br>$$</p>
<p>这里$\phi$是激活函数，$x_1, \ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \ldots, w_{4i}$，偏差参数为$b_i$。</p>
<p>当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。</p>
<p>丢弃概率是丢弃法的超参数。具体来说，设随机变量$\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i’$</p>
<p>$$<br> h_i’ = \frac{\xi_i}{1-p} h_i<br>$$</p>
<p>由于$E(\xi_i) = 1-p$，因此</p>
<p>$$<br> E(h_i’) = \frac{E(\xi_i)}{1-p}h_i = h_i<br>$$</p>
<p>即丢弃法不改变其输入的期望值。</p>
<p>对之前多层感知机的神经网络中的隐藏层使用丢弃法，一种可能的结果如图所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \ldots, h_5$中的任一个。</p>
<center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWpkNjlpbjNtLnBuZw?x-oss-process=image/format,png" /></center>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h4 id="丢弃函数"><a href="#丢弃函数" class="headerlink" title="丢弃函数"></a>丢弃函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">‘’‘</span><br><span class="line">:param:drop_prob:丢失率</span><br><span class="line">:param:keep_prob:保存率</span><br><span class="line">’‘’</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(X, drop_prob)</span>:</span></span><br><span class="line">    X = X.float()</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= drop_prob &lt;= <span class="number">1</span></span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    <span class="comment"># 这种情况下把全部元素都丢弃 keep_prob 是保存率</span></span><br><span class="line">    <span class="keyword">if</span> keep_prob == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    mask = (torch.rand(X.shape) &lt; keep_prob).float()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mask * X / keep_prob</span><br></pre></td></tr></table></figure>

<h4 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 参数的初始化</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b1 = torch.zeros(num_hiddens1, requires_grad=<span class="literal">True</span>)</span><br><span class="line">W2 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b2 = torch.zeros(num_hiddens2, requires_grad=<span class="literal">True</span>)</span><br><span class="line">W3 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b3 = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2, W3, b3]</span><br></pre></td></tr></table></figure>

<h4 id="添加丢弃层"><a href="#添加丢弃层" class="headerlink" title="添加丢弃层"></a>添加丢弃层</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">drop_prob1, drop_prob2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X, is_training=True)</span>:</span></span><br><span class="line">    X = X.view(<span class="number">-1</span>, num_inputs)</span><br><span class="line">    H1 = (torch.matmul(X, W1) + b1).relu()</span><br><span class="line">    <span class="keyword">if</span> is_training:  <span class="comment"># 只在训练模型时使用丢弃法</span></span><br><span class="line">        H1 = dropout(H1, drop_prob1)  <span class="comment"># 在第一层全连接后添加丢弃层</span></span><br><span class="line">    H2 = (torch.matmul(H1, W2) + b2).relu()</span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">        H2 = dropout(H2, drop_prob2)  <span class="comment"># 在第二层全连接后添加丢弃层</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(H2, W3) + b3</span><br></pre></td></tr></table></figure>

<h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net)</span>:</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">if</span> isinstance(net, torch.nn.Module):</span><br><span class="line">            net.eval()  <span class="comment"># 评估模式, 这会关闭dropout</span></span><br><span class="line">            acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">            net.train()  <span class="comment"># 改回训练模式</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 自定义的模型</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="string">'is_training'</span> <span class="keyword">in</span> net.__code__.co_varnames):  <span class="comment"># 如果有is_training这个参数</span></span><br><span class="line">                <span class="comment"># 将is_training设置成False</span></span><br><span class="line">                acc_sum += (net(X, is_training=<span class="literal">False</span>).argmax(dim=<span class="number">1</span>)</span><br><span class="line">                            == y).float().sum().item()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br></pre></td></tr></table></figure>
<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs, lr, batch_size = <span class="number">5</span>, <span class="number">100.0</span>, <span class="number">256</span>  <span class="comment"># 这里的学习率设置的很大，原因与之前相同。</span></span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(</span><br><span class="line">    batch_size, root=<span class="string">'path to FashionMNIST2065'</span>)</span><br><span class="line">d2l.train_ch3(</span><br><span class="line">    net,</span><br><span class="line">    train_iter,</span><br><span class="line">    test_iter,</span><br><span class="line">    loss,</span><br><span class="line">    num_epochs,</span><br><span class="line">    batch_size,</span><br><span class="line">    params,</span><br><span class="line">    lr)</span><br></pre></td></tr></table></figure>
<h4 id="丢弃法的简化"><a href="#丢弃法的简化" class="headerlink" title="丢弃法的简化"></a>丢弃法的简化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens1),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob1),</span><br><span class="line">        nn.Linear(num_hiddens1, num_hiddens2), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob2),</span><br><span class="line">        nn.Linear(num_hiddens2, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    nn.init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>

<center><b>综上<b/><center/><br/>

<p>对于训练误差较低但是泛化误差依然较高，二者相差较大的过拟合 </p>
<ul>
<li>权重衰减</li>
<li>丢弃法</li>
<li>更加全面的方案【<a href="https://www.cnblogs.com/XDU-Lakers/p/10536101.html" target="_blank" rel="noopener">https://www.cnblogs.com/XDU-Lakers/p/10536101.html</a>】</li>
</ul>
<p>对于模型无法达到一个较低的误差的欠拟合</p>
<ul>
<li><p>增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间</p>
</li>
<li><p>添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强</p>
</li>
<li><p>减少正则化参数，正则化的目的是用来防止过拟合的，但是模型出现了欠拟合，则需要减少正则化参数</p>
</li>
<li><p>使用非线性模型，比如核SVM 、决策树、深度学习等模型</p>
</li>
<li><p>调整模型的容量(capacity)，通俗地，模型的容量是指其拟合各种函数的能力</p>
</li>
<li><p>容量低的模型可能很难拟合训练集；使用集成学习方法，如Bagging ,将多个弱学习器Bagging</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>拟合现象</tag>
        <tag>训练误差</tag>
        <tag>泛化误差</tag>
        <tag>L2范数正则化</tag>
      </tags>
  </entry>
  <entry>
    <title>ModernRNN</title>
    <url>/2020/02/28/ModernRNN/</url>
    <content><![CDATA[<p>在循环神经网络的基础上进行了 RNN 的改进，我将介绍四种进化版的循环神经网络</p>
<ol>
<li>GRU</li>
<li>LSTM</li>
<li>深度循环神经网络</li>
<li>双向循环神经网络</li>
</ol>
<blockquote>
<p>循环神经网络初识：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104307813</a><br/><br>RNN 出现的梯度爆炸和梯度衰减问题</p>
<blockquote>
<p>解决梯度爆炸的裁剪梯度方法：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104307813</a></p>
</blockquote>
</blockquote>
<blockquote>
<p>解决梯度衰减问题</p>
</blockquote>
<h2 id="GRU-RNN"><a href="#GRU-RNN" class="headerlink" title="GRU RNN"></a>GRU RNN</h2><p>称为 [ 门控循环神经网络 ] ：通过捕捉时间序列中时间步较大的依赖关系</p>
<p>对比  <code>普通神经网络</code> 与 <code>GRU</code></p>
<center>
    <img src="https://img-blog.csdnimg.cn/20200216134327427.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70"/>
</center>
<br/>
<center>
    <img src="https://img-blog.csdnimg.cn/20200216134348404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70"/>
</center>

<p>• 重置⻔ : 有助于捕捉时间序列⾥短期的依赖关系;<br/><br>• 更新⻔ : 有助于捕捉时间序列⾥⻓期的依赖关系。  </p>
<hr>
<p><strong>参数详释</strong><br/><br>根据参数理解需要初始化的参数，首先表达式中的权重和偏置，6个W和3个b，是更新门、重置门和候选隐藏状态的初始化，紧接着作为下一个 GRU 的 $H_{t-1}$ ,此阶段输出时需要初始化输出层参数。</p>
<p>那么假如是第一层这样没有再上一层的$H_{t-1}$输入,就需要初始化最初的状态</p>
<hr>
<h3 id="实践中理解设计"><a href="#实践中理解设计" class="headerlink" title="实践中理解设计"></a>实践中理解设计</h3><blockquote>
<p>尽管一个 nn.GRU 包揽全盘，但是为了理解 GRU 的设计…</p>
</blockquote>
<h4 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line"><span class="comment"># print('will use', device)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32) <span class="comment">#正态分布</span></span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line">     </span><br><span class="line">    W_xz, W_hz, b_z = _three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = _three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = _three()  <span class="comment"># 候选隐藏状态参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_gru_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span>   <span class="comment">#隐藏状态初始化</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>
<h4 id="GRU-模型"><a href="#GRU-模型" class="headerlink" title="GRU 模型"></a>GRU 模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gru</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh(torch.matmul(X, W_xh) + R * torch.matmul(H, W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H,)</span><br></pre></td></tr></table></figure>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><blockquote>
<p>较 GRUB，LSTM 多了记忆功能，也就是结构上多了个记忆细胞</p>
</blockquote>
<p>包含三个门，引入了记忆细胞，和隐藏状态相似，用来记忆额外的信息</p>
<center>
<img src="https://img-blog.csdnimg.cn/20200216140544366.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" />
</center>

<center>长短期记忆 ( long short-term memory )</center><br />

<ul>
<li>遗忘门:控制上一时间步的记忆细胞 </li>
<li>输入门:控制当前时间步的输入  </li>
<li>输出门:控制从记忆细胞到隐藏状态  </li>
<li>记忆细胞：⼀种特殊的隐藏状态的信息的流动  </li>
</ul>
<h3 id="实践中理解设计-1"><a href="#实践中理解设计-1" class="headerlink" title="实践中理解设计"></a>实践中理解设计</h3><h4 id="初始化参数-1"><a href="#初始化参数-1" class="headerlink" title="初始化参数"></a>初始化参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">print(<span class="string">'will use'</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    W_xi, W_hi, b_i = _three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = _three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = _three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = _three()  <span class="comment"># 候选记忆细胞参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), </span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure>

<h4 id="LSTM-模型"><a href="#LSTM-模型" class="headerlink" title="LSTM 模型"></a>LSTM 模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">print(<span class="string">'will use'</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    W_xi, W_hi, b_i = _three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = _three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = _three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = _three()  <span class="comment"># 候选记忆细胞参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), </span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure>

<h2 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h2><blockquote>
<p>深度代表高度，对于神经网络隐藏层来说，并非如此，层数的加深会导致收敛困难</p>
</blockquote>
<p>对比循环神经网络：<a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">https://blog.csdn.net/RokoBasilisk/article/details/104307813</a></p>
<div align=center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWprbTB2NDRpLnBuZw?x-oss-process=image/format,png" /></div>
深度循环神经网络就是多了隐藏层
<center><img src="https://img-blog.csdnimg.cn/20200216141700404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h4 id="实现方式相同，改变的是-num-layer"><a href="#实现方式相同，改变的是-num-layer" class="headerlink" title="实现方式相同，改变的是 num_layer"></a>实现方式相同，改变的是 num_layer</h4><h2 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h2><blockquote>
<p>常用于 NLP</p>
<blockquote>
<p>特点：预测不再仅依赖于前面的元素，而是同时结合了前后元素，一个词：content <br/><br>两层隐藏层之间的连接采用了concat的方式</p>
</blockquote>
</blockquote>
<center><img src="https://img-blog.csdnimg.cn/20200216143253637.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<h4 id="改变的是-bidirectional-参数"><a href="#改变的是-bidirectional-参数" class="headerlink" title="改变的是 bidirectional 参数"></a>改变的是 bidirectional 参数</h4>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>梯度现象</tag>
        <tag>GRU</tag>
        <tag>LSTM</tag>
        <tag>深度循环神经网络</tag>
        <tag>双向循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Language Model &amp; Data Sampling</title>
    <url>/2020/02/28/Language%20Model%20&amp;%20Data%20Sampling/</url>
    <content><![CDATA[<h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><p>一段自然语言文本可以看作是一个离散时间序列，给定一个长度为$T$的词的序列$w_1, w_2, \ldots, w_T$。</p>
<p>语言模型的目标就是评估该序列是否合理，即计算该序列的概率：</p>
<p>$$<br>P(w_1, w_2, \ldots, w_T).<br>$$<br>假设序列$w_1, w_2, \ldots, w_T$中的每个词是依次生成的，则有</p>
<p>$$<br>\begin{aligned}<br>P(w_1, w_2, \ldots, w_T)<br>&amp;= \prod_{t=1}^T P(w_t \mid w_1, \ldots, w_{t-1})\\<br>&amp;= P(w_1)P(w_2 \mid w_1) \cdots P(w_T \mid w_1w_2\cdots w_{T-1})<br>\end {aligned}<br>$$<br>例如，一段含有4个词的文本序列的概率</p>
<p>$$<br>P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3).<br>$$</p>
<p>语言模型的参数就是词的概率以及给定前几个词情况下的条件概率。设训练数据集为一个大型文本语料库，如维基百科的所有条目.</p>
<p>词的概率可以通过该词在训练数据集中的相对词频来计算，例如，$w_1$的概率可以计算为：</p>
<p>$$<br>\hat P(w_1) = \frac{n(w_1)}{n}<br>$$</p>
<blockquote>
<p>其中$n(w_1)$为语料库中以$w_1$作为第一个词的文本的数量，$n$为语料库中文本的总数量。</p>
</blockquote>
<p>类似的，给定$w_1$情况下，$w_2$的条件概率可以计算为：</p>
<p>$$<br>\hat P(w_2 \mid w_1) = \frac{n(w_1, w_2)}{n(w_1)}<br>$$</p>
<blockquote>
<p>其中$n(w_1, w_2)$为语料库中以$w_1$作为第一个词，$w_2$作为第二个词的文本的数量。</p>
</blockquote>
<hr>
<center>加点料 < 统计学知识 ></center>

<h2 id="n-元语法"><a href="#n-元语法" class="headerlink" title="n 元语法"></a>n 元语法</h2><p>序列长度增加，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设简化模型，马尔科夫假设是指一个词的出现只与前面$n$个词相关，即$n$阶马尔可夫链（Markov chain of order $n$），如果$n=1$，那么有$P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)$。基于$n-1$阶马尔可夫链，我们可以将语言模型改写为</p>
<p>$$<br>P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1}) .<br>$$</p>
<p>以上也叫$n$元语法（$n$-grams），它是基于$n - 1$阶马尔可夫链的概率语言模型。例如，当$n=2$时，含有4个词的文本序列的概率就可以改写为:</p>
<p>$$<br>\begin{aligned}<br>P(w_1, w_2, w_3, w_4)<br>&amp;= P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3)\\<br>&amp;= P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3)<br>\end{aligned}<br>$$</p>
<p>当$n$分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列$w_1, w_2, w_3, w_4$在一元语法、二元语法和三元语法中的概率分别为</p>
<p>$$<br>\begin{aligned}<br>P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\<br>P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3) ,\\<br>P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_2, w_3) .<br>\end{aligned}<br>$$</p>
<p>当$n$较小时，$n$元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当$n$较大时，$n$元语法需要计算并存储大量的词频和多词相邻频率。</p>
<hr>
<blockquote>
<p>深入理解元语法的缺陷</p>
</blockquote>
<ul>
<li><p>参数空间过大 </p>
<p>  n 元语法当 n 足够大的时候词频和使用频率的计算会越来越大</p>
</li>
<li><p>数据稀疏 </p>
<p>  齐夫定律：按频率递减顺序排列的频率词表中，单词的频率与它的序号之间存在“幂律”(power law)关系，即如果把单词按使用频率排序，那么使用频率与序号之间几乎恰好成反比。</p>
</li>
</ul>
<blockquote>
<p>在缺陷的基础上寻找问题的解决办法</p>
<blockquote>
<p>数据采样</p>
</blockquote>
<p>随机采样 &amp;&amp; 相邻采样</p>
</blockquote>
<h3 id="引入数据集"><a href="#引入数据集" class="headerlink" title="引入数据集"></a>引入数据集</h3><blockquote>
<p>利用周杰伦的歌词作为数据集 jaychou_lyrics.txt</p>
<blockquote>
<p><a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/data/jaychou_lyrics.txt.zip?raw=true" target="_blank" rel="noopener">下载地址</a></p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># read data</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'path to jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    corpus_chars = f.read()</span><br><span class="line">print(len(corpus_chars))</span><br><span class="line">print(corpus_chars[: <span class="number">40</span>])</span><br><span class="line">corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">corpus_chars = corpus_chars[: <span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># build character index</span></span><br><span class="line">idx_to_char = list(set(corpus_chars)) <span class="comment"># 去重，得到索引到字符的映射</span></span><br><span class="line">char_to_idx = &#123;char: i <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)&#125; <span class="comment"># 字符到索引的映射 enumerate枚举</span></span><br><span class="line">vocab_size = len(char_to_idx)</span><br><span class="line">print(vocab_size)</span><br><span class="line">corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]  <span class="comment"># 将每个字符转化为索引，得到一个索引的序列</span></span><br><span class="line">sample = corpus_indices[: <span class="number">20</span>]</span><br><span class="line">print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample])) <span class="comment">#join 进行字符的拼接</span></span><br><span class="line">print(<span class="string">'indices:'</span>, sample)</span><br></pre></td></tr></table></figure>

<h3 id="数据采样"><a href="#数据采样" class="headerlink" title="数据采样"></a>数据采样</h3><p>在训练中我们需要每次随机读取小批量样本和标签。时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”，即$X$=“想要有直升”，$Y$=“要有直升机”。</p>
<p>现在我们考虑序列“想要有直升机，想要和你飞到宇宙去”，如果时间步数为5，有以下可能的样本和标签：</p>
<ul>
<li>$X$：“想要有直升”，$Y$：“要有直升机”</li>
<li>$X$：“要有直升机”，$Y$：“有直升机，”</li>
<li>$X$：“有直升机，”，$Y$：“直升机，想”</li>
<li>…</li>
<li>$X$：“要和你飞到”，$Y$：“和你飞到宇”</li>
<li>$X$：“和你飞到宇”，$Y$：“你飞到宇宙”</li>
<li>$X$：“你飞到宇宙”，$Y$：“飞到宇宙去”</li>
</ul>
<p>可以看到，如果序列的长度为$T$，时间步数为$n$，那么一共有$T-n$个合法的样本，但是这些样本有大量的重合，我们通常采用更加高效的采样方式。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。</p>
<h3 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h3><p>下面的代码每次从数据里随机采样一个小批量。其中批量大小<code>batch_size</code>是每个小批量的样本数，<code>num_steps</code>是每个样本所包含的时间步数。<br>在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。</p>
<div align=center>
<img src= "https://img-blog.csdnimg.cn/20200214095637385.png" />
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, device=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为对于长度为n的序列，X最多只有包含其中的前n - 1个字符</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps  <span class="comment"># 下取整，得到不重叠情况下的样本个数</span></span><br><span class="line">    example_indices = [i * num_steps <span class="keyword">for</span> i <span class="keyword">in</span> range(num_examples)]  <span class="comment"># 每个样本的第一个字符在corpus_indices中的下标</span></span><br><span class="line">    random.shuffle(example_indices) <span class="comment">#因为做随机采样，shuffle进行捣乱</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(i)</span>:</span></span><br><span class="line">        <span class="comment"># 返回从i开始的长为num_steps的序列</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[i: i + num_steps]</span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        <span class="comment"># 每次选出batch_size个随机样本</span></span><br><span class="line">        batch_indices = example_indices[i: i + batch_size]  <span class="comment"># 当前batch的各个样本的首字符的下标</span></span><br><span class="line">        X = [_data(j) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X, device=device), torch.tensor(Y, device=device)</span><br></pre></td></tr></table></figure>

<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_seq = list(range(<span class="number">30</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">6</span>):</span><br><span class="line">    print(<span class="string">'X: '</span>, X, <span class="string">'\nY:'</span>, Y, <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">X:  tensor([[ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">        [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">        [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]]) </span><br><span class="line"></span><br><span class="line">X:  tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [<span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="相邻采样"><a href="#相邻采样" class="headerlink" title="相邻采样"></a>相邻采样</h3><p>在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。</p>
<div align=center> <img src="https://img-blog.csdnimg.cn/20200214101159177.png" /></div>

<blockquote>
<p>三部分堆叠构成二维的 tensor</p>
</blockquote>
<div align=center><img src="https://img-blog.csdnimg.cn/20200214101954759.png" /></div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, device=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">    corpus_len = len(corpus_indices) // batch_size * batch_size  <span class="comment"># 保留下来的序列的长度</span></span><br><span class="line">    corpus_indices = corpus_indices[: corpus_len]  <span class="comment"># 仅保留前corpus_len个字符</span></span><br><span class="line">    indices = torch.tensor(corpus_indices, device=device)</span><br><span class="line">    indices = indices.view(batch_size, <span class="number">-1</span>)  <span class="comment"># resize成(batch_size, )</span></span><br><span class="line">    batch_num = (indices.shape[<span class="number">1</span>] - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_num):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i: i + num_steps] <span class="comment">#构建索引 Ｘ是样本</span></span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>: i + num_steps + <span class="number">1</span>]　<span class="comment"># Y 是标签</span></span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure>

<h4 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter_consecutive(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">6</span>):</span><br><span class="line">    print(<span class="string">'X: '</span>, X, <span class="string">'\nY:'</span>, Y, <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">X:  tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>]]) </span><br><span class="line"></span><br><span class="line">X:  tensor([[ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">        [<span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">        [<span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>]])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>语言模型</tag>
        <tag>概率论</tag>
        <tag>元语法</tag>
        <tag>数据采样</tag>
      </tags>
  </entry>
  <entry>
    <title>Text Preprocessing</title>
    <url>/2020/02/28/Text%20Preprocessing/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>打开 Google， 输入搜索关键词，显示上百条搜索结果</p>
</blockquote>
<blockquote>
<p>打开 Google Translate， 输入待翻译文本，翻译结果框中显示出翻译结果</p>
</blockquote>
<p>以上二者的共同点便是文本预处理 Pre-Processing</p>
</blockquote>
<p>在 NLP 项目中，文本预处理占据了超过半数的时间，其重要性不言而喻。</p>
<hr>
<div align=center>
当然<br>
也可以利用完备且效率可观的工具可以快速完成项目
</div>
For Example： 我一直在使用的由 哈工大社会计算与信息检索研究中心开发的 （LTP，Language Technology Platform ）语言技术平台

<hr>
<h1 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h1><p>文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：</p>
<p>【较为简单的步骤，如果需要专门做NLP相关的时候，要进行特殊的预处理】</p>
<ol>
<li>读入文本</li>
<li>分词</li>
<li>建立字典，将每个词映射到一个唯一的索引（index）</li>
<li>将文本从词的序列转换为索引的序列，方便输入模型</li>
</ol>
<h2 id="中英文文本预处理的特点"><a href="#中英文文本预处理的特点" class="headerlink" title="中英文文本预处理的特点"></a>中英文文本预处理的特点</h2><ul>
<li><p>中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般我们需要用分词算法来完成分词。</p>
</li>
<li><p>英文文本的预处理特殊在拼写问题，很多时候，对英文预处理要包括拼写检查，比如“Helo World”这样的错误，我们不能在分析的时候再去纠错。还有就是词干提取(stemming)和词形还原(lemmatization)，主要是因为英文中一个词会存在不同的形式，这个步骤有点像孙悟空的火眼金睛，直接得到单词的原始形态。For Example：” faster “、” fastest “ -&gt; “ fast “；“ leafs ”、“ leaves ” -&gt;  “ leaf “ 。</p>
</li>
</ul>
<blockquote>
<p>本文进行简要的介绍和实现<br>详细可参考：<a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/</a></p>
</blockquote>
<h3 id="读入文本"><a href="#读入文本" class="headerlink" title="读入文本"></a>读入文本</h3><p>引入数据源：<a href="http://www.gutenberg.org/ebooks/35" target="_blank" rel="noopener">http://www.gutenberg.org/ebooks/35</a>【小说 Time Machine】</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'path to timemachine.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f: <span class="comment">#每次处理一行</span></span><br><span class="line">        lines = [re.sub(<span class="string">'[^a-z]+'</span>, <span class="string">' '</span>, line.strip().lower()) <span class="keyword">for</span> line <span class="keyword">in</span> f]  <span class="comment">#正则表达式</span></span><br><span class="line">    <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(<span class="string">'# sentences %d'</span> % len(lines))</span><br></pre></td></tr></table></figure>

<h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>对每个句子进行分词，也就是将一个句子划分成若干个词（token），转换为一个词的序列。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentences, token=<span class="string">'word'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Split sentences into word or char tokens"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'word'</span>:</span><br><span class="line">        <span class="keyword">return</span> [sentence.split(<span class="string">' '</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>:</span><br><span class="line">        <span class="keyword">return</span> [list(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'ERROR: unkown token type '</span>+token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">tokens[<span class="number">0</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#分词结果:</span></span><br><span class="line">[[<span class="string">'the'</span>, <span class="string">'time'</span>, <span class="string">'machine'</span>, <span class="string">'by'</span>, <span class="string">'h'</span>, <span class="string">'g'</span>, <span class="string">'wells'</span>, <span class="string">''</span>], [<span class="string">''</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="建立字典"><a href="#建立字典" class="headerlink" title="建立字典"></a>建立字典</h3><p>为了方便模型处理，将字符串转换为数字。因此先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 构建Vocab类时注意：句子长度统计与构建字典无关 | 所以不必要进行句子长度统计</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">        counter = count_corpus(tokens)  <span class="comment">#&lt;key,value&gt;:&lt;词，词频&gt; </span></span><br><span class="line">        self.token_freqs = list(counter.items()) <span class="comment">#去重并统计词频</span></span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">            <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">            self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            self.idx_to_token += [<span class="string">'&lt;pad&gt;'</span>, <span class="string">'&lt;bos&gt;'</span>, <span class="string">'&lt;eos&gt;'</span>, <span class="string">'&lt;unk&gt;'</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.unk = <span class="number">0</span></span><br><span class="line">            self.idx_to_token += [<span class="string">'&lt;unk&gt;'</span>]</span><br><span class="line">        self.idx_to_token += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> self.token_freqs</span><br><span class="line">                        <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.idx_to_token]</span><br><span class="line">        self.token_to_idx = dict()</span><br><span class="line">        <span class="keyword">for</span> idx, token <span class="keyword">in</span> enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] = idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span> <span class="comment"># 返回字典的大小</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span>  <span class="comment"># 词到索引的映射</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span>  <span class="comment">#索引到词的映射</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    tokens = [tk <span class="keyword">for</span> st <span class="keyword">in</span> sentences <span class="keyword">for</span> tk <span class="keyword">in</span> st]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)  <span class="comment"># 返回一个字典，记录每个词的出现次数</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>&lt; unk &gt;    较为特殊，表示为登录词，无论 use_special_token 参数是否为真，都会用到</p>
</blockquote>
<h3 id="将词转为索引"><a href="#将词转为索引" class="headerlink" title="将词转为索引"></a>将词转为索引</h3><p>使用字典，我们可以将原文本中的句子从单词序列转换为索引序列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'words:'</span>, tokens[i])</span><br><span class="line">    print(<span class="string">'indices:'</span>, vocab[tokens[i]])</span><br></pre></td></tr></table></figure>
<hr>
<p>到此，按照常见步骤已经介绍完了</p>
<p>我们对于分词这一重要关卡需要考虑的更多，上边实现的简要分词许多情形还未完善，只能实现部分特定的分词</p>
<ol>
<li>标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了</li>
<li>类似“shouldn’t”, “doesn’t”这样的词会被错误地处理</li>
<li>类似”Mr.”, “Dr.”这样的词会被错误地处理</li>
</ol>
<p>一者，我们可以通过引入更复杂的规则来解决这些问题，但是事实上，有一些现有的工具可以很好地进行分词，在这里简单介绍其中的两个：<a href="https://spacy.io/" target="_blank" rel="noopener">spaCy</a>和<a href="https://www.nltk.org/" target="_blank" rel="noopener">NLTK</a>。</p>
<p>For Example :</p>
<blockquote>
<p>text = “Mr. Chen doesn’t agree with my suggestion.”</p>
</blockquote>
<h2 id="spaCy"><a href="#spaCy" class="headerlink" title="spaCy"></a>spaCy</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">doc = nlp(text)</span><br><span class="line">print([token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">[<span class="string">'Mr.'</span>, <span class="string">'Chen'</span>, <span class="string">'does'</span>, <span class="string">"n't"</span>, <span class="string">'agree'</span>, <span class="string">'with'</span>, <span class="string">'my'</span>, <span class="string">'suggestion'</span>, <span class="string">'.'</span>]</span><br></pre></td></tr></table></figure>

<h2 id="NLTK"><a href="#NLTK" class="headerlink" title="NLTK"></a>NLTK</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> data</span><br><span class="line">data.path.append(<span class="string">'/home/kesci/input/nltk_data3784/nltk_data'</span>)</span><br><span class="line">print(word_tokenize(text))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Result:</span><br><span class="line">[<span class="string">'Mr.'</span>, <span class="string">'Chen'</span>, <span class="string">'does'</span>, <span class="string">"n't"</span>, <span class="string">'agree'</span>, <span class="string">'with'</span>, <span class="string">'my'</span>, <span class="string">'suggestion'</span>, <span class="string">'.'</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>文本预处理</tag>
        <tag>spaCy</tag>
        <tag>NLTK</tag>
      </tags>
  </entry>
  <entry>
    <title>Multilayer Perceptron &amp; Classify image</title>
    <url>/2020/02/28/Multilayer%20Perceptron%20&amp;%20Classify%20image/</url>
    <content><![CDATA[<h2 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h2><p>以多层感知机为例，概述多层神经网络</p>
<h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>此图为多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。</p>
<div align=center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWhvNjg0am1oLnBuZw?x-oss-process=image/format,png"/> </div>

<h3 id="表达公式"><a href="#表达公式" class="headerlink" title="表达公式"></a>表达公式</h3><p>具体来说，给定一个小批量样本$\boldsymbol{X} \in \mathbb{R}^{n \times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层【其中隐藏单元个数为$h$】记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\boldsymbol{H}$，有$\boldsymbol{H} \in \mathbb{R}^{n \times h}$。</p>
<p>因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\boldsymbol{W}_h \in \mathbb{R}^{d \times h}$和 $\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，输出层的权重和偏差参数分别为$\boldsymbol{W}_o \in \mathbb{R}^{h \times q}$和$\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}$。</p>
<p>含单隐藏层的多层感知机的设计。其输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算为</p>
<p>$$<br> \begin{aligned} \boldsymbol{H} &amp;= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\\ \boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}<br>$$</p>
<p>也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到</p>
<p>$$<br> \boldsymbol{O} = (\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X} \boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o.<br>$$</p>
<blockquote>
<blockquote>
<p>存在的问题</p>
</blockquote>
<ul>
<li>虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络</li>
<li>其中输出层权重参数为$\boldsymbol{W}_h\boldsymbol{W}_o$，偏差参数为$\boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o$、</li>
</ul>
</blockquote>
<blockquote>
<p>结论：隐藏层未起到作用</p>
</blockquote>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><blockquote>
<blockquote>
<p>问题解释</p>
</blockquote>
<p>全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。</p>
</blockquote>
<blockquote>
<blockquote>
<p>解决方法</p>
</blockquote>
<p>引入非线性变换<br>Example:对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。<br>这个非线性函数被称为激活函数（activation function）。</p>
</blockquote>
<ul>
<li><h4 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h4>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为</li>
</ul>
<p>$$<br>\text{ReLU}(x) = \max(x, 0).<br>$$</p>
<p>可以看出，ReLU函数只保留正数元素，并将负数元素清零。</p>
<div align=center><img src=" https://img-blog.csdnimg.cn/20200228104905655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70 " /></div>


<ul>
<li><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4>sigmoid函数可以将元素的值变换到0和1之间：</li>
</ul>
<p>$$<br>\text{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.<br>$$</p>
<div align=center><img src=" https://img-blog.csdnimg.cn/20200228104942978.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70 " /> </div>

<ul>
<li><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h4>tanh（双曲正切）函数可以将元素的值变换到-1和1之间：</li>
</ul>
<p>$$<br>\text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.<br>$$</p>
<p>我们接着绘制tanh函数。当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。</p>
<div align=center><img src="https://img-blog.csdnimg.cn/20200228105017679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></div>

<h4 id="激活函数的选择"><a href="#激活函数的选择" class="headerlink" title="激活函数的选择"></a>激活函数的选择</h4><blockquote>
<blockquote>
<p>ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</p>
</blockquote>
<blockquote>
<p>用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。  </p>
</blockquote>
<blockquote>
<p>在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</p>
</blockquote>
<blockquote>
<p>在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</p>
</blockquote>
</blockquote>
<p>那么之前表达式中输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算变为：</p>
<p>$$<br> \begin{aligned} \boldsymbol{H} &amp;= \phi(\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h),\\ \boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}<br>$$</p>
<p>其中$\phi$表示激活函数。</p>
<h2 id="多层感知机实现"><a href="#多层感知机实现" class="headerlink" title="多层感知机实现"></a>多层感知机实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure>
<h3 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,</span><br><span class="line">			root=<span class="string">'path to FashionMNIST.zip'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="定义模型参数"><a href="#定义模型参数" class="headerlink" title="定义模型参数"></a>定义模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, num_hiddens)), dtype=torch.float)</span><br><span class="line">b1 = torch.zeros(num_hiddens, dtype=torch.float)</span><br><span class="line">W2 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_hiddens, num_outputs)), dtype=torch.float)</span><br><span class="line">b2 = torch.zeros(num_outputs, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="定义激活函数"><a href="#定义激活函数" class="headerlink" title="定义激活函数"></a>定义激活函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.max(input=X, other=torch.tensor(<span class="number">0.0</span>))</span><br><span class="line"><span class="comment">#进行0和Ｘ的大小比较</span></span><br></pre></td></tr></table></figure>

<h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    X = X.view((<span class="number">-1</span>, num_inputs))</span><br><span class="line">    H = relu(torch.matmul(X, W1) + b1)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(H, W2) + b2</span><br></pre></td></tr></table></figure>

<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">100.0</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="多层感知机Pytorch简化"><a href="#多层感知机Pytorch简化" class="headerlink" title="多层感知机Pytorch简化"></a>多层感知机Pytorch简化</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init model and param</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">    </span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(num_hiddens, num_outputs), </span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> params <span class="keyword">in</span> net.parameters():</span><br><span class="line">    init.normal_(params, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>

<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=<span class="string">'path to FashionMNIST.zip'</span>)</span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>MLP</tag>
        <tag>Network</tag>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>Softmax &amp; 分类模型</title>
    <url>/2020/02/28/Softmax%20&amp;%20%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h1><blockquote>
<p>与候选采样相对</p>
</blockquote>
<p>Softmax function, a wonderful activation function that turns numbers aka logits into probabilities that sum to one. Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes.</p>
<p>一种函数，可提供多类别分类模型中每个可能类别的概率。这些概率的总和正好为 1.0。</p>
<p>Example: softmax 可能会得出某个图像是狗、猫和马的概率分别是 0.9、0.08 和 0.02。（也称为完整 softmax。）<img src="https://img-blog.csdnimg.cn/20200212195050635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="softmax的基本概念"><a href="#softmax的基本概念" class="headerlink" title="softmax的基本概念"></a>softmax的基本概念</h2><ul>
<li><p>分类问题<br>一个简单的图像分类问题，输入图像的高和宽均为2像素，色彩为灰度。<br>图像中的4像素分别记为$x_1, x_2, x_3, x_4$。<br>假设真实标签为狗、猫或者鸡，这些标签对应的离散值为$y_1, y_2, y_3$。<br>我们通常使用离散的数值来表示类别，例如$y_1=1, y_2=2, y_3=3$。</p>
</li>
<li><p>权重矢量  </p>
</li>
</ul>
<p>$$<br> \begin{aligned} o_1 &amp;= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1 \end{aligned}<br>$$</p>
<p>$$<br> \begin{aligned} o_2 &amp;= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2 \end{aligned}<br>$$</p>
<p>$$<br> \begin{aligned} o_3 &amp;= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3 \end{aligned}<br>$$</p>
<ul>
<li>神经网络图<br>下图用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$o_1, o_2, o_3$的计算都要依赖于所有的输入$x_1, x_2, x_3, x_4$，softmax回归的输出层也是一个全连接层。</li>
</ul>
<div align=center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWhteW1lem9nLnBuZw?x-oss-process=image/format,png" /></div> 

<p>$$<br>\begin{aligned}softmax回归是一个单层神经网络\end{aligned}<br>$$</p>
<p>既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$当作预测类别是$i$的置信度，并将值最大的输出所对应的类作为预测输出，即输出 $\underset{i}{\arg\max} o_i$。例如，如果$o_1,o_2,o_3$分别为$0.1,10,0.1$，由于$o_2$最大，那么预测类别为2，其代表猫。</p>
<ul>
<li>输出问题<br>直接使用输出层的输出有两个问题：<ol>
<li>一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果$o_1=o_3=10^3$，那么输出值10却又表示图像类别为猫的概率很低。</li>
<li>另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。</li>
</ol>
</li>
</ul>
<p>softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：</p>
<p>$$<br> \hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3)<br>$$</p>
<p>其中</p>
<p>$$<br> \hat{y}1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.<br>$$</p>
<p>容易看出$\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$且$0 \leq \hat{y}_1, \hat{y}_2, \hat{y}_3 \leq 1$，因此$\hat{y}_1, \hat{y}_2, \hat{y}_3$是一个合法的概率分布。这时候，如果$\hat{y}_2=0.8$，不管$\hat{y}_1$和$\hat{y}_3$的值是多少，我们都知道图像类别为猫的概率是80%。此外，我们注意到</p>
<p>$$<br> \underset{i}{\arg\max} o_i = \underset{i}{\arg\max} \hat{y}_i<br>$$</p>
<p>因此softmax运算不改变预测类别输出。</p>
<ul>
<li>计算效率<ul>
<li>单样本矢量计算表达式<br>为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为</li>
</ul>
</li>
</ul>
<p>$$<br> \boldsymbol{W} = \begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} \\ w_{21} &amp; w_{22} &amp; w_{23} \\ w_{31} &amp; w_{32} &amp; w_{33} \\ w_{41} &amp; w_{42} &amp; w_{43} \end{bmatrix},\quad \boldsymbol{b} = \begin{bmatrix} b_1 &amp; b_2 &amp; b_3 \end{bmatrix},<br>$$</p>
<p>设高和宽分别为2个像素的图像样本$i$的特征为</p>
<p>$$<br>\boldsymbol{x}^{(i)} = \begin{bmatrix}x_1^{(i)} &amp; x_2^{(i)} &amp; x_3^{(i)} &amp; x_4^{(i)}\end{bmatrix},<br>$$</p>
<p>输出层的输出为</p>
<p>$$<br>\boldsymbol{o}^{(i)} = \begin{bmatrix}o_1^{(i)} &amp; o_2^{(i)} &amp; o_3^{(i)}\end{bmatrix},<br>$$</p>
<p>预测为狗、猫或鸡的概率分布为</p>
<p>$$<br>\boldsymbol{\hat{y}}^{(i)} = \begin{bmatrix}\hat{y}_1^{(i)} &amp; \hat{y}_2^{(i)} &amp; \hat{y}_3^{(i)}\end{bmatrix}.<br>$$</p>
<p>softmax回归对样本$i$分类的矢量计算表达式为</p>
<p>$$<br> \begin{aligned} \boldsymbol{o}^{(i)} &amp;= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{y}}^{(i)} &amp;= \text{softmax}(\boldsymbol{o}^{(i)}). \end{aligned}<br>$$</p>
<ul>
<li>小批量矢量计算表达式<br>  为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为$n$，输入个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$\boldsymbol{X} \in \mathbb{R}^{n \times d}$。假设softmax回归的权重和偏差参数分别为$\boldsymbol{W} \in \mathbb{R}^{d \times q}$和$\boldsymbol{b} \in \mathbb{R}^{1 \times q}$。softmax回归的矢量计算表达式为</li>
</ul>
<p>$$<br> \begin{aligned} \boldsymbol{O} &amp;= \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{Y}} &amp;= \text{softmax}(\boldsymbol{O}), \end{aligned}<br>$$</p>
<p>其中的加法运算使用了广播机制，$\boldsymbol{O}, \boldsymbol{\hat{Y}} \in \mathbb{R}^{n \times q}$且这两个矩阵的第$i$行分别为样本$i$的输出$\boldsymbol{o}^{(i)}$和概率分布$\boldsymbol{\hat{y}}^{(i)}$。</p>
<hr>
<p>两种操作对比</p>
<blockquote>
<p>numpy 操作：np.exp(x) / np.sum(np.exp(x), axis=0)<br>pytorch 操作：torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1,1)</p>
</blockquote>
<h2 id="引入Fashion-MNIST"><a href="#引入Fashion-MNIST" class="headerlink" title="引入Fashion-MNIST"></a>引入Fashion-MNIST</h2><blockquote>
<p>为方便介绍 Softmax， 为了更加直观的观察到算法之间的差异<br>引入较为复杂的多分类图像分类数据集</p>
<blockquote>
<p>导入：torchvision 包【构建计算机视觉模型】</p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import needed package</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(torch.__version__)</span></span><br><span class="line"><span class="comment"># print(torchvision.__version__)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get dataset</span></span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">'path to file storge FashionMNIST.zip'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">'path to file storge FashionMNIST.zip'</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fashion_mnist_labels</span><span class="params">(labels)</span>:</span></span><br><span class="line">    text_labels = [<span class="string">'t-shirt'</span>, <span class="string">'trouser'</span>, <span class="string">'pullover'</span>, <span class="string">'dress'</span>, <span class="string">'coat'</span>,</span><br><span class="line">                   <span class="string">'sandal'</span>, <span class="string">'shirt'</span>, <span class="string">'sneaker'</span>, <span class="string">'bag'</span>, <span class="string">'ankle boot'</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[int(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_fashion_mnist</span><span class="params">(images, labels)</span>:</span></span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    <span class="comment"># 这里的_表示我们忽略（不使用）的变量</span></span><br><span class="line">    _, figs = plt.subplots(<span class="number">1</span>, len(images), figsize=(<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> f, img, lbl <span class="keyword">in</span> zip(figs, images, labels):</span><br><span class="line">        f.imshow(img.view((<span class="number">28</span>, <span class="number">28</span>)).numpy())</span><br><span class="line">        f.set_title(lbl)</span><br><span class="line">        f.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        f.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">X, y = [], []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    X.append(mnist_train[i][<span class="number">0</span>]) <span class="comment"># 将第i个feature加到X中</span></span><br><span class="line">    y.append(mnist_train[i][<span class="number">1</span>]) <span class="comment"># 将第i个label加到y中</span></span><br><span class="line">show_fashion_mnist(X, get_fashion_mnist_labels(y))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># read data</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">num_workers = <span class="number">4</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line">print(<span class="string">'%.2f sec'</span> % (time.time() - start))</span><br></pre></td></tr></table></figure>

<h2 id="Softmax-手动实现"><a href="#Softmax-手动实现" class="headerlink" title="Softmax 手动实现"></a>Softmax 手动实现</h2><h3 id="import-package-and-module"><a href="#import-package-and-module" class="headerlink" title="import package and module"></a>import package and module</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br><span class="line">print(torchvision.__version__)</span><br></pre></td></tr></table></figure>
<h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>

<h3 id="模型参数初始化"><a href="#模型参数初始化" class="headerlink" title="模型参数初始化"></a>模型参数初始化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init module param </span></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">print(<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, num_outputs)), dtype=torch.float)</span><br><span class="line">b = torch.zeros(num_outputs, dtype=torch.float)</span><br><span class="line">W.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line">b.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Sofmax-定义"><a href="#Sofmax-定义" class="headerlink" title="Sofmax 定义"></a>Sofmax 定义</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define softmax function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></span><br><span class="line">    X_exp = X.exp()</span><br><span class="line">    partition = X_exp.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># print("X size is ", X_exp.size())</span></span><br><span class="line">    <span class="comment"># print("partition size is ", partition, partition.size())</span></span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure>

<h3 id="softmax-回归模型"><a href="#softmax-回归模型" class="headerlink" title="softmax 回归模型"></a>softmax 回归模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define regression model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> softmax(torch.mm(X.view((<span class="number">-1</span>, num_inputs)), W) + b)</span><br></pre></td></tr></table></figure>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat.gather(<span class="number">1</span>, y.view(<span class="number">-1</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="准确率"><a href="#准确率" class="headerlink" title="准确率"></a>准确率</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat.argmax(dim=<span class="number">1</span>) == y).float().mean().item()</span><br></pre></td></tr></table></figure>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch3</span><span class="params">(net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">              params=None, lr=None, optimizer=None)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y).sum()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">            <span class="keyword">elif</span> params <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line">            </span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                d2l.sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                optimizer.step() </span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            train_l_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).sum().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)</span><br></pre></td></tr></table></figure>
<h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = iter(test_iter).next()</span><br><span class="line"></span><br><span class="line">true_labels = d2l.get_fashion_mnist_labels(y.numpy())</span><br><span class="line">pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=<span class="number">1</span>).numpy())</span><br><span class="line">titles = [true + <span class="string">'\n'</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> zip(true_labels, pred_labels)]</span><br><span class="line"></span><br><span class="line">d2l.show_fashion_mnist(X[<span class="number">0</span>:<span class="number">9</span>], titles[<span class="number">0</span>:<span class="number">9</span>])</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Pytorch-改进"><a href="#Pytorch-改进" class="headerlink" title="Pytorch 改进"></a>Pytorch 改进</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import package and module</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"path to file storge d2lzh1981"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<h3 id="初始化参数和获取数据"><a href="#初始化参数和获取数据" class="headerlink" title="初始化参数和获取数据"></a>初始化参数和获取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init param and get data</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<h3 id="定义网络模型"><a href="#定义网络模型" class="headerlink" title="定义网络模型"></a>定义网络模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_inputs, num_outputs)</span>:</span></span><br><span class="line">        super(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(num_inputs, num_outputs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># x 的形状: (batch, 1, 28, 28)</span></span><br><span class="line">        y = self.linear(x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line"><span class="comment"># net = LinearNet(num_inputs, num_outputs)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FlattenLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(FlattenLayer, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># x 的形状: (batch, *, *, ...)</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        <span class="comment"># FlattenLayer(),</span></span><br><span class="line">        <span class="comment"># LinearNet(num_inputs, num_outputs) </span></span><br><span class="line">        OrderedDict([</span><br><span class="line">           (<span class="string">'flatten'</span>, FlattenLayer()),</span><br><span class="line">           (<span class="string">'linear'</span>, nn.Linear(num_inputs, num_outputs))]) <span class="comment"># 或者写成我们自己定义的 LinearNet(num_inputs, num_outputs) 也可以</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init module param</span></span><br><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net.linear.bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss() <span class="comment"># 下面是他的函数原型</span></span><br><span class="line"><span class="comment"># class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')</span></span><br></pre></td></tr></table></figure>

<h3 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>) <span class="comment"># 下面是函数原型</span></span><br><span class="line"><span class="comment"># class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)</span></span><br></pre></td></tr></table></figure>

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>

<h3 id="训练结果分析"><a href="#训练结果分析" class="headerlink" title="训练结果分析"></a>训练结果分析</h3><p><img src="https://img-blog.csdnimg.cn/20200212213623439.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>最开始：训练数据集上的准确率低于测试数据集上的准确率</p>
</blockquote>
<blockquote>
<blockquote>
<p>原因</p>
</blockquote>
<p>训练集上的准确率是在一个epoch的过程中计算得到的<br>测试集上的准确率是在一个epoch结束后计算得到的<br>Result: 后者的模型参数更优</p>
</blockquote>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Softmax</tag>
        <tag>classify</tag>
        <tag>Fashion-MNIST</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title>Design and Realization of Linear Regression</title>
    <url>/2020/02/28/Design%20and%20Realization%20of%20Linear%20Regression/</url>
    <content><![CDATA[<h2 id="线性回归的基本要素"><a href="#线性回归的基本要素" class="headerlink" title="线性回归的基本要素"></a>线性回归的基本要素</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>$$<br>\mathrm{y} = w \cdot \mathrm{x} + b<br>$$</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们在收集到的数据中寻找合适的模型参数来使模型的预测价格与真实价格的误差最小。被训练的数据的集合称为训练数据集（training data set）或训练集（training set），每一条数据的主体作为一个样本（sample），被预测值称作标签（label），用来预测标签的因素叫作特征（feature）。特征用来表征样本的特点。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为 $i$ 的样本误差的表达式为</p>
<p>$$<br>l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,<br>$$</p>
<p>$$<br>L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.<br>$$</p>
<h3 id="优化函数-随机梯度下降"><a href="#优化函数-随机梯度下降" class="headerlink" title="优化函数 - 随机梯度下降"></a>优化函数 - 随机梯度下降</h3><p>鉴于大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值，在这种求数值解的优化算法中，通常使用小批量随机梯度下降（mini-batch stochastic gradient descent）。</p>
<blockquote>
<p>算法</p>
</blockquote>
<p>先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。   </p>
<p>$$<br>(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)<br>$$</p>
<p>学习率: $\eta$代表在每次优化中，能够学习的步长的大小<br>批量大小: $\mathcal{B}$是小批量计算中的批量大小batch size   </p>
<p>总结一下，优化函数的有以下两个步骤：</p>
<ul>
<li>初始化模型参数，一般使用随机初始化；</li>
<li>在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。</li>
</ul>
<h2 id="线性回归模型的实现"><a href="#线性回归模型的实现" class="headerlink" title="线性回归模型的实现"></a>线性回归模型的实现</h2><blockquote>
<p>以房屋为样本，价格为标签，面积和房龄为特征<br>$$<br>\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b<br>$$</p>
</blockquote>
<h3 id="导入包和模块"><a href="#导入包和模块" class="headerlink" title="导入包和模块"></a>导入包和模块</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import packages and modules</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>使用线性模型来生成数据集，生成一个 1000 个样本的数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set input feature number </span></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line"><span class="comment"># set example number</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set true weight and bias in order to generate corresponded label</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line"></span><br><span class="line">features = torch.randn(num_examples, num_inputs,</span><br><span class="line">                      dtype=torch.float32) <span class="comment">#1000*2 vector</span></span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()),</span><br><span class="line">                       dtype=torch.float32) <span class="comment">#偏差通过正态分布随机生成</span></span><br></pre></td></tr></table></figure>
<h3 id="通过图像观察生成的数据合适程度"><a href="#通过图像观察生成的数据合适程度" class="headerlink" title="通过图像观察生成的数据合适程度"></a>通过图像观察生成的数据合适程度</h3><p><img src="https://img-blog.csdnimg.cn/20200211213555899.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(batch_size, features, labels)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(indices)  <span class="comment"># random read 10 samples</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) <span class="comment"># the last time may be not enough for a whole batch</span></span><br><span class="line">        <span class="keyword">yield</span>  features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span> <span class="comment"># read 10 samples</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels): <span class="comment"># input features and labels</span></span><br><span class="line">    print(X, <span class="string">'\n'</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># init parameter</span></span><br><span class="line">w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, <span class="number">1</span>)), dtype=torch.float32)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">w.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line">b.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linreg</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.mm(X, w) + b</span><br></pre></td></tr></table></figure>
<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><blockquote>
<p>均方误差损失函数<br>$$<br>l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,<br>$$</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span><span class="params">(y_hat, y)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.view(y_hat.size())) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><blockquote>
<p>小批量随机梯度下降优化</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define optimization function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span> </span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size <span class="comment"># ues .data to operate param without gradient track</span></span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># super parameters init</span></span><br><span class="line">lr = <span class="number">0.03</span> <span class="comment">#学习率</span></span><br><span class="line">num_epochs = <span class="number">5</span> <span class="comment">#训练周期</span></span><br><span class="line"></span><br><span class="line">net = linreg <span class="comment">#单层网络</span></span><br><span class="line">loss = squared_loss <span class="comment">#均方误差损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):  <span class="comment"># training repeats num_epochs times</span></span><br><span class="line">    <span class="comment"># in each epoch, all the samples in dataset will be used once</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># X is the feature and y is the label of a batch sample</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y).sum()  </span><br><span class="line">        <span class="comment"># calculate the gradient of batch sample loss </span></span><br><span class="line">        l.backward()  </span><br><span class="line">        <span class="comment"># using small batch random gradient descent to iter model parameters</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  </span><br><span class="line">        <span class="comment"># reset parameter gradient</span></span><br><span class="line">        w.grad.data.zero_()<span class="comment">#参数梯度清零，为防止叠加</span></span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, train_l.mean().item()))</span><br></pre></td></tr></table></figure>
<h3 id="检验训练结果"><a href="#检验训练结果" class="headerlink" title="检验训练结果"></a>检验训练结果</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># output result of trainning </span></span><br><span class="line">w, true_w, b, true_b</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="使用-torch-简化代码"><a href="#使用-torch-简化代码" class="headerlink" title="使用 torch 简化代码"></a>使用 torch 简化代码</h2><blockquote>
<p>未单独重写的步骤默认与上一致</p>
</blockquote>
<h3 id="读取数据集-1"><a href="#读取数据集-1" class="headerlink" title="读取数据集"></a>读取数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># combine featues and labels of dataset</span></span><br><span class="line">dataset = Data.TensorDataset(features, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># put dataset into DataLoader</span></span><br><span class="line">data_iter = Data.DataLoader(</span><br><span class="line">    dataset=dataset,            <span class="comment"># torch TensorDataset format</span></span><br><span class="line">    batch_size=batch_size,      <span class="comment"># mini batch size</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,               <span class="comment"># whether shuffle the data or not</span></span><br><span class="line">    num_workers=<span class="number">2</span>,              <span class="comment"># read data in multithreading</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">    print(X, <span class="string">'\n'</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature)</span>:</span></span><br><span class="line">        super(LinearNet, self).__init__()      <span class="comment"># call father function to init </span></span><br><span class="line">        self.linear = nn.Linear(n_feature, <span class="number">1</span>)  <span class="comment"># function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line">net = LinearNet(num_inputs) </span><br><span class="line">print(net) <span class="comment">#单层线性网络</span></span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line">init.normal_(net[<span class="number">0</span>].weight, mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net[<span class="number">0</span>].bias, val=<span class="number">0.0</span>)  <span class="comment"># or you can use `net[0].bias.data.fill_(0)` to modify it directly</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    print(param)</span><br></pre></td></tr></table></figure>

<h3 id="定义损失函数-1"><a href="#定义损失函数-1" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.MSELoss()    <span class="comment"># nn built-in squared loss function</span></span><br><span class="line">                       <span class="comment"># function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`</span></span><br></pre></td></tr></table></figure>

<h3 id="定义优化函数-1"><a href="#定义优化函数-1" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)   <span class="comment"># built-in random gradient descent function</span></span><br><span class="line">print(optimizer)  <span class="comment"># function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`</span></span><br></pre></td></tr></table></figure>
<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)   <span class="comment"># built-in random gradient descent function</span></span><br><span class="line">print(optimizer)  <span class="comment"># function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`</span></span><br></pre></td></tr></table></figure>

<h3 id="检验训练结果-1"><a href="#检验训练结果-1" class="headerlink" title="检验训练结果"></a>检验训练结果</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># result comparision</span></span><br><span class="line">dense = net[<span class="number">0</span>]</span><br><span class="line">print(true_w, dense.weight.data)</span><br><span class="line">print(true_b, dense.bias.data)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Linear Regression</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer(Google 机器翻译模型)</title>
    <url>/2020/02/26/Transformer(Goole%20%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B)/</url>
    <content><![CDATA[<center><b><font size=5>双壁合一</font></b></center>

<p><strong>卷积神经网络(CNNS)</strong></p>
<ul>
<li><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104380762" target="_blank" rel="noopener">Fundamentals of Convolutional Neural Networks</a></p>
</li>
<li><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104381637" target="_blank" rel="noopener">LeNet &amp;&amp; ModernCNN</a></p>
</li>
</ul>
<blockquote>
<p>CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。</p>
</blockquote>
<p><strong>循环神经网络(RNNS)</strong></p>
<ul>
<li><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104307813" target="_blank" rel="noopener">Fundamentals of Recurrent Neural Network</a></p>
</li>
<li><p><a href="https://blog.csdn.net/RokoBasilisk/article/details/104340035" target="_blank" rel="noopener">ModernRNN</a></p>
</li>
</ul>
<blockquote>
<p>RNNs 适合捕捉长距离变长序列的依赖，但是自身的recurrent特性却难以实现并行化处理序列。</p>
</blockquote>
<p><strong>整合CNN和RNN的优势，Vaswani et al., 2017 创新性地使用注意力机制设计了 Transformer 模型。</strong></p>
<hr>
<p>该模型利用 attention 机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的 tokens ，上述优势使得 Transformer 模型在性能优异的同时大大减少了训练时间。</p>
<hr>
<p>如图展示了 Transformer 模型的架构，与<a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">机器翻译及其相关技术介绍</a>中介绍的 seq2seq <strong>相似</strong>，Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：</p>
<ol>
<li>Transformer blocks：$循环网络_{seq2seq模型}$–&gt; Transformer Blocks<br/><br>Transform Blocks模块包含一个多头注意力层（Multi-head Attention Layers）以及两个 position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。</li>
<li>Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理<br>该层包含残差结构以及<font color=gree>层归一化</font>。</li>
<li>Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。</li>
</ol>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwYmoyY2o1LnBuZw?x-oss-process=image/format,png" alt="Fig. 10.3.1 The Transformer architecture."></p>
<p>$$<br>Transformer 架构.<br>$$</p>
<p>鉴于新子块第一次出现，在此前 CNNS 和 RNNS 的基础上，实现 Transform 子模块，并且就<a href="https://blog.csdn.net/RokoBasilisk/article/details/104367653" target="_blank" rel="noopener">机器翻译及其相关技术介绍</a>中的英法翻译数据集实现一个新的机器翻译模型。</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'path to file storge d2lzh1981'</span>)</span><br><span class="line"><span class="keyword">import</span> d2l</span><br></pre></td></tr></table></figure>

<h2 id="masked-softmax"><a href="#masked-softmax" class="headerlink" title="masked softmax"></a>masked softmax</h2><blockquote>
<p>参考<a href="https://blog.csdn.net/RokoBasilisk/article/details/104369799" target="_blank" rel="noopener">$注意力机制和Seq2seq模型_{工具1}$</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">-1e6</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    X_len = X_len.to(X.device)</span><br><span class="line">    <span class="comment">#print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\n',X_len[:, None] )</span></span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float, device=X.device)</span><br><span class="line">    mask = mask[<span class="literal">None</span>, :] &lt; X_len[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment">#print(mask)</span></span><br><span class="line">    X[~mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span><span class="params">(X, valid_length)</span>:</span></span><br><span class="line">    <span class="comment"># X: 3-D tensor, valid_length: 1-D or 2-D tensor</span></span><br><span class="line">    softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> softmax(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_length.dim() == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = valid_length.reshape((<span class="number">-1</span>,))</span><br><span class="line">        <span class="comment"># fill masked elements with a large negative, whose exp is 0</span></span><br><span class="line">        X = SequenceMask(X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>])), valid_length)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> softmax(X).reshape(shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># query: (batch_size, #queries, d)</span></span><br><span class="line">    <span class="comment"># key: (batch_size, #kv_pairs, d)</span></span><br><span class="line">    <span class="comment"># value: (batch_size, #kv_pairs, dim_v)</span></span><br><span class="line">    <span class="comment"># valid_length: either (batch_size, ) or (batch_size, xx)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length=None)</span>:</span></span><br><span class="line">        d = query.shape[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># set transpose_b=True to swap the last two dimensions of key</span></span><br><span class="line">        scores = torch.bmm(query, key.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure>
<h2 id="多头注意力层"><a href="#多头注意力层" class="headerlink" title="多头注意力层"></a>多头注意力层</h2><hr>
<p>引入:<strong>自注意力（self-attention）</strong></p>
<p>自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。</p>
<p>与循环神经网络相比，自注意力对每个元素输出的<strong>计算是并行</strong>的，所以我们可以<strong>高效</strong>的实现这个模块。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwY2t2MzhxLnBuZw?x-oss-process=image/format,png" alt="Fig. 10.3.2 自注意力结构"></p>
<p>$$<br>自注意力结构<br>$$</p>
<p>$$<br>输出了一个与输入长度相同的表征序列<br>$$</p>
<hr>
<p>多头注意力层包含$h$个<strong>并行的自注意力层</strong>，每一个这种层被成为一个head。</p>
<p>对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，这$h$个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwY3NvemlkLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>多头注意力<br>$$</p>
<p>假设query，key和value的维度分别是$d_q$、$d_k$和$d_v$。那么对于每一个头$i=1,\ldots,h$，我们可以训练相应的模型权重$W_q^{(i)} \in \mathbb{R}^{p_q\times d_q}$、$W_k^{(i)} \in \mathbb{R}^{p_k\times d_k}$和$W_v^{(i)} \in \mathbb{R}^{p_v\times d_v}$，以得到每个头的输出：</p>
<p>$$<br>o^{(i)} = attention(W_q^{(i)}q, W_k^{(i)}k, W_v^{(i)}v)<br>$$</p>
<p>这里的attention可以是任意的attention function，之后我们将所有head对应的输出拼接起来，送入最后一个线性层进行整合，这个层的权重可以表示为$W_o\in \mathbb{R}^{d_0 \times hp_v}$</p>
<p>$$<br>o = W_o[o^{(1)}, \ldots, o^{(h)}]<br>$$</p>
<p>接下来实现多头注意力，假设有h个头，隐藏层权重 $hidden_size = p_q = p_k = p_v$ 与query，key，value的维度一致。除此之外，因为多头注意力层保持输入与输出张量的维度不变，所以输出feature 的维度也设置为 $d_0 = hidden_size$。</p>
<h3 id="MultiHeadAttention-class"><a href="#MultiHeadAttention-class" class="headerlink" title="MultiHeadAttention class"></a><div id ="MultiHeadAttention">MultiHeadAttention class</div></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, num_heads, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_k = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_v = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_o = nn.Linear(hidden_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length)</span>:</span></span><br><span class="line">        <span class="comment"># query, key, and value shape: (batch_size, seq_len, dim),</span></span><br><span class="line">        <span class="comment"># where seq_len is the length of input sequence</span></span><br><span class="line">        <span class="comment"># valid_length shape is either (batch_size, )</span></span><br><span class="line">        <span class="comment"># or (batch_size, seq_len).</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Project and transpose query, key, and value from</span></span><br><span class="line">        <span class="comment"># (batch_size, seq_len, hidden_size * num_heads) to</span></span><br><span class="line">        <span class="comment"># (batch_size * num_heads, seq_len, hidden_size).</span></span><br><span class="line">        </span><br><span class="line">        query = transpose_qkv(self.W_q(query), self.num_heads)</span><br><span class="line">        key = transpose_qkv(self.W_k(key), self.num_heads)</span><br><span class="line">        value = transpose_qkv(self.W_v(value), self.num_heads)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Copy valid_length by num_heads times</span></span><br><span class="line">            device = valid_length.device</span><br><span class="line">            valid_length = valid_length.cpu().numpy() <span class="keyword">if</span> valid_length.is_cuda <span class="keyword">else</span> valid_length.numpy()</span><br><span class="line">            <span class="keyword">if</span> valid_length.ndim == <span class="number">1</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">            valid_length = valid_length.to(device)</span><br><span class="line">            </span><br><span class="line">        output = self.attention(query, key, value, valid_length)</span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure>
<h3 id="转置函数"><a href="#转置函数" class="headerlink" title="转置函数"></a>转置函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_qkv</span><span class="params">(X, num_heads)</span>:</span></span><br><span class="line">    <span class="comment"># Original X shape: (batch_size, seq_len, hidden_size * num_heads),</span></span><br><span class="line">    <span class="comment"># -1 means inferring its value, after first reshape, X shape:</span></span><br><span class="line">    <span class="comment"># (batch_size, seq_len, num_heads, hidden_size)</span></span><br><span class="line">    X = X.view(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)</span></span><br><span class="line">    X = X.transpose(<span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Merge the first two dimensions. Use reverse=True to infer shape from</span></span><br><span class="line">    <span class="comment"># right to left.</span></span><br><span class="line">    <span class="comment"># output shape: (batch_size * num_heads, seq_len, hidden_size)</span></span><br><span class="line">    output = X.view(<span class="number">-1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Saved in the d2l package for later use</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_output</span><span class="params">(X, num_heads)</span>:</span></span><br><span class="line">    <span class="comment"># A reversed version of transpose_qkv</span></span><br><span class="line">    X = X.view(<span class="number">-1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.transpose(<span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">    <span class="keyword">return</span> X.view(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], <span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="测试输出"><a href="#测试输出" class="headerlink" title="测试输出"></a>测试输出</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cell = MultiHeadAttention(<span class="number">5</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">0.5</span>)</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">valid_length = torch.FloatTensor([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">cell(X, X, X, valid_length).shape</span><br></pre></td></tr></table></figure>

<h2 id="Position-Wise-Feed-Forward-Networks"><a href="#Position-Wise-Feed-Forward-Networks" class="headerlink" title="Position-Wise Feed-Forward Networks"></a>Position-Wise Feed-Forward Networks</h2><p>Transformer 模块另一个非常重要的部分就是<strong>基于位置的前馈网络（FFN）</strong>，它接受一个形状为（batch_size，seq_length, feature_size）的三维张量。</p>
<p>Position-wise FFN 由两个全连接层组成，它们<strong>作用在最后一维</strong>上。因为序列的每个位置的状态都会被单独地更新，所以我们称为 position-wise，其等效于一个 1x1 的卷积。</p>
<h3 id="Position-wise-FFN-class"><a href="#Position-wise-FFN-class" class="headerlink" title="Position-wise FFN  class"></a>Position-wise FFN  class</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionWiseFFN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, ffn_hidden_size, hidden_size_out, **kwargs)</span>:</span></span><br><span class="line">        super(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.ffn_1 = nn.Linear(input_size, ffn_hidden_size)</span><br><span class="line">        self.ffn_2 = nn.Linear(ffn_hidden_size, hidden_size_out)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.ffn_2(F.relu(self.ffn_1(X)))</span><br></pre></td></tr></table></figure>
<p>与<a href="#MultiHeadAttention">多头注意力层</a>相似，FFN层同样只会对最后一维的大小进行改变；除此之外，对于两个完全相同的输入，FFN层的输出也将相等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">out = ffn(torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line">print(out, out.shape)</span><br></pre></td></tr></table></figure>
<h2 id="Add-and-Norm"><a href="#Add-and-Norm" class="headerlink" title="Add and Norm"></a>Add and Norm</h2><blockquote>
<p>Transformer还有一个重要的<strong>相加归一化层</strong></p>
</blockquote>
<p>它可以平滑地整合输入和其他层的输出，因此我们在每个多头注意力层和 FFN 层后面都添加一个含残差连接的Layer Norm层。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener">Layer Norm 相似于 Batch Norm</a> </p>
</blockquote>
<p><strong>唯一的区别</strong></p>
<blockquote>
<ul>
<li>Batch Norm是对于batch size这个维度进行计算均值和方差的，</li>
<li>Layer Norm则是对最后一维进行计算。<br/></li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">layernorm = nn.LayerNorm(normalized_shape=<span class="number">2</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">batchnorm = nn.BatchNorm1d(num_features=<span class="number">2</span>, affine=<span class="literal">True</span>)</span><br><span class="line">X = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">print(<span class="string">'layer norm:'</span>, layernorm(X))</span><br><span class="line">print(<span class="string">'batch norm:'</span>, batchnorm(X))</span><br></pre></td></tr></table></figure>

<p><font color=gree>层归一化</font>可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能。</p>
<h3 id="AddNorm-class"><a href="#AddNorm-class" class="headerlink" title="AddNorm class"></a>AddNorm class</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.norm = nn.LayerNorm(hidden_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>【注意】：由于残差连接，X和Y需要有相同的维度。</p>
</blockquote>
<h3 id="模块测试"><a href="#模块测试" class="headerlink" title="模块测试"></a>模块测试</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_norm = AddNorm(<span class="number">4</span>, <span class="number">0.5</span>)</span><br><span class="line">add_norm(torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)), torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))).shape</span><br></pre></td></tr></table></figure>

<hr>
<p>以上是Transformer 模型的三个模块，还记得 Transformer 模型高效并行的特性，得益于：</p>
<blockquote>
<p>多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新</p>
</blockquote>
<p>但是在这种特性下，却丢失了重要的序列顺序的信息，为了更好的捕捉序列信息，需要一种可以保持序列元素位置的模块，因而引入位置编码。</p>
<h1 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h1><p>假设输入序列的嵌入表示 $X\in \mathbb{R}^{l\times d}$, 序列长度为$l$嵌入向量维度为$d$，则其位置编码为$P \in \mathbb{R}^{l\times d}$ ，输出的向量就是二者相加 $X + P$。</p>
<p>位置编码是一个二维的矩阵，i对应着序列中的顺序，j对应其embedding vector内部的维度索引。我们可以通过以下等式计算位置编码：</p>
<p>$$<br>P_{i,2j} = sin(i/10000^{2j/d})<br>$$</p>
<p>$$<br>P_{i,2j+1} = cos(i/10000^{2j/d})<br>$$</p>
<p>$$<br>for\ i=0,\ldots, l-1\ and\ j=0,\ldots,\lfloor (d-1)/2 \rfloor<br>$$</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwZTBsdTM4LnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<p>$$<br>位置编码<br>$$</p>
<h2 id="PositionalEncoding-class"><a href="#PositionalEncoding-class" class="headerlink" title="PositionalEncoding class"></a>PositionalEncoding class</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, dropout, max_len=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.P = np.zeros((<span class="number">1</span>, max_len, embedding_size))</span><br><span class="line">        X = np.arange(<span class="number">0</span>, max_len).reshape(<span class="number">-1</span>, <span class="number">1</span>) / np.power(</span><br><span class="line">            <span class="number">10000</span>, np.arange(<span class="number">0</span>, embedding_size, <span class="number">2</span>)/embedding_size)</span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = np.sin(X)</span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = np.cos(X)</span><br><span class="line">        self.P = torch.FloatTensor(self.P)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> X.is_cuda <span class="keyword">and</span> <span class="keyword">not</span> self.P.is_cuda:</span><br><span class="line">            self.P = self.P.cuda()</span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br></pre></td></tr></table></figure>
<h2 id="模块测试-1"><a href="#模块测试-1" class="headerlink" title="模块测试"></a>模块测试</h2><p>可视化其中四个维度，可以看到，第 4 维和第 5 维有相同的频率但偏置不同。第 6 维和第 7 维具有更低的频率；因此 positional encoding 对于不同维度具有可区分性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">Y = pe(torch.zeros((<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))).numpy()</span><br><span class="line">d2l.plot(np.arange(<span class="number">100</span>), Y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].T, figsize=(<span class="number">6</span>, <span class="number">2.5</span>),</span><br><span class="line">         legend=[<span class="string">"dim %d"</span> % p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br></pre></td></tr></table></figure>
<center><img src="https://img-blog.csdnimg.cn/20200219115650513.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Jva29CYXNpbGlzaw==,size_16,color_FFFFFF,t_70" /></center>

<p>$$<br>Result-to-Test<br>$$</p>
<h1 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h1><p>有了组成Transformer的各个模块，可以搭建一个编码器。</p>
<p>编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层。</p>
<p>对于attention模型以及FFN模型，由于残差连接导致输出维度都是与 embedding 维度一致的，因此要将前一层的输出与原始输入相加并归一化。</p>
<h2 id="Encoder-Block基础块"><a href="#Encoder-Block基础块" class="headerlink" title="Encoder Block基础块"></a>Encoder Block基础块</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, ffn_hidden_size, num_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, **kwargs)</span>:</span></span><br><span class="line">        super(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_1 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)</span><br><span class="line">        self.addnorm_2 = AddNorm(embedding_size, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, valid_length)</span>:</span></span><br><span class="line">        Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm_2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># batch_size = 2, seq_len = 100, embedding_size = 24</span></span><br><span class="line"><span class="comment"># ffn_hidden_size = 48, num_head = 8, dropout = 0.5</span></span><br><span class="line"></span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">encoder_blk = EncoderBlock(<span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder_blk(X, valid_length).shape</span><br></pre></td></tr></table></figure>
<p>整个编码器由 n 个 Encoder Block 堆叠而成，利用 Encoder Block 基础块实现 Transformer 编码器。</p>
<p><strong>两个注意点：</strong></p>
<ul>
<li>残差连接的缘故，中间状态的维度始终与嵌入向量的维度 d 一致；</li>
<li>同时注意到我们把嵌入向量乘以 $\sqrt{d}$ 以防止其值过小。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span><span class="params">(d2l.Encoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_size, ffn_hidden_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads, num_layers, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size)</span><br><span class="line">        self.pos_encoding = PositionalEncoding(embedding_size, dropout)</span><br><span class="line">        self.blks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.blks.append(</span><br><span class="line">                EncoderBlock(embedding_size, ffn_hidden_size,</span><br><span class="line">                             num_heads, dropout))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, valid_length, *args)</span>:</span></span><br><span class="line">        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X = blk(X, valid_length)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test encoder</span></span><br><span class="line">encoder = TransformerEncoder(<span class="number">200</span>, <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>)).long(), valid_length).shape</span><br></pre></td></tr></table></figure>
<h1 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h1><p>Transformer 模型的解码器与编码器结构类似，然而，除了之前介绍的几个模块之外，解码器部分有另一个子模块。该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。</p>
<p>与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和<font color=gree>层归一化</font>将各个子层的输出相连。</p>
<p>在第t个时间步，当前输入$x_t$是query，那么self attention接受了第t步以及前t-1步的所有输入$x_1,\ldots, x_{t-1}$。在训练时，由于第t位置的输入可以观测到全部的序列，这与预测阶段的情形项矛盾，所以我们要通过将第t个时间步所对应的可观测长度设置为t，以消除不需要看到的未来的信息。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWtwZWZoY3lnLnBuZw?x-oss-process=image/format,png" alt="Image Name"></p>
<h2 id="Decoder-Block-基础块"><a href="#Decoder-Block-基础块" class="headerlink" title="Decoder Block 基础块"></a>Decoder Block 基础块</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, ffn_hidden_size, num_heads,dropout,i,**kwargs)</span>:</span></span><br><span class="line">        super(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention_1 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_1 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.attention_2 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_2 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)</span><br><span class="line">        self.addnorm_3 = AddNorm(embedding_size, dropout)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        enc_outputs, enc_valid_length = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Example Demo:</span></span><br><span class="line">        <span class="comment"># love dogs ! [EOS]</span></span><br><span class="line">        <span class="comment">#  |    |   |   |</span></span><br><span class="line">        <span class="comment">#   Transformer </span></span><br><span class="line">        <span class="comment">#    Decoder</span></span><br><span class="line">        <span class="comment">#  |   |   |   |</span></span><br><span class="line">        <span class="comment">#  I love dogs !</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># shape of key_values = (batch_size, t, hidden_size)</span></span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), dim=<span class="number">1</span>) </span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, seq_len, _ = X.shape</span><br><span class="line">            <span class="comment"># Shape: (batch_size, seq_len), the values in the j-th column are j+1</span></span><br><span class="line">            valid_length = torch.FloatTensor(np.tile(np.arange(<span class="number">1</span>, seq_len+<span class="number">1</span>), (batch_size, <span class="number">1</span>))) </span><br><span class="line">            valid_length = valid_length.to(X.device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        X2 = self.attention_1(X, key_values, key_values, valid_length)</span><br><span class="line">        Y = self.addnorm_1(X, X2)</span><br><span class="line">        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length)</span><br><span class="line">        Z = self.addnorm_2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm_3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(<span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_length), valid_length, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>
<p>对于 Transformer 解码器来说，构造方式与编码器一样，除了最后一层添加一个 dense layer 以获得输出的置信度分数。</p>
<p><strong>Transformer Decoder 参数设置：</strong></p>
<ul>
<li>编码器的输出 enc_outputs </li>
<li>句子有效长度 enc_valid_length</li>
<li>常规的超参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_size, ffn_hidden_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads, num_layers, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size)</span><br><span class="line">        self.pos_encoding = PositionalEncoding(embedding_size, dropout)</span><br><span class="line">        self.blks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.blks.append(</span><br><span class="line">                DecoderBlock(embedding_size, ffn_hidden_size, num_heads,</span><br><span class="line">                             dropout, i))</span><br><span class="line">        self.dense = nn.Linear(embedding_size, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, enc_valid_length, *args)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_length, [<span class="literal">None</span>]*self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br></pre></td></tr></table></figure>

<h1 id="机器翻译模型-Transformer-训练"><a href="#机器翻译模型-Transformer-训练" class="headerlink" title="机器翻译模型 Transformer 训练"></a>机器翻译模型 Transformer 训练</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">    <span class="comment"># sort by frequency and token</span></span><br><span class="line">    counter = collections.Counter(tokens)</span><br><span class="line">    token_freqs = sorted(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    token_freqs.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">      <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">      self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">      tokens = [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.unk = <span class="number">0</span></span><br><span class="line">      tokens = [<span class="string">''</span>]</span><br><span class="line">    tokens += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs <span class="keyword">if</span> freq &gt;= min_freq]</span><br><span class="line">    self.idx_to_token = []</span><br><span class="line">    self.token_to_idx = dict()</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">      self.idx_to_token.append(token)</span><br><span class="line">      self.token_to_idx[token] = len(self.idx_to_token) - <span class="number">1</span></span><br><span class="line">      </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">      <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">      <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span><span class="params">(batch_size, max_len, num_examples=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Download an NMT dataset, return its vocabulary and data iterator."""</span></span><br><span class="line">    <span class="comment"># Download and preprocess</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_raw</span><span class="params">(text)</span>:</span></span><br><span class="line">        text = text.replace(<span class="string">'\u202f'</span>, <span class="string">' '</span>).replace(<span class="string">'\xa0'</span>, <span class="string">' '</span>)</span><br><span class="line">        out = <span class="string">''</span></span><br><span class="line">        <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(text.lower()):</span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">in</span> (<span class="string">','</span>, <span class="string">'!'</span>, <span class="string">'.'</span>) <span class="keyword">and</span> text[i<span class="number">-1</span>] != <span class="string">' '</span>:</span><br><span class="line">                out += <span class="string">' '</span></span><br><span class="line">            out += char</span><br><span class="line">        <span class="keyword">return</span> out </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'path to data dile'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">      raw_text = f.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    text = preprocess_raw(raw_text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(text.split(<span class="string">'\n'</span>)):</span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">if</span> len(parts) &gt;= <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">' '</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build vocab</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(tokens)</span>:</span></span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">        <span class="keyword">return</span> Vocab(tokens, min_freq=<span class="number">3</span>, use_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert to index arrays</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pad</span><span class="params">(line, max_len, padding_token)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(line) &gt; max_len:</span><br><span class="line">            <span class="keyword">return</span> line[:max_len]</span><br><span class="line">        <span class="keyword">return</span> line + [padding_token] * (max_len - len(line))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_array</span><span class="params">(lines, vocab, max_len, is_source)</span>:</span></span><br><span class="line">        lines = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_source:</span><br><span class="line">            lines = [[vocab.bos] + line + [vocab.eos] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        array = torch.tensor([pad(line, max_len, vocab.pad) <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line">        valid_len = (array != vocab.pad).sum(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line">    src_array, src_valid_len = build_array(source, src_vocab, max_len, <span class="literal">True</span>)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, <span class="literal">False</span>)</span><br><span class="line">    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    train_iter = data.DataLoader(train_data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> src_vocab, tgt_vocab, train_iter</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> d2l</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"1"</span></span><br><span class="line"></span><br><span class="line">embed_size, embedding_size, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.05</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, ctx = <span class="number">0.005</span>, <span class="number">250</span>, d2l.try_gpu()</span><br><span class="line">print(ctx)</span><br><span class="line">num_hiddens, num_heads = <span class="number">64</span>, <span class="number">4</span></span><br><span class="line"></span><br><span class="line">src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    len(src_vocab), embedding_size, num_hiddens, num_heads, num_layers,</span><br><span class="line">    dropout)</span><br><span class="line">decoder = TransformerDecoder(</span><br><span class="line">    len(src_vocab), embedding_size, num_hiddens, num_heads, num_layers,</span><br><span class="line">    dropout)</span><br><span class="line">model = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_s2s_ch9(model, train_iter, lr, num_epochs, ctx)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.eval()</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> [<span class="string">'Go .'</span>, <span class="string">'Wow !'</span>, <span class="string">"I'm OK ."</span>, <span class="string">'I won !'</span>]:</span><br><span class="line">    print(sentence + <span class="string">' =&gt; '</span> + d2l.predict_s2s_ch9(</span><br><span class="line">        model, sentence, src_vocab, tgt_vocab, num_steps, ctx))</span><br></pre></td></tr></table></figure>
<hr>
<p>以上是利用 Transformer 机器翻译模型实现翻译 Demo 的全部分块，针对其中的层归一化进行总结：</p>
<p>层归一化</p>
<ol>
<li>层归一化有利于加快收敛，减少训练时间成本</li>
<li>层归一化对一个中间层的所有神经元进行归一化 </li>
<li>层归一化的效果不会受到batch大小的影响</li>
</ol>
<p>补充：</p>
<p>&ensp;&ensp;&ensp;批归一化</p>
<p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;每个神经元的输入数据以mini-batch为单位进行汇总</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>masked softmax</tag>
        <tag>多头注意</tag>
        <tag>位置编码</tag>
        <tag>编/译码器</tag>
      </tags>
  </entry>
</search>
